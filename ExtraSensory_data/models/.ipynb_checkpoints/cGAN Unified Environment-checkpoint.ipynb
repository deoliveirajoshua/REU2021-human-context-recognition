{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b295ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from prettytable import PrettyTable\n",
    "from pylab import *\n",
    "from scipy.stats import wasserstein_distance\n",
    "import random\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092fed2",
   "metadata": {},
   "source": [
    "### Load acceleration data for 3 users engaging in 3 activities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5f99f",
   "metadata": {},
   "source": [
    "The classifiers were trained on three specific users and with the acceleration features of sitting, sleeping, and walking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d92dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acceleration_features(filepath):\n",
    "    ### filepath should be to the aggregated data file\n",
    "    data = pd.read_csv(filepath)\n",
    "    data = data[(data['UUID'] == '0BFC35E2-4817-4865-BFA7-764742302A2D') | (data['UUID'] == '0A986513-7828-4D53-AA1F-E02D6DF9561B') | (data['UUID'] == '00EABED2-271D-49D8-B599-1D4A09240601')] \n",
    "    data.drop(columns = ['timestamp'], inplace = True)\n",
    "\n",
    "    only_walking = data[(data['label:FIX_walking'] == 1) & (data['label:SITTING'] == 0) & (data['label:SLEEPING'] == 0)]\n",
    "    only_walking = only_walking.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz'] \n",
    "    only_walking['label'] = \"WALKING\"\n",
    "\n",
    "    only_sitting = data[(data['label:FIX_walking'] == 0) & (data['label:SITTING'] == 1) & (data['label:SLEEPING'] == 0)]\n",
    "    only_sitting = only_sitting.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz'] \n",
    "    only_sitting['label'] = \"SITTING\"\n",
    "\n",
    "    only_sleeping = data[(data['label:FIX_walking'] == 0) & (data['label:SITTING'] == 0) & (data['label:SLEEPING'] == 1)]\n",
    "    only_sleeping = only_sleeping.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz'] \n",
    "    only_sleeping['label'] = \"SLEEPING\"\n",
    "\n",
    "    df = pd.concat([only_walking, only_sitting, only_sleeping], axis = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793684de",
   "metadata": {},
   "source": [
    "### Move Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8b56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaryClare's\n",
    "#os.chdir('/Users/maryclaremartin/Documents/jup/ExtraSensory')\n",
    "\n",
    "# Josh's\n",
    "os.chdir(\"D:/Research_Projects/REU2021-human-context-recognition/ExtraSensory_data\")\n",
    "\n",
    "# Harrison's\n",
    "#os.chdir(\"/Users/hkimr/Desktop/WPI Github/REU2021-human-context-recognition/ExtraSensory_data\")\n",
    "softmax = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2518e4",
   "metadata": {},
   "source": [
    "### Load & Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929125c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load and scale data\n",
    "#returns scaled data (X) and labels (Y)\n",
    "#file_name: string, file with data to be used\n",
    "#label: array, list of activities to use\n",
    "#users: array, list of users whose data is to be used\n",
    "\n",
    "def start_data(file_name, label, users):\n",
    "    #read csv into dataframe\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data[data['UUID'].isin(users)]\n",
    "\n",
    "    #seperate only acceleration data\n",
    "    X = data.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz']    \n",
    "    y = data[label]\n",
    "\n",
    "    #seperate only \"on\" labels\n",
    "    X = X[(y!=0).any(axis=1)]\n",
    "    y = y[(y!=0).any(axis=1)]\n",
    "    \n",
    "    #interpolate averages per column\n",
    "    X = interpolation(X).values\n",
    "    y = interpolation(y).values\n",
    "    \n",
    "    #scale the data\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X = sc.fit_transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f925998",
   "metadata": {},
   "source": [
    "### The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2620d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    #torch.manual_seed(0)\n",
    "    return torch.randn(n_samples, z_dim).to(device)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, int(hidden_dim/2)),\n",
    "            generator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            generator_block(int(hidden_dim/4), 30),\n",
    "            generator_block(30, 28),\n",
    "            nn.Linear(28, feature_dim)\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "##calculates generator loss\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#criterion1: loss function1\n",
    "#criterion2: loss function2\n",
    "#batch_size: batch size\n",
    "#z_dim: number of dimensions in the latent space\n",
    "def get_gen_loss(gen, disc, act, usr, criterion1, criterion2, batch_size, z_dim, activities, users):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, activities)\n",
    "    usr_vectors = get_usr_matrix(batch_size, users)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "    fake_features = gen(to_gen)\n",
    "    \n",
    "    pred_disc = disc(fake_features.to(device))\n",
    "    pred_act = act(fake_features.to(device)) ### CrossEntropyLoss Criterion automatically applies softmax and torch.max\n",
    "    pred_usr = usr(fake_features.to(device))\n",
    "    \n",
    "    d_loss = criterion1(pred_disc, torch.ones_like(pred_disc))\n",
    "    act_loss = criterion2(pred_act, act_vectors[0].to(device))\n",
    "    usr_loss = criterion2(pred_usr, usr_vectors[0].to(device))\n",
    "    \n",
    "    gen_loss = d_loss + act_loss + usr_loss\n",
    "    return gen_loss\n",
    "    \n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    #print(indexes)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c1320",
   "metadata": {},
   "source": [
    "### Create Fake Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f8ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_samples(gen, batch_size, z_dim):\n",
    "    \"\"\"\n",
    "    Generates fake acceleration features given a batch size, latent vector dimension, and trained generator.\n",
    "    \n",
    "    \"\"\"\n",
    "    latent_vectors = get_noise(batch_size, z_dim) ### Retrieves a 2D tensor of noise\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    \n",
    "    return fake_features ### Returns a 2D tensor of fake features of size batch_size x z_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3171dc9",
   "metadata": {},
   "source": [
    "### The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480f06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "#defines discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim = 26, hidden_dim = 16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            discriminator_block(feature_dim, hidden_dim),\n",
    "            discriminator_block(hidden_dim, int(hidden_dim/2)),\n",
    "            discriminator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            nn.Linear(int(hidden_dim/4), 1),\n",
    "            nn.Sigmoid()                    \n",
    "        )\n",
    "    def forward(self, feature_vector):\n",
    "        return self.disc(feature_vector)\n",
    "    \n",
    "def get_disc_loss(gen, disc, criterion, real_features, batch_size, z_dim, a_dim, u_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, a_dim)\n",
    "    usr_vectors = get_usr_matrix(batch_size, u_dim)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors.to(device), act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "    fake_features = gen(to_gen)\n",
    "    pred_fake = disc(fake_features.detach())\n",
    "    \n",
    "    ground_truth = torch.zeros_like(pred_fake)\n",
    "    loss_fake = criterion(pred_fake, ground_truth)\n",
    "    \n",
    "    pred_real = disc(real_features.to(device))\n",
    "    ground_truth = torch.ones_like(pred_real)\n",
    "    loss_real = criterion(pred_real, ground_truth)\n",
    "    \n",
    "    disc_loss = (loss_fake + loss_real) / 2\n",
    "    return disc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1a69b",
   "metadata": {},
   "source": [
    "### User Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a2370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class User_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(User_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bb4e3",
   "metadata": {},
   "source": [
    "### Activity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9617998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Activity_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Activity_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            classifier_block(10, 5),\n",
    "            nn.Linear(5, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #softmax = nn.Softmax(dim = 1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e674e",
   "metadata": {},
   "source": [
    "### Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29773593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replaces Nan values with average values\n",
    "#df: data frame of data to use\n",
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b0303",
   "metadata": {},
   "source": [
    "### Visualize Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d2c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##prints a plot of a generator batch\n",
    "#gen: generator\n",
    "#b_size: batch size\n",
    "#epochs: current epoch (-1)\n",
    "def visualize_gen_batch(gen, b_size, epochs = -1):\n",
    "    #print(str(b_size))\n",
    "    latent_vectors = get_noise(b_size, z_dim)\n",
    "    #print(latent_vectors.shape)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    #print(fake_features.shape)\n",
    "    \n",
    "    w_img = fake_features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Generated Batch at Epoch ' + str(epochs), fontweight =\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "##prints a plot of a batch of real data\n",
    "#features: real data\n",
    "def visualize_real_batch(features):\n",
    "    w_img = features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Real Batch of Data', fontweight =\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0526a",
   "metadata": {},
   "source": [
    "### Calculate Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9c5b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculates performance statistics for each epoch of training\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#b_size: batch size\n",
    "#z_dim: number of dimensions of the latent space\n",
    "##returns accuracy, precision, recall, fpR, and f1 score\n",
    "def performance_stats(gen, disc, b_size, z_dim, a_dim, u_dim, batch = None):\n",
    "    tp = 0 #true positive\n",
    "    fp = 0 #false positive\n",
    "    tn = 0 #true negative\n",
    "    fn = 0 #false negative\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if batch is None:\n",
    "            latent_vectors = get_noise(b_size, z_dim)\n",
    "            \n",
    "            act_vectors = get_act_matrix(b_size, a_dim)\n",
    "            usr_vectors = get_usr_matrix(b_size, u_dim)\n",
    "            to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "            fake_features = gen(to_gen)\n",
    "\n",
    "            y_hat = torch.round(disc(fake_features))\n",
    "            y_label = [0] * b_size\n",
    "            y_label = torch.Tensor(y_label).to(device)\n",
    "        else:\n",
    "            latent_vectors = get_noise(b_size, z_dim)\n",
    "            act_vectors = get_act_matrix(b_size, a_dim)\n",
    "            usr_vectors = get_usr_matrix(b_size, u_dim)\n",
    "            \n",
    "            to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "            fake_features = gen(to_gen)\n",
    "        \n",
    "            y_hat = torch.round(disc(fake_features.to(device)))\n",
    "            y_label = torch.Tensor([0] * b_size)\n",
    "            \n",
    "            real_y_hat = torch.round(disc(batch.to(device)))\n",
    "            y_add = torch.Tensor([1] * b_size)\n",
    "            y_label = torch.cat((y_label, y_add), dim = 0)\n",
    "            #for i in range(0, int(b_size/2)):\n",
    "            # y_label.append(1)\n",
    "            y_hat = torch.cat((y_hat, real_y_hat), dim = 0).to(device)\n",
    "            \n",
    "            #print(y_hat)\n",
    "            #print(y_label)\n",
    "         \n",
    "        \n",
    "        for k in range(len(y_label)):\n",
    "            if y_label[k] == 1 and y_hat[k] == 1:\n",
    "                tp += 1\n",
    "            elif y_label[k] == 1 and y_hat[k] == 0:\n",
    "                fn += 1\n",
    "            elif y_label[k] == 0 and y_hat[k] == 0:\n",
    "                tn += 1\n",
    "            elif y_label[k] == 0 and y_hat[k] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "                exit()\n",
    "            \n",
    "        class_acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"TP: \"  + str(tp))\n",
    "        print(\"FP: \"  + str(fp))\n",
    "        print(\"TN: \"  + str(tn))\n",
    "        print(\"FN: \"  + str(fn))\n",
    "        \n",
    "        \n",
    "        \n",
    "        if tp + fp == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            \n",
    "        if tp + fn == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "            \n",
    "        if fp + tn == 0:\n",
    "            fpR = 0\n",
    "        else: \n",
    "            fpR = fp / (fp + tn)\n",
    "\n",
    "        #print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "        #print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "        #print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "        #print(f'F-1 Score: {2*(precision * recall / (precision + recall + 0.001)):.2f}')\n",
    "        return class_acc, precision, recall, fpR, 2*(precision * recall / (precision + recall + 0.001))\n",
    "\n",
    "def performance_stats_class(gen, classifier, machine, batch_size, z_dim, a_dim, u_dim):\n",
    "    tp = 0 #true positive\n",
    "    fp = 0 #false positive\n",
    "    tn = 0 #true negative\n",
    "    fn = 0 #false negative\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent_vectors = get_noise(batch_size, z_dim)\n",
    "        act_vectors = get_act_matrix(batch_size, a_dim)\n",
    "        usr_vectors = get_usr_matrix(batch_size, u_dim)\n",
    "    \n",
    "        to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "        fake_features = gen(to_gen)\n",
    "        #print(fake_features)\n",
    "    \n",
    "        _, pred_class = torch.max(softmax(classifier(fake_features.to(device))), dim = 1)\n",
    "        #print(pred_class)\n",
    "        labels = []\n",
    "        if machine == \"act\":\n",
    "            labels = act_vectors\n",
    "        else:\n",
    "            labels = usr_vectors\n",
    "        \n",
    "        return torch.eq(labels[0].to(device), pred_class).sum()/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121c7d0",
   "metadata": {},
   "source": [
    "### Create Density Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9401d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and plot density curves for mean, x, y, z acceleration\n",
    "#reals: real data\n",
    "#fakes: generated data\n",
    "def density_curves(reals, fakes):\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    subplot(2, 2, 1)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,0], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,0], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 2)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,18], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,18], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean X-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 3)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,19], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,19], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Y-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 4)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,20], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,20], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Z-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574b48f",
   "metadata": {},
   "source": [
    "### Calculate Wassertein distance for each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe452c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate Waaserstein distances for each dimension\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "#feature_dim: number ofd dimensions in the feature space\n",
    "#sample: sample of data\n",
    "def all_Wasserstein_dists(gen, z_dim, feature_dim, sample):\n",
    "    wasser_dim = []\n",
    "    latent_vectors = get_noise(len(sample), z_dim)\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    for k in range(feature_dim):\n",
    "        wasser_dim.append(wasserstein_distance(fake_features[:, k].cpu().detach().numpy(), sample[:, k].cpu().detach().numpy()))\n",
    "    return torch.tensor(wasser_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d4d8f",
   "metadata": {},
   "source": [
    "### Visualizing Generation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf2423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates and prints a plot of the generated vs real data\n",
    "#data: data used\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "def visualize_gen(data, gen, z_dim, a_dim, u_dim):\n",
    "    #Number of datum to visualize\n",
    "    sample_size = len(data)\n",
    "    reals = data[0:sample_size, :]\n",
    "    fakes = get_fake_samples(gen, sample_size, z_dim).detach()\n",
    "    density_curves(reals, fakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd0ff5",
   "metadata": {},
   "source": [
    "### Initialize Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aebeaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initalize parameters that depend on training loop parameters\n",
    "#X: acceleration data\n",
    "#y: labels associated with X data (fake or real)\n",
    "#z_dim: number of dimensions to the latent space\n",
    "#disc_lr: discriminator learning rate\n",
    "#gen_lr: generator learning rate\n",
    "#DISCRIMINATOR: 1 to indicate if discriminator is training\n",
    "#batch_size: batch size\n",
    "#disc: initialized discrimiantor\n",
    "\n",
    "def initialize_params(X, y, z_dim, a_dim, u_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc):\n",
    "    #initialize generator\n",
    "    gen = Generator(z_dim + a_dim + u_dim).to(device)\n",
    "    #indicate that discriminator is training\n",
    "    to_train = DISCRIMINATOR\n",
    "    #create training features\n",
    "    train_features = torch.tensor(X)\n",
    "    #create training labels\n",
    "    train_labels = torch.tensor(y)\n",
    "    #concatenate to create training data\n",
    "    train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    #create data loader for training data\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    #initialize generator and discriminator optimizers\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr = disc_lr)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr = gen_lr)\n",
    "    \n",
    "    return gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462505c",
   "metadata": {},
   "source": [
    "# Save / Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ccf4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path and name of the Generator and Discriminator accordingly\n",
    "def save_model(gen, disc, model_name):\n",
    "    torch.save(gen.state_dict(), f\"saved_models/{model_name}_gen\")\n",
    "    torch.save(disc.state_dict(), f\"saved_models/{model_name}_disc\")\n",
    "    \n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5222799",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eddb1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Training loop to train GAN\n",
    "\n",
    "#Parameters to specifiy: \n",
    "    #X: starting accelerometer data\n",
    "    #y: starting labels for X data (fake or real)\n",
    "    \n",
    "#Set parameters (do not change)\n",
    "    #criterion: loss function (BCE)\n",
    "    #dig: number of significant digits for printing (5)\n",
    "    #feature_dim: Number of dimensions of output from generator (26)\n",
    "    #GENERATOR: set generator to zero for training\n",
    "    #DISCRIMINATOR: set discriminator to one for training\n",
    "    #train_string: starting machine to train (DISC)\n",
    "    #disc: initalize discriminator\n",
    "    #rel_epochs: Epochs passed since last switch (constant training) (0)\n",
    "    #rows: initialization of array to save data of each epoch to CSV file ([])\n",
    "    #heading: array of column headings for table ([\"Epoch\", \"Machine Training\", \"Discriminator Loss\", \n",
    "                    #\"Generator Loss\", \"FPR\", \"Recall\", \"Median Wasserstein\", \"Mean Wasserstein\"])\n",
    "    #table: intialize a table as a pretty table to save epoch data\n",
    "    #switch_count: number of switches in dynamic training (0)\n",
    "    \n",
    "#Set parameters (can change):\n",
    "    #z_dim: number of dimensions of latent vector (100)\n",
    "    #gen_lr: generator learning rate (.001)\n",
    "    #disc_lr: discriminator learning rate (.001) (shoud be equal to gen_lr)\n",
    "    #batch_size: batch size (75)\n",
    "    #print_batches: Show model performance per batch (False)\n",
    "    #n_epochs: number of epochs to train (100)\n",
    "    #constant_train_flag: (False)\n",
    "        #Set to true to train based on constant # of epochs per machine \n",
    "        #Set to false to train dynamically based on machine performance\n",
    "        \n",
    "    #Constant training approach:\n",
    "        #disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (5)\n",
    "        #gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #epoch_threshold: Epoch number to change training epoch ratio (50)\n",
    "        #disc_epochs_change: New number of consecutive epochs to train discriminator after epoch threshold is exceeded (1)\n",
    "        #gen_epochs_change: New number of consecutive epochs to train generator after epoch threshold is exceeded (50)\n",
    "    \n",
    "    #Dynamic training approach:                        \n",
    "        #static_threshold: Epoch number to change from static ratio to dynamic (18)\n",
    "        #static_disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (4)\n",
    "        #static_gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #pull_threshold: Accuracy threshold for switching machine training when the generator is no longer competitive (0.4)\n",
    "        #push_threshold: Accuracy threshold for switching machine training when the discriminator is no longer competitive (0.6)\n",
    "        #recall_threshold: threshold for recall to switch machine training when discriminator is training well\n",
    "        #switch_flag: indicates if we should switch our training machine (False)\n",
    "        \n",
    "def training_loop(X, y, act, usr, criterion1 = nn.BCELoss(), criterion2 = nn.CrossEntropyLoss(), gan_id = \"Mod Test Gan\", dig = 5, feature_dim = 26, \n",
    "                  GENERATOR = 0, DISCRIMINATOR = 1, train_string = \"DISC\", disc = Discriminator(), z_dim = 100, a_dim = 3, u_dim = 3, \n",
    "                  gen_lr =  0.001, disc_lr = 0.001, batch_size = 100, constant_train_flag = False, disc_epochs = 5,\n",
    "                  gen_epochs = 2, epoch_threshold = 50, disc_epochs_change = 5, gen_epochs_change = 2, rel_epochs = 0,\n",
    "                 static_threshold = 28, static_disc_epochs = 5, static_gen_epochs = 2, pull_threshold = 0.3,\n",
    "                 push_threshold = 0.7, recall_threshold = 0.75, print_batches = False, n_epochs = 1000, rows = [],\n",
    "                 heading = [\"Epoch\", \"Training\", \"Discriminator Loss\", \"Generator Loss\", \"D_Accuracy\", \"D_fpR\", \"D_Recall\", \"A_fpR\", \"U_fpR\"],\n",
    "                 table = PrettyTable(), switch_flag = False, switch_count = 0, last_real_features = []):\n",
    "    \n",
    "    disc.to(device)\n",
    "    #returns generator, sets discriminator training, creates training tensor, loads data, and initializes optimizers\n",
    "    gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen = initialize_params(X, y, z_dim, a_dim, u_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc)\n",
    "\n",
    "    #set pretty table field names\n",
    "    table.field_names = heading\n",
    "    \n",
    "    #visualize_gen(X, gen, z_dim, a_dim, u_dim)\n",
    "\n",
    "    gen_epochs = 0\n",
    "    \n",
    "    last_D_loss = -1.0\n",
    "    last_G_loss = -1.0\n",
    "    \n",
    "    mean_mean = []\n",
    "    mean_median = []\n",
    "    \n",
    "    for epoch in range(n_epochs):  \n",
    "        if constant_train_flag:\n",
    "            if to_train == DISCRIMINATOR and rel_epochs >= disc_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GEN\"\n",
    "\n",
    "            elif to_train == GENERATOR and rel_epochs >= gen_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = DISCRIMINATOR\n",
    "                train_string = \"DISC\"\n",
    "\n",
    "            # Change epoch ratio after intial 'leveling out'\n",
    "            if epoch == epoch_threshold:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GENERATOR\"\n",
    "\n",
    "                old_ratio = gen_epochs / disc_epochs\n",
    "                gen_epochs = gen_epochs_change\n",
    "                disc_epochs = disc_epochs_change\n",
    "                new_ratio = gen_epochs / disc_epochs\n",
    "                print(f'\\n\\nTraining ratio of G/D switched from {old_ratio:.{dig}f} to {new_ratio:.{dig}f}\\n\\n')\n",
    "        else:\n",
    "            if epoch < static_threshold:\n",
    "                if to_train == DISCRIMINATOR and rel_epochs >= static_disc_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "\n",
    "                elif to_train == GENERATOR and rel_epochs >= static_gen_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "\n",
    "            else:\n",
    "                if not switch_flag:\n",
    "                    print(\"\\nSwitching to Dynamic Training\\n\")\n",
    "                    switch_flag = True\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "                if to_train == DISCRIMINATOR and fpR <= pull_threshold and R >= recall_threshold:\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "                    print(\"\\nPull Generator\\n\")\n",
    "                    switch_count += 1\n",
    "                if to_train == GENERATOR and fpR >= push_threshold:\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "                    print(\"\\nPush Generator\\n\")\n",
    "                    switch_count += 1\n",
    "        print(f'Epoch[{epoch + 1}/{n_epochs}] Train: {train_string} ', end ='')\n",
    "        for batch_idx, (real_features, _) in enumerate(train_loader):\n",
    "            #batch_size = len(real_features)\n",
    "            #print(len(real_features))\n",
    "            \n",
    "            if print_batches:\n",
    "                    print(f'\\n\\tBatch[{batch_idx + 1}/{len(train_loader)}] |', end ='')\n",
    "\n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Training Discriminator\n",
    "                #visualize_real_batch(real_features.float())\n",
    "                opt_disc.zero_grad()\n",
    "                disc_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), len(real_features), z_dim, a_dim, u_dim)\n",
    "                disc_loss.backward(retain_graph = True)\n",
    "                #disc_loss.backward()\n",
    "                opt_disc.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), z_dim, a_dim, u_dim, batch = real_features.float())\n",
    "                A_fpR = performance_stats_class(gen, act, \"act\", batch_size, z_dim, a_dim, u_dim)\n",
    "                U_fpR = performance_stats_class(gen, usr, \"usr\", batch_size, z_dim, a_dim, u_dim)\n",
    "                #w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                #median_w_dist = torch.median(w_dist)\n",
    "                #mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                #mean_mean.append(mean_w_dist)\n",
    "                #mean_median.append(median_w_dist)\n",
    "\n",
    "                last_D_loss = disc_loss.item()\n",
    "                \n",
    "                if last_G_loss == -1.0:\n",
    "                    last_G_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, len(real_features), z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f}')\n",
    "            else:\n",
    "                ### Training Generator\n",
    "                opt_gen.zero_grad()\n",
    "                gen_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, len(real_features), z_dim, a_dim, u_dim)\n",
    "                gen_loss.backward()\n",
    "                opt_gen.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), z_dim, a_dim, u_dim, batch = real_features.float())\n",
    "                A_fpR = performance_stats_class(gen, act, \"act\", batch_size, z_dim, a_dim, u_dim)\n",
    "                U_fpR = performance_stats_class(gen, usr, \"usr\", batch_size, z_dim, a_dim, u_dim)\n",
    "                #w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                #median_w_dist = torch.median(w_dist)\n",
    "                #mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                #mean_mean.append(mean_w_dist)\n",
    "                #mean_median.append(median_w_dist)\n",
    "                \n",
    "                last_G_loss = gen_loss.item()\n",
    "                \n",
    "                if last_D_loss == -1.0:\n",
    "                    last_D_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), len(real_features), z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f}')\n",
    "\n",
    "        if not print_batches:\n",
    "            \n",
    "            mean_mean_w = torch.mean(torch.Tensor(mean_mean)) \n",
    "            \n",
    "            mean_median_w = torch.mean(torch.Tensor(mean_median))\n",
    "            \n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Currently doesn't print Median/Mean Wasserstein --> Change if needed\n",
    "                print(f'| LossD: {last_D_loss:.{dig}f}, LossG: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | R: {R:.{dig}f} | A_fpR: {A_fpR:.{dig}f} | U_fpR: {U_fpR:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Disc\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{A_fpR:.{dig}f}\", f\"{U_fpR:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "            else:\n",
    "                print(f'| LossD: {last_D_loss:.{dig}f}, LossG: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | R: {R:.{dig}f} | A_fpR: {A_fpR:.{dig}f} | U_fpR: {U_fpR:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Gen\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{A_fpR:.{dig}f}\", f\"{U_fpR:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "                gen_epochs += 1\n",
    "        mean_mean.clear()\n",
    "        mean_median.clear()\n",
    "        rel_epochs += 1\n",
    "    print(\"\\n\\nTraining Session Finished\")\n",
    "    print(f\"Encountered {switch_count} non-trivial training swaps\")\n",
    "    percent = gen_epochs / n_epochs\n",
    "    print(f\"Trained Generator {gen_epochs} out of {n_epochs} ({percent:.3f})\")\n",
    "    f = open(\"model_outputs/\" + gan_id + \".txt\", \"w\")\n",
    "    f.write(table.get_string())\n",
    "    f.close()\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".txt\\\"\")\n",
    "\n",
    "    with open(\"model_outputs/\" + gan_id + \".csv\", \"w\") as csvfile: \n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "        # writing the fields \n",
    "        csvwriter.writerow(heading)\n",
    "        # writing the data rows \n",
    "        csvwriter.writerows(rows)\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".csv\\\"\")\n",
    "    save_model(gen, disc, gan_id)\n",
    "    model_output = pd.read_csv(\"model_outputs/\" + gan_id + \".csv\")\n",
    "    #visualize_gen(X, gen, z_dim)\n",
    "    \n",
    "    # Change path and name of the Generator and Discriminator accordingly\n",
    "    save_model(gen, disc, gan_id)\n",
    "    \n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8f1fd",
   "metadata": {},
   "source": [
    "# Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9924892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot metrics based on data (csv)\n",
    "def plot_metrics(data, vanilla = True):\n",
    "    if vanilla:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "    \n",
    "        plt.figure(figsize = (15, 15))\n",
    "        subplot(2, 2, 1)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'FPR', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 2)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'Recall', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 3)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 4)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "        plt.figure(figsize = (15, 8))\n",
    "        \n",
    "        subplot(1, 2, 1)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(1, 2, 2)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c10e2",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdf46cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5000] Train: DISC 8691\n",
      "| LossD: 0.69293, LossG: 11.91434 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.33000 | U_fpR: 0.34127\n",
      "Epoch[2/5000] Train: DISC 8691\n",
      "| LossD: 0.68937, LossG: 11.91434 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.32505 | U_fpR: 0.34173\n",
      "Epoch[3/5000] Train: DISC 8691\n",
      "| LossD: 0.68561, LossG: 11.91434 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.33034 | U_fpR: 0.33759\n",
      "Epoch[4/5000] Train: DISC 8691\n",
      "| LossD: 0.68184, LossG: 11.91434 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.33471 | U_fpR: 0.33552\n",
      "Epoch[5/5000] Train: DISC 8691\n",
      "| LossD: 0.67784, LossG: 11.91434 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.33379 | U_fpR: 0.33529\n",
      "Epoch[6/5000] Train: GEN 8691\n",
      "| LossD: 0.67784, LossG: 11.59522 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.32954 | U_fpR: 0.34369\n",
      "Epoch[7/5000] Train: GEN 8691\n",
      "| LossD: 0.67784, LossG: 8.05136 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.33840 | U_fpR: 0.35922\n",
      "Epoch[8/5000] Train: DISC 8691\n",
      "| LossD: 0.67358, LossG: 8.05136 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.34208 | U_fpR: 0.34173\n",
      "Epoch[9/5000] Train: DISC 8691\n",
      "| LossD: 0.66848, LossG: 8.05136 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.32367 | U_fpR: 0.35266\n",
      "Epoch[10/5000] Train: DISC 8691\n",
      "| LossD: 0.66366, LossG: 8.05136 | Acc: 0.50000 | fpR: 1.00000 | R: 1.00000 | A_fpR: 0.33218 | U_fpR: 0.34449\n",
      "Epoch[11/5000] Train: DISC 8691\n",
      "| LossD: 0.65811, LossG: 8.05136 | Acc: 0.49983 | fpR: 1.00000 | R: 0.99965 | A_fpR: 0.33115 | U_fpR: 0.34691\n",
      "Epoch[12/5000] Train: DISC 8691\n",
      "| LossD: 0.65212, LossG: 8.05136 | Acc: 0.49971 | fpR: 1.00000 | R: 0.99942 | A_fpR: 0.33069 | U_fpR: 0.36049\n",
      "Epoch[13/5000] Train: GEN 8691\n",
      "| LossD: 0.65212, LossG: 7.16729 | Acc: 0.49988 | fpR: 0.99988 | R: 0.99965 | A_fpR: 0.33310 | U_fpR: 0.35002\n",
      "Epoch[14/5000] Train: GEN 8691\n",
      "| LossD: 0.65212, LossG: 6.64721 | Acc: 0.49994 | fpR: 0.99977 | R: 0.99965 | A_fpR: 0.32724 | U_fpR: 0.35347\n",
      "Epoch[15/5000] Train: DISC 8691\n",
      "| LossD: 0.64495, LossG: 6.64721 | Acc: 0.50293 | fpR: 0.99356 | R: 0.99942 | A_fpR: 0.32908 | U_fpR: 0.34300\n",
      "Epoch[16/5000] Train: DISC 8691\n",
      "| LossD: 0.63732, LossG: 6.64721 | Acc: 0.53964 | fpR: 0.91992 | R: 0.99919 | A_fpR: 0.33310 | U_fpR: 0.35036\n",
      "Epoch[17/5000] Train: DISC 8691\n",
      "| LossD: 0.62990, LossG: 6.64721 | Acc: 0.68111 | fpR: 0.63710 | R: 0.99931 | A_fpR: 0.33529 | U_fpR: 0.35531\n",
      "Epoch[18/5000] Train: DISC 8691\n",
      "| LossD: 0.62039, LossG: 6.64721 | Acc: 0.78081 | fpR: 0.43574 | R: 0.99735 | A_fpR: 0.34887 | U_fpR: 0.35462\n",
      "Epoch[19/5000] Train: DISC 8691\n",
      "| LossD: 0.60991, LossG: 6.64721 | Acc: 0.83143 | fpR: 0.33425 | R: 0.99712 | A_fpR: 0.33207 | U_fpR: 0.34657\n",
      "Epoch[20/5000] Train: GEN 8691\n",
      "| LossD: 0.60991, LossG: 6.37977 | Acc: 0.83379 | fpR: 0.32954 | R: 0.99712 | A_fpR: 0.32850 | U_fpR: 0.34507\n",
      "Epoch[21/5000] Train: GEN 8691\n",
      "| LossD: 0.60991, LossG: 6.04542 | Acc: 0.83477 | fpR: 0.32689 | R: 0.99643 | A_fpR: 0.33897 | U_fpR: 0.34783\n",
      "Epoch[22/5000] Train: DISC 8691\n",
      "| LossD: 0.59944, LossG: 6.04542 | Acc: 0.85439 | fpR: 0.28616 | R: 0.99494 | A_fpR: 0.33978 | U_fpR: 0.34265\n",
      "Epoch[23/5000] Train: DISC 8691\n",
      "| LossD: 0.58728, LossG: 6.04542 | Acc: 0.87867 | fpR: 0.23611 | R: 0.99344 | A_fpR: 0.32574 | U_fpR: 0.34403\n",
      "Epoch[24/5000] Train: DISC 8691\n",
      "| LossD: 0.57496, LossG: 6.04542 | Acc: 0.89190 | fpR: 0.20792 | R: 0.99172 | A_fpR: 0.32298 | U_fpR: 0.34484\n",
      "Epoch[25/5000] Train: DISC 8691\n",
      "| LossD: 0.56127, LossG: 6.04542 | Acc: 0.90323 | fpR: 0.18502 | R: 0.99149 | A_fpR: 0.34047 | U_fpR: 0.34841\n",
      "Epoch[26/5000] Train: DISC 8691\n",
      "| LossD: 0.54727, LossG: 6.04542 | Acc: 0.91255 | fpR: 0.16350 | R: 0.98861 | A_fpR: 0.33425 | U_fpR: 0.34380\n",
      "Epoch[27/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 5.63781 | Acc: 0.91629 | fpR: 0.15740 | R: 0.98999 | A_fpR: 0.33356 | U_fpR: 0.34438\n",
      "Epoch[28/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 5.29293 | Acc: 0.91186 | fpR: 0.16523 | R: 0.98895 | A_fpR: 0.32608 | U_fpR: 0.34703\n",
      "\n",
      "Switching to Dynamic Training\n",
      "\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[29/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 5.06751 | Acc: 0.91054 | fpR: 0.16834 | R: 0.98941 | A_fpR: 0.32574 | U_fpR: 0.34415\n",
      "Epoch[30/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 4.84511 | Acc: 0.90651 | fpR: 0.17639 | R: 0.98941 | A_fpR: 0.32942 | U_fpR: 0.34841\n",
      "Epoch[31/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 4.70363 | Acc: 0.91100 | fpR: 0.16615 | R: 0.98815 | A_fpR: 0.33356 | U_fpR: 0.34576\n",
      "Epoch[32/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 4.47660 | Acc: 0.90933 | fpR: 0.16995 | R: 0.98861 | A_fpR: 0.33540 | U_fpR: 0.35416\n",
      "Epoch[33/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 4.29780 | Acc: 0.90864 | fpR: 0.17110 | R: 0.98838 | A_fpR: 0.33471 | U_fpR: 0.35404\n",
      "Epoch[34/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 4.12118 | Acc: 0.90674 | fpR: 0.17443 | R: 0.98792 | A_fpR: 0.33138 | U_fpR: 0.35807\n",
      "Epoch[35/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 4.03433 | Acc: 0.90381 | fpR: 0.18214 | R: 0.98976 | A_fpR: 0.33460 | U_fpR: 0.36060\n",
      "Epoch[36/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.98463 | Acc: 0.90128 | fpR: 0.18709 | R: 0.98964 | A_fpR: 0.33506 | U_fpR: 0.36175\n",
      "Epoch[37/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.89366 | Acc: 0.90203 | fpR: 0.18559 | R: 0.98964 | A_fpR: 0.33172 | U_fpR: 0.36037\n",
      "Epoch[38/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.77862 | Acc: 0.90594 | fpR: 0.17789 | R: 0.98976 | A_fpR: 0.34346 | U_fpR: 0.36095\n",
      "Epoch[39/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.74361 | Acc: 0.90018 | fpR: 0.18893 | R: 0.98930 | A_fpR: 0.33817 | U_fpR: 0.38154\n",
      "Epoch[40/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.64537 | Acc: 0.89783 | fpR: 0.19399 | R: 0.98964 | A_fpR: 0.33920 | U_fpR: 0.37993\n",
      "Epoch[41/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.60442 | Acc: 0.89829 | fpR: 0.19330 | R: 0.98987 | A_fpR: 0.33644 | U_fpR: 0.38269\n",
      "Epoch[42/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.53088 | Acc: 0.89219 | fpR: 0.20412 | R: 0.98849 | A_fpR: 0.33241 | U_fpR: 0.38764\n",
      "Epoch[43/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.53217 | Acc: 0.89282 | fpR: 0.20389 | R: 0.98953 | A_fpR: 0.33609 | U_fpR: 0.39466\n",
      "Epoch[44/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.46516 | Acc: 0.89052 | fpR: 0.20999 | R: 0.99103 | A_fpR: 0.33264 | U_fpR: 0.39915\n",
      "Epoch[45/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.44308 | Acc: 0.89086 | fpR: 0.20734 | R: 0.98907 | A_fpR: 0.33253 | U_fpR: 0.40617\n",
      "Epoch[46/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.44652 | Acc: 0.88960 | fpR: 0.21079 | R: 0.98999 | A_fpR: 0.33460 | U_fpR: 0.40421\n",
      "Epoch[47/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.37498 | Acc: 0.88546 | fpR: 0.21965 | R: 0.99056 | A_fpR: 0.34380 | U_fpR: 0.41284\n",
      "Epoch[48/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.32927 | Acc: 0.88321 | fpR: 0.22276 | R: 0.98918 | A_fpR: 0.33379 | U_fpR: 0.41227\n",
      "Epoch[49/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.31599 | Acc: 0.88154 | fpR: 0.22610 | R: 0.98918 | A_fpR: 0.33794 | U_fpR: 0.42032\n",
      "Epoch[50/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.31345 | Acc: 0.87395 | fpR: 0.24197 | R: 0.98987 | A_fpR: 0.33011 | U_fpR: 0.42297\n",
      "Epoch[51/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.23710 | Acc: 0.87907 | fpR: 0.23070 | R: 0.98884 | A_fpR: 0.34564 | U_fpR: 0.42918\n",
      "Epoch[52/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.25224 | Acc: 0.87453 | fpR: 0.24082 | R: 0.98987 | A_fpR: 0.33023 | U_fpR: 0.42734\n",
      "Epoch[53/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.22847 | Acc: 0.87671 | fpR: 0.23565 | R: 0.98907 | A_fpR: 0.33598 | U_fpR: 0.42745\n",
      "Epoch[54/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.20062 | Acc: 0.87044 | fpR: 0.24819 | R: 0.98907 | A_fpR: 0.34127 | U_fpR: 0.44701\n",
      "Epoch[55/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.17011 | Acc: 0.87205 | fpR: 0.24508 | R: 0.98918 | A_fpR: 0.32839 | U_fpR: 0.43838\n",
      "Epoch[56/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.14226 | Acc: 0.87073 | fpR: 0.24899 | R: 0.99045 | A_fpR: 0.32896 | U_fpR: 0.43896\n",
      "Epoch[57/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.12979 | Acc: 0.86699 | fpR: 0.25360 | R: 0.98757 | A_fpR: 0.33379 | U_fpR: 0.44161\n",
      "Epoch[58/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.09802 | Acc: 0.86124 | fpR: 0.26694 | R: 0.98941 | A_fpR: 0.33771 | U_fpR: 0.45645\n",
      "Epoch[59/5000] Train: GEN 8691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| LossD: 0.54727, LossG: 3.05675 | Acc: 0.86239 | fpR: 0.26384 | R: 0.98861 | A_fpR: 0.34265 | U_fpR: 0.45818\n",
      "Epoch[60/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.03254 | Acc: 0.85583 | fpR: 0.27753 | R: 0.98918 | A_fpR: 0.33425 | U_fpR: 0.46830\n",
      "Epoch[61/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.01496 | Acc: 0.85859 | fpR: 0.27074 | R: 0.98792 | A_fpR: 0.33494 | U_fpR: 0.46726\n",
      "Epoch[62/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 3.00072 | Acc: 0.85042 | fpR: 0.28811 | R: 0.98895 | A_fpR: 0.34219 | U_fpR: 0.48567\n",
      "Epoch[63/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.97959 | Acc: 0.84800 | fpR: 0.29341 | R: 0.98941 | A_fpR: 0.33529 | U_fpR: 0.49419\n",
      "Epoch[64/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.95796 | Acc: 0.83529 | fpR: 0.31930 | R: 0.98987 | A_fpR: 0.34369 | U_fpR: 0.49453\n",
      "Epoch[65/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.94678 | Acc: 0.83224 | fpR: 0.32505 | R: 0.98953 | A_fpR: 0.33966 | U_fpR: 0.48924\n",
      "Epoch[66/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.91154 | Acc: 0.82459 | fpR: 0.34104 | R: 0.99022 | A_fpR: 0.34426 | U_fpR: 0.50846\n",
      "Epoch[67/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.87741 | Acc: 0.81636 | fpR: 0.35635 | R: 0.98907 | A_fpR: 0.34737 | U_fpR: 0.51525\n",
      "Epoch[68/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.85011 | Acc: 0.80060 | fpR: 0.38914 | R: 0.99033 | A_fpR: 0.34311 | U_fpR: 0.53135\n",
      "Epoch[69/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.83276 | Acc: 0.78886 | fpR: 0.41215 | R: 0.98987 | A_fpR: 0.34829 | U_fpR: 0.54252\n",
      "Epoch[70/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.82106 | Acc: 0.77408 | fpR: 0.44161 | R: 0.98976 | A_fpR: 0.33299 | U_fpR: 0.54194\n",
      "Epoch[71/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.79872 | Acc: 0.75043 | fpR: 0.48786 | R: 0.98872 | A_fpR: 0.34680 | U_fpR: 0.55770\n",
      "Epoch[72/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.77478 | Acc: 0.73634 | fpR: 0.51709 | R: 0.98976 | A_fpR: 0.34242 | U_fpR: 0.56553\n",
      "Epoch[73/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.75128 | Acc: 0.71983 | fpR: 0.54792 | R: 0.98757 | A_fpR: 0.34795 | U_fpR: 0.58164\n",
      "Epoch[74/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.71888 | Acc: 0.70487 | fpR: 0.57853 | R: 0.98826 | A_fpR: 0.34104 | U_fpR: 0.57048\n",
      "Epoch[75/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.71510 | Acc: 0.69204 | fpR: 0.60511 | R: 0.98918 | A_fpR: 0.33989 | U_fpR: 0.58290\n",
      "Epoch[76/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.71324 | Acc: 0.68019 | fpR: 0.62812 | R: 0.98849 | A_fpR: 0.34564 | U_fpR: 0.58187\n",
      "Epoch[77/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.64819 | Acc: 0.66868 | fpR: 0.65470 | R: 0.99206 | A_fpR: 0.34472 | U_fpR: 0.58049\n",
      "Epoch[78/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.67968 | Acc: 0.65303 | fpR: 0.68335 | R: 0.98941 | A_fpR: 0.33966 | U_fpR: 0.60212\n",
      "Epoch[79/5000] Train: GEN 8691\n",
      "| LossD: 0.54727, LossG: 2.65411 | Acc: 0.63968 | fpR: 0.70878 | R: 0.98815 | A_fpR: 0.33828 | U_fpR: 0.60914\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[80/5000] Train: DISC 8691\n",
      "| LossD: 0.57701, LossG: 2.65411 | Acc: 0.67311 | fpR: 0.64296 | R: 0.98918 | A_fpR: 0.34645 | U_fpR: 0.60545\n",
      "Epoch[81/5000] Train: DISC 8691\n",
      "| LossD: 0.56194, LossG: 2.65411 | Acc: 0.71430 | fpR: 0.55943 | R: 0.98803 | A_fpR: 0.33748 | U_fpR: 0.61098\n",
      "Epoch[82/5000] Train: DISC 8691\n",
      "| LossD: 0.54474, LossG: 2.65411 | Acc: 0.76056 | fpR: 0.46669 | R: 0.98780 | A_fpR: 0.33656 | U_fpR: 0.60051\n",
      "Epoch[83/5000] Train: DISC 8691\n",
      "| LossD: 0.52704, LossG: 2.65411 | Acc: 0.81987 | fpR: 0.34749 | R: 0.98723 | A_fpR: 0.33621 | U_fpR: 0.59694\n",
      "Epoch[84/5000] Train: DISC 8691\n",
      "| LossD: 0.50868, LossG: 2.65411 | Acc: 0.86630 | fpR: 0.25221 | R: 0.98481 | A_fpR: 0.34553 | U_fpR: 0.60453\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[85/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.70539 | Acc: 0.85232 | fpR: 0.28133 | R: 0.98596 | A_fpR: 0.33759 | U_fpR: 0.61592\n",
      "Epoch[86/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.70520 | Acc: 0.83644 | fpR: 0.31458 | R: 0.98746 | A_fpR: 0.33920 | U_fpR: 0.61052\n",
      "Epoch[87/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.69422 | Acc: 0.81291 | fpR: 0.36037 | R: 0.98619 | A_fpR: 0.34403 | U_fpR: 0.62375\n",
      "Epoch[88/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.65477 | Acc: 0.78650 | fpR: 0.41307 | R: 0.98608 | A_fpR: 0.34495 | U_fpR: 0.62616\n",
      "Epoch[89/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.61280 | Acc: 0.77039 | fpR: 0.44310 | R: 0.98389 | A_fpR: 0.34185 | U_fpR: 0.63422\n",
      "Epoch[90/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.61395 | Acc: 0.74169 | fpR: 0.50328 | R: 0.98665 | A_fpR: 0.34104 | U_fpR: 0.63548\n",
      "Epoch[91/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.61379 | Acc: 0.72126 | fpR: 0.54424 | R: 0.98677 | A_fpR: 0.33989 | U_fpR: 0.63226\n",
      "Epoch[92/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.56592 | Acc: 0.70475 | fpR: 0.57772 | R: 0.98723 | A_fpR: 0.34300 | U_fpR: 0.62743\n",
      "Epoch[93/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.54152 | Acc: 0.68646 | fpR: 0.61282 | R: 0.98573 | A_fpR: 0.35531 | U_fpR: 0.64837\n",
      "Epoch[94/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.53632 | Acc: 0.67012 | fpR: 0.64883 | R: 0.98907 | A_fpR: 0.35243 | U_fpR: 0.65551\n",
      "Epoch[95/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.50615 | Acc: 0.65159 | fpR: 0.68324 | R: 0.98642 | A_fpR: 0.34518 | U_fpR: 0.65044\n",
      "Epoch[96/5000] Train: GEN 8691\n",
      "| LossD: 0.50868, LossG: 2.49431 | Acc: 0.63612 | fpR: 0.71522 | R: 0.98746 | A_fpR: 0.34622 | U_fpR: 0.66264\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[97/5000] Train: DISC 8691\n",
      "| LossD: 0.55540, LossG: 2.49431 | Acc: 0.67167 | fpR: 0.64400 | R: 0.98734 | A_fpR: 0.34277 | U_fpR: 0.66379\n",
      "Epoch[98/5000] Train: DISC 8691\n",
      "| LossD: 0.53224, LossG: 2.49431 | Acc: 0.71511 | fpR: 0.55563 | R: 0.98585 | A_fpR: 0.34898 | U_fpR: 0.64757\n",
      "Epoch[99/5000] Train: DISC 8691\n",
      "| LossD: 0.50592, LossG: 2.49431 | Acc: 0.76844 | fpR: 0.45150 | R: 0.98838 | A_fpR: 0.34208 | U_fpR: 0.66609\n",
      "Epoch[100/5000] Train: DISC 8691\n",
      "| LossD: 0.48180, LossG: 2.49431 | Acc: 0.83178 | fpR: 0.32252 | R: 0.98608 | A_fpR: 0.33851 | U_fpR: 0.66287\n",
      "Epoch[101/5000] Train: DISC 8691\n",
      "| LossD: 0.45541, LossG: 2.49431 | Acc: 0.89282 | fpR: 0.20067 | R: 0.98631 | A_fpR: 0.33805 | U_fpR: 0.66529\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[102/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.63136 | Acc: 0.86843 | fpR: 0.25060 | R: 0.98746 | A_fpR: 0.34035 | U_fpR: 0.66529\n",
      "Epoch[103/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.59060 | Acc: 0.83713 | fpR: 0.31343 | R: 0.98769 | A_fpR: 0.34369 | U_fpR: 0.67138\n",
      "Epoch[104/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.59657 | Acc: 0.80411 | fpR: 0.37959 | R: 0.98780 | A_fpR: 0.35197 | U_fpR: 0.67645\n",
      "Epoch[105/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.56471 | Acc: 0.78253 | fpR: 0.42251 | R: 0.98757 | A_fpR: 0.34910 | U_fpR: 0.67461\n",
      "Epoch[106/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.52922 | Acc: 0.74905 | fpR: 0.49062 | R: 0.98872 | A_fpR: 0.36129 | U_fpR: 0.68462\n",
      "Epoch[107/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.50237 | Acc: 0.72650 | fpR: 0.53400 | R: 0.98700 | A_fpR: 0.35612 | U_fpR: 0.69348\n",
      "Epoch[108/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.46461 | Acc: 0.70176 | fpR: 0.58543 | R: 0.98895 | A_fpR: 0.35727 | U_fpR: 0.69727\n",
      "Epoch[109/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.43481 | Acc: 0.67351 | fpR: 0.64055 | R: 0.98757 | A_fpR: 0.34634 | U_fpR: 0.69900\n",
      "Epoch[110/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.41947 | Acc: 0.64624 | fpR: 0.69382 | R: 0.98631 | A_fpR: 0.35025 | U_fpR: 0.69785\n",
      "Epoch[111/5000] Train: GEN 8691\n",
      "| LossD: 0.45541, LossG: 2.35424 | Acc: 0.62444 | fpR: 0.73789 | R: 0.98677 | A_fpR: 0.34254 | U_fpR: 0.71212\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[112/5000] Train: DISC 8691\n",
      "| LossD: 0.56780, LossG: 2.35424 | Acc: 0.66391 | fpR: 0.65769 | R: 0.98550 | A_fpR: 0.34415 | U_fpR: 0.70820\n",
      "Epoch[113/5000] Train: DISC 8691\n",
      "| LossD: 0.52815, LossG: 2.35424 | Acc: 0.71258 | fpR: 0.56093 | R: 0.98608 | A_fpR: 0.34956 | U_fpR: 0.71833\n",
      "Epoch[114/5000] Train: DISC 8691\n",
      "| LossD: 0.48872, LossG: 2.35424 | Acc: 0.77166 | fpR: 0.44356 | R: 0.98688 | A_fpR: 0.36014 | U_fpR: 0.70510\n",
      "Epoch[115/5000] Train: DISC 8691\n",
      "| LossD: 0.45090, LossG: 2.35424 | Acc: 0.84248 | fpR: 0.30146 | R: 0.98642 | A_fpR: 0.35255 | U_fpR: 0.70809\n",
      "Epoch[116/5000] Train: DISC 8691\n",
      "| LossD: 0.41601, LossG: 2.35424 | Acc: 0.90853 | fpR: 0.16914 | R: 0.98619 | A_fpR: 0.35278 | U_fpR: 0.70533\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[117/5000] Train: GEN 8691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| LossD: 0.41601, LossG: 2.63106 | Acc: 0.88051 | fpR: 0.22460 | R: 0.98562 | A_fpR: 0.35612 | U_fpR: 0.71004\n",
      "Epoch[118/5000] Train: GEN 8691\n",
      "| LossD: 0.41601, LossG: 2.57693 | Acc: 0.83305 | fpR: 0.31953 | R: 0.98562 | A_fpR: 0.35692 | U_fpR: 0.71936\n",
      "Epoch[119/5000] Train: GEN 8691\n",
      "| LossD: 0.41601, LossG: 2.52495 | Acc: 0.79243 | fpR: 0.39961 | R: 0.98447 | A_fpR: 0.35301 | U_fpR: 0.72753\n",
      "Epoch[120/5000] Train: GEN 8691\n",
      "| LossD: 0.41601, LossG: 2.47495 | Acc: 0.75066 | fpR: 0.48337 | R: 0.98470 | A_fpR: 0.34898 | U_fpR: 0.71913\n",
      "Epoch[121/5000] Train: GEN 8691\n",
      "| LossD: 0.41601, LossG: 2.42835 | Acc: 0.70423 | fpR: 0.57703 | R: 0.98550 | A_fpR: 0.33909 | U_fpR: 0.72765\n",
      "Epoch[122/5000] Train: GEN 8691\n",
      "| LossD: 0.41601, LossG: 2.37778 | Acc: 0.66546 | fpR: 0.65412 | R: 0.98504 | A_fpR: 0.34518 | U_fpR: 0.72454\n",
      "Epoch[123/5000] Train: GEN 8691\n",
      "| LossD: 0.41601, LossG: 2.32396 | Acc: 0.63025 | fpR: 0.72339 | R: 0.98389 | A_fpR: 0.34795 | U_fpR: 0.74456\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[124/5000] Train: DISC 8691\n",
      "| LossD: 0.57921, LossG: 2.32396 | Acc: 0.67812 | fpR: 0.62663 | R: 0.98286 | A_fpR: 0.34553 | U_fpR: 0.72696\n",
      "Epoch[125/5000] Train: DISC 8691\n",
      "| LossD: 0.52366, LossG: 2.32396 | Acc: 0.74140 | fpR: 0.49764 | R: 0.98044 | A_fpR: 0.34162 | U_fpR: 0.73939\n",
      "Epoch[126/5000] Train: DISC 8691\n",
      "| LossD: 0.46673, LossG: 2.32396 | Acc: 0.82338 | fpR: 0.33103 | R: 0.97779 | A_fpR: 0.33840 | U_fpR: 0.73536\n",
      "Epoch[127/5000] Train: DISC 8691\n",
      "| LossD: 0.41824, LossG: 2.32396 | Acc: 0.90778 | fpR: 0.15395 | R: 0.96951 | A_fpR: 0.34507 | U_fpR: 0.73317\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[128/5000] Train: GEN 8691\n",
      "| LossD: 0.41824, LossG: 2.57362 | Acc: 0.84996 | fpR: 0.27350 | R: 0.97342 | A_fpR: 0.34139 | U_fpR: 0.74502\n",
      "Epoch[129/5000] Train: GEN 8691\n",
      "| LossD: 0.41824, LossG: 2.51384 | Acc: 0.78725 | fpR: 0.39719 | R: 0.97169 | A_fpR: 0.33529 | U_fpR: 0.75043\n",
      "Epoch[130/5000] Train: GEN 8691\n",
      "| LossD: 0.41824, LossG: 2.45702 | Acc: 0.73053 | fpR: 0.51237 | R: 0.97342 | A_fpR: 0.34898 | U_fpR: 0.75469\n",
      "Epoch[131/5000] Train: GEN 8691\n",
      "| LossD: 0.41824, LossG: 2.34471 | Acc: 0.67288 | fpR: 0.62893 | R: 0.97469 | A_fpR: 0.34392 | U_fpR: 0.76631\n",
      "Epoch[132/5000] Train: GEN 8691\n",
      "| LossD: 0.41824, LossG: 2.27975 | Acc: 0.62116 | fpR: 0.72915 | R: 0.97146 | A_fpR: 0.34657 | U_fpR: 0.76113\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[133/5000] Train: DISC 8691\n",
      "| LossD: 0.61276, LossG: 2.27975 | Acc: 0.67777 | fpR: 0.60799 | R: 0.96353 | A_fpR: 0.34875 | U_fpR: 0.76504\n",
      "Epoch[134/5000] Train: DISC 8691\n",
      "| LossD: 0.54201, LossG: 2.27975 | Acc: 0.75170 | fpR: 0.44736 | R: 0.95075 | A_fpR: 0.34070 | U_fpR: 0.76735\n",
      "Epoch[135/5000] Train: DISC 8691\n",
      "| LossD: 0.47905, LossG: 2.27975 | Acc: 0.84507 | fpR: 0.24462 | R: 0.93476 | A_fpR: 0.35543 | U_fpR: 0.76735\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[136/5000] Train: GEN 8691\n",
      "| LossD: 0.47905, LossG: 2.49150 | Acc: 0.76389 | fpR: 0.40950 | R: 0.93729 | A_fpR: 0.34737 | U_fpR: 0.77413\n",
      "Epoch[137/5000] Train: GEN 8691\n",
      "| LossD: 0.47905, LossG: 2.41135 | Acc: 0.68910 | fpR: 0.55609 | R: 0.93430 | A_fpR: 0.34047 | U_fpR: 0.77494\n",
      "Epoch[138/5000] Train: GEN 8691\n",
      "| LossD: 0.47905, LossG: 2.28831 | Acc: 0.62358 | fpR: 0.68853 | R: 0.93568 | A_fpR: 0.34426 | U_fpR: 0.77713\n",
      "Epoch[139/5000] Train: GEN 8691\n",
      "| LossD: 0.47905, LossG: 2.18930 | Acc: 0.57606 | fpR: 0.78161 | R: 0.93372 | A_fpR: 0.36129 | U_fpR: 0.80037\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[140/5000] Train: DISC 8691\n",
      "| LossD: 0.71074, LossG: 2.18930 | Acc: 0.61506 | fpR: 0.67380 | R: 0.90392 | A_fpR: 0.34841 | U_fpR: 0.77678\n",
      "Epoch[141/5000] Train: DISC 8691\n",
      "| LossD: 0.63159, LossG: 2.18930 | Acc: 0.68111 | fpR: 0.49960 | R: 0.86181 | A_fpR: 0.35025 | U_fpR: 0.77609\n",
      "Epoch[142/5000] Train: DISC 8691\n",
      "| LossD: 0.56030, LossG: 2.18930 | Acc: 0.74623 | fpR: 0.31193 | R: 0.80440 | A_fpR: 0.34668 | U_fpR: 0.78725\n",
      "Epoch[143/5000] Train: DISC 8691\n",
      "| LossD: 0.51349, LossG: 2.18930 | Acc: 0.81130 | fpR: 0.11103 | R: 0.73363 | A_fpR: 0.34864 | U_fpR: 0.77632\n",
      "Epoch[144/5000] Train: DISC 8691\n",
      "| LossD: 0.49128, LossG: 2.18930 | Acc: 0.80192 | fpR: 0.05155 | R: 0.65539 | A_fpR: 0.34818 | U_fpR: 0.77954\n",
      "Epoch[145/5000] Train: DISC 8691\n",
      "| LossD: 0.48610, LossG: 2.18930 | Acc: 0.79128 | fpR: 0.02957 | R: 0.61213 | A_fpR: 0.34541 | U_fpR: 0.78679\n",
      "Epoch[146/5000] Train: DISC 8691\n",
      "| LossD: 0.48856, LossG: 2.18930 | Acc: 0.78443 | fpR: 0.01277 | R: 0.58164 | A_fpR: 0.34426 | U_fpR: 0.77575\n",
      "Epoch[147/5000] Train: DISC 8691\n",
      "| LossD: 0.48988, LossG: 2.18930 | Acc: 0.78472 | fpR: 0.00782 | R: 0.57726 | A_fpR: 0.34265 | U_fpR: 0.79059\n",
      "Epoch[148/5000] Train: DISC 8691\n",
      "| LossD: 0.48619, LossG: 2.18930 | Acc: 0.78650 | fpR: 0.00518 | R: 0.57818 | A_fpR: 0.34116 | U_fpR: 0.77609\n",
      "Epoch[149/5000] Train: DISC 8691\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e5948f5942b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0muser_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved_models/5000Epochs_0.01LR_MutualExclusiveLabelClassifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cGAN_trained\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_lr\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant_train_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#plot_metrics(model_output, True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-15f87178b9ed>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(X, y, act, usr, criterion1, criterion2, gan_id, dig, feature_dim, GENERATOR, DISCRIMINATOR, train_string, disc, z_dim, a_dim, u_dim, gen_lr, disc_lr, batch_size, constant_train_flag, disc_epochs, gen_epochs, epoch_threshold, disc_epochs_change, gen_epochs_change, rel_epochs, static_threshold, static_disc_epochs, static_gen_epochs, pull_threshold, push_threshold, recall_threshold, print_batches, n_epochs, rows, heading, table, switch_flag, switch_count, last_real_features)\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[1;31m#disc_loss.backward()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mopt_disc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperformance_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreal_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[0mA_fpR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperformance_stats_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"act\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mU_fpR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperformance_stats_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"usr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-2722c0bcff39>\u001b[0m in \u001b[0;36mperformance_stats\u001b[1;34m(gen, disc, b_size, z_dim, a_dim, u_dim, batch)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0my_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0my_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mtn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0my_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#X, y = start_data(\"aggregated_data/aggregated_data.csv\", \"label:SITTING\")\n",
    "#X, y = start_data(\"raw_data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv\", \"label:SITTING\" ) \n",
    "X, y = start_data(\"aggregated_data/aggregated_data.csv\", [\"label:SITTING\", \"label:FIX_walking\", \"label:SLEEPING\"], [\"0BFC35E2-4817-4865-BFA7-764742302A2D\", \"0A986513-7828-4D53-AA1F-E02D6DF9561B\", \"00EABED2-271D-49D8-B599-1D4A09240601\"])\n",
    "#print(len(X))\n",
    "activity_classifier = Activity_Classifier()\n",
    "user_classifier = User_Classifier()\n",
    "activity_classifier.to(device)\n",
    "user_classifier.to(device)\n",
    "\n",
    "activity_classifier.load_state_dict(torch.load('saved_models/2000Epochs_0.01LR_UserClassifier'), strict = False)\n",
    "user_classifier.load_state_dict(torch.load(\"saved_models/5000Epochs_0.01LR_MutualExclusiveLabelClassifier\"), strict = False)\n",
    "\n",
    "model_output = training_loop(X,y, activity_classifier, user_classifier, gan_id=\"cGAN_trained\", batch_size = len(X), gen_lr=.005, disc_lr =.005, n_epochs=5000, dig=5, constant_train_flag=False, print_batches = False)\n",
    "#plot_metrics(model_output, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c2ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
