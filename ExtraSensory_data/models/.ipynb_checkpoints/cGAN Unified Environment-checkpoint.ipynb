{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d598477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from prettytable import PrettyTable\n",
    "from pylab import *\n",
    "from scipy.stats import wasserstein_distance\n",
    "import random\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a8f94",
   "metadata": {},
   "source": [
    "# Move Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1885423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaryClare's\n",
    "#os.chdir('/Users/maryclaremartin/Documents/jup/ExtraSensory')\n",
    "\n",
    "# Josh's\n",
    "os.chdir(\"/Users/jdeoliveira/REU2021-human-context-recognition/ExtraSensory_data\")\n",
    "\n",
    "softmax = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ef613",
   "metadata": {},
   "source": [
    "# Load & Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf8748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load and scale data\n",
    "#returns scaled data (X) and labels (Y)\n",
    "#file_name: string, file with data to be used\n",
    "#label: string, activity to do\n",
    "def start_data(file_name, label):\n",
    "    #read csv into dataframe\n",
    "    data = pd.read_csv(file_name)\n",
    "    \n",
    "    #seperate only acceleration data\n",
    "    X = data.iloc[:,1:27]\n",
    "    y = data[[label]]\n",
    "\n",
    "    #seperate only \"on\" labels\n",
    "    X = X[y[label] == 1]\n",
    "    y = y[y[label] == 1]\n",
    "    \n",
    "    #interpolate averages per column\n",
    "    X = interpolation(X).values\n",
    "    y = interpolation(y).values\n",
    "    \n",
    "    #scale the data\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X = sc.fit_transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97af8e",
   "metadata": {},
   "source": [
    "# The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304a6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    #torch.manual_seed(0)\n",
    "    return torch.randn(n_samples, z_dim).to(device)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, int(hidden_dim/2)),\n",
    "            generator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            generator_block(int(hidden_dim/4), 30),\n",
    "            generator_block(30, 28),\n",
    "            nn.Linear(28, feature_dim)\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "##calculates generator loss\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#criterion1: loss function1\n",
    "#criterion2: loss function2\n",
    "#batch_size: batch size\n",
    "#z_dim: number of dimensions in the latent space\n",
    "def get_gen_loss(gen, disc, act, usr, criterion1, criterion2, batch_size, z_dim, activities, users):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, activities)\n",
    "    usr_vectors = get_usr_matrix(batch_size, users)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "    fake_features = gen(to_gen)\n",
    "    \n",
    "    pred_disc = disc(fake_features.to(device))\n",
    "    pred_act = softmax(act(fake_features.to(device)))\n",
    "    pred_usr = softmax(usr(fake_features.to(device)))\n",
    "    \n",
    "    d_loss = criterion1(pred_disc, torch.ones_like(pred_disc))\n",
    "    act_loss = criterion2(pred_act, act_vectors[0].to(device))\n",
    "    usr_loss = criterion2(pred_usr, usr_vectors[0].to(device))\n",
    "    \n",
    "    gen_loss = d_loss + act_loss + usr_loss\n",
    "    return gen_loss\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    #print(indexes)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287679b9",
   "metadata": {},
   "source": [
    "# Create Fake Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6b90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_samples(gen, batch_size, z_dim):\n",
    "    \"\"\"\n",
    "    Generates fake acceleration features given a batch size, latent vector dimension, and trained generator.\n",
    "    \n",
    "    \"\"\"\n",
    "    latent_vectors = get_noise(batch_size, z_dim) ### Retrieves a 2D tensor of noise\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    \n",
    "    return fake_features ### Returns a 2D tensor of fake features of size batch_size x z_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b67ffa",
   "metadata": {},
   "source": [
    "# The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7e2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "#defines discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim = 26, hidden_dim = 16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            discriminator_block(feature_dim, hidden_dim),\n",
    "            discriminator_block(hidden_dim, int(hidden_dim/2)),\n",
    "            discriminator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            nn.Linear(int(hidden_dim/4), 1),\n",
    "            nn.Sigmoid()                    \n",
    "        )\n",
    "    def forward(self, feature_vector):\n",
    "        return self.disc(feature_vector)\n",
    "    \n",
    "def get_disc_loss(gen, disc, criterion, real_features, batch_size, z_dim, a_dim, u_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, a_dim)\n",
    "    usr_vectors = get_usr_matrix(batch_size, u_dim)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors.to(device), act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "    fake_features = gen(to_gen)\n",
    "    pred_fake = disc(fake_features.detach())\n",
    "    \n",
    "    ground_truth = torch.zeros_like(pred_fake)\n",
    "    loss_fake = criterion(pred_fake, ground_truth)\n",
    "    \n",
    "    pred_real = disc(real_features.to(device))\n",
    "    ground_truth = torch.ones_like(pred_real)\n",
    "    loss_real = criterion(pred_real, ground_truth)\n",
    "    \n",
    "    disc_loss = (loss_fake + loss_real) / 2\n",
    "    return disc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba8536",
   "metadata": {},
   "source": [
    "# User Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25d4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class User_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(User_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f6223",
   "metadata": {},
   "source": [
    "# Activity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490d22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Activity_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Activity_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            classifier_block(10, 5),\n",
    "            nn.Linear(5, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #softmax = nn.Softmax(dim = 1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b73410",
   "metadata": {},
   "source": [
    "# Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d634b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replaces Nan values with average values\n",
    "#df: data frame of data to use\n",
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe834a8f",
   "metadata": {},
   "source": [
    "# Visualize Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9436c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "##prints a plot of a generator batch\n",
    "#gen: generator\n",
    "#b_size: batch size\n",
    "#epochs: current epoch (-1)\n",
    "def visualize_gen_batch(gen, b_size, epochs = -1):\n",
    "    #print(str(b_size))\n",
    "    latent_vectors = get_noise(b_size, z_dim)\n",
    "    #print(latent_vectors.shape)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    #print(fake_features.shape)\n",
    "    \n",
    "    w_img = fake_features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Generated Batch at Epoch ' + str(epochs), fontweight =\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "##prints a plot of a batch of real data\n",
    "#features: real data\n",
    "def visualize_real_batch(features):\n",
    "    w_img = features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Real Batch of Data', fontweight =\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a26fd",
   "metadata": {},
   "source": [
    "# Calculate Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21a605a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculates performance statistics for each epoch of training\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#b_size: batch size\n",
    "#z_dim: number of dimensions of the latent space\n",
    "##returns accuracy, precision, recall, fpR, and f1 score\n",
    "def performance_stats(gen, disc, b_size, z_dim, a_dim, u_dim, batch = None):\n",
    "    tp = 0 #true positive\n",
    "    fp = 0 #false positive\n",
    "    tn = 0 #true negative\n",
    "    fn = 0 #false negative\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if batch is None:\n",
    "            latent_vectors = get_noise(b_size, z_dim)\n",
    "            \n",
    "            act_vectors = get_act_matrix(b_size, a_dim)\n",
    "            usr_vectors = get_usr_matrix(b_size, u_dim)\n",
    "            to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "            fake_features = gen(to_gen)\n",
    "\n",
    "            y_hat = torch.round(disc(fake_features))\n",
    "            y_label = [0] * b_size\n",
    "            y_label = torch.Tensor(y_label).to(device)\n",
    "        else:\n",
    "            latent_vectors = get_noise(int(b_size/2), z_dim)\n",
    "            act_vectors = get_act_matrix(int(b_size/2), a_dim)\n",
    "            usr_vectors = get_usr_matrix(int(b_size/2), u_dim)\n",
    "            \n",
    "            to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "            fake_features = gen(to_gen)\n",
    "        \n",
    "            y_hat = torch.round(disc(fake_features.to(device)))\n",
    "            y_label = torch.Tensor([0] * int(b_size/2))\n",
    "            \n",
    "            real_y_hat = torch.round(disc(batch[:int(b_size/2)].to(device)))\n",
    "            y_add = torch.Tensor([1] * int(b_size/2))\n",
    "            y_label = torch.cat((y_label, y_add), dim = 0)\n",
    "            #for i in range(0, int(b_size/2)):\n",
    "            # y_label.append(1)\n",
    "            y_hat = torch.cat((y_hat, real_y_hat), dim = 0).to(device)\n",
    "            \n",
    "            #print(y_hat)\n",
    "            #print(y_label)\n",
    "         \n",
    "        \n",
    "        for k in range(len(y_hat)):\n",
    "            if y_label[k] == 1:\n",
    "                if y_hat[k] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            elif y_hat[k] == 0:\n",
    "                tn += 1\n",
    "            elif y_hat[k] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "                exit()\n",
    "            \n",
    "        class_acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "        \n",
    "        if tp + fp == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            \n",
    "        if tp + fn == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "            \n",
    "        if fp + tn == 0:\n",
    "            fpR = 0\n",
    "        else: \n",
    "            fpR = fp / (fp + tn)\n",
    "\n",
    "        #print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "        #print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "        #print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "        #print(f'F-1 Score: {2*(precision * recall / (precision + recall + 0.001)):.2f}')\n",
    "        return class_acc, precision, recall, fpR, 2*(precision * recall / (precision + recall + 0.001))\n",
    "\n",
    "def performance_stats_class(gen, classifier, machine, batch_size, z_dim, a_dim, u_dim):\n",
    "    tp = 0 #true positive\n",
    "    fp = 0 #false positive\n",
    "    tn = 0 #true negative\n",
    "    fn = 0 #false negative\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent_vectors = get_noise(batch_size, z_dim)\n",
    "        act_vectors = get_act_matrix(batch_size, a_dim)\n",
    "        usr_vectors = get_usr_matrix(batch_size, u_dim)\n",
    "    \n",
    "        to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "        fake_features = gen(to_gen)\n",
    "        #print(fake_features)\n",
    "    \n",
    "        _, pred_class = torch.max(softmax(classifier(fake_features.to(device))), dim = 1)\n",
    "        #print(pred_class)\n",
    "        labels = []\n",
    "        if machine == \"act\":\n",
    "            labels = act_vectors\n",
    "        else:\n",
    "            labels = usr_vectors\n",
    "        \n",
    "        return torch.eq(labels[0].to(device), pred_class).sum()/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ebc6e",
   "metadata": {},
   "source": [
    "# Create Density Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c62bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and plot density curves for mean, x, y, z acceleration\n",
    "#reals: real data\n",
    "#fakes: generated data\n",
    "def density_curves(reals, fakes):\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    subplot(2, 2, 1)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,0], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,0], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 2)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,18], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,18], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean X-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 3)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,19], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,19], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Y-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 4)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,20], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,20], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Z-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc3660",
   "metadata": {},
   "source": [
    "# Calculate Wassertein distance for each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbdc0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate Waaserstein distances for each dimension\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "#feature_dim: number ofd dimensions in the feature space\n",
    "#sample: sample of data\n",
    "def all_Wasserstein_dists(gen, z_dim, feature_dim, sample):\n",
    "    wasser_dim = []\n",
    "    latent_vectors = get_noise(len(sample), z_dim)\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    for k in range(feature_dim):\n",
    "        wasser_dim.append(wasserstein_distance(fake_features[:, k].cpu().detach().numpy(), sample[:, k].cpu().detach().numpy()))\n",
    "    return torch.tensor(wasser_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5615c5",
   "metadata": {},
   "source": [
    "# Visualizing Generation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccedc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates and prints a plot of the generated vs real data\n",
    "#data: data used\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "def visualize_gen(data, gen, z_dim, a_dim, u_dim):\n",
    "    #Number of datum to visualize\n",
    "    sample_size = len(data)\n",
    "    reals = data[0:sample_size, :]\n",
    "    fakes = get_fake_samples(gen, sample_size, z_dim).detach()\n",
    "    density_curves(reals, fakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e240e",
   "metadata": {},
   "source": [
    "# Initialize Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff586316",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initalize parameters that depend on training loop parameters\n",
    "#X: acceleration data\n",
    "#y: labels associated with X data (fake or real)\n",
    "#z_dim: number of dimensions to the latent space\n",
    "#disc_lr: discriminator learning rate\n",
    "#gen_lr: generator learning rate\n",
    "#DISCRIMINATOR: 1 to indicate if discriminator is training\n",
    "#batch_size: batch size\n",
    "#disc: initialized discrimiantor\n",
    "\n",
    "def initialize_params(X, y, z_dim, a_dim, u_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc):\n",
    "    #initialize generator\n",
    "    gen = Generator(z_dim + a_dim + u_dim).to(device)\n",
    "    #indicate that discriminator is training\n",
    "    to_train = DISCRIMINATOR\n",
    "    #create training features\n",
    "    train_features = torch.tensor(X)\n",
    "    #create training labels\n",
    "    train_labels = torch.tensor(y)\n",
    "    #concatenate to create training data\n",
    "    train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    #create data loader for training data\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    #initialize generator and discriminator optimizers\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr = disc_lr)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr = gen_lr)\n",
    "    \n",
    "    return gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129fb6e1",
   "metadata": {},
   "source": [
    "# Save / Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0803c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path and name of the Generator and Discriminator accordingly\n",
    "def save_model(gen, disc, model_name):\n",
    "    torch.save(gen.state_dict(), f\"saved_models/{model_name}_gen\")\n",
    "    torch.save(disc.state_dict(), f\"saved_models/{model_name}_disc\")\n",
    "    \n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c08d0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba594b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Training loop to train GAN\n",
    "\n",
    "#Parameters to specifiy: \n",
    "    #X: starting accelerometer data\n",
    "    #y: starting labels for X data (fake or real)\n",
    "    \n",
    "#Set parameters (do not change)\n",
    "    #criterion: loss function (BCE)\n",
    "    #dig: number of significant digits for printing (5)\n",
    "    #feature_dim: Number of dimensions of output from generator (26)\n",
    "    #GENERATOR: set generator to zero for training\n",
    "    #DISCRIMINATOR: set discriminator to one for training\n",
    "    #train_string: starting machine to train (DISC)\n",
    "    #disc: initalize discriminator\n",
    "    #rel_epochs: Epochs passed since last switch (constant training) (0)\n",
    "    #rows: initialization of array to save data of each epoch to CSV file ([])\n",
    "    #heading: array of column headings for table ([\"Epoch\", \"Machine Training\", \"Discriminator Loss\", \n",
    "                    #\"Generator Loss\", \"FPR\", \"Recall\", \"Median Wasserstein\", \"Mean Wasserstein\"])\n",
    "    #table: intialize a table as a pretty table to save epoch data\n",
    "    #switch_count: number of switches in dynamic training (0)\n",
    "    \n",
    "#Set parameters (can change):\n",
    "    #z_dim: number of dimensions of latent vector (100)\n",
    "    #gen_lr: generator learning rate (.001)\n",
    "    #disc_lr: discriminator learning rate (.001) (shoud be equal to gen_lr)\n",
    "    #batch_size: batch size (75)\n",
    "    #print_batches: Show model performance per batch (False)\n",
    "    #n_epochs: number of epochs to train (100)\n",
    "    #constant_train_flag: (False)\n",
    "        #Set to true to train based on constant # of epochs per machine \n",
    "        #Set to false to train dynamically based on machine performance\n",
    "        \n",
    "    #Constant training approach:\n",
    "        #disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (5)\n",
    "        #gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #epoch_threshold: Epoch number to change training epoch ratio (50)\n",
    "        #disc_epochs_change: New number of consecutive epochs to train discriminator after epoch threshold is exceeded (1)\n",
    "        #gen_epochs_change: New number of consecutive epochs to train generator after epoch threshold is exceeded (50)\n",
    "    \n",
    "    #Dynamic training approach:                        \n",
    "        #static_threshold: Epoch number to change from static ratio to dynamic (18)\n",
    "        #static_disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (4)\n",
    "        #static_gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #pull_threshold: Accuracy threshold for switching machine training when the generator is no longer competitive (0.4)\n",
    "        #push_threshold: Accuracy threshold for switching machine training when the discriminator is no longer competitive (0.6)\n",
    "        #recall_threshold: threshold for recall to switch machine training when discriminator is training well\n",
    "        #switch_flag: indicates if we should switch our training machine (False)\n",
    "        \n",
    "def training_loop(X, y, act, usr, criterion1 = nn.BCELoss(), criterion2 = nn.CrossEntropyLoss(), gan_id = \"Mod Test Gan\", dig = 5, feature_dim = 26, \n",
    "                  GENERATOR = 0, DISCRIMINATOR = 1, train_string = \"DISC\", disc = Discriminator(), z_dim = 100, a_dim = 3, u_dim = 3, \n",
    "                  gen_lr =  0.001, disc_lr = 0.001, batch_size = 100, constant_train_flag = False, disc_epochs = 5,\n",
    "                  gen_epochs = 2, epoch_threshold = 50, disc_epochs_change = 5, gen_epochs_change = 2, rel_epochs = 0,\n",
    "                 static_threshold = 28, static_disc_epochs = 5, static_gen_epochs = 2, pull_threshold = 0.3,\n",
    "                 push_threshold = 0.7, recall_threshold = 0.75, print_batches = False, n_epochs = 1000, rows = [],\n",
    "                 heading = [\"Epoch\", \"Training\", \"Discriminator Loss\", \"Generator Loss\", \"D_Accuracy\", \"D_fpR\", \"D_Recall\", \"A_fpR\", \"U_fpR\"],\n",
    "                 table = PrettyTable(), switch_flag = False, switch_count = 0, last_real_features = []):\n",
    "    \n",
    "    disc.to(device)\n",
    "    #returns generator, sets discriminator training, creates training tensor, loads data, and initializes optimizers\n",
    "    gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen = initialize_params(X, y, z_dim, a_dim, u_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc)\n",
    "\n",
    "    #set pretty table field names\n",
    "    table.field_names = heading\n",
    "    \n",
    "    #visualize_gen(X, gen, z_dim, a_dim, u_dim)\n",
    "\n",
    "    gen_epochs = 0\n",
    "    \n",
    "    last_D_loss = -1.0\n",
    "    last_G_loss = -1.0\n",
    "    \n",
    "    mean_mean = []\n",
    "    mean_median = []\n",
    "    \n",
    "    for epoch in range(n_epochs):  \n",
    "        if constant_train_flag:\n",
    "            if to_train == DISCRIMINATOR and rel_epochs >= disc_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GEN\"\n",
    "\n",
    "            elif to_train == GENERATOR and rel_epochs >= gen_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = DISCRIMINATOR\n",
    "                train_string = \"DISC\"\n",
    "\n",
    "            # Change epoch ratio after intial 'leveling out'\n",
    "            if epoch == epoch_threshold:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GENERATOR\"\n",
    "\n",
    "                old_ratio = gen_epochs / disc_epochs\n",
    "                gen_epochs = gen_epochs_change\n",
    "                disc_epochs = disc_epochs_change\n",
    "                new_ratio = gen_epochs / disc_epochs\n",
    "                print(f'\\n\\nTraining ratio of G/D switched from {old_ratio:.{dig}f} to {new_ratio:.{dig}f}\\n\\n')\n",
    "        else:\n",
    "            if epoch < static_threshold:\n",
    "                if to_train == DISCRIMINATOR and rel_epochs >= static_disc_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "\n",
    "                elif to_train == GENERATOR and rel_epochs >= static_gen_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "\n",
    "            else:\n",
    "                #to_train = DISCRIMINATOR\n",
    "                #train_string = \"DISC\"\n",
    "                if not switch_flag:\n",
    "                    print(\"\\nSwitching to Dynamic Training\\n\")\n",
    "                    switch_flag = True\n",
    "                if to_train == DISCRIMINATOR and fpR <= pull_threshold and R >= recall_threshold:\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "                    print(\"\\nPull Generator\\n\")\n",
    "                    switch_count += 1\n",
    "                if to_train == GENERATOR and fpR >= push_threshold:\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "                    print(\"\\nPush Generator\\n\")\n",
    "                    switch_count += 1\n",
    "        print(f'Epoch[{epoch + 1}/{n_epochs}] Train: {train_string} ', end ='')\n",
    "        for batch_idx, (real_features, _) in enumerate(train_loader):\n",
    "            #batch_size = len(real_features)\n",
    "            \n",
    "            if print_batches:\n",
    "                    print(f'\\n\\tBatch[{batch_idx + 1}/{len(train_loader)}] |', end ='')\n",
    "\n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Training Discriminator\n",
    "                #visualize_real_batch(real_features.float())\n",
    "                opt_disc.zero_grad()\n",
    "                disc_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), len(real_features), z_dim, a_dim, u_dim)\n",
    "                disc_loss.backward(retain_graph = True)\n",
    "                opt_disc.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), z_dim, a_dim, u_dim, batch = real_features.float())\n",
    "                A_fpR = performance_stats_class(gen, act, \"act\", batch_size, z_dim, a_dim, u_dim)\n",
    "                U_fpR = performance_stats_class(gen, usr, \"usr\", batch_size, z_dim, a_dim, u_dim)\n",
    "                #w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                #median_w_dist = torch.median(w_dist)\n",
    "                #mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                #mean_mean.append(mean_w_dist)\n",
    "                #mean_median.append(median_w_dist)\n",
    "                \n",
    "                mean_mean.append(0)\n",
    "                mean_median.append(0)\n",
    "\n",
    "                last_D_loss = disc_loss.item()\n",
    "                \n",
    "                if last_G_loss == -1.0:\n",
    "                    last_G_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, len(real_features), z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f}')\n",
    "            else:\n",
    "                ### Training Generator\n",
    "                opt_gen.zero_grad()\n",
    "                gen_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, batch_size, z_dim, a_dim, u_dim)\n",
    "                gen_loss.backward()\n",
    "                opt_gen.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), z_dim, a_dim, u_dim, batch = real_features.float())\n",
    "                A_fpR = performance_stats_class(gen, act, \"act\", batch_size, z_dim, a_dim, u_dim)\n",
    "                U_fpR = performance_stats_class(gen, usr, \"usr\", batch_size, z_dim, a_dim, u_dim)\n",
    "                #w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                #median_w_dist = torch.median(w_dist)\n",
    "                #mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                #mean_mean.append(mean_w_dist)\n",
    "                #mean_median.append(median_w_dist)\n",
    "                \n",
    "                mean_mean.append(0)\n",
    "                mean_median.append(0)\n",
    "                \n",
    "                last_G_loss = gen_loss.item()\n",
    "                \n",
    "                if last_D_loss == -1.0:\n",
    "                    last_D_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), batch_size, z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f}')\n",
    "\n",
    "        if not print_batches:\n",
    "            \n",
    "            mean_mean_w = torch.mean(torch.Tensor(mean_mean)) \n",
    "            \n",
    "            mean_median_w = torch.mean(torch.Tensor(mean_median))\n",
    "            \n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Currently doesn't print Median/Mean Wasserstein --> Change if needed\n",
    "                print(f'| LossD: {last_D_loss:.{dig}f}, LossG: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | R: {R:.{dig}f} | A_fpR: {A_fpR:.{dig}f} | U_fpR: {U_fpR:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Discriminator\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{A_fpR:.{dig}f}\", f\"{U_fpR:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "            else:\n",
    "                print(f'| Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | R: {R:.{dig}f} | A_fpR: {A_fpR:.{dig}f} | U_fpR: {U_fpR:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Generator\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{A_fpR:.{dig}f}\", f\"{U_fpR:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "                gen_epochs += 1\n",
    "        mean_mean.clear()\n",
    "        mean_median.clear()\n",
    "        rel_epochs += 1\n",
    "    print(\"\\n\\nTraining Session Finished\")\n",
    "    print(f\"Encountered {switch_count} non-trivial training swaps\")\n",
    "    percent = gen_epochs / n_epochs\n",
    "    print(f\"Trained Generator {gen_epochs} out of {n_epochs} ({percent:.3f})\")\n",
    "    f = open(\"model_outputs/\" + gan_id + \".txt\", \"w\")\n",
    "    f.write(table.get_string())\n",
    "    f.close()\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".txt\\\"\")\n",
    "\n",
    "    with open(\"model_outputs/\" + gan_id + \".csv\", \"w\") as csvfile: \n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "        # writing the fields \n",
    "        csvwriter.writerow(heading)\n",
    "        # writing the data rows \n",
    "        csvwriter.writerows(rows)\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".csv\\\"\")\n",
    "    save_model(gen, disc, gan_id)\n",
    "    model_output = pd.read_csv(\"model_outputs/\" + gan_id + \".csv\")\n",
    "    #visualize_gen(X, gen, z_dim)\n",
    "    \n",
    "    # Change path and name of the Generator and Discriminator accordingly\n",
    "    save_model(gen, disc, gan_id)\n",
    "    \n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2d413",
   "metadata": {},
   "source": [
    "# Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df42357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot metrics based on data (csv)\n",
    "def plot_metrics(data, vanilla = True):\n",
    "    if vanilla:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "    \n",
    "        plt.figure(figsize = (15, 15))\n",
    "        subplot(2, 2, 1)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'FPR', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 2)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'Recall', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 3)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 4)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "        plt.figure(figsize = (15, 8))\n",
    "        \n",
    "        subplot(1, 2, 1)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(1, 2, 2)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ace97d",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a8de1da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/10000] Train: DISC | LossD: 0.739, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.365 | U_fpR: 0.345\n",
      "Epoch[2/10000] Train: DISC | LossD: 0.720, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.400 | U_fpR: 0.345\n",
      "Epoch[3/10000] Train: DISC | LossD: 0.710, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.440 | U_fpR: 0.325\n",
      "Epoch[4/10000] Train: DISC | LossD: 0.728, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.310\n",
      "Epoch[5/10000] Train: DISC | LossD: 0.709, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.385 | U_fpR: 0.370\n",
      "Epoch[6/10000] Train: GEN | Loss D: 0.709, Loss G: 2.713 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.305 | U_fpR: 0.250\n",
      "Epoch[7/10000] Train: GEN | Loss D: 0.709, Loss G: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.350 | U_fpR: 0.280\n",
      "Epoch[8/10000] Train: DISC | LossD: 0.715, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.345\n",
      "Epoch[9/10000] Train: DISC | LossD: 0.710, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.335\n",
      "Epoch[10/10000] Train: DISC | LossD: 0.712, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.350 | U_fpR: 0.335\n",
      "Epoch[11/10000] Train: DISC | LossD: 0.697, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.330\n",
      "Epoch[12/10000] Train: DISC | LossD: 0.727, LossG: 2.704 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.345\n",
      "Epoch[13/10000] Train: GEN | Loss D: 0.727, Loss G: 2.689 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.380 | U_fpR: 0.305\n",
      "Epoch[14/10000] Train: GEN | Loss D: 0.727, Loss G: 2.613 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.355\n",
      "Epoch[15/10000] Train: DISC | LossD: 0.711, LossG: 2.613 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.395 | U_fpR: 0.370\n",
      "Epoch[16/10000] Train: DISC | LossD: 0.735, LossG: 2.613 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.275 | U_fpR: 0.315\n",
      "Epoch[17/10000] Train: DISC | LossD: 0.712, LossG: 2.613 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.315\n",
      "Epoch[18/10000] Train: DISC | LossD: 0.709, LossG: 2.613 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.305 | U_fpR: 0.350\n",
      "Epoch[19/10000] Train: DISC | LossD: 0.719, LossG: 2.613 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.295\n",
      "Epoch[20/10000] Train: GEN | Loss D: 0.719, Loss G: 2.629 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.305\n",
      "Epoch[21/10000] Train: GEN | Loss D: 0.719, Loss G: 2.681 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.290 | U_fpR: 0.315\n",
      "Epoch[22/10000] Train: DISC | LossD: 0.712, LossG: 2.681 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.405 | U_fpR: 0.310\n",
      "Epoch[23/10000] Train: DISC | LossD: 0.712, LossG: 2.681 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.355\n",
      "Epoch[24/10000] Train: DISC | LossD: 0.726, LossG: 2.681 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.330\n",
      "Epoch[25/10000] Train: DISC | LossD: 0.701, LossG: 2.681 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.340\n",
      "Epoch[26/10000] Train: DISC | LossD: 0.708, LossG: 2.681 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.385 | U_fpR: 0.320\n",
      "Epoch[27/10000] Train: GEN | Loss D: 0.708, Loss G: 2.675 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.380 | U_fpR: 0.300\n",
      "Epoch[28/10000] Train: GEN | Loss D: 0.708, Loss G: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.365 | U_fpR: 0.300\n",
      "\n",
      "Switching to Dynamic Training\n",
      "\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[29/10000] Train: DISC | LossD: 0.701, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.365\n",
      "Epoch[30/10000] Train: DISC | LossD: 0.673, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.330 | U_fpR: 0.295\n",
      "Epoch[31/10000] Train: DISC | LossD: 0.722, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.375 | U_fpR: 0.335\n",
      "Epoch[32/10000] Train: DISC | LossD: 0.715, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.365 | U_fpR: 0.390\n",
      "Epoch[33/10000] Train: DISC | LossD: 0.714, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.350\n",
      "Epoch[34/10000] Train: DISC | LossD: 0.722, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.330 | U_fpR: 0.350\n",
      "Epoch[35/10000] Train: DISC | LossD: 0.717, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.270\n",
      "Epoch[36/10000] Train: DISC | LossD: 0.716, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.340\n",
      "Epoch[37/10000] Train: DISC | LossD: 0.731, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.325\n",
      "Epoch[38/10000] Train: DISC | LossD: 0.715, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.300 | U_fpR: 0.275\n",
      "Epoch[39/10000] Train: DISC | LossD: 0.696, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.330 | U_fpR: 0.305\n",
      "Epoch[40/10000] Train: DISC | LossD: 0.704, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.275 | U_fpR: 0.320\n",
      "Epoch[41/10000] Train: DISC | LossD: 0.695, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.395 | U_fpR: 0.375\n",
      "Epoch[42/10000] Train: DISC | LossD: 0.717, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.305 | U_fpR: 0.330\n",
      "Epoch[43/10000] Train: DISC | LossD: 0.686, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.385\n",
      "Epoch[44/10000] Train: DISC | LossD: 0.696, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.310 | U_fpR: 0.315\n",
      "Epoch[45/10000] Train: DISC | LossD: 0.715, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.295\n",
      "Epoch[46/10000] Train: DISC | LossD: 0.686, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.375 | U_fpR: 0.335\n",
      "Epoch[47/10000] Train: DISC | LossD: 0.700, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.265 | U_fpR: 0.345\n",
      "Epoch[48/10000] Train: DISC | LossD: 0.697, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.330\n",
      "Epoch[49/10000] Train: DISC | LossD: 0.684, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.310\n",
      "Epoch[50/10000] Train: DISC | LossD: 0.693, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.280 | U_fpR: 0.310\n",
      "Epoch[51/10000] Train: DISC | LossD: 0.706, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.325\n",
      "Epoch[52/10000] Train: DISC | LossD: 0.685, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.310\n",
      "Epoch[53/10000] Train: DISC | LossD: 0.700, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.330\n",
      "Epoch[54/10000] Train: DISC | LossD: 0.684, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.350\n",
      "Epoch[55/10000] Train: DISC | LossD: 0.705, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.300\n",
      "Epoch[56/10000] Train: DISC | LossD: 0.691, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.295\n",
      "Epoch[57/10000] Train: DISC | LossD: 0.702, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.345\n",
      "Epoch[58/10000] Train: DISC | LossD: 0.690, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.360\n",
      "Epoch[59/10000] Train: DISC | LossD: 0.703, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.270 | U_fpR: 0.390\n",
      "Epoch[60/10000] Train: DISC | LossD: 0.707, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.320\n",
      "Epoch[61/10000] Train: DISC | LossD: 0.700, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.370\n",
      "Epoch[62/10000] Train: DISC | LossD: 0.708, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.350 | U_fpR: 0.380\n",
      "Epoch[63/10000] Train: DISC | LossD: 0.692, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.345\n",
      "Epoch[64/10000] Train: DISC | LossD: 0.691, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.305 | U_fpR: 0.345\n",
      "Epoch[65/10000] Train: DISC | LossD: 0.693, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.270 | U_fpR: 0.340\n",
      "Epoch[66/10000] Train: DISC | LossD: 0.679, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[67/10000] Train: DISC | LossD: 0.669, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.270 | U_fpR: 0.330\n",
      "Epoch[68/10000] Train: DISC | LossD: 0.692, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.360\n",
      "Epoch[69/10000] Train: DISC | LossD: 0.695, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.375 | U_fpR: 0.325\n",
      "Epoch[70/10000] Train: DISC | LossD: 0.678, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.280 | U_fpR: 0.335\n",
      "Epoch[71/10000] Train: DISC | LossD: 0.687, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.310 | U_fpR: 0.315\n",
      "Epoch[72/10000] Train: DISC | LossD: 0.692, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.265 | U_fpR: 0.305\n",
      "Epoch[73/10000] Train: DISC | LossD: 0.667, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.345\n",
      "Epoch[74/10000] Train: DISC | LossD: 0.712, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.315\n",
      "Epoch[75/10000] Train: DISC | LossD: 0.668, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.385 | U_fpR: 0.320\n",
      "Epoch[76/10000] Train: DISC | LossD: 0.700, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.345\n",
      "Epoch[77/10000] Train: DISC | LossD: 0.676, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.310 | U_fpR: 0.340\n",
      "Epoch[78/10000] Train: DISC | LossD: 0.694, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.280 | U_fpR: 0.335\n",
      "Epoch[79/10000] Train: DISC | LossD: 0.666, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.405\n",
      "Epoch[80/10000] Train: DISC | LossD: 0.690, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.365 | U_fpR: 0.390\n",
      "Epoch[81/10000] Train: DISC | LossD: 0.670, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.345\n",
      "Epoch[82/10000] Train: DISC | LossD: 0.684, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.265 | U_fpR: 0.310\n",
      "Epoch[83/10000] Train: DISC | LossD: 0.695, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.330 | U_fpR: 0.350\n",
      "Epoch[84/10000] Train: DISC | LossD: 0.662, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.355\n",
      "Epoch[85/10000] Train: DISC | LossD: 0.673, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.285 | U_fpR: 0.335\n",
      "Epoch[86/10000] Train: DISC | LossD: 0.691, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.340\n",
      "Epoch[87/10000] Train: DISC | LossD: 0.692, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.290\n",
      "Epoch[88/10000] Train: DISC | LossD: 0.679, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.280\n",
      "Epoch[89/10000] Train: DISC | LossD: 0.675, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.340\n",
      "Epoch[90/10000] Train: DISC | LossD: 0.678, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.280\n",
      "Epoch[91/10000] Train: DISC | LossD: 0.702, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.335\n",
      "Epoch[92/10000] Train: DISC | LossD: 0.681, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.300\n",
      "Epoch[93/10000] Train: DISC | LossD: 0.677, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.305 | U_fpR: 0.300\n",
      "Epoch[94/10000] Train: DISC | LossD: 0.678, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.340\n",
      "Epoch[95/10000] Train: DISC | LossD: 0.678, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.375 | U_fpR: 0.335\n",
      "Epoch[96/10000] Train: DISC | LossD: 0.669, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.290 | U_fpR: 0.275\n",
      "Epoch[97/10000] Train: DISC | LossD: 0.682, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.355\n",
      "Epoch[98/10000] Train: DISC | LossD: 0.643, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.310\n",
      "Epoch[99/10000] Train: DISC | LossD: 0.683, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.285\n",
      "Epoch[100/10000] Train: DISC | LossD: 0.671, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.380\n",
      "Epoch[101/10000] Train: DISC | LossD: 0.690, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.320\n",
      "Epoch[102/10000] Train: DISC | LossD: 0.665, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.350 | U_fpR: 0.380\n",
      "Epoch[103/10000] Train: DISC | LossD: 0.677, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.360\n",
      "Epoch[104/10000] Train: DISC | LossD: 0.680, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.365 | U_fpR: 0.330\n",
      "Epoch[105/10000] Train: DISC | LossD: 0.662, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.375 | U_fpR: 0.335\n",
      "Epoch[106/10000] Train: DISC | LossD: 0.684, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.360\n",
      "Epoch[107/10000] Train: DISC | LossD: 0.671, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.350\n",
      "Epoch[108/10000] Train: DISC | LossD: 0.666, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.365 | U_fpR: 0.355\n",
      "Epoch[109/10000] Train: DISC | LossD: 0.668, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.320\n",
      "Epoch[110/10000] Train: DISC | LossD: 0.672, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.360\n",
      "Epoch[111/10000] Train: DISC | LossD: 0.685, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.360\n",
      "Epoch[112/10000] Train: DISC | LossD: 0.676, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.360\n",
      "Epoch[113/10000] Train: DISC | LossD: 0.671, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.325\n",
      "Epoch[114/10000] Train: DISC | LossD: 0.666, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.270\n",
      "Epoch[115/10000] Train: DISC | LossD: 0.677, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.300 | U_fpR: 0.350\n",
      "Epoch[116/10000] Train: DISC | LossD: 0.666, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.330 | U_fpR: 0.355\n",
      "Epoch[117/10000] Train: DISC | LossD: 0.684, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.290\n",
      "Epoch[118/10000] Train: DISC | LossD: 0.653, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.365 | U_fpR: 0.315\n",
      "Epoch[119/10000] Train: DISC | LossD: 0.666, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.315\n",
      "Epoch[120/10000] Train: DISC | LossD: 0.679, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.405\n",
      "Epoch[121/10000] Train: DISC | LossD: 0.669, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.330\n",
      "Epoch[122/10000] Train: DISC | LossD: 0.657, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.375\n",
      "Epoch[123/10000] Train: DISC | LossD: 0.668, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.290 | U_fpR: 0.330\n",
      "Epoch[124/10000] Train: DISC | LossD: 0.671, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.320\n",
      "Epoch[125/10000] Train: DISC | LossD: 0.664, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.350\n",
      "Epoch[126/10000] Train: DISC | LossD: 0.660, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.325\n",
      "Epoch[127/10000] Train: DISC | LossD: 0.644, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.345\n",
      "Epoch[128/10000] Train: DISC | LossD: 0.667, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.380 | U_fpR: 0.320\n",
      "Epoch[129/10000] Train: DISC | LossD: 0.660, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.295\n",
      "Epoch[130/10000] Train: DISC | LossD: 0.633, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.380 | U_fpR: 0.345\n",
      "Epoch[131/10000] Train: DISC | LossD: 0.656, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.285 | U_fpR: 0.340\n",
      "Epoch[132/10000] Train: DISC | LossD: 0.651, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[133/10000] Train: DISC | LossD: 0.679, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.315\n",
      "Epoch[134/10000] Train: DISC | LossD: 0.666, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.330 | U_fpR: 0.345\n",
      "Epoch[135/10000] Train: DISC | LossD: 0.660, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.305 | U_fpR: 0.330\n",
      "Epoch[136/10000] Train: DISC | LossD: 0.653, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.345\n",
      "Epoch[137/10000] Train: DISC | LossD: 0.685, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.370\n",
      "Epoch[138/10000] Train: DISC | LossD: 0.647, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.340\n",
      "Epoch[139/10000] Train: DISC | LossD: 0.667, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.325 | U_fpR: 0.335\n",
      "Epoch[140/10000] Train: DISC | LossD: 0.668, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.360\n",
      "Epoch[141/10000] Train: DISC | LossD: 0.656, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.370 | U_fpR: 0.300\n",
      "Epoch[142/10000] Train: DISC | LossD: 0.661, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.310 | U_fpR: 0.310\n",
      "Epoch[143/10000] Train: DISC | LossD: 0.660, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.325\n",
      "Epoch[144/10000] Train: DISC | LossD: 0.655, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.370 | U_fpR: 0.250\n",
      "Epoch[145/10000] Train: DISC | LossD: 0.654, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.375\n",
      "Epoch[146/10000] Train: DISC | LossD: 0.662, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.320\n",
      "Epoch[147/10000] Train: DISC | LossD: 0.661, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.315 | U_fpR: 0.325\n",
      "Epoch[148/10000] Train: DISC | LossD: 0.665, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.270 | U_fpR: 0.335\n",
      "Epoch[149/10000] Train: DISC | LossD: 0.649, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.310 | U_fpR: 0.360\n",
      "Epoch[150/10000] Train: DISC | LossD: 0.654, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.305 | U_fpR: 0.315\n",
      "Epoch[151/10000] Train: DISC | LossD: 0.659, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.315\n",
      "Epoch[152/10000] Train: DISC | LossD: 0.658, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.345\n",
      "Epoch[153/10000] Train: DISC | LossD: 0.650, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.320 | U_fpR: 0.435\n",
      "Epoch[154/10000] Train: DISC | LossD: 0.655, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.265 | U_fpR: 0.290\n",
      "Epoch[155/10000] Train: DISC | LossD: 0.648, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.385 | U_fpR: 0.430\n",
      "Epoch[156/10000] Train: DISC | LossD: 0.654, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.340\n",
      "Epoch[157/10000] Train: DISC | LossD: 0.647, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.345 | U_fpR: 0.350\n",
      "Epoch[158/10000] Train: DISC | LossD: 0.641, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.335 | U_fpR: 0.390\n",
      "Epoch[159/10000] Train: DISC | LossD: 0.656, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.295 | U_fpR: 0.315\n",
      "Epoch[160/10000] Train: DISC | LossD: 0.641, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.360 | U_fpR: 0.265\n",
      "Epoch[161/10000] Train: DISC | LossD: 0.665, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.350 | U_fpR: 0.325\n",
      "Epoch[162/10000] Train: DISC | LossD: 0.646, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.340 | U_fpR: 0.320\n",
      "Epoch[163/10000] Train: DISC | LossD: 0.651, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.355 | U_fpR: 0.275\n",
      "Epoch[164/10000] Train: DISC | LossD: 0.647, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.330 | U_fpR: 0.380\n",
      "Epoch[165/10000] Train: DISC | LossD: 0.632, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.350 | U_fpR: 0.315\n",
      "Epoch[166/10000] Train: DISC | LossD: 0.644, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.420 | U_fpR: 0.335\n",
      "Epoch[167/10000] Train: DISC | LossD: 0.632, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.350 | U_fpR: 0.325\n",
      "Epoch[168/10000] Train: DISC | LossD: 0.639, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.400 | U_fpR: 0.385\n",
      "Epoch[169/10000] Train: DISC | LossD: 0.637, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.380 | U_fpR: 0.380\n",
      "Epoch[170/10000] Train: DISC | LossD: 0.644, LossG: 2.708 | Acc: 0.500 | fpR: 1.000 | R: 1.000 | A_fpR: 0.385 | U_fpR: 0.265\n",
      "Epoch[171/10000] Train: DISC "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c01488dd1b8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0muser_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved_models/5000Epochs_0.01LR_MutualExclusiveLabelClassifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"TEST_cGAN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_lr\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant_train_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m#plot_metrics(model_output, True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-73536174680e>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(X, y, act, usr, criterion1, criterion2, gan_id, dig, feature_dim, GENERATOR, DISCRIMINATOR, train_string, disc, z_dim, a_dim, u_dim, gen_lr, disc_lr, batch_size, constant_train_flag, disc_epochs, gen_epochs, epoch_threshold, disc_epochs_change, gen_epochs_change, rel_epochs, static_threshold, static_disc_epochs, static_gen_epochs, pull_threshold, push_threshold, recall_threshold, print_batches, n_epochs, rows, heading, table, switch_flag, switch_count, last_real_features)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mdisc_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mopt_disc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperformance_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreal_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                 \u001b[0mA_fpR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperformance_stats_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"act\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mU_fpR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperformance_stats_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"usr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-bbd30456bbcf>\u001b[0m in \u001b[0;36mperformance_stats\u001b[1;34m(gen, disc, b_size, z_dim, a_dim, u_dim, batch)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[0mfn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[0mtn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#X, y = start_data(\"aggregated_data/aggregated_data.csv\", \"label:SITTING\")\n",
    "X, y = start_data(\"raw_data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv\", \"label:SITTING\") \n",
    "\n",
    "activity_classifier = Activity_Classifier()\n",
    "user_classifier = User_Classifier()\n",
    "activity_classifier.to(device)\n",
    "user_classifier.to(device)\n",
    "\n",
    "activity_classifier.load_state_dict(torch.load('saved_models/2000Epochs_0.01LR_UserClassifier'), strict = False)\n",
    "user_classifier.load_state_dict(torch.load(\"saved_models/5000Epochs_0.01LR_MutualExclusiveLabelClassifier\"), strict = False)\n",
    "\n",
    "model_output = training_loop(X,y, activity_classifier, user_classifier, gan_id=\"TEST_cGAN\", batch_size = 200, gen_lr=.00001, disc_lr =.00001, n_epochs=10000, dig=3, constant_train_flag=False, print_batches = False)\n",
    "#plot_metrics(model_output, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90371cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fcec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
