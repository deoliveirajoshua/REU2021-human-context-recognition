{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from prettytable import PrettyTable\n",
    "from pylab import *\n",
    "from scipy.stats import wasserstein_distance\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>raw_acc:magnitude_stats:time_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>label:STAIRS_-_GOING_DOWN</th>\n",
       "      <th>label:ELEVATOR</th>\n",
       "      <th>label:OR_standing</th>\n",
       "      <th>label:AT_SCHOOL</th>\n",
       "      <th>label:PHONE_IN_HAND</th>\n",
       "      <th>label:PHONE_IN_BAG</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "      <th>label:WITH_CO-WORKERS</th>\n",
       "      <th>label:WITH_FRIENDS</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1449601597</td>\n",
       "      <td>1.000371</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>-0.016173</td>\n",
       "      <td>0.027860</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>1.000739</td>\n",
       "      <td>1.003265</td>\n",
       "      <td>0.891038</td>\n",
       "      <td>6.684582</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1449601657</td>\n",
       "      <td>1.000243</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.998463</td>\n",
       "      <td>1.000373</td>\n",
       "      <td>1.002088</td>\n",
       "      <td>1.647929</td>\n",
       "      <td>6.684605</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1449601717</td>\n",
       "      <td>1.000811</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>1.000928</td>\n",
       "      <td>1.002032</td>\n",
       "      <td>1.960286</td>\n",
       "      <td>6.684610</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1449601777</td>\n",
       "      <td>1.001245</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.999188</td>\n",
       "      <td>1.001425</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>1.614524</td>\n",
       "      <td>6.684601</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1449601855</td>\n",
       "      <td>1.001354</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>-0.096520</td>\n",
       "      <td>0.165298</td>\n",
       "      <td>1.000807</td>\n",
       "      <td>1.002259</td>\n",
       "      <td>1.003631</td>\n",
       "      <td>0.837790</td>\n",
       "      <td>6.682252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0  1449601597                      1.000371                     0.007671   \n",
       "1  1449601657                      1.000243                     0.003782   \n",
       "2  1449601717                      1.000811                     0.002082   \n",
       "3  1449601777                      1.001245                     0.004715   \n",
       "4  1449601855                      1.001354                     0.065186   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.016173                         0.027860   \n",
       "1                        -0.002713                         0.007046   \n",
       "2                        -0.001922                         0.003575   \n",
       "3                        -0.002895                         0.008881   \n",
       "4                        -0.096520                         0.165298   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              0.998221                              1.000739   \n",
       "1                              0.998463                              1.000373   \n",
       "2                              0.999653                              1.000928   \n",
       "3                              0.999188                              1.001425   \n",
       "4                              1.000807                              1.002259   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              1.003265   \n",
       "1                              1.002088   \n",
       "2                              1.002032   \n",
       "3                              1.003500   \n",
       "4                              1.003631   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  \\\n",
       "0                               0.891038   \n",
       "1                               1.647929   \n",
       "2                               1.960286   \n",
       "3                               1.614524   \n",
       "4                               0.837790   \n",
       "\n",
       "   raw_acc:magnitude_stats:time_entropy  ...  label:STAIRS_-_GOING_DOWN  \\\n",
       "0                              6.684582  ...                        NaN   \n",
       "1                              6.684605  ...                        NaN   \n",
       "2                              6.684610  ...                        NaN   \n",
       "3                              6.684601  ...                        NaN   \n",
       "4                              6.682252  ...                        0.0   \n",
       "\n",
       "   label:ELEVATOR  label:OR_standing  label:AT_SCHOOL  label:PHONE_IN_HAND  \\\n",
       "0             NaN                NaN              NaN                  NaN   \n",
       "1             NaN                NaN              NaN                  NaN   \n",
       "2             NaN                NaN              NaN                  NaN   \n",
       "3             NaN                NaN              NaN                  NaN   \n",
       "4             NaN                0.0              1.0                  NaN   \n",
       "\n",
       "   label:PHONE_IN_BAG  label:PHONE_ON_TABLE  label:WITH_CO-WORKERS  \\\n",
       "0                 NaN                   NaN                    NaN   \n",
       "1                 NaN                   NaN                    NaN   \n",
       "2                 NaN                   NaN                    NaN   \n",
       "3                 NaN                   NaN                    NaN   \n",
       "4                 NaN                   NaN                    NaN   \n",
       "\n",
       "   label:WITH_FRIENDS  label_source  \n",
       "0                 NaN            -1  \n",
       "1                 NaN            -1  \n",
       "2                 NaN            -1  \n",
       "3                 NaN            -1  \n",
       "4                 0.0             2  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Change the data file directory below appropriately\n",
    "data = pd.read_csv('../raw_data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv')\n",
    "#data = pd.read_csv('../aggregated_data/aggregated_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolating columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data and loading it into a PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253 2253\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,1:27]\n",
    "y = data[['label:SITTING']]\n",
    "\n",
    "X = X[y['label:SITTING'] == 1]\n",
    "y = y[y['label:SITTING'] == 1]\n",
    "\n",
    "X = interpolation(X).values\n",
    "y = interpolation(y).values\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "             generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 40),\n",
    "            generator_block(40, 28),\n",
    "            nn.Linear(28, feature_dim)\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def discriminator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim = 26, hidden_dim = 16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            discriminator_block(feature_dim, hidden_dim),\n",
    "            discriminator_block(hidden_dim, int(hidden_dim/2)),\n",
    "            discriminator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            nn.Linear(int(hidden_dim/4), 1),\n",
    "            nn.Sigmoid()                    \n",
    "        )\n",
    "    def forward(self, feature_vector):\n",
    "        return self.disc(feature_vector)\n",
    "\n",
    "def get_disc_loss(gen, disc, criterion, real_features, batch_size, z_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    pred_fake = disc(fake_features.detach())\n",
    "    \n",
    "    ground_truth = torch.zeros_like(pred_fake)\n",
    "    loss_fake = criterion(pred_fake, ground_truth)\n",
    "    \n",
    "    pred_real = disc(real_features)\n",
    "    ground_truth = torch.ones_like(pred_real)\n",
    "    loss_real = criterion(pred_real, ground_truth)\n",
    "    \n",
    "    disc_loss = (loss_fake + loss_real) / 2\n",
    "    return disc_loss\n",
    "\n",
    "def get_gen_loss(gen, disc, criterion, batch_size, z_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    pred = disc(fake_features)\n",
    "    gen_loss = criterion(pred, torch.ones_like(pred))\n",
    "    return gen_loss\n",
    "\n",
    "def visualize_gen_batch(gen, b_size, epochs = -1):\n",
    "    #print(str(b_size))\n",
    "    latent_vectors = get_noise(b_size, z_dim)\n",
    "    #print(latent_vectors.shape)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    #print(fake_features.shape)\n",
    "    \n",
    "    w_img = fake_features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Generated Batch at Epoch ' + str(epochs), fontweight =\"bold\")\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_real_batch(features):\n",
    "    w_img = features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Real Batch of Data', fontweight =\"bold\")\n",
    "    plt.show()\n",
    "    \n",
    "def performance_stats(gen, disc, b_size, batch = None):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if batch is None:\n",
    "            latent_vectors = get_noise(b_size, z_dim)\n",
    "            fake_features = gen(latent_vectors)\n",
    "            y_hat = torch.round(disc(fake_features))\n",
    "            y_label = [0] * b_size\n",
    "            y_label = torch.Tensor(y_label)\n",
    "        else:\n",
    "            latent_vectors = get_noise(int(b_size/2), z_dim)\n",
    "            fake_features = gen(latent_vectors)\n",
    "            y_hat = torch.round(disc(fake_features))\n",
    "            y_label = [0] * int(b_size/2)\n",
    "            \n",
    "            real_y_hat = torch.round(disc(batch[:int(b_size/2)]))\n",
    "            for i in range(0, int(b_size/2)):\n",
    "                y_label.append(1)\n",
    "            y_hat = torch.cat((y_hat, real_y_hat), dim = 0)\n",
    "            \n",
    "            #print(y_hat)\n",
    "            #print(y_label)\n",
    "         \n",
    "        \n",
    "        for k in range(len(y_hat)):\n",
    "            #True positive\n",
    "            if y_label[k] == 1 and y_hat[k] == 1:\n",
    "                tp += 1\n",
    "            #False Negative\n",
    "            elif y_label[k] == 1 and y_hat[k] == 0:\n",
    "                fn += 1\n",
    "            #True Negative\n",
    "            elif y_label[k] == 0 and y_hat[k] == 0:\n",
    "                tn += 1\n",
    "            elif y_label[k] == 0 and y_hat[k] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "                exit()\n",
    "            \n",
    "        class_acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "        \n",
    "        if tp + fp == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            \n",
    "        if tp + fn == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "            \n",
    "        if fp + tn == 0:\n",
    "            fpR = 0\n",
    "        else: \n",
    "            fpR = fp / (fp + tn)\n",
    "\n",
    "        #print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "        #print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "        #print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "        #print(f'F-1 Score: {2*(precision * recall / (precision + recall + 0.001)):.2f}')\n",
    "        return class_acc, precision, recall, fpR, 2*(precision * recall / (precision + recall + 0.0001))\n",
    "    \n",
    "    \n",
    "def density_curves(reals, fakes):\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    subplot(2, 2, 1)\n",
    "    sns.kdeplot(fakes.numpy()[:,0], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,0], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 2)\n",
    "    sns.kdeplot(fakes.numpy()[:,18], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,18], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean X-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 3)\n",
    "    sns.kdeplot(fakes.numpy()[:,19], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,19], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Y-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 4)\n",
    "    sns.kdeplot(fakes.numpy()[:,20], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,20], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Z-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def get_fake_samples(gen, batch_size, z_dim):\n",
    "    \"\"\"\n",
    "    Generates fake acceleration features given a batch size, latent vector dimension, and trained generator.\n",
    "    \n",
    "    \"\"\"\n",
    "    latent_vectors = get_noise(batch_size, z_dim) ### Retrieves a 2D tensor of noise\n",
    "    fake_features = gen(latent_vectors)\n",
    "    \n",
    "    return fake_features ### Returns a 2D tensor of fake features of size batch_size x z_dim\n",
    "\n",
    "def all_Wasserstein_dists(gen, z_dim, feature_dim, sample):\n",
    "    wasser_dim = []\n",
    "    latent_vectors = get_noise(len(sample), z_dim)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    for k in range(feature_dim):\n",
    "        wasser_dim.append(wasserstein_distance(fake_features[:, k].detach().numpy(), sample[:, k].detach().numpy()))\n",
    "    return torch.tensor(wasser_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters (Always Run Again Before Starting Training Loop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss function for model\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#GAN Name (used for saving model and its output)\n",
    "gan_id = \"20k_5_2_constant_GAN\"\n",
    "\n",
    "# Digit Precision for printouts\n",
    "dig = 5\n",
    "\n",
    "# Max epochs to run\n",
    "n_epochs = 20000\n",
    "\n",
    "# Number of dimensions of output from generator\n",
    "feature_dim = 26\n",
    "\n",
    "# Number of dimensions of latent vector\n",
    "z_dim = 100\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "\n",
    "# Learning Rates for Generator (Gen) and Discriminator (Disc)\n",
    "gen_lr =  0.0001\n",
    "disc_lr = 0.0001\n",
    "\n",
    "# Constant epochs approach to train Discriminator, Generator\n",
    "constant_train_flag = True # Set to true to train based on constant # of epochs per machine \n",
    "                            # Set to false to train dynamically based on machine performance\n",
    "disc_epochs = 5             # Number of consecutive epochs to train discriminator before epoch threshold\n",
    "gen_epochs = 2              # Number of consecutive epochs to train generator before epoch threshold\n",
    "epoch_threshold = 50        # Epoch number to change training epoch ratio\n",
    "disc_epochs_change = 5      # New number of consecutive epochs to train discriminator\n",
    "gen_epochs_change = 2      # New number of consecutive epochs to train generator\n",
    "rel_epochs = 0              # Epochs passed since last switch (always set to 0)\n",
    "\n",
    "\n",
    "# Dynamic number of epochs to train Discriminator, Generator\n",
    "static_threshold = 77   # Epoch number to change from static ratio to dynamic\n",
    "static_disc_epochs = 5  # Number of consecutive epochs to train discriminator before epoch threshold\n",
    "static_gen_epochs = 2   # Number of consecutive epochs to train generator before epoch threshold\n",
    "pull_threshold = 0.2    # Accuracy threshold for switching machine training when the generator is no longer competitive\n",
    "push_threshold = 0.8    # Accuracy threshold for switching machine training when the discriminator is no longer competitive\n",
    "recall_threshold = 0.80\n",
    "\n",
    "# Which machine to train (0 for Generator, 1 for Discriminator) !!!(do not change unless for good reason)!!!\n",
    "GENERATOR = 0\n",
    "DISCRIMINATOR = 1\n",
    "to_train = DISCRIMINATOR\n",
    "train_string = \"DISC\"\n",
    "\n",
    "# Show model performance per batch (will always show summary for each epoch)\n",
    "print_batches = False\n",
    "\n",
    "# Moving corpus data into a pyTorch format !!!(do not change unless for good reason)!!!\n",
    "train_features = torch.tensor(X)\n",
    "train_labels = torch.tensor(y)\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Initializiing the Machines !!!(do not change unless for good reason)!!!\n",
    "disc = Discriminator()\n",
    "gen = Generator(z_dim)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr = disc_lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr = gen_lr)\n",
    "\n",
    "torch.save(gen.state_dict(), \"../saved_models/20k_5_2_constant_gan\")\n",
    "torch.save(disc.state_dict(), \"../saved_models/20k_5_2_constant_disc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20000] Training: DISC | Loss D: 0.73119, Loss G: 0.98274 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.54281\n",
      "Epoch [2/20000] Training: DISC | Loss D: 0.73307, Loss G: 0.96530 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.58612\n",
      "Epoch [3/20000] Training: DISC | Loss D: 0.72313, Loss G: 0.96933 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.49133\n",
      "Epoch [4/20000] Training: DISC | Loss D: 0.72340, Loss G: 0.95834 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.50525\n",
      "Epoch [5/20000] Training: DISC | Loss D: 0.72169, Loss G: 0.95163 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.52981\n",
      "Epoch [6/20000] Training: GEN | Loss D: 0.72340, Loss G: 0.96332 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.48440\n",
      "Epoch [7/20000] Training: GEN | Loss D: 0.73338, Loss G: 0.95598 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.53978\n",
      "Epoch [8/20000] Training: DISC | Loss D: 0.72509, Loss G: 0.94439 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.51744\n",
      "Epoch [9/20000] Training: DISC | Loss D: 0.72374, Loss G: 0.94308 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.69457\n",
      "Epoch [10/20000] Training: DISC | Loss D: 0.71694, Loss G: 0.94757 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.45206\n",
      "Epoch [11/20000] Training: DISC | Loss D: 0.71745, Loss G: 0.93766 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43404\n",
      "Epoch [12/20000] Training: DISC | Loss D: 0.71099, Loss G: 0.93305 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.50837\n",
      "Epoch [13/20000] Training: GEN | Loss D: 0.71408, Loss G: 0.92918 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.52327\n",
      "Epoch [14/20000] Training: GEN | Loss D: 0.71291, Loss G: 0.93056 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.48054\n",
      "Epoch [15/20000] Training: DISC | Loss D: 0.71869, Loss G: 0.92753 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.64479\n",
      "Epoch [16/20000] Training: DISC | Loss D: 0.71516, Loss G: 0.92302 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.52102\n",
      "Epoch [17/20000] Training: DISC | Loss D: 0.71108, Loss G: 0.92002 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.44458\n",
      "Epoch [18/20000] Training: DISC | Loss D: 0.70812, Loss G: 0.92057 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.45308\n",
      "Epoch [19/20000] Training: DISC | Loss D: 0.70348, Loss G: 0.91782 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43978\n",
      "Epoch [20/20000] Training: GEN | Loss D: 0.70669, Loss G: 0.90959 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.47891\n",
      "Epoch [21/20000] Training: GEN | Loss D: 0.70728, Loss G: 0.90479 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43966\n",
      "Epoch [22/20000] Training: DISC | Loss D: 0.70648, Loss G: 0.90084 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.42881\n",
      "Epoch [23/20000] Training: DISC | Loss D: 0.70405, Loss G: 0.90262 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.56919\n",
      "Epoch [24/20000] Training: DISC | Loss D: 0.70527, Loss G: 0.90088 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.64593\n",
      "Epoch [25/20000] Training: DISC | Loss D: 0.69726, Loss G: 0.90250 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.49925\n",
      "Epoch [26/20000] Training: DISC | Loss D: 0.69942, Loss G: 0.89650 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43922\n",
      "Epoch [27/20000] Training: GEN | Loss D: 0.69883, Loss G: 0.89223 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.46449\n",
      "Epoch [28/20000] Training: GEN | Loss D: 0.70219, Loss G: 0.89035 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.39141\n",
      "Epoch [29/20000] Training: DISC | Loss D: 0.69986, Loss G: 0.88192 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.42437\n",
      "Epoch [30/20000] Training: DISC | Loss D: 0.69761, Loss G: 0.89028 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.55121\n",
      "Epoch [31/20000] Training: DISC | Loss D: 0.70233, Loss G: 0.89382 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.57378\n",
      "Epoch [32/20000] Training: DISC | Loss D: 0.69412, Loss G: 0.88995 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.42880\n",
      "Epoch [33/20000] Training: DISC | Loss D: 0.69149, Loss G: 0.89250 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43062\n",
      "Epoch [34/20000] Training: GEN | Loss D: 0.69072, Loss G: 0.88107 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.40800\n",
      "Epoch [35/20000] Training: GEN | Loss D: 0.70081, Loss G: 0.86921 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.50617\n",
      "Epoch [36/20000] Training: DISC | Loss D: 0.69772, Loss G: 0.87365 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.44054\n",
      "Epoch [37/20000] Training: DISC | Loss D: 0.69552, Loss G: 0.87759 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.49996\n",
      "Epoch [38/20000] Training: DISC | Loss D: 0.68783, Loss G: 0.88215 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.45114\n",
      "Epoch [39/20000] Training: DISC | Loss D: 0.68291, Loss G: 0.88135 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.59585\n",
      "Epoch [40/20000] Training: DISC | Loss D: 0.68457, Loss G: 0.89255 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.50067\n",
      "Epoch [41/20000] Training: GEN | Loss D: 0.69068, Loss G: 0.87548 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43163\n",
      "Epoch [42/20000] Training: GEN | Loss D: 0.69083, Loss G: 0.86084 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.46848\n",
      "Epoch [43/20000] Training: DISC | Loss D: 0.69433, Loss G: 0.87327 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.48104\n",
      "Epoch [44/20000] Training: DISC | Loss D: 0.68351, Loss G: 0.88009 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43417\n",
      "Epoch [45/20000] Training: DISC | Loss D: 0.68177, Loss G: 0.88765 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.43277\n",
      "Epoch [46/20000] Training: DISC | Loss D: 0.67627, Loss G: 0.89681 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.35211\n",
      "Epoch [47/20000] Training: DISC | Loss D: 0.67098, Loss G: 0.90536 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.42945\n",
      "Epoch [48/20000] Training: GEN | Loss D: 0.67777, Loss G: 0.87377 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.45726\n",
      "Epoch [49/20000] Training: GEN | Loss D: 0.69699, Loss G: 0.85062 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.44895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/20000] Training: DISC | Loss D: 0.68092, Loss G: 0.86181 | Accuracy: 0.51923 | fpR: 0.00000 | P: 1.00000 | R: 0.03846 | F1: 0.07407 | Mean Wasserstein: 0.38685\n",
      "\n",
      "\n",
      "Training ratio of G/D switched from 0.40000 to 0.40000\n",
      "\n",
      "\n",
      "Epoch [51/20000] Training: GENERATOR | Loss D: 0.69232, Loss G: 0.84509 | Accuracy: 0.48077 | fpR: 0.03846 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.44547\n",
      "Epoch [52/20000] Training: GENERATOR | Loss D: 0.70258, Loss G: 0.84437 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.40234\n",
      "Epoch [53/20000] Training: DISC | Loss D: 0.68847, Loss G: 0.83204 | Accuracy: 0.48077 | fpR: 0.03846 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.41508\n",
      "Epoch [54/20000] Training: DISC | Loss D: 0.69688, Loss G: 0.83297 | Accuracy: 0.51923 | fpR: 0.00000 | P: 1.00000 | R: 0.03846 | F1: 0.07407 | Mean Wasserstein: 0.53960\n",
      "Epoch [55/20000] Training: DISC | Loss D: 0.69080, Loss G: 0.83633 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.32330\n",
      "Epoch [56/20000] Training: DISC | Loss D: 0.68259, Loss G: 0.83930 | Accuracy: 0.51923 | fpR: 0.00000 | P: 1.00000 | R: 0.03846 | F1: 0.07407 | Mean Wasserstein: 0.41749\n",
      "Epoch [57/20000] Training: DISC | Loss D: 0.68363, Loss G: 0.84920 | Accuracy: 0.51923 | fpR: 0.00000 | P: 1.00000 | R: 0.03846 | F1: 0.07407 | Mean Wasserstein: 0.53217\n",
      "Epoch [58/20000] Training: GEN | Loss D: 0.67978, Loss G: 0.83149 | Accuracy: 0.50000 | fpR: 0.03846 | P: 0.50000 | R: 0.03846 | F1: 0.07142 | Mean Wasserstein: 0.35731\n",
      "Epoch [59/20000] Training: GEN | Loss D: 0.68409, Loss G: 0.82415 | Accuracy: 0.48077 | fpR: 0.03846 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.52462\n",
      "Epoch [60/20000] Training: DISC | Loss D: 0.68656, Loss G: 0.81745 | Accuracy: 0.50000 | fpR: 0.00000 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.61499\n",
      "Epoch [61/20000] Training: DISC | Loss D: 0.68849, Loss G: 0.82043 | Accuracy: 0.50000 | fpR: 0.07692 | P: 0.50000 | R: 0.07692 | F1: 0.13331 | Mean Wasserstein: 0.53777\n",
      "Epoch [62/20000] Training: DISC | Loss D: 0.68757, Loss G: 0.83843 | Accuracy: 0.51923 | fpR: 0.00000 | P: 1.00000 | R: 0.03846 | F1: 0.07407 | Mean Wasserstein: 0.41190\n",
      "Epoch [63/20000] Training: DISC | Loss D: 0.67816, Loss G: 0.82952 | Accuracy: 0.53846 | fpR: 0.00000 | P: 1.00000 | R: 0.07692 | F1: 0.14284 | Mean Wasserstein: 0.38634\n",
      "Epoch [64/20000] Training: DISC | Loss D: 0.67844, Loss G: 0.85085 | Accuracy: 0.51923 | fpR: 0.00000 | P: 1.00000 | R: 0.03846 | F1: 0.07407 | Mean Wasserstein: 0.39306\n",
      "Epoch [65/20000] Training: GEN | Loss D: 0.69215, Loss G: 0.82184 | Accuracy: 0.51923 | fpR: 0.03846 | P: 0.66667 | R: 0.07692 | F1: 0.13791 | Mean Wasserstein: 0.47149\n",
      "Epoch [66/20000] Training: GEN | Loss D: 0.70102, Loss G: 0.79611 | Accuracy: 0.50000 | fpR: 0.07692 | P: 0.50000 | R: 0.07692 | F1: 0.13331 | Mean Wasserstein: 0.49066\n",
      "Epoch [67/20000] Training: DISC | Loss D: 0.69814, Loss G: 0.79884 | Accuracy: 0.53846 | fpR: 0.00000 | P: 1.00000 | R: 0.07692 | F1: 0.14284 | Mean Wasserstein: 0.40702\n",
      "Epoch [68/20000] Training: DISC | Loss D: 0.68792, Loss G: 0.82097 | Accuracy: 0.51923 | fpR: 0.03846 | P: 0.66667 | R: 0.07692 | F1: 0.13791 | Mean Wasserstein: 0.47362\n",
      "Epoch [69/20000] Training: DISC | Loss D: 0.68700, Loss G: 0.82470 | Accuracy: 0.51923 | fpR: 0.07692 | P: 0.60000 | R: 0.11538 | F1: 0.19352 | Mean Wasserstein: 0.39914\n",
      "Epoch [70/20000] Training: DISC | Loss D: 0.68550, Loss G: 0.82879 | Accuracy: 0.48077 | fpR: 0.03846 | P: 0.00000 | R: 0.00000 | F1: 0.00000 | Mean Wasserstein: 0.35779\n",
      "Epoch [71/20000] Training: DISC | Loss D: 0.67298, Loss G: 0.84807 | Accuracy: 0.51923 | fpR: 0.00000 | P: 1.00000 | R: 0.03846 | F1: 0.07407 | Mean Wasserstein: 0.33018\n",
      "Epoch [72/20000] Training: GEN | Loss D: 0.67249, Loss G: 0.81923 | Accuracy: 0.55769 | fpR: 0.03846 | P: 0.80000 | R: 0.15385 | F1: 0.25804 | Mean Wasserstein: 0.37594\n",
      "Epoch [73/20000] Training: GEN | Loss D: 0.69334, Loss G: 0.79635 | Accuracy: 0.50000 | fpR: 0.11538 | P: 0.50000 | R: 0.11538 | F1: 0.18747 | Mean Wasserstein: 0.41562\n",
      "Epoch [74/20000] Training: DISC | Loss D: 0.69203, Loss G: 0.82328 | Accuracy: 0.44231 | fpR: 0.15385 | P: 0.20000 | R: 0.03846 | F1: 0.06449 | Mean Wasserstein: 0.43138\n",
      "Epoch [75/20000] Training: DISC | Loss D: 0.68970, Loss G: 0.83118 | Accuracy: 0.48077 | fpR: 0.07692 | P: 0.33333 | R: 0.03846 | F1: 0.06895 | Mean Wasserstein: 0.37921\n",
      "Epoch [76/20000] Training: DISC | Loss D: 0.69178, Loss G: 0.84505 | Accuracy: 0.51923 | fpR: 0.07692 | P: 0.60000 | R: 0.11538 | F1: 0.19352 | Mean Wasserstein: 0.38612\n",
      "Epoch [77/20000] Training: DISC | Loss D: 0.67217, Loss G: 0.84876 | Accuracy: 0.59615 | fpR: 0.00000 | P: 1.00000 | R: 0.19231 | F1: 0.32255 | Mean Wasserstein: 0.42289\n",
      "Epoch [78/20000] Training: DISC | Loss D: 0.66526, Loss G: 0.84606 | Accuracy: 0.57692 | fpR: 0.00000 | P: 1.00000 | R: 0.15385 | F1: 0.26664 | Mean Wasserstein: 0.42219\n",
      "Epoch [79/20000] Training: GEN | Loss D: 0.67924, Loss G: 0.82814 | Accuracy: 0.55769 | fpR: 0.03846 | P: 0.80000 | R: 0.15385 | F1: 0.25804 | Mean Wasserstein: 0.40778\n",
      "Epoch [80/20000] Training: GEN | Loss D: 0.69113, Loss G: 0.80054 | Accuracy: 0.53846 | fpR: 0.03846 | P: 0.75000 | R: 0.11538 | F1: 0.19998 | Mean Wasserstein: 0.37350\n",
      "Epoch [81/20000] Training: DISC | Loss D: 0.68209, Loss G: 0.79909 | Accuracy: 0.50000 | fpR: 0.07692 | P: 0.50000 | R: 0.07692 | F1: 0.13331 | Mean Wasserstein: 0.47382\n",
      "Epoch [82/20000] Training: DISC | Loss D: 0.68464, Loss G: 0.80651 | Accuracy: 0.57692 | fpR: 0.00000 | P: 1.00000 | R: 0.15385 | F1: 0.26664 | Mean Wasserstein: 0.47784\n",
      "Epoch [83/20000] Training: DISC | Loss D: 0.67938, Loss G: 0.82117 | Accuracy: 0.53846 | fpR: 0.00000 | P: 1.00000 | R: 0.07692 | F1: 0.14284 | Mean Wasserstein: 0.40251\n",
      "Epoch [84/20000] Training: DISC | Loss D: 0.67537, Loss G: 0.84190 | Accuracy: 0.55769 | fpR: 0.11538 | P: 0.66667 | R: 0.23077 | F1: 0.34282 | Mean Wasserstein: 0.49146\n",
      "Epoch [85/20000] Training: DISC | Loss D: 0.66480, Loss G: 0.85684 | Accuracy: 0.57692 | fpR: 0.00000 | P: 1.00000 | R: 0.15385 | F1: 0.26664 | Mean Wasserstein: 0.38163\n",
      "Epoch [86/20000] Training: GEN | Loss D: 0.67379, Loss G: 0.83712 | Accuracy: 0.50000 | fpR: 0.11538 | P: 0.50000 | R: 0.11538 | F1: 0.18747 | Mean Wasserstein: 0.41947\n",
      "Epoch [87/20000] Training: GEN | Loss D: 0.69197, Loss G: 0.80203 | Accuracy: 0.48077 | fpR: 0.11538 | P: 0.40000 | R: 0.07692 | F1: 0.12901 | Mean Wasserstein: 0.42505\n",
      "Epoch [88/20000] Training: DISC | Loss D: 0.68489, Loss G: 0.79313 | Accuracy: 0.50000 | fpR: 0.11538 | P: 0.50000 | R: 0.11538 | F1: 0.18747 | Mean Wasserstein: 0.34918\n",
      "Epoch [89/20000] Training: DISC | Loss D: 0.68998, Loss G: 0.80325 | Accuracy: 0.57692 | fpR: 0.03846 | P: 0.83333 | R: 0.19231 | F1: 0.31247 | Mean Wasserstein: 0.48231\n",
      "Epoch [90/20000] Training: DISC | Loss D: 0.67955, Loss G: 0.82790 | Accuracy: 0.46154 | fpR: 0.11538 | P: 0.25000 | R: 0.03846 | F1: 0.06664 | Mean Wasserstein: 0.33501\n",
      "Epoch [91/20000] Training: DISC | Loss D: 0.68536, Loss G: 0.83724 | Accuracy: 0.53846 | fpR: 0.00000 | P: 1.00000 | R: 0.07692 | F1: 0.14284 | Mean Wasserstein: 0.35206\n",
      "Epoch [92/20000] Training: DISC | Loss D: 0.66476, Loss G: 0.84176 | Accuracy: 0.59615 | fpR: 0.11538 | P: 0.72727 | R: 0.30769 | F1: 0.43239 | Mean Wasserstein: 0.47485\n",
      "Epoch [93/20000] Training: GEN | Loss D: 0.67445, Loss G: 0.82485 | Accuracy: 0.53846 | fpR: 0.07692 | P: 0.66667 | R: 0.15385 | F1: 0.24997 | Mean Wasserstein: 0.45136\n",
      "Epoch [94/20000] Training: GEN | Loss D: 0.68403, Loss G: 0.78037 | Accuracy: 0.57692 | fpR: 0.07692 | P: 0.75000 | R: 0.23077 | F1: 0.35291 | Mean Wasserstein: 0.38426\n",
      "Epoch [95/20000] Training: DISC | Loss D: 0.68367, Loss G: 0.80038 | Accuracy: 0.51923 | fpR: 0.11538 | P: 0.57143 | R: 0.15385 | F1: 0.24239 | Mean Wasserstein: 0.41892\n",
      "Epoch [96/20000] Training: DISC | Loss D: 0.68151, Loss G: 0.78716 | Accuracy: 0.55769 | fpR: 0.15385 | P: 0.63636 | R: 0.26923 | F1: 0.37834 | Mean Wasserstein: 0.52998\n",
      "Epoch [97/20000] Training: DISC | Loss D: 0.66206, Loss G: 0.82266 | Accuracy: 0.61538 | fpR: 0.15385 | P: 0.71429 | R: 0.38462 | F1: 0.49995 | Mean Wasserstein: 0.46656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/20000] Training: DISC | Loss D: 0.67571, Loss G: 0.81101 | Accuracy: 0.61538 | fpR: 0.07692 | P: 0.80000 | R: 0.30769 | F1: 0.44440 | Mean Wasserstein: 0.37795\n",
      "Epoch [99/20000] Training: DISC | Loss D: 0.67182, Loss G: 0.82304 | Accuracy: 0.59615 | fpR: 0.00000 | P: 1.00000 | R: 0.19231 | F1: 0.32255 | Mean Wasserstein: 0.43631\n",
      "Epoch [100/20000] Training: GEN | Loss D: 0.68257, Loss G: 0.81857 | Accuracy: 0.57692 | fpR: 0.11538 | P: 0.70000 | R: 0.26923 | F1: 0.38885 | Mean Wasserstein: 0.46740\n",
      "Epoch [101/20000] Training: GEN | Loss D: 0.70040, Loss G: 0.78544 | Accuracy: 0.57692 | fpR: 0.19231 | P: 0.64286 | R: 0.34615 | F1: 0.44995 | Mean Wasserstein: 0.33667\n",
      "Epoch [102/20000] Training: DISC | Loss D: 0.68378, Loss G: 0.77679 | Accuracy: 0.59615 | fpR: 0.19231 | P: 0.66667 | R: 0.38462 | F1: 0.48776 | Mean Wasserstein: 0.33885\n",
      "Epoch [103/20000] Training: DISC | Loss D: 0.67083, Loss G: 0.81344 | Accuracy: 0.53846 | fpR: 0.15385 | P: 0.60000 | R: 0.23077 | F1: 0.33329 | Mean Wasserstein: 0.45183\n",
      "Epoch [104/20000] Training: DISC | Loss D: 0.67872, Loss G: 0.80308 | Accuracy: 0.61538 | fpR: 0.00000 | P: 1.00000 | R: 0.23077 | F1: 0.37497 | Mean Wasserstein: 0.41329\n",
      "Epoch [105/20000] Training: DISC | Loss D: 0.66073, Loss G: 0.82264 | Accuracy: 0.59615 | fpR: 0.03846 | P: 0.85714 | R: 0.23077 | F1: 0.36360 | Mean Wasserstein: 0.45670\n",
      "Epoch [106/20000] Training: DISC | Loss D: 0.66683, Loss G: 0.81142 | Accuracy: 0.63462 | fpR: 0.07692 | P: 0.81818 | R: 0.34615 | F1: 0.48644 | Mean Wasserstein: 0.40488\n",
      "Epoch [107/20000] Training: GEN | Loss D: 0.66310, Loss G: 0.79936 | Accuracy: 0.71154 | fpR: 0.15385 | P: 0.78947 | R: 0.57692 | F1: 0.66662 | Mean Wasserstein: 0.39943\n",
      "Epoch [108/20000] Training: GEN | Loss D: 0.67778, Loss G: 0.77643 | Accuracy: 0.59615 | fpR: 0.15385 | P: 0.69231 | R: 0.34615 | F1: 0.46149 | Mean Wasserstein: 0.38572\n",
      "Epoch [109/20000] Training: DISC | Loss D: 0.68196, Loss G: 0.77094 | Accuracy: 0.51923 | fpR: 0.23077 | P: 0.53846 | R: 0.26923 | F1: 0.35893 | Mean Wasserstein: 0.32340\n",
      "Epoch [110/20000] Training: DISC | Loss D: 0.64177, Loss G: 0.79973 | Accuracy: 0.53846 | fpR: 0.15385 | P: 0.60000 | R: 0.23077 | F1: 0.33329 | Mean Wasserstein: 0.47895\n",
      "Epoch [111/20000] Training: DISC | Loss D: 0.66288, Loss G: 0.79966 | Accuracy: 0.61538 | fpR: 0.07692 | P: 0.80000 | R: 0.30769 | F1: 0.44440 | Mean Wasserstein: 0.38794\n",
      "Epoch [112/20000] Training: DISC | Loss D: 0.64186, Loss G: 0.78882 | Accuracy: 0.69231 | fpR: 0.03846 | P: 0.91667 | R: 0.42308 | F1: 0.57890 | Mean Wasserstein: 0.44043\n",
      "Epoch [113/20000] Training: DISC | Loss D: 0.64182, Loss G: 0.81087 | Accuracy: 0.71154 | fpR: 0.07692 | P: 0.86667 | R: 0.50000 | F1: 0.63410 | Mean Wasserstein: 0.38957\n",
      "Epoch [114/20000] Training: GEN | Loss D: 0.65742, Loss G: 0.78429 | Accuracy: 0.65385 | fpR: 0.19231 | P: 0.72222 | R: 0.50000 | F1: 0.59086 | Mean Wasserstein: 0.44863\n",
      "Epoch [115/20000] Training: GEN | Loss D: 0.64319, Loss G: 0.77574 | Accuracy: 0.59615 | fpR: 0.19231 | P: 0.66667 | R: 0.38462 | F1: 0.48776 | Mean Wasserstein: 0.45981\n",
      "Epoch [116/20000] Training: DISC | Loss D: 0.65373, Loss G: 0.77900 | Accuracy: 0.67308 | fpR: 0.19231 | P: 0.73684 | R: 0.53846 | F1: 0.62217 | Mean Wasserstein: 0.38558\n",
      "Epoch [117/20000] Training: DISC | Loss D: 0.65017, Loss G: 0.78362 | Accuracy: 0.59615 | fpR: 0.23077 | P: 0.64706 | R: 0.42308 | F1: 0.51158 | Mean Wasserstein: 0.37213\n",
      "Epoch [118/20000] Training: DISC | Loss D: 0.66108, Loss G: 0.77784 | Accuracy: 0.63462 | fpR: 0.07692 | P: 0.81818 | R: 0.34615 | F1: 0.48644 | Mean Wasserstein: 0.33889\n",
      "Epoch [119/20000] Training: DISC | Loss D: 0.63995, Loss G: 0.79946 | Accuracy: 0.67308 | fpR: 0.15385 | P: 0.76471 | R: 0.50000 | F1: 0.60460 | Mean Wasserstein: 0.39893\n",
      "Epoch [120/20000] Training: DISC | Loss D: 0.62116, Loss G: 0.80692 | Accuracy: 0.59615 | fpR: 0.15385 | P: 0.69231 | R: 0.34615 | F1: 0.46149 | Mean Wasserstein: 0.41985\n",
      "Epoch [121/20000] Training: GEN | Loss D: 0.62332, Loss G: 0.78800 | Accuracy: 0.71154 | fpR: 0.11538 | P: 0.82353 | R: 0.53846 | F1: 0.65111 | Mean Wasserstein: 0.39680\n",
      "Epoch [122/20000] Training: GEN | Loss D: 0.66810, Loss G: 0.74956 | Accuracy: 0.42308 | fpR: 0.38462 | P: 0.37500 | R: 0.23077 | F1: 0.28567 | Mean Wasserstein: 0.36694\n",
      "Epoch [123/20000] Training: DISC | Loss D: 0.66620, Loss G: 0.76838 | Accuracy: 0.55769 | fpR: 0.19231 | P: 0.61538 | R: 0.30769 | F1: 0.41021 | Mean Wasserstein: 0.34078\n",
      "Epoch [124/20000] Training: DISC | Loss D: 0.62986, Loss G: 0.78995 | Accuracy: 0.57692 | fpR: 0.34615 | P: 0.59091 | R: 0.50000 | F1: 0.54162 | Mean Wasserstein: 0.44192\n",
      "Epoch [125/20000] Training: DISC | Loss D: 0.64149, Loss G: 0.77742 | Accuracy: 0.61538 | fpR: 0.15385 | P: 0.71429 | R: 0.38462 | F1: 0.49995 | Mean Wasserstein: 0.36942\n",
      "Epoch [126/20000] Training: DISC | Loss D: 0.61582, Loss G: 0.76287 | Accuracy: 0.75000 | fpR: 0.11538 | P: 0.84211 | R: 0.61538 | F1: 0.71106 | Mean Wasserstein: 0.51461\n",
      "Epoch [127/20000] Training: DISC | Loss D: 0.64891, Loss G: 0.79389 | Accuracy: 0.59615 | fpR: 0.19231 | P: 0.66667 | R: 0.38462 | F1: 0.48776 | Mean Wasserstein: 0.30944\n",
      "Epoch [128/20000] Training: GEN | Loss D: 0.64433, Loss G: 0.77867 | Accuracy: 0.61538 | fpR: 0.34615 | P: 0.62500 | R: 0.57692 | F1: 0.59995 | Mean Wasserstein: 0.40833\n",
      "Epoch [129/20000] Training: GEN | Loss D: 0.68299, Loss G: 0.73851 | Accuracy: 0.48077 | fpR: 0.38462 | P: 0.47368 | R: 0.34615 | F1: 0.39995 | Mean Wasserstein: 0.34851\n",
      "Epoch [130/20000] Training: DISC | Loss D: 0.65922, Loss G: 0.77810 | Accuracy: 0.63462 | fpR: 0.26923 | P: 0.66667 | R: 0.53846 | F1: 0.59570 | Mean Wasserstein: 0.41901\n",
      "Epoch [131/20000] Training: DISC | Loss D: 0.63762, Loss G: 0.77175 | Accuracy: 0.67308 | fpR: 0.19231 | P: 0.73684 | R: 0.53846 | F1: 0.62217 | Mean Wasserstein: 0.47453\n",
      "Epoch [132/20000] Training: DISC | Loss D: 0.65500, Loss G: 0.77313 | Accuracy: 0.67308 | fpR: 0.19231 | P: 0.73684 | R: 0.53846 | F1: 0.62217 | Mean Wasserstein: 0.32061\n",
      "Epoch [133/20000] Training: DISC | Loss D: 0.64059, Loss G: 0.80535 | Accuracy: 0.65385 | fpR: 0.23077 | P: 0.70000 | R: 0.53846 | F1: 0.60865 | Mean Wasserstein: 0.34463\n",
      "Epoch [134/20000] Training: DISC | Loss D: 0.60688, Loss G: 0.80811 | Accuracy: 0.71154 | fpR: 0.23077 | P: 0.73913 | R: 0.65385 | F1: 0.69383 | Mean Wasserstein: 0.45374\n",
      "Epoch [135/20000] Training: GEN | Loss D: 0.63021, Loss G: 0.77433 | Accuracy: 0.55769 | fpR: 0.42308 | P: 0.56000 | R: 0.53846 | F1: 0.54897 | Mean Wasserstein: 0.36423\n",
      "Epoch [136/20000] Training: GEN | Loss D: 0.69243, Loss G: 0.74280 | Accuracy: 0.61538 | fpR: 0.42308 | P: 0.60714 | R: 0.65385 | F1: 0.62958 | Mean Wasserstein: 0.28154\n",
      "Epoch [137/20000] Training: DISC | Loss D: 0.64753, Loss G: 0.72151 | Accuracy: 0.59615 | fpR: 0.46154 | P: 0.58621 | R: 0.65385 | F1: 0.61813 | Mean Wasserstein: 0.37660\n",
      "Epoch [138/20000] Training: DISC | Loss D: 0.64170, Loss G: 0.76942 | Accuracy: 0.69231 | fpR: 0.26923 | P: 0.70833 | R: 0.65385 | F1: 0.67995 | Mean Wasserstein: 0.45874\n",
      "Epoch [139/20000] Training: DISC | Loss D: 0.62518, Loss G: 0.78889 | Accuracy: 0.73077 | fpR: 0.19231 | P: 0.77273 | R: 0.65385 | F1: 0.70828 | Mean Wasserstein: 0.42492\n",
      "Epoch [140/20000] Training: DISC | Loss D: 0.63418, Loss G: 0.80570 | Accuracy: 0.69231 | fpR: 0.23077 | P: 0.72727 | R: 0.61538 | F1: 0.66662 | Mean Wasserstein: 0.46829\n",
      "Epoch [141/20000] Training: DISC | Loss D: 0.61082, Loss G: 0.82703 | Accuracy: 0.63462 | fpR: 0.30769 | P: 0.65217 | R: 0.57692 | F1: 0.61220 | Mean Wasserstein: 0.47871\n",
      "Epoch [142/20000] Training: GEN | Loss D: 0.63257, Loss G: 0.75915 | Accuracy: 0.65385 | fpR: 0.38462 | P: 0.64286 | R: 0.69231 | F1: 0.66662 | Mean Wasserstein: 0.36591\n",
      "Epoch [143/20000] Training: GEN | Loss D: 0.67267, Loss G: 0.72415 | Accuracy: 0.55769 | fpR: 0.46154 | P: 0.55556 | R: 0.57692 | F1: 0.56599 | Mean Wasserstein: 0.36425\n",
      "Epoch [144/20000] Training: DISC | Loss D: 0.66143, Loss G: 0.74222 | Accuracy: 0.59615 | fpR: 0.42308 | P: 0.59259 | R: 0.61538 | F1: 0.60372 | Mean Wasserstein: 0.41272\n",
      "Epoch [145/20000] Training: DISC | Loss D: 0.59708, Loss G: 0.76938 | Accuracy: 0.69231 | fpR: 0.30769 | P: 0.69231 | R: 0.69231 | F1: 0.69226 | Mean Wasserstein: 0.37363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [146/20000] Training: DISC | Loss D: 0.65002, Loss G: 0.76480 | Accuracy: 0.65385 | fpR: 0.23077 | P: 0.70000 | R: 0.53846 | F1: 0.60865 | Mean Wasserstein: 0.30108\n",
      "Epoch [147/20000] Training: DISC | Loss D: 0.63343, Loss G: 0.82427 | Accuracy: 0.65385 | fpR: 0.23077 | P: 0.70000 | R: 0.53846 | F1: 0.60865 | Mean Wasserstein: 0.37065\n",
      "Epoch [148/20000] Training: DISC | Loss D: 0.62322, Loss G: 0.86281 | Accuracy: 0.61538 | fpR: 0.11538 | P: 0.75000 | R: 0.34615 | F1: 0.47364 | Mean Wasserstein: 0.32154\n",
      "Epoch [149/20000] Training: GEN | Loss D: 0.61979, Loss G: 0.79511 | Accuracy: 0.80769 | fpR: 0.19231 | P: 0.80769 | R: 0.80769 | F1: 0.80764 | Mean Wasserstein: 0.37676\n",
      "Epoch [150/20000] Training: GEN | Loss D: 0.64276, Loss G: 0.73144 | Accuracy: 0.57692 | fpR: 0.46154 | P: 0.57143 | R: 0.61538 | F1: 0.59254 | Mean Wasserstein: 0.45257\n",
      "Epoch [151/20000] Training: DISC | Loss D: 0.62606, Loss G: 0.75992 | Accuracy: 0.63462 | fpR: 0.23077 | P: 0.68421 | R: 0.50000 | F1: 0.57773 | Mean Wasserstein: 0.40630\n",
      "Epoch [152/20000] Training: DISC | Loss D: 0.61901, Loss G: 0.78699 | Accuracy: 0.65385 | fpR: 0.30769 | P: 0.66667 | R: 0.61538 | F1: 0.63995 | Mean Wasserstein: 0.37341\n",
      "Epoch [153/20000] Training: DISC | Loss D: 0.60248, Loss G: 0.86249 | Accuracy: 0.67308 | fpR: 0.26923 | P: 0.69565 | R: 0.61538 | F1: 0.65301 | Mean Wasserstein: 0.41978\n",
      "Epoch [154/20000] Training: DISC | Loss D: 0.62555, Loss G: 0.88297 | Accuracy: 0.69231 | fpR: 0.11538 | P: 0.81250 | R: 0.50000 | F1: 0.61900 | Mean Wasserstein: 0.38255\n",
      "Epoch [155/20000] Training: DISC | Loss D: 0.61445, Loss G: 0.90393 | Accuracy: 0.61538 | fpR: 0.15385 | P: 0.71429 | R: 0.38462 | F1: 0.49995 | Mean Wasserstein: 0.34040\n",
      "Epoch [156/20000] Training: GEN | Loss D: 0.59942, Loss G: 0.80596 | Accuracy: 0.67308 | fpR: 0.23077 | P: 0.71429 | R: 0.57692 | F1: 0.63825 | Mean Wasserstein: 0.39482\n",
      "Epoch [157/20000] Training: GEN | Loss D: 0.63566, Loss G: 0.81083 | Accuracy: 0.59615 | fpR: 0.46154 | P: 0.58621 | R: 0.65385 | F1: 0.61813 | Mean Wasserstein: 0.39423\n",
      "Epoch [158/20000] Training: DISC | Loss D: 0.62144, Loss G: 0.78147 | Accuracy: 0.59615 | fpR: 0.38462 | P: 0.60000 | R: 0.57692 | F1: 0.58819 | Mean Wasserstein: 0.35229\n",
      "Epoch [159/20000] Training: DISC | Loss D: 0.58897, Loss G: 0.82143 | Accuracy: 0.57692 | fpR: 0.42308 | P: 0.57692 | R: 0.57692 | F1: 0.57687 | Mean Wasserstein: 0.45233\n",
      "Epoch [160/20000] Training: DISC | Loss D: 0.61638, Loss G: 0.84024 | Accuracy: 0.67308 | fpR: 0.19231 | P: 0.73684 | R: 0.53846 | F1: 0.62217 | Mean Wasserstein: 0.42905\n",
      "Epoch [161/20000] Training: DISC | Loss D: 0.58413, Loss G: 0.83049 | Accuracy: 0.65385 | fpR: 0.23077 | P: 0.70000 | R: 0.53846 | F1: 0.60865 | Mean Wasserstein: 0.35318\n",
      "Epoch [162/20000] Training: DISC | Loss D: 0.59480, Loss G: 0.86998 | Accuracy: 0.63462 | fpR: 0.23077 | P: 0.68421 | R: 0.50000 | F1: 0.57773 | Mean Wasserstein: 0.34783\n",
      "Epoch [163/20000] Training: GEN | Loss D: 0.60935, Loss G: 0.83512 | Accuracy: 0.67308 | fpR: 0.19231 | P: 0.73684 | R: 0.53846 | F1: 0.62217 | Mean Wasserstein: 0.33951\n",
      "Epoch [164/20000] Training: GEN | Loss D: 0.70358, Loss G: 0.72513 | Accuracy: 0.63462 | fpR: 0.30769 | P: 0.65217 | R: 0.57692 | F1: 0.61220 | Mean Wasserstein: 0.37356\n",
      "Epoch [165/20000] Training: DISC | Loss D: 0.67189, Loss G: 0.75910 | Accuracy: 0.61538 | fpR: 0.26923 | P: 0.65000 | R: 0.50000 | F1: 0.56517 | Mean Wasserstein: 0.37249\n",
      "Epoch [166/20000] Training: DISC | Loss D: 0.69552, Loss G: 0.82583 | Accuracy: 0.55769 | fpR: 0.19231 | P: 0.61538 | R: 0.30769 | F1: 0.41021 | Mean Wasserstein: 0.30106\n",
      "Epoch [167/20000] Training: DISC | Loss D: 0.62713, Loss G: 0.85117 | Accuracy: 0.61538 | fpR: 0.23077 | P: 0.66667 | R: 0.46154 | F1: 0.54541 | Mean Wasserstein: 0.34203\n",
      "Epoch [168/20000] Training: DISC | Loss D: 0.62559, Loss G: 0.90521 | Accuracy: 0.69231 | fpR: 0.15385 | P: 0.77778 | R: 0.53846 | F1: 0.63632 | Mean Wasserstein: 0.43833\n",
      "Epoch [169/20000] Training: DISC | Loss D: 0.59154, Loss G: 0.90306 | Accuracy: 0.67308 | fpR: 0.19231 | P: 0.73684 | R: 0.53846 | F1: 0.62217 | Mean Wasserstein: 0.37742\n",
      "Epoch [170/20000] Training: GEN | Loss D: 0.63470, Loss G: 0.77349 | Accuracy: 0.63462 | fpR: 0.26923 | P: 0.66667 | R: 0.53846 | F1: 0.59570 | Mean Wasserstein: 0.40319\n",
      "Epoch [171/20000] Training: GEN | Loss D: 0.65884, Loss G: 0.78454 | Accuracy: 0.57692 | fpR: 0.34615 | P: 0.59091 | R: 0.50000 | F1: 0.54162 | Mean Wasserstein: 0.40542\n",
      "Epoch [172/20000] Training: DISC | Loss D: 0.67221, Loss G: 0.74444 | Accuracy: 0.51923 | fpR: 0.38462 | P: 0.52381 | R: 0.42308 | F1: 0.46804 | Mean Wasserstein: 0.43350\n",
      "Epoch [173/20000] Training: DISC | Loss D: 0.64245, Loss G: 0.86368 | Accuracy: 0.53846 | fpR: 0.30769 | P: 0.55556 | R: 0.38462 | F1: 0.45450 | Mean Wasserstein: 0.34527\n",
      "Epoch [174/20000] Training: DISC | Loss D: 0.63517, Loss G: 0.87621 | Accuracy: 0.57692 | fpR: 0.23077 | P: 0.62500 | R: 0.38462 | F1: 0.47614 | Mean Wasserstein: 0.36979\n",
      "Epoch [175/20000] Training: DISC | Loss D: 0.57605, Loss G: 0.89518 | Accuracy: 0.61538 | fpR: 0.30769 | P: 0.63636 | R: 0.53846 | F1: 0.58328 | Mean Wasserstein: 0.49749\n",
      "Epoch [176/20000] Training: DISC | Loss D: 0.61773, Loss G: 0.90454 | Accuracy: 0.67308 | fpR: 0.11538 | P: 0.80000 | R: 0.46154 | F1: 0.58532 | Mean Wasserstein: 0.32840\n",
      "Epoch [177/20000] Training: GEN | Loss D: 0.61585, Loss G: 0.79854 | Accuracy: 0.65385 | fpR: 0.15385 | P: 0.75000 | R: 0.46154 | F1: 0.57138 | Mean Wasserstein: 0.39870\n",
      "Epoch [178/20000] Training: GEN | Loss D: 0.64264, Loss G: 0.80540 | Accuracy: 0.55769 | fpR: 0.30769 | P: 0.57895 | R: 0.42308 | F1: 0.48884 | Mean Wasserstein: 0.44973\n",
      "Epoch [179/20000] Training: DISC | Loss D: 0.65297, Loss G: 0.78699 | Accuracy: 0.57692 | fpR: 0.19231 | P: 0.64286 | R: 0.34615 | F1: 0.44995 | Mean Wasserstein: 0.38888\n",
      "Epoch [180/20000] Training: DISC | Loss D: 0.60623, Loss G: 0.87391 | Accuracy: 0.65385 | fpR: 0.15385 | P: 0.75000 | R: 0.46154 | F1: 0.57138 | Mean Wasserstein: 0.48393\n",
      "Epoch [181/20000] Training: DISC | Loss D: 0.59438, Loss G: 0.89315 | Accuracy: 0.65385 | fpR: 0.34615 | P: 0.65385 | R: 0.65385 | F1: 0.65380 | Mean Wasserstein: 0.42204\n",
      "Epoch [182/20000] Training: DISC | Loss D: 0.58519, Loss G: 0.89459 | Accuracy: 0.73077 | fpR: 0.07692 | P: 0.87500 | R: 0.53846 | F1: 0.66662 | Mean Wasserstein: 0.47888\n",
      "Epoch [183/20000] Training: DISC | Loss D: 0.58155, Loss G: 0.92188 | Accuracy: 0.67308 | fpR: 0.19231 | P: 0.73684 | R: 0.53846 | F1: 0.62217 | Mean Wasserstein: 0.39793\n",
      "Epoch [184/20000] Training: GEN | Loss D: 0.62991, Loss G: 0.87787 | Accuracy: 0.65385 | fpR: 0.11538 | P: 0.78571 | R: 0.42308 | F1: 0.54995 | Mean Wasserstein: 0.36164\n",
      "Epoch [185/20000] Training: GEN | Loss D: 0.61401, Loss G: 0.76804 | Accuracy: 0.76923 | fpR: 0.07692 | P: 0.88889 | R: 0.61538 | F1: 0.72722 | Mean Wasserstein: 0.48356\n",
      "Epoch [186/20000] Training: DISC | Loss D: 0.57606, Loss G: 0.84762 | Accuracy: 0.75000 | fpR: 0.11538 | P: 0.84211 | R: 0.61538 | F1: 0.71106 | Mean Wasserstein: 0.47140\n",
      "Epoch [187/20000] Training: DISC | Loss D: 0.60164, Loss G: 0.84305 | Accuracy: 0.69231 | fpR: 0.11538 | P: 0.81250 | R: 0.50000 | F1: 0.61900 | Mean Wasserstein: 0.43599\n",
      "Epoch [188/20000] Training: DISC | Loss D: 0.60299, Loss G: 0.86417 | Accuracy: 0.73077 | fpR: 0.07692 | P: 0.87500 | R: 0.53846 | F1: 0.66662 | Mean Wasserstein: 0.40697\n",
      "Epoch [189/20000] Training: DISC | Loss D: 0.57880, Loss G: 0.92518 | Accuracy: 0.71154 | fpR: 0.11538 | P: 0.82353 | R: 0.53846 | F1: 0.65111 | Mean Wasserstein: 0.33842\n",
      "Epoch [190/20000] Training: DISC | Loss D: 0.57263, Loss G: 0.87840 | Accuracy: 0.73077 | fpR: 0.11538 | P: 0.83333 | R: 0.57692 | F1: 0.68177 | Mean Wasserstein: 0.36012\n",
      "Epoch [191/20000] Training: GEN | Loss D: 0.57955, Loss G: 0.87324 | Accuracy: 0.59615 | fpR: 0.15385 | P: 0.69231 | R: 0.34615 | F1: 0.46149 | Mean Wasserstein: 0.37997\n",
      "Epoch [192/20000] Training: GEN | Loss D: 0.62808, Loss G: 0.77431 | Accuracy: 0.65385 | fpR: 0.15385 | P: 0.75000 | R: 0.46154 | F1: 0.57138 | Mean Wasserstein: 0.37646\n",
      "Epoch [193/20000] Training: DISC | Loss D: 0.58998, Loss G: 0.85696 | Accuracy: 0.73077 | fpR: 0.15385 | P: 0.80000 | R: 0.61538 | F1: 0.69560 | Mean Wasserstein: 0.40343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [194/20000] Training: DISC | Loss D: 0.58876, Loss G: 0.87844 | Accuracy: 0.63462 | fpR: 0.15385 | P: 0.73333 | R: 0.42308 | F1: 0.53654 | Mean Wasserstein: 0.43954\n",
      "Epoch [195/20000] Training: DISC | Loss D: 0.60147, Loss G: 0.90389 | Accuracy: 0.78846 | fpR: 0.03846 | P: 0.94118 | R: 0.61538 | F1: 0.74414 | Mean Wasserstein: 0.39631\n",
      "Epoch [196/20000] Training: DISC | Loss D: 0.61108, Loss G: 0.90869 | Accuracy: 0.57692 | fpR: 0.11538 | P: 0.70000 | R: 0.26923 | F1: 0.38885 | Mean Wasserstein: 0.34362\n",
      "Epoch [197/20000] Training: DISC | Loss D: 0.57280, Loss G: 0.91262 | Accuracy: 0.65385 | fpR: 0.23077 | P: 0.70000 | R: 0.53846 | F1: 0.60865 | Mean Wasserstein: 0.37779\n",
      "Epoch [198/20000] Training: GEN | Loss D: 0.60259, Loss G: 0.82631 | Accuracy: 0.69231 | fpR: 0.15385 | P: 0.77778 | R: 0.53846 | F1: 0.63632 | Mean Wasserstein: 0.37227\n",
      "Epoch [199/20000] Training: GEN | Loss D: 0.65060, Loss G: 0.73455 | Accuracy: 0.53846 | fpR: 0.46154 | P: 0.53846 | R: 0.53846 | F1: 0.53841 | Mean Wasserstein: 0.34981\n",
      "Epoch [200/20000] Training: DISC | Loss D: 0.61455, Loss G: 0.82987 | Accuracy: 0.59615 | fpR: 0.23077 | P: 0.64706 | R: 0.42308 | F1: 0.51158 | Mean Wasserstein: 0.47996\n",
      "Epoch [201/20000] Training: DISC | Loss D: 0.62580, Loss G: 0.82627 | Accuracy: 0.61538 | fpR: 0.19231 | P: 0.68750 | R: 0.42308 | F1: 0.52376 | Mean Wasserstein: 0.33790\n",
      "Epoch [202/20000] Training: DISC | Loss D: 0.56078, Loss G: 0.93327 | Accuracy: 0.67308 | fpR: 0.11538 | P: 0.80000 | R: 0.46154 | F1: 0.58532 | Mean Wasserstein: 0.35169\n",
      "Epoch [203/20000] Training: DISC | Loss D: 0.60433, Loss G: 0.88728 | Accuracy: 0.63462 | fpR: 0.15385 | P: 0.73333 | R: 0.42308 | F1: 0.53654 | Mean Wasserstein: 0.33439\n",
      "Epoch [204/20000] Training: DISC | Loss D: 0.57751, Loss G: 0.92220 | Accuracy: 0.73077 | fpR: 0.15385 | P: 0.80000 | R: 0.61538 | F1: 0.69560 | Mean Wasserstein: 0.50324\n",
      "Epoch [205/20000] Training: GEN | Loss D: 0.61660, Loss G: 0.86367 | Accuracy: 0.65385 | fpR: 0.19231 | P: 0.72222 | R: 0.50000 | F1: 0.59086 | Mean Wasserstein: 0.43177\n",
      "Epoch [206/20000] Training: GEN | Loss D: 0.62775, Loss G: 0.70332 | Accuracy: 0.53846 | fpR: 0.42308 | P: 0.54167 | R: 0.50000 | F1: 0.51995 | Mean Wasserstein: 0.39438\n",
      "Epoch [207/20000] Training: DISC | Loss D: 0.61180, Loss G: 0.77121 | Accuracy: 0.55769 | fpR: 0.46154 | P: 0.55556 | R: 0.57692 | F1: 0.56599 | Mean Wasserstein: 0.36702\n",
      "Epoch [208/20000] Training: DISC | Loss D: 0.62624, Loss G: 0.84874 | Accuracy: 0.63462 | fpR: 0.26923 | P: 0.66667 | R: 0.53846 | F1: 0.59570 | Mean Wasserstein: 0.34447\n",
      "Epoch [209/20000] Training: DISC | Loss D: 0.60407, Loss G: 0.90640 | Accuracy: 0.63462 | fpR: 0.23077 | P: 0.68421 | R: 0.50000 | F1: 0.57773 | Mean Wasserstein: 0.41014\n",
      "Epoch [210/20000] Training: DISC | Loss D: 0.57440, Loss G: 0.93576 | Accuracy: 0.69231 | fpR: 0.19231 | P: 0.75000 | R: 0.57692 | F1: 0.65212 | Mean Wasserstein: 0.47156\n",
      "Epoch [211/20000] Training: DISC "
     ]
    }
   ],
   "source": [
    "#For saving prettyTable.txt file\n",
    "heading = [\"Epoch\", \"Machine Training\", \"Discriminator Loss\", \"Generator Loss\", \"FPR\", \"Recall\", \"Median Wasserstein\", \"Mean Wasserstein\"]\n",
    "table = PrettyTable()\n",
    "table.field_names = heading\n",
    "\n",
    "#For saving .csv file\n",
    "rows = []\n",
    "\n",
    "last_real_features = []\n",
    "switch_flag = False\n",
    "switch_count = 0\n",
    "\n",
    "for epoch in range(n_epochs):  \n",
    "    if constant_train_flag:\n",
    "        if to_train == DISCRIMINATOR and rel_epochs >= disc_epochs:\n",
    "            rel_epochs = 0\n",
    "            to_train = GENERATOR\n",
    "            train_string = \"GEN\"\n",
    "\n",
    "        elif to_train == GENERATOR and rel_epochs >= gen_epochs:\n",
    "            rel_epochs = 0\n",
    "            to_train = DISCRIMINATOR\n",
    "            train_string = \"DISC\"\n",
    "        \n",
    "        # Change epoch ratio after intial 'leveling out'\n",
    "        if epoch == epoch_threshold:\n",
    "            rel_epochs = 0\n",
    "            to_train = GENERATOR\n",
    "            train_string = \"GENERATOR\"\n",
    "\n",
    "            old_ratio = gen_epochs / disc_epochs\n",
    "            gen_epochs = gen_epochs_change\n",
    "            disc_epochs = disc_epochs_change\n",
    "            new_ratio = gen_epochs / disc_epochs\n",
    "            print(f'\\n\\nTraining ratio of G/D switched from {old_ratio:.{dig}f} to {new_ratio:.{dig}f}\\n\\n')\n",
    "    else:\n",
    "        if epoch < static_threshold:\n",
    "            if to_train == DISCRIMINATOR and rel_epochs >= static_disc_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GEN\"\n",
    "\n",
    "            elif to_train == GENERATOR and rel_epochs >= static_gen_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = DISCRIMINATOR\n",
    "                train_string = \"DISC\"\n",
    "        \n",
    "        else:\n",
    "            if not switch_flag:\n",
    "                print(\"\\nSwitching to Dynamic Training\\n\")\n",
    "                switch_flag = True\n",
    "                to_train = DISCRIMINATOR\n",
    "                train_string = \"DISC\"\n",
    "            if to_train == DISCRIMINATOR and fpR <= pull_threshold and R >= recall_threshold:\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GEN\"\n",
    "                print(\"\\nPull Generator\\n\")\n",
    "                switch_count += 1\n",
    "            if to_train == GENERATOR and fpR >= push_threshold:\n",
    "                to_train = DISCRIMINATOR\n",
    "                train_string = \"DISC\"\n",
    "                print(\"\\nPush Generator\\n\")\n",
    "                switch_count += 1\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}] Training: {train_string} ', end ='')\n",
    "    for batch_idx, (real_features, _) in enumerate(train_loader):\n",
    "        #batch_size = len(real_features)\n",
    "    \n",
    "        if print_batches:\n",
    "                print(f'\\n\\tBatch [{batch_idx + 1} / {len(train_loader)}] |', end ='')\n",
    "    \n",
    "        if to_train == DISCRIMINATOR:\n",
    "            ### Training Discriminator\n",
    "            #visualize_real_batch(real_features.float())\n",
    "            opt_disc.zero_grad()\n",
    "            disc_loss = get_disc_loss(gen, disc, criterion, real_features.float(), len(real_features), z_dim)\n",
    "            disc_loss.backward(retain_graph = True)\n",
    "            opt_disc.step()\n",
    "            acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), batch = real_features.float())\n",
    "            w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "            median_w_dist = torch.median(w_dist)\n",
    "            mean_w_dist = torch.mean(w_dist)\n",
    "            if print_batches:\n",
    "                print(f'Loss D: {disc_loss.item():.digf}, Loss G: {get_gen_loss(gen, disc, criterion, len(real_features), z_dim):.{dig}f} | Accuracy: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Median Wasserstein: {median_w_dist:.{dig}f} | Mean Wasserstein: {mean_w_dist:.{dig}f}')\n",
    "        else:\n",
    "            ### Training Generator\n",
    "            opt_gen.zero_grad()\n",
    "            gen_loss = get_gen_loss(gen, disc, criterion, len(real_features), z_dim)\n",
    "            gen_loss.backward()\n",
    "            opt_gen.step()\n",
    "            acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), batch = real_features.float())\n",
    "            w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "            median_w_dist = torch.median(w_dist)\n",
    "            mean_w_dist = torch.mean(w_dist)\n",
    "            if print_batches:\n",
    "                print(f'Loss D: {disc_loss.item():.digf}, Loss G: {get_gen_loss(gen, disc, criterion, len(real_features), z_dim):.{dig}f} | Accuracy: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Median Wasserstein: {median_w_dist:.{dig}f} | Mean Wasserstein: {mean_w_dist:.{dig}f}')\n",
    "        \n",
    "    if not print_batches:\n",
    "        if to_train == DISCRIMINATOR:\n",
    "            ### Currently doesn't print Median/Mean Wasserstein --> Change if needed\n",
    "            print(f'| Loss D: {disc_loss.item():.{dig}f}, Loss G: {get_gen_loss(gen, disc, criterion, len(real_features), z_dim):.{dig}f} | Accuracy: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Mean Wasserstein: {mean_w_dist:.{dig}f}')\n",
    "            row_to_add = [f\"{epoch + 1}\", \"Discriminator\", f\"{disc_loss.item():.{dig}f}\", f\"{get_gen_loss(gen, disc, criterion, len(real_features), z_dim):.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{median_w_dist:.{dig}f}\", f\"{mean_w_dist:.{dig}f}\"]\n",
    "            table.add_row(row_to_add)\n",
    "            rows.append(row_to_add)\n",
    "        else:\n",
    "            print(f'| Loss D: {get_disc_loss(gen, disc, criterion, real_features.float(), len(real_features), z_dim):.{dig}f}, Loss G: {gen_loss.item():.{dig}f} | Accuracy: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Mean Wasserstein: {mean_w_dist:.{dig}f}')\n",
    "            row_to_add = [f\"{epoch + 1}\", \"Generator\", f\"{disc_loss.item():.{dig}f}\", f\"{get_gen_loss(gen, disc, criterion, len(real_features), z_dim):.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{median_w_dist:.{dig}f}\", f\"{mean_w_dist:.{dig}f}\"]\n",
    "            table.add_row(row_to_add)\n",
    "            rows.append(row_to_add)\n",
    "    rel_epochs += 1\n",
    "    \n",
    "    if mean_w_dist <= 0.1:\n",
    "        break\n",
    "\n",
    "    \n",
    "print(\"\\n\\nTraining Session Finished\")\n",
    "print(f\"Encountered {switch_count} non-trivial training swaps\")\n",
    "\n",
    "f = open(\"../model_outputs/\" + gan_id + \".txt\", \"w\")\n",
    "f.write(table.get_string())\n",
    "f.close()\n",
    "print(\"Model Results Sucessfully Saved to \\\"../model_outputs/\" + gan_id + \".txt\\\"\")\n",
    "\n",
    "with open(\"../model_outputs/\" + gan_id + \".csv\", \"w\") as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(heading)\n",
    "    # writing the data rows \n",
    "    csvwriter.writerows(rows)\n",
    "print(\"Model Results Sucessfully Saved to \\\"../model_outputs/\" + gan_id + \".csv\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change path and name of the Generator and Discriminator accordingly\n",
    "torch.save(gen.state_dict(), \"../saved_models/20k_5_2_trained_constant_gan\")\n",
    "torch.save(disc.state_dict(), \"../saved_models/20k_5_2_trained_constant_disc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Generation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Number of datum to visualize\n",
    "sample_size = len(X)\n",
    "reals = X[0:sample_size, :]\n",
    "fakes = get_fake_samples(gen, sample_size, z_dim).detach()\n",
    "density_curves(reals, fakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_output = pd.read_csv('../model_outputs/Test Gan.csv')\n",
    "model_output.head()\n",
    "\n",
    "def plot_metrics(data, vanilla = True):\n",
    "    if vanilla:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "    \n",
    "        plt.figure(figsize = (15, 15))\n",
    "        subplot(2, 2, 1)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'FPR', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 2)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'Recall', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 3)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 4)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "        plt.figure(figsize = (15, 8))\n",
    "        \n",
    "        subplot(1, 2, 1)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(1, 2, 2)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(model_output, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real/Fake Train Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_real_dataset(gen, data, scaler):\n",
    "    data = interpolation(data) #Interpolates entire dataframe\n",
    "    not_sitting = data[data['label:SITTING'] == 0] #Only selects rows where sitting is 0\n",
    "    real_features = not_sitting.iloc[:,1:27] #Selects only acceleration columns\n",
    "    \n",
    "    real_features = real_features.values #Converting to a numpy array\n",
    "    real_features = scaler.transform(real_features) # These are all the scaled acceleration features for non-sitting real data\n",
    "    real_features_size = real_features.shape[0] #Number of real samples\n",
    "    y_label_notsitting = [0] * real_features_size #0 corresponds to the non-sitting class\n",
    "    y_label_notsitting = np.asarray(y_label_notsitting).reshape(-1, 1) #Reshaping into a 2D column vector\n",
    "    \n",
    "    #Generating fake acceleration features for sitting data\n",
    "    latent_vectors = get_noise(real_features_size, z_dim) #Generate the same number of fake sitting samples as real non-sitting samples\n",
    "    fake_features = gen(latent_vectors).detach().numpy() #Generator already creates scaled features, so no scaling necessary\n",
    "    y_label_sitting = [1] * real_features_size\n",
    "    y_label_sitting = np.asarray(y_label_sitting).reshape(-1, 1)\n",
    "    \n",
    "    #Concatenating fake/real features and labels\n",
    "    all_features = np.concatenate((real_features, fake_features), axis = 0) #Vertical concatenation\n",
    "    all_labels = np.concatenate((y_label_notsitting, y_label_sitting), axis = 0).flatten() #Flatten 2D vector into 1D array for LogisticRegression\n",
    "    \n",
    "    #Splitting into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size = 0.2, shuffle = True)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def generate_real_dataset(data, scaler):\n",
    "    data = interpolation(data) #Interpolating the entire dataframe\n",
    "    features = data.iloc[:,1:27] #Selecting only acceleration columns\n",
    "    features = features.values #Converting to a numpy array\n",
    "    features = scaler.transform(features) #Scaling all rows in the dataframe\n",
    "    labels = data['label:SITTING'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, shuffle = True)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train_test_real_fake(X_train, y_train, X_test, y_test):\n",
    "    classifier = LogisticRegression(penalty = 'l2', C = 0.8)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    f1 = classifier_performance(y_pred, y_test)\n",
    "    return f1\n",
    "\n",
    "def train_test_real(X_train, y_train, X_test, y_test): \n",
    "    classifier = LogisticRegression(penalty = 'l2', C = 0.8)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    f1 = classifier_performance(y_pred, y_test)\n",
    "    return f1\n",
    "\n",
    "def classifier_performance(y_pred, y_test):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for k in range(len(y_pred)):\n",
    "        #True positive\n",
    "        if y_test[k] == 1 and y_pred[k] == 1:\n",
    "            tp += 1\n",
    "        #False Negative\n",
    "        elif y_test[k] == 1 and y_pred[k] == 0:\n",
    "            fn += 1\n",
    "        #True Negative\n",
    "        elif y_test[k] == 0 and y_pred[k] == 0:\n",
    "            tn += 1\n",
    "        elif y_test[k] == 0 and y_pred[k] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "            exit()\n",
    "            \n",
    "    acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "\n",
    "    if tp + fp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    \n",
    "    f1 = 2*(precision * recall / (precision + recall + 0.001))\n",
    "    \n",
    "    print(f'Precision: {precision:.3f} Recall: {recall:.3f} F-1 Score: {f1:.3f}')\n",
    "    \n",
    "    return acc, f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
