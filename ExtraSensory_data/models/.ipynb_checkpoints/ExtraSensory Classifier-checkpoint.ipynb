{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program uses a standard fully-connected neural network with ReLU activations to predict whether a person is sitting based on their accelerometer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>raw_acc:magnitude_stats:time_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>label:STAIRS_-_GOING_DOWN</th>\n",
       "      <th>label:ELEVATOR</th>\n",
       "      <th>label:OR_standing</th>\n",
       "      <th>label:AT_SCHOOL</th>\n",
       "      <th>label:PHONE_IN_HAND</th>\n",
       "      <th>label:PHONE_IN_BAG</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "      <th>label:WITH_CO-WORKERS</th>\n",
       "      <th>label:WITH_FRIENDS</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1449601597</td>\n",
       "      <td>1.000371</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>-0.016173</td>\n",
       "      <td>0.027860</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>1.000739</td>\n",
       "      <td>1.003265</td>\n",
       "      <td>0.891038</td>\n",
       "      <td>6.684582</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1449601657</td>\n",
       "      <td>1.000243</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.998463</td>\n",
       "      <td>1.000373</td>\n",
       "      <td>1.002088</td>\n",
       "      <td>1.647929</td>\n",
       "      <td>6.684605</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1449601717</td>\n",
       "      <td>1.000811</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>1.000928</td>\n",
       "      <td>1.002032</td>\n",
       "      <td>1.960286</td>\n",
       "      <td>6.684610</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1449601777</td>\n",
       "      <td>1.001245</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.999188</td>\n",
       "      <td>1.001425</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>1.614524</td>\n",
       "      <td>6.684601</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1449601855</td>\n",
       "      <td>1.001354</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>-0.096520</td>\n",
       "      <td>0.165298</td>\n",
       "      <td>1.000807</td>\n",
       "      <td>1.002259</td>\n",
       "      <td>1.003631</td>\n",
       "      <td>0.837790</td>\n",
       "      <td>6.682252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0  1449601597                      1.000371                     0.007671   \n",
       "1  1449601657                      1.000243                     0.003782   \n",
       "2  1449601717                      1.000811                     0.002082   \n",
       "3  1449601777                      1.001245                     0.004715   \n",
       "4  1449601855                      1.001354                     0.065186   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.016173                         0.027860   \n",
       "1                        -0.002713                         0.007046   \n",
       "2                        -0.001922                         0.003575   \n",
       "3                        -0.002895                         0.008881   \n",
       "4                        -0.096520                         0.165298   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              0.998221                              1.000739   \n",
       "1                              0.998463                              1.000373   \n",
       "2                              0.999653                              1.000928   \n",
       "3                              0.999188                              1.001425   \n",
       "4                              1.000807                              1.002259   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              1.003265   \n",
       "1                              1.002088   \n",
       "2                              1.002032   \n",
       "3                              1.003500   \n",
       "4                              1.003631   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  \\\n",
       "0                               0.891038   \n",
       "1                               1.647929   \n",
       "2                               1.960286   \n",
       "3                               1.614524   \n",
       "4                               0.837790   \n",
       "\n",
       "   raw_acc:magnitude_stats:time_entropy  ...  label:STAIRS_-_GOING_DOWN  \\\n",
       "0                              6.684582  ...                        NaN   \n",
       "1                              6.684605  ...                        NaN   \n",
       "2                              6.684610  ...                        NaN   \n",
       "3                              6.684601  ...                        NaN   \n",
       "4                              6.682252  ...                        0.0   \n",
       "\n",
       "   label:ELEVATOR  label:OR_standing  label:AT_SCHOOL  label:PHONE_IN_HAND  \\\n",
       "0             NaN                NaN              NaN                  NaN   \n",
       "1             NaN                NaN              NaN                  NaN   \n",
       "2             NaN                NaN              NaN                  NaN   \n",
       "3             NaN                NaN              NaN                  NaN   \n",
       "4             NaN                0.0              1.0                  NaN   \n",
       "\n",
       "   label:PHONE_IN_BAG  label:PHONE_ON_TABLE  label:WITH_CO-WORKERS  \\\n",
       "0                 NaN                   NaN                    NaN   \n",
       "1                 NaN                   NaN                    NaN   \n",
       "2                 NaN                   NaN                    NaN   \n",
       "3                 NaN                   NaN                    NaN   \n",
       "4                 NaN                   NaN                    NaN   \n",
       "\n",
       "   label:WITH_FRIENDS  label_source  \n",
       "0                 NaN            -1  \n",
       "1                 NaN            -1  \n",
       "2                 NaN            -1  \n",
       "3                 NaN            -1  \n",
       "4                 0.0             2  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change the data file directory below appropriately\n",
    "data = pd.read_csv('../raw_data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.000371,  0.007671, -0.016173, ..., -0.329743,  0.382219,\n",
       "         -0.121107],\n",
       "        [ 1.000243,  0.003782, -0.002713, ...,  0.20286 ,  0.335481,\n",
       "          0.10547 ],\n",
       "        [ 1.000811,  0.002082, -0.001922, ...,  0.111225,  0.48802 ,\n",
       "          0.154312],\n",
       "        ...,\n",
       "        [ 1.002523,  0.028048,  0.027043, ...,  0.524328,  0.286613,\n",
       "          0.012429],\n",
       "        [ 1.00259 ,  0.005246, -0.001691, ..., -0.081698,  0.466467,\n",
       "          0.545858],\n",
       "        [ 1.002413,  0.003424,  0.004579, ...,  0.175483, -0.0208  ,\n",
       "         -0.128086]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:,1:27]\n",
    "y = data[['label:SITTING']]\n",
    "\n",
    "X = interpolation(X).values\n",
    "y = interpolation(y).values\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data and loading it into a PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 50, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_layer, output_layer):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_layer, 200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 50)\n",
    "        self.output = nn.Linear(50, output_layer)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = train_features.shape[1]\n",
    "output_layer = 1\n",
    "\n",
    "model = Classifier(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Total Batch Loss: 42.02975445985794\n",
      "Epoch: 2 | Total Batch Loss: 35.77012872695923\n",
      "Epoch: 3 | Total Batch Loss: 33.23016256093979\n",
      "Epoch: 4 | Total Batch Loss: 32.07504230737686\n",
      "Epoch: 5 | Total Batch Loss: 31.372679203748703\n",
      "Epoch: 6 | Total Batch Loss: 29.845425486564636\n",
      "Epoch: 7 | Total Batch Loss: 29.079235583543777\n",
      "Epoch: 8 | Total Batch Loss: 28.515376150608063\n",
      "Epoch: 9 | Total Batch Loss: 28.098766565322876\n",
      "Epoch: 10 | Total Batch Loss: 26.746616810560226\n",
      "Epoch: 11 | Total Batch Loss: 26.832252383232117\n",
      "Epoch: 12 | Total Batch Loss: 27.156780689954758\n",
      "Epoch: 13 | Total Batch Loss: 26.984860360622406\n",
      "Epoch: 14 | Total Batch Loss: 25.61850230395794\n",
      "Epoch: 15 | Total Batch Loss: 25.622392877936363\n",
      "Epoch: 16 | Total Batch Loss: 24.556797668337822\n",
      "Epoch: 17 | Total Batch Loss: 24.272471517324448\n",
      "Epoch: 18 | Total Batch Loss: 24.860372230410576\n",
      "Epoch: 19 | Total Batch Loss: 23.90154267847538\n",
      "Epoch: 20 | Total Batch Loss: 23.894635915756226\n",
      "Epoch: 21 | Total Batch Loss: 23.75466887652874\n",
      "Epoch: 22 | Total Batch Loss: 23.172027736902237\n",
      "Epoch: 23 | Total Batch Loss: 23.19660359621048\n",
      "Epoch: 24 | Total Batch Loss: 23.392291754484177\n",
      "Epoch: 25 | Total Batch Loss: 22.52563688158989\n",
      "Epoch: 26 | Total Batch Loss: 22.57310988008976\n",
      "Epoch: 27 | Total Batch Loss: 23.13104298710823\n",
      "Epoch: 28 | Total Batch Loss: 22.208971083164215\n",
      "Epoch: 29 | Total Batch Loss: 21.913023188710213\n",
      "Epoch: 30 | Total Batch Loss: 21.696467489004135\n",
      "Epoch: 31 | Total Batch Loss: 21.658597096800804\n",
      "Epoch: 32 | Total Batch Loss: 21.035207107663155\n",
      "Epoch: 33 | Total Batch Loss: 21.295177832245827\n",
      "Epoch: 34 | Total Batch Loss: 20.870919838547707\n",
      "Epoch: 35 | Total Batch Loss: 20.9906387925148\n",
      "Epoch: 36 | Total Batch Loss: 20.431021586060524\n",
      "Epoch: 37 | Total Batch Loss: 20.74433745443821\n",
      "Epoch: 38 | Total Batch Loss: 20.480102956295013\n",
      "Epoch: 39 | Total Batch Loss: 19.896148025989532\n",
      "Epoch: 40 | Total Batch Loss: 19.94101992249489\n",
      "Epoch: 41 | Total Batch Loss: 19.810599118471146\n",
      "Epoch: 42 | Total Batch Loss: 19.45912778377533\n",
      "Epoch: 43 | Total Batch Loss: 19.016641587018967\n",
      "Epoch: 44 | Total Batch Loss: 19.53327138721943\n",
      "Epoch: 45 | Total Batch Loss: 20.05871769040823\n",
      "Epoch: 46 | Total Batch Loss: 18.81175796687603\n",
      "Epoch: 47 | Total Batch Loss: 18.94028812646866\n",
      "Epoch: 48 | Total Batch Loss: 18.2826661542058\n",
      "Epoch: 49 | Total Batch Loss: 18.20738735795021\n",
      "Epoch: 50 | Total Batch Loss: 17.769285656511784\n",
      "Epoch: 51 | Total Batch Loss: 19.297025695443153\n",
      "Epoch: 52 | Total Batch Loss: 18.28544157743454\n",
      "Epoch: 53 | Total Batch Loss: 17.476850047707558\n",
      "Epoch: 54 | Total Batch Loss: 18.173824548721313\n",
      "Epoch: 55 | Total Batch Loss: 18.10654367506504\n",
      "Epoch: 56 | Total Batch Loss: 17.76157495379448\n",
      "Epoch: 57 | Total Batch Loss: 17.013297609984875\n",
      "Epoch: 58 | Total Batch Loss: 16.429819241166115\n",
      "Epoch: 59 | Total Batch Loss: 16.529134519398212\n",
      "Epoch: 60 | Total Batch Loss: 17.008132591843605\n",
      "Epoch: 61 | Total Batch Loss: 16.946384608745575\n",
      "Epoch: 62 | Total Batch Loss: 16.035012915730476\n",
      "Epoch: 63 | Total Batch Loss: 15.956961080431938\n",
      "Epoch: 64 | Total Batch Loss: 16.251577630639076\n",
      "Epoch: 65 | Total Batch Loss: 16.87173594534397\n",
      "Epoch: 66 | Total Batch Loss: 15.420765571296215\n",
      "Epoch: 67 | Total Batch Loss: 15.75423513352871\n",
      "Epoch: 68 | Total Batch Loss: 15.662312917411327\n",
      "Epoch: 69 | Total Batch Loss: 15.6682830452919\n",
      "Epoch: 70 | Total Batch Loss: 15.849283084273338\n",
      "Epoch: 71 | Total Batch Loss: 15.452309392392635\n",
      "Epoch: 72 | Total Batch Loss: 15.667075701057911\n",
      "Epoch: 73 | Total Batch Loss: 15.138482443988323\n",
      "Epoch: 74 | Total Batch Loss: 15.092786014080048\n",
      "Epoch: 75 | Total Batch Loss: 14.964625045657158\n",
      "Epoch: 76 | Total Batch Loss: 14.703726436942816\n",
      "Epoch: 77 | Total Batch Loss: 14.37384993582964\n",
      "Epoch: 78 | Total Batch Loss: 14.712848603725433\n",
      "Epoch: 79 | Total Batch Loss: 14.452994138002396\n",
      "Epoch: 80 | Total Batch Loss: 14.609961368143559\n",
      "Epoch: 81 | Total Batch Loss: 14.02345323562622\n",
      "Epoch: 82 | Total Batch Loss: 14.263652585446835\n",
      "Epoch: 83 | Total Batch Loss: 13.9017119333148\n",
      "Epoch: 84 | Total Batch Loss: 14.63210966438055\n",
      "Epoch: 85 | Total Batch Loss: 13.663845285773277\n",
      "Epoch: 86 | Total Batch Loss: 13.501731988042593\n",
      "Epoch: 87 | Total Batch Loss: 13.464397106319666\n",
      "Epoch: 88 | Total Batch Loss: 13.11389122158289\n",
      "Epoch: 89 | Total Batch Loss: 13.215754948556423\n",
      "Epoch: 90 | Total Batch Loss: 12.884365044534206\n",
      "Epoch: 91 | Total Batch Loss: 13.272739119827747\n",
      "Epoch: 92 | Total Batch Loss: 13.866982780396938\n",
      "Epoch: 93 | Total Batch Loss: 13.053903348743916\n",
      "Epoch: 94 | Total Batch Loss: 12.726436220109463\n",
      "Epoch: 95 | Total Batch Loss: 12.554985843598843\n",
      "Epoch: 96 | Total Batch Loss: 12.430014409124851\n",
      "Epoch: 97 | Total Batch Loss: 13.010785847902298\n",
      "Epoch: 98 | Total Batch Loss: 13.340543247759342\n",
      "Epoch: 99 | Total Batch Loss: 12.621804356575012\n",
      "Epoch: 100 | Total Batch Loss: 13.346684776246548\n",
      "Epoch: 101 | Total Batch Loss: 12.43466691672802\n",
      "Epoch: 102 | Total Batch Loss: 11.818100064992905\n",
      "Epoch: 103 | Total Batch Loss: 13.197824746370316\n",
      "Epoch: 104 | Total Batch Loss: 12.915820084512234\n",
      "Epoch: 105 | Total Batch Loss: 13.012121669948101\n",
      "Epoch: 106 | Total Batch Loss: 12.88640183955431\n",
      "Epoch: 107 | Total Batch Loss: 11.580281898379326\n",
      "Epoch: 108 | Total Batch Loss: 11.281548626720905\n",
      "Epoch: 109 | Total Batch Loss: 11.667077749967575\n",
      "Epoch: 110 | Total Batch Loss: 11.48532384634018\n",
      "Epoch: 111 | Total Batch Loss: 11.004350394010544\n",
      "Epoch: 112 | Total Batch Loss: 11.436878107488155\n",
      "Epoch: 113 | Total Batch Loss: 11.216538690030575\n",
      "Epoch: 114 | Total Batch Loss: 11.375262677669525\n",
      "Epoch: 115 | Total Batch Loss: 11.007287189364433\n",
      "Epoch: 116 | Total Batch Loss: 11.562810063362122\n",
      "Epoch: 117 | Total Batch Loss: 11.242610275745392\n",
      "Epoch: 118 | Total Batch Loss: 11.576220020651817\n",
      "Epoch: 119 | Total Batch Loss: 11.212479807436466\n",
      "Epoch: 120 | Total Batch Loss: 11.171720165759325\n",
      "Epoch: 121 | Total Batch Loss: 11.255930684506893\n",
      "Epoch: 122 | Total Batch Loss: 11.005671042948961\n",
      "Epoch: 123 | Total Batch Loss: 11.382526829838753\n",
      "Epoch: 124 | Total Batch Loss: 11.283326610922813\n",
      "Epoch: 125 | Total Batch Loss: 10.046615783125162\n",
      "Epoch: 126 | Total Batch Loss: 10.90522438287735\n",
      "Epoch: 127 | Total Batch Loss: 11.056494735181332\n",
      "Epoch: 128 | Total Batch Loss: 10.262187652289867\n",
      "Epoch: 129 | Total Batch Loss: 10.372458070516586\n",
      "Epoch: 130 | Total Batch Loss: 10.24041859805584\n",
      "Epoch: 131 | Total Batch Loss: 9.953922554850578\n",
      "Epoch: 132 | Total Batch Loss: 10.647660456597805\n",
      "Epoch: 133 | Total Batch Loss: 11.077049978077412\n",
      "Epoch: 134 | Total Batch Loss: 9.853703256696463\n",
      "Epoch: 135 | Total Batch Loss: 9.946190226823092\n",
      "Epoch: 136 | Total Batch Loss: 9.767433051019907\n",
      "Epoch: 137 | Total Batch Loss: 10.008178729563951\n",
      "Epoch: 138 | Total Batch Loss: 9.960652489215136\n",
      "Epoch: 139 | Total Batch Loss: 10.62429752573371\n",
      "Epoch: 140 | Total Batch Loss: 10.422933839261532\n",
      "Epoch: 141 | Total Batch Loss: 9.38795131072402\n",
      "Epoch: 142 | Total Batch Loss: 9.48078827187419\n",
      "Epoch: 143 | Total Batch Loss: 10.57775342464447\n",
      "Epoch: 144 | Total Batch Loss: 10.804492510855198\n",
      "Epoch: 145 | Total Batch Loss: 9.516586367040873\n",
      "Epoch: 146 | Total Batch Loss: 9.606595478951931\n",
      "Epoch: 147 | Total Batch Loss: 9.289465446025133\n",
      "Epoch: 148 | Total Batch Loss: 10.221353519707918\n",
      "Epoch: 149 | Total Batch Loss: 9.899690214544535\n",
      "Epoch: 150 | Total Batch Loss: 9.662898771464825\n",
      "Epoch: 151 | Total Batch Loss: 9.065615128725767\n",
      "Epoch: 152 | Total Batch Loss: 10.053679767996073\n",
      "Epoch: 153 | Total Batch Loss: 8.849274899810553\n",
      "Epoch: 154 | Total Batch Loss: 8.58097866922617\n",
      "Epoch: 155 | Total Batch Loss: 8.528621770441532\n",
      "Epoch: 156 | Total Batch Loss: 8.688439834862947\n",
      "Epoch: 157 | Total Batch Loss: 8.59298300743103\n",
      "Epoch: 158 | Total Batch Loss: 8.397692289203405\n",
      "Epoch: 159 | Total Batch Loss: 9.028577666729689\n",
      "Epoch: 160 | Total Batch Loss: 8.687947800382972\n",
      "Epoch: 161 | Total Batch Loss: 8.55166013352573\n",
      "Epoch: 162 | Total Batch Loss: 9.376148089766502\n",
      "Epoch: 163 | Total Batch Loss: 8.159642148762941\n",
      "Epoch: 164 | Total Batch Loss: 8.029495360329747\n",
      "Epoch: 165 | Total Batch Loss: 8.025790622457862\n",
      "Epoch: 166 | Total Batch Loss: 8.162353079766035\n",
      "Epoch: 167 | Total Batch Loss: 8.064138082787395\n",
      "Epoch: 168 | Total Batch Loss: 8.319851096719503\n",
      "Epoch: 169 | Total Batch Loss: 8.121662400662899\n",
      "Epoch: 170 | Total Batch Loss: 7.749153647571802\n",
      "Epoch: 171 | Total Batch Loss: 7.693773757666349\n",
      "Epoch: 172 | Total Batch Loss: 8.343621740117669\n",
      "Epoch: 173 | Total Batch Loss: 8.282789446413517\n",
      "Epoch: 174 | Total Batch Loss: 9.404481045901775\n",
      "Epoch: 175 | Total Batch Loss: 9.641541056334972\n",
      "Epoch: 176 | Total Batch Loss: 8.317310250364244\n",
      "Epoch: 177 | Total Batch Loss: 8.268574248999357\n",
      "Epoch: 178 | Total Batch Loss: 7.922239890322089\n",
      "Epoch: 179 | Total Batch Loss: 7.62696323916316\n",
      "Epoch: 180 | Total Batch Loss: 7.824132435023785\n",
      "Epoch: 181 | Total Batch Loss: 7.571273684501648\n",
      "Epoch: 182 | Total Batch Loss: 7.487541984766722\n",
      "Epoch: 183 | Total Batch Loss: 7.96033550798893\n",
      "Epoch: 184 | Total Batch Loss: 8.65131601691246\n",
      "Epoch: 185 | Total Batch Loss: 8.364323399960995\n",
      "Epoch: 186 | Total Batch Loss: 7.884050181135535\n",
      "Epoch: 187 | Total Batch Loss: 8.256835902109742\n",
      "Epoch: 188 | Total Batch Loss: 7.618347249925137\n",
      "Epoch: 189 | Total Batch Loss: 7.075951030477881\n",
      "Epoch: 190 | Total Batch Loss: 7.103043278679252\n",
      "Epoch: 191 | Total Batch Loss: 7.6080429293215275\n",
      "Epoch: 192 | Total Batch Loss: 7.679259434342384\n",
      "Epoch: 193 | Total Batch Loss: 7.767561844550073\n",
      "Epoch: 194 | Total Batch Loss: 7.320601100102067\n",
      "Epoch: 195 | Total Batch Loss: 8.207156656309962\n",
      "Epoch: 196 | Total Batch Loss: 6.905025236308575\n",
      "Epoch: 197 | Total Batch Loss: 7.237531024962664\n",
      "Epoch: 198 | Total Batch Loss: 7.260558350011706\n",
      "Epoch: 199 | Total Batch Loss: 7.8720053024590015\n",
      "Epoch: 200 | Total Batch Loss: 7.240076400339603\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        y_preds = model(features.float())\n",
    "        loss = criterion(y_preds, labels.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch: {epoch + 1} | Total Batch Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 84.72\n",
      "Precision: 0.87\n",
      "Recall: 0.87\n",
      "F-1 Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "total_wrong = 0\n",
    "positive_preds = 0 #tp + fp\n",
    "true_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test_data, y_test in test_loader:\n",
    "        y_test_preds = model(X_test_data.float())\n",
    "        y_test_preds = torch.round(y_test_preds)\n",
    "\n",
    "        for k in range(len(y_test_preds)):\n",
    "            if y_test_preds[k].item() == 1:\n",
    "                positive_preds += 1\n",
    "            if y_test_preds[k].item() == y_test[k].item() == 1:\n",
    "                true_positives += 1\n",
    "            if y_test_preds[k].item() == 0 and y_test[k].item() == 1:\n",
    "                false_negatives += 1\n",
    "\n",
    "        current_wrong = (abs(y_test_preds - y_test)).sum().item()\n",
    "        total_wrong += current_wrong\n",
    "\n",
    "    class_acc = (len(X_test) - total_wrong) / len(X_test) * 100\n",
    "    precision = true_positives / positive_preds\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "    print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "    print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "    print(f'F-1 Score: {2*(precision * recall / (precision + recall)):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
