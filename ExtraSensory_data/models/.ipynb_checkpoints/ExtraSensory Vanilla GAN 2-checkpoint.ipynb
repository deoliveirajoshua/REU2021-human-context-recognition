{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Change the data file directory below appropriately\n",
    "data = pd.read_csv('data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:27]\n",
    "y = data[['label:SITTING']]\n",
    "\n",
    "X = interpolation(X)\n",
    "y = interpolation(y)\n",
    "\n",
    "X = X[y['label:SITTING'] == 1].reset_index() ### Select samples of acceleration where the person is sitting\n",
    "X.drop(columns = ['index'], inplace = True)\n",
    "y = y[y['label:SITTING'] == 1].reset_index()\n",
    "y.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data into the range (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0306, 0.1106, 0.1286,  ..., 0.4425, 0.7359, 0.4850],\n",
       "        [0.0327, 0.1152, 0.3959,  ..., 0.6198, 0.4337, 0.2250],\n",
       "        [0.0315, 0.0019, 0.2265,  ..., 0.6134, 0.4755, 0.7132],\n",
       "        ...,\n",
       "        [0.0328, 0.0467, 0.2553,  ..., 0.7668, 0.6482, 0.5024],\n",
       "        [0.0330, 0.0075, 0.2258,  ..., 0.4555, 0.7392, 0.7749],\n",
       "        [0.0326, 0.0044, 0.2323,  ..., 0.5876, 0.4927, 0.4306]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "X = mm.fit_transform(X)\n",
    "train_features = torch.tensor(X)\n",
    "train_labels = torch.tensor(y)\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model_fake(gen, loss_function, n_epochs, z_dim, data, batch_size, input_size, output_size)\n",
    "#train_model_real(loss_function, n_epochs, data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, hidden_dim * 2),\n",
    "            generator_block(hidden_dim * 2, hidden_dim),\n",
    "            generator_block(hidden_dim, int(hidden_dim * 0.5)),\n",
    "            nn.Linear(int(hidden_dim * 0.5), feature_dim),\n",
    "            #generator_block(int(hidden_dim * 0.5), feature_dim),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def discriminator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout()\n",
    "    )\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            discriminator_block(feature_dim, hidden_dim),\n",
    "            discriminator_block(hidden_dim, int(hidden_dim * 0.5)),\n",
    "            discriminator_block(int(hidden_dim * 0.5), hidden_dim),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, feature_vector):\n",
    "        return self.disc(feature_vector)\n",
    "\n",
    "def get_disc_loss(gen, disc, criterion, real_features, batch_size, z_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    pred_fake = disc(fake_features.detach())\n",
    "    \n",
    "    ground_truth = torch.zeros_like(pred_fake)\n",
    "    loss_fake = criterion(pred_fake, ground_truth)\n",
    "    \n",
    "    pred_real = disc(real_features)\n",
    "    ground_truth = torch.ones_like(pred_real)\n",
    "    loss_real = criterion(pred_real, ground_truth)\n",
    "    \n",
    "    disc_loss = (loss_fake + loss_real) / 2\n",
    "    \n",
    "    return disc_loss\n",
    "\n",
    "def get_gen_loss(gen, disc, criterion, batch_size, z_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    pred = disc(fake_features)\n",
    "    gen_loss = criterion(pred, torch.ones_like(pred))\n",
    "    \n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake data quality test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "def get_fake_samples(gen, batch_size, z_dim):\n",
    "    \"\"\"\n",
    "    Generates fake acceleration features given a batch size, latent vector dimension, and trained generator.\n",
    "    \n",
    "    \"\"\"\n",
    "    latent_vectors = get_noise(batch_size, z_dim) ### Retrieves a 2D tensor of noise\n",
    "    fake_features = gen(latent_vectors)\n",
    "    \n",
    "    return fake_features ### Returns a 2D tensor of fake features of size batch_size x z_dim\n",
    "    \n",
    "\n",
    "def create_fake_dataset(gen, z_dim, data):\n",
    "    '''\n",
    "    Creates a training/test set with 50% fake sitting features and 50% real non-sitting features.\n",
    "    '''\n",
    "    ### Retrieve random real samples where the user wasn't sitting\n",
    "    X = interpolation(data.iloc[:,1:27]) \n",
    "    y = interpolation(data[['label:SITTING']]) \n",
    "    X = X[y['label:SITTING'] == 0].reset_index().drop(columns = ['index']) ### Selects non-sitting features\n",
    "    y = y[y['label:SITTING'] == 0].reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    X = X.values ### Converts the dataframes into arrays\n",
    "    y = y.values \n",
    "    \n",
    "    X_real, _, _, _ = train_test_split(X, y, test_size = 0.2) ### Use 80% of real non-sitting samples\n",
    "    \n",
    "    X_real_length = len(X_real) ### Storing the length to create an equal number of fake samples\n",
    "    \n",
    "    X_real = torch.tensor(X_real) ### All real samples where the user wasn't sitting\n",
    "    \n",
    "    fake_sitting = get_fake_samples(gen, X_real_length, z_dim)\n",
    "    \n",
    "    dataset = torch.cat((fake_sitting, X_real), dim = 0).detach()\n",
    "    \n",
    "    one_labels = torch.ones(len(fake_sitting), 1) ### 1s correspond to sitting\n",
    "    zero_labels = torch.zeros(len(X_real), 1) ### 0s correspond to not sitting\n",
    "    labels = torch.cat((one_labels, zero_labels), dim = 0).detach()\n",
    "    \n",
    "    ### Splitting into training and testing sets\n",
    "    dataset = dataset.numpy()\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.2)\n",
    "    \n",
    "    ### Converting to tensors\n",
    "    X_train = torch.tensor(X_train)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_test = torch.tensor(y_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_real_dataset(data):\n",
    "    \"\"\"\n",
    "    Returns a train/test split of the real dataset.\n",
    "    \"\"\"\n",
    "    ### Not a guaranteed 50-50 split between sitting and not sitting\n",
    "    X = interpolation(data.iloc[:,1:27])\n",
    "    y = interpolation(data[['label:SITTING']])\n",
    "\n",
    "    ### Converting dataframe to arrays\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    ### Converting to tensors\n",
    "    X_train = torch.tensor(X_train)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_test = torch.tensor(y_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "def create_dataloader(X_train, y_train, X_test, y_test, batch_size):\n",
    "    \"\"\"\n",
    "    Creates the train_loader and test_loader iterables.\n",
    "    \"\"\"\n",
    "    train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    test_data = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True) \n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model_fake(gen, criterion, n_epochs, z_dim, data, batch_size, input_size, output_size):\n",
    "    \"\"\"\n",
    "    Trains a classifier on a combination of real and fake training examples. Evaluates it on real testing examples.\n",
    "    \"\"\"\n",
    "    ### Create a training/test set with fake features\n",
    "    X_train, y_train, X_test, y_test = create_fake_dataset(gen, z_dim, data)\n",
    "    train_loader, test_loader = create_dataloader(X_train, y_train, X_test, y_test, batch_size)\n",
    "    \n",
    "    ### Instantiate the model and optimizer\n",
    "    model = Classifier(input_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "            y_preds = model(features.float())\n",
    "            loss = criterion(y_preds, labels.float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch: {epoch + 1} | Total Batch Loss: {total_loss}')\n",
    "        \n",
    "    ### Evalute Model's Performance\n",
    "    evaluate_model(X_test, y_test, test_loader, model)\n",
    "\n",
    "def train_model_real(criterion, n_epochs, data, batch_size):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a classifier on only real training and testing examples.\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test = create_real_dataset(data)\n",
    "    train_loader, test_loader = create_dataloader(X_train, y_train, X_test, y_test, batch_size)\n",
    "    \n",
    "    model = Classifier(input_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "            y_preds = model(features.float())\n",
    "            loss = criterion(y_preds, labels.float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch: {epoch + 1} | Total Batch Loss: {total_loss}')\n",
    "        \n",
    "    ### Evalute Model's Performance\n",
    "    evaluate_model(X_test, y_test, test_loader, model)\n",
    "\n",
    "def evaluate_model(X_test, y_test, test_loader, model):\n",
    "    \"\"\"\n",
    "    Returns the classification accuracy, precision, recall, and F-1 score of a model.\n",
    "    \"\"\"\n",
    "    total_wrong = 0\n",
    "    positive_preds = 0 \n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test_data, y_test in test_loader:\n",
    "            y_test_preds = model(X_test_data.float())\n",
    "            y_test_preds = torch.round(y_test_preds)\n",
    "\n",
    "            for k in range(len(y_test_preds)):\n",
    "                if y_test_preds[k].item() == 1:\n",
    "                    positive_preds += 1\n",
    "                if y_test_preds[k].item() == y_test[k].item() == 1:\n",
    "                    true_positives += 1\n",
    "                if y_test_preds[k].item() == 0 and y_test[k].item() == 1:\n",
    "                    false_negatives += 1\n",
    "\n",
    "            current_wrong = (abs(y_test_preds - y_test)).sum().item()\n",
    "            total_wrong += current_wrong\n",
    "\n",
    "        class_acc = (len(X_test) - total_wrong) / len(X_test) * 100\n",
    "        precision = true_positives / positive_preds\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "        print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "        print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "        print(f'F-1 Score: {2*(precision * recall / (precision + recall)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() ### For GAN training\n",
    "loss_function = nn.BCELoss() ### For classifier training\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True) ### 66 batches\n",
    "\n",
    "batch_size = 60\n",
    "n_epochs = 20\n",
    "z_dim = 100\n",
    "lr = 0.000001\n",
    "\n",
    "### For Classifier\n",
    "input_size = 26\n",
    "output_size = 1\n",
    "\n",
    "### Instantiating discriminator/generator objects\n",
    "disc = Discriminator()\n",
    "gen = Generator(z_dim)\n",
    "\n",
    "### Optimizers for the GAN\n",
    "opt_disc = optim.Adam(disc.parameters(), lr = lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 20] Loss D: 0.6921, Loss G: 0.6810 \n",
      "Epoch [2 / 20] Loss D: 0.6928, Loss G: 0.6811 \n",
      "Epoch [3 / 20] Loss D: 0.6900, Loss G: 0.6976 \n",
      "Epoch [4 / 20] Loss D: 0.6948, Loss G: 0.6827 \n",
      "Epoch [5 / 20] Loss D: 0.6899, Loss G: 0.6896 \n",
      "Epoch [6 / 20] Loss D: 0.6921, Loss G: 0.6880 \n",
      "Epoch [7 / 20] Loss D: 0.6872, Loss G: 0.6933 \n",
      "Epoch [8 / 20] Loss D: 0.6894, Loss G: 0.6890 \n",
      "Epoch [9 / 20] Loss D: 0.6850, Loss G: 0.6981 \n",
      "Epoch [10 / 20] Loss D: 0.6835, Loss G: 0.6899 \n",
      "Epoch: 1 | Total Batch Loss: 36.82591310143471\n",
      "Epoch: 2 | Total Batch Loss: 22.252367809414864\n",
      "Epoch: 3 | Total Batch Loss: 11.742404691874981\n",
      "Epoch: 4 | Total Batch Loss: 5.683568447828293\n",
      "Epoch: 5 | Total Batch Loss: 2.8281019795686007\n",
      "Epoch: 6 | Total Batch Loss: 1.5518566872924566\n",
      "Epoch: 7 | Total Batch Loss: 0.9347840175032616\n",
      "Epoch: 8 | Total Batch Loss: 0.5986886224709451\n",
      "Epoch: 9 | Total Batch Loss: 0.40491782885510474\n",
      "Epoch: 10 | Total Batch Loss: 0.29497301089577377\n",
      "Epoch: 11 | Total Batch Loss: 0.2215801632264629\n",
      "Epoch: 12 | Total Batch Loss: 0.17362346686422825\n",
      "Epoch: 13 | Total Batch Loss: 0.1386216952232644\n",
      "Epoch: 14 | Total Batch Loss: 0.11289327626582235\n",
      "Epoch: 15 | Total Batch Loss: 0.09407747845398262\n",
      "Epoch: 16 | Total Batch Loss: 0.07884478371124715\n",
      "Epoch: 17 | Total Batch Loss: 0.06748128001345322\n",
      "Epoch: 18 | Total Batch Loss: 0.0582179413177073\n",
      "Epoch: 19 | Total Batch Loss: 0.05063873092876747\n",
      "Epoch: 20 | Total Batch Loss: 0.04412186410627328\n",
      "Classification Accuracy: 100.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F-1 Score: 1.00\n",
      "Epoch: 1 | Total Batch Loss: 61.25247150659561\n",
      "Epoch: 2 | Total Batch Loss: 53.2802232503891\n",
      "Epoch: 3 | Total Batch Loss: 49.92663300037384\n",
      "Epoch: 4 | Total Batch Loss: 46.684381514787674\n",
      "Epoch: 5 | Total Batch Loss: 45.955338299274445\n",
      "Epoch: 6 | Total Batch Loss: 44.83048704266548\n",
      "Epoch: 7 | Total Batch Loss: 42.43295909464359\n",
      "Epoch: 8 | Total Batch Loss: 42.051531836390495\n",
      "Epoch: 9 | Total Batch Loss: 40.4033515304327\n",
      "Epoch: 10 | Total Batch Loss: 39.32036344707012\n",
      "Epoch: 11 | Total Batch Loss: 39.16399113833904\n",
      "Epoch: 12 | Total Batch Loss: 39.23799730837345\n",
      "Epoch: 13 | Total Batch Loss: 39.35999310016632\n",
      "Epoch: 14 | Total Batch Loss: 38.580856025218964\n",
      "Epoch: 15 | Total Batch Loss: 38.198076248168945\n",
      "Epoch: 16 | Total Batch Loss: 37.237729132175446\n",
      "Epoch: 17 | Total Batch Loss: 36.27818042039871\n",
      "Epoch: 18 | Total Batch Loss: 36.96012634038925\n",
      "Epoch: 19 | Total Batch Loss: 35.924647361040115\n",
      "Epoch: 20 | Total Batch Loss: 35.87048718333244\n",
      "Classification Accuracy: 82.20\n",
      "Precision: 0.89\n",
      "Recall: 0.78\n",
      "F-1 Score: 0.83\n",
      "Epoch [11 / 20] Loss D: 0.6834, Loss G: 0.7015 \n",
      "Epoch [12 / 20] Loss D: 0.6863, Loss G: 0.7006 \n",
      "Epoch [13 / 20] Loss D: 0.6845, Loss G: 0.7083 \n",
      "Epoch [14 / 20] Loss D: 0.6806, Loss G: 0.7003 \n",
      "Epoch [15 / 20] Loss D: 0.6802, Loss G: 0.7072 \n",
      "Epoch [16 / 20] Loss D: 0.6717, Loss G: 0.7067 \n",
      "Epoch [17 / 20] Loss D: 0.6792, Loss G: 0.7046 \n",
      "Epoch [18 / 20] Loss D: 0.6753, Loss G: 0.7087 \n",
      "Epoch [19 / 20] Loss D: 0.6753, Loss G: 0.7240 \n",
      "Epoch [20 / 20] Loss D: 0.6736, Loss G: 0.7193 \n",
      "Epoch: 1 | Total Batch Loss: 35.96161976456642\n",
      "Epoch: 2 | Total Batch Loss: 22.58241619169712\n",
      "Epoch: 3 | Total Batch Loss: 13.049097087234259\n",
      "Epoch: 4 | Total Batch Loss: 6.437284417450428\n",
      "Epoch: 5 | Total Batch Loss: 2.9879044331610203\n",
      "Epoch: 6 | Total Batch Loss: 1.5225567817687988\n",
      "Epoch: 7 | Total Batch Loss: 0.8797419173642993\n",
      "Epoch: 8 | Total Batch Loss: 0.5605253335088491\n",
      "Epoch: 9 | Total Batch Loss: 0.3896030685864389\n",
      "Epoch: 10 | Total Batch Loss: 0.28118213126435876\n",
      "Epoch: 11 | Total Batch Loss: 0.21052496414631605\n",
      "Epoch: 12 | Total Batch Loss: 0.16447234619408846\n",
      "Epoch: 13 | Total Batch Loss: 0.13103666750248522\n",
      "Epoch: 14 | Total Batch Loss: 0.10744031541980803\n",
      "Epoch: 15 | Total Batch Loss: 0.0889212858164683\n",
      "Epoch: 16 | Total Batch Loss: 0.07562461739871651\n",
      "Epoch: 17 | Total Batch Loss: 0.06291516777127981\n",
      "Epoch: 18 | Total Batch Loss: 0.053859591571381316\n",
      "Epoch: 19 | Total Batch Loss: 0.04680865298723802\n",
      "Epoch: 20 | Total Batch Loss: 0.04093080305028707\n",
      "Classification Accuracy: 100.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F-1 Score: 1.00\n",
      "Epoch: 1 | Total Batch Loss: 61.464009523391724\n",
      "Epoch: 2 | Total Batch Loss: 53.88804167509079\n",
      "Epoch: 3 | Total Batch Loss: 50.22853094339371\n",
      "Epoch: 4 | Total Batch Loss: 48.10946995019913\n",
      "Epoch: 5 | Total Batch Loss: 45.88317009806633\n",
      "Epoch: 6 | Total Batch Loss: 43.77019593119621\n",
      "Epoch: 7 | Total Batch Loss: 42.27963209152222\n",
      "Epoch: 8 | Total Batch Loss: 40.82645443081856\n",
      "Epoch: 9 | Total Batch Loss: 40.21003380417824\n",
      "Epoch: 10 | Total Batch Loss: 39.748085632920265\n",
      "Epoch: 11 | Total Batch Loss: 38.96895954012871\n",
      "Epoch: 12 | Total Batch Loss: 37.636586889624596\n",
      "Epoch: 13 | Total Batch Loss: 37.382342010736465\n",
      "Epoch: 14 | Total Batch Loss: 37.50016303360462\n",
      "Epoch: 15 | Total Batch Loss: 36.21473963558674\n",
      "Epoch: 16 | Total Batch Loss: 36.37555278837681\n",
      "Epoch: 17 | Total Batch Loss: 35.856494426727295\n",
      "Epoch: 18 | Total Batch Loss: 36.08894930779934\n",
      "Epoch: 19 | Total Batch Loss: 34.85070137679577\n",
      "Epoch: 20 | Total Batch Loss: 35.97971171140671\n",
      "Classification Accuracy: 80.30\n",
      "Precision: 0.87\n",
      "Recall: 0.78\n",
      "F-1 Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (real_features, _) in enumerate(train_loader):\n",
    "        batch_size = len(real_features)\n",
    "        \n",
    "        ### Training Discriminator\n",
    "        for k in range(5):\n",
    "            opt_disc.zero_grad()\n",
    "            disc_loss = get_disc_loss(gen, disc, criterion, real_features.float(), batch_size, z_dim)\n",
    "            disc_loss.backward(retain_graph = True)\n",
    "            opt_disc.step()\n",
    "\n",
    "        ### Training Generator\n",
    "        opt_gen.zero_grad()\n",
    "        gen_loss = get_gen_loss(gen, disc, criterion, batch_size, z_dim)\n",
    "        gen_loss.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f'Epoch [{epoch + 1} / {n_epochs}] Loss D: {disc_loss.item():.4f}, Loss G: {gen_loss.item():.4f} '\n",
    "            )\n",
    "            \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        train_model_fake(gen, loss_function, n_epochs, z_dim, data, batch_size, input_size, output_size)\n",
    "        train_model_real(loss_function, n_epochs, data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
