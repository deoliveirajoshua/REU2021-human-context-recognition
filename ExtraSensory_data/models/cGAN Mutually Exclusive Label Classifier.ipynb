{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>label:STAIRS_-_GOING_DOWN</th>\n",
       "      <th>label:ELEVATOR</th>\n",
       "      <th>label:OR_standing</th>\n",
       "      <th>label:AT_SCHOOL</th>\n",
       "      <th>label:PHONE_IN_HAND</th>\n",
       "      <th>label:PHONE_IN_BAG</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "      <th>label:WITH_CO-WORKERS</th>\n",
       "      <th>label:WITH_FRIENDS</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079161</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.995203</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.998502</td>\n",
       "      <td>1.748756</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079221</td>\n",
       "      <td>0.996864</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>1.935573</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079281</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.994797</td>\n",
       "      <td>0.996614</td>\n",
       "      <td>0.998704</td>\n",
       "      <td>2.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079341</td>\n",
       "      <td>0.996874</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>0.995050</td>\n",
       "      <td>0.996907</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>1.865318</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079431</td>\n",
       "      <td>0.997371</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.102332</td>\n",
       "      <td>0.995548</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.460806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   UUID   timestamp  \\\n",
       "0  00EABED2-271D-49D8-B599-1D4A09240601  1444079161   \n",
       "1  00EABED2-271D-49D8-B599-1D4A09240601  1444079221   \n",
       "2  00EABED2-271D-49D8-B599-1D4A09240601  1444079281   \n",
       "3  00EABED2-271D-49D8-B599-1D4A09240601  1444079341   \n",
       "4  00EABED2-271D-49D8-B599-1D4A09240601  1444079431   \n",
       "\n",
       "   raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0                      0.996815                     0.003529   \n",
       "1                      0.996864                     0.004172   \n",
       "2                      0.996825                     0.003667   \n",
       "3                      0.996874                     0.003541   \n",
       "4                      0.997371                     0.037653   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.002786                         0.006496   \n",
       "1                        -0.003110                         0.007050   \n",
       "2                         0.003094                         0.006076   \n",
       "3                         0.000626                         0.006059   \n",
       "4                         0.043389                         0.102332   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              0.995203                              0.996825   \n",
       "1                              0.994957                              0.996981   \n",
       "2                              0.994797                              0.996614   \n",
       "3                              0.995050                              0.996907   \n",
       "4                              0.995548                              0.996860   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              0.998502   \n",
       "1                              0.998766   \n",
       "2                              0.998704   \n",
       "3                              0.998690   \n",
       "4                              0.998205   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  ...  label:STAIRS_-_GOING_DOWN  \\\n",
       "0                               1.748756  ...                        NaN   \n",
       "1                               1.935573  ...                        NaN   \n",
       "2                               2.031780  ...                        NaN   \n",
       "3                               1.865318  ...                        NaN   \n",
       "4                               0.460806  ...                        NaN   \n",
       "\n",
       "   label:ELEVATOR  label:OR_standing  label:AT_SCHOOL  label:PHONE_IN_HAND  \\\n",
       "0             NaN                0.0              NaN                  NaN   \n",
       "1             NaN                0.0              NaN                  NaN   \n",
       "2             NaN                0.0              NaN                  NaN   \n",
       "3             NaN                0.0              NaN                  NaN   \n",
       "4             NaN                0.0              NaN                  NaN   \n",
       "\n",
       "   label:PHONE_IN_BAG  label:PHONE_ON_TABLE  label:WITH_CO-WORKERS  \\\n",
       "0                 NaN                   1.0                    1.0   \n",
       "1                 NaN                   1.0                    1.0   \n",
       "2                 NaN                   1.0                    1.0   \n",
       "3                 NaN                   1.0                    1.0   \n",
       "4                 NaN                   1.0                    1.0   \n",
       "\n",
       "   label:WITH_FRIENDS  label_source  \n",
       "0                 NaN             2  \n",
       "1                 NaN             2  \n",
       "2                 NaN             2  \n",
       "3                 NaN             2  \n",
       "4                 NaN             2  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Aggregated User Data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['UUID'] == '0BFC35E2-4817-4865-BFA7-764742302A2D') | (data['UUID'] == '0A986513-7828-4D53-AA1F-E02D6DF9561B') | (data['UUID'] == '00EABED2-271D-49D8-B599-1D4A09240601')] \n",
    "data.drop(columns = ['timestamp'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new dataframe with only sitting, walking, and sleeping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>raw_acc:magnitude_stats:time_entropy</th>\n",
       "      <th>raw_acc:magnitude_spectrum:log_energy_band0</th>\n",
       "      <th>...</th>\n",
       "      <th>raw_acc:3d:mean_x</th>\n",
       "      <th>raw_acc:3d:mean_y</th>\n",
       "      <th>raw_acc:3d:mean_z</th>\n",
       "      <th>raw_acc:3d:std_x</th>\n",
       "      <th>raw_acc:3d:std_y</th>\n",
       "      <th>raw_acc:3d:std_z</th>\n",
       "      <th>raw_acc:3d:ro_xy</th>\n",
       "      <th>raw_acc:3d:ro_xz</th>\n",
       "      <th>raw_acc:3d:ro_yz</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.003052</td>\n",
       "      <td>0.139768</td>\n",
       "      <td>0.115192</td>\n",
       "      <td>0.273656</td>\n",
       "      <td>0.991920</td>\n",
       "      <td>0.996238</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>1.328158</td>\n",
       "      <td>6.674634</td>\n",
       "      <td>5.027699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051719</td>\n",
       "      <td>-0.168622</td>\n",
       "      <td>-0.866269</td>\n",
       "      <td>0.250667</td>\n",
       "      <td>0.343037</td>\n",
       "      <td>0.252212</td>\n",
       "      <td>-0.030830</td>\n",
       "      <td>0.143926</td>\n",
       "      <td>-0.530033</td>\n",
       "      <td>WALKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.005091</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>0.190056</td>\n",
       "      <td>0.292741</td>\n",
       "      <td>0.826441</td>\n",
       "      <td>0.948630</td>\n",
       "      <td>1.176819</td>\n",
       "      <td>2.745002</td>\n",
       "      <td>6.658351</td>\n",
       "      <td>5.013784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255740</td>\n",
       "      <td>-0.433965</td>\n",
       "      <td>-0.801786</td>\n",
       "      <td>0.276395</td>\n",
       "      <td>0.190099</td>\n",
       "      <td>0.235251</td>\n",
       "      <td>0.105712</td>\n",
       "      <td>0.016350</td>\n",
       "      <td>0.027738</td>\n",
       "      <td>WALKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.988339</td>\n",
       "      <td>0.025706</td>\n",
       "      <td>-0.012189</td>\n",
       "      <td>0.044177</td>\n",
       "      <td>0.979990</td>\n",
       "      <td>0.987836</td>\n",
       "      <td>0.996713</td>\n",
       "      <td>1.783550</td>\n",
       "      <td>6.684273</td>\n",
       "      <td>5.043187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028900</td>\n",
       "      <td>-0.594037</td>\n",
       "      <td>-0.772274</td>\n",
       "      <td>0.136729</td>\n",
       "      <td>0.065971</td>\n",
       "      <td>0.065611</td>\n",
       "      <td>-0.603787</td>\n",
       "      <td>0.630796</td>\n",
       "      <td>-0.799983</td>\n",
       "      <td>WALKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.005461</td>\n",
       "      <td>0.105208</td>\n",
       "      <td>0.128558</td>\n",
       "      <td>0.199233</td>\n",
       "      <td>0.992822</td>\n",
       "      <td>0.994770</td>\n",
       "      <td>0.997684</td>\n",
       "      <td>1.459081</td>\n",
       "      <td>6.679381</td>\n",
       "      <td>5.039396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152049</td>\n",
       "      <td>-0.164488</td>\n",
       "      <td>-0.791628</td>\n",
       "      <td>0.272913</td>\n",
       "      <td>0.297443</td>\n",
       "      <td>0.426869</td>\n",
       "      <td>-0.372997</td>\n",
       "      <td>0.652161</td>\n",
       "      <td>-0.524372</td>\n",
       "      <td>WALKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.001464</td>\n",
       "      <td>0.121184</td>\n",
       "      <td>0.129053</td>\n",
       "      <td>0.209801</td>\n",
       "      <td>0.942805</td>\n",
       "      <td>0.996160</td>\n",
       "      <td>1.052143</td>\n",
       "      <td>2.060366</td>\n",
       "      <td>6.677517</td>\n",
       "      <td>5.029112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149694</td>\n",
       "      <td>-0.014688</td>\n",
       "      <td>-0.913222</td>\n",
       "      <td>0.299310</td>\n",
       "      <td>0.237561</td>\n",
       "      <td>0.122455</td>\n",
       "      <td>0.601753</td>\n",
       "      <td>-0.072091</td>\n",
       "      <td>-0.003273</td>\n",
       "      <td>WALKING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "155                      1.003052                     0.139768   \n",
       "156                      1.005091                     0.232927   \n",
       "157                      0.988339                     0.025706   \n",
       "158                      1.005461                     0.105208   \n",
       "159                      1.001464                     0.121184   \n",
       "\n",
       "     raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "155                         0.115192                         0.273656   \n",
       "156                         0.190056                         0.292741   \n",
       "157                        -0.012189                         0.044177   \n",
       "158                         0.128558                         0.199233   \n",
       "159                         0.129053                         0.209801   \n",
       "\n",
       "     raw_acc:magnitude_stats:percentile25  \\\n",
       "155                              0.991920   \n",
       "156                              0.826441   \n",
       "157                              0.979990   \n",
       "158                              0.992822   \n",
       "159                              0.942805   \n",
       "\n",
       "     raw_acc:magnitude_stats:percentile50  \\\n",
       "155                              0.996238   \n",
       "156                              0.948630   \n",
       "157                              0.987836   \n",
       "158                              0.994770   \n",
       "159                              0.996160   \n",
       "\n",
       "     raw_acc:magnitude_stats:percentile75  \\\n",
       "155                              0.999979   \n",
       "156                              1.176819   \n",
       "157                              0.996713   \n",
       "158                              0.997684   \n",
       "159                              1.052143   \n",
       "\n",
       "     raw_acc:magnitude_stats:value_entropy  \\\n",
       "155                               1.328158   \n",
       "156                               2.745002   \n",
       "157                               1.783550   \n",
       "158                               1.459081   \n",
       "159                               2.060366   \n",
       "\n",
       "     raw_acc:magnitude_stats:time_entropy  \\\n",
       "155                              6.674634   \n",
       "156                              6.658351   \n",
       "157                              6.684273   \n",
       "158                              6.679381   \n",
       "159                              6.677517   \n",
       "\n",
       "     raw_acc:magnitude_spectrum:log_energy_band0  ...  raw_acc:3d:mean_x  \\\n",
       "155                                     5.027699  ...          -0.051719   \n",
       "156                                     5.013784  ...           0.255740   \n",
       "157                                     5.043187  ...          -0.028900   \n",
       "158                                     5.039396  ...           0.152049   \n",
       "159                                     5.029112  ...           0.149694   \n",
       "\n",
       "     raw_acc:3d:mean_y  raw_acc:3d:mean_z  raw_acc:3d:std_x  raw_acc:3d:std_y  \\\n",
       "155          -0.168622          -0.866269          0.250667          0.343037   \n",
       "156          -0.433965          -0.801786          0.276395          0.190099   \n",
       "157          -0.594037          -0.772274          0.136729          0.065971   \n",
       "158          -0.164488          -0.791628          0.272913          0.297443   \n",
       "159          -0.014688          -0.913222          0.299310          0.237561   \n",
       "\n",
       "     raw_acc:3d:std_z  raw_acc:3d:ro_xy  raw_acc:3d:ro_xz  raw_acc:3d:ro_yz  \\\n",
       "155          0.252212         -0.030830          0.143926         -0.530033   \n",
       "156          0.235251          0.105712          0.016350          0.027738   \n",
       "157          0.065611         -0.603787          0.630796         -0.799983   \n",
       "158          0.426869         -0.372997          0.652161         -0.524372   \n",
       "159          0.122455          0.601753         -0.072091         -0.003273   \n",
       "\n",
       "       label  \n",
       "155  WALKING  \n",
       "156  WALKING  \n",
       "157  WALKING  \n",
       "158  WALKING  \n",
       "159  WALKING  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_walking = data[(data['label:FIX_walking'] == 1) & (data['label:SITTING'] == 0) & (data['label:SLEEPING'] == 0)]\n",
    "only_walking = only_walking.iloc[:,1:27]\n",
    "only_walking['label'] = \"WALKING\"\n",
    "\n",
    "only_sitting = data[(data['label:FIX_walking'] == 0) & (data['label:SITTING'] == 1) & (data['label:SLEEPING'] == 0)]\n",
    "only_sitting = only_sitting.iloc[:,1:27]\n",
    "only_sitting['label'] = \"SITTING\"\n",
    "\n",
    "only_sleeping = data[(data['label:FIX_walking'] == 0) & (data['label:SITTING'] == 0) & (data['label:SLEEPING'] == 1)]\n",
    "only_sleeping = only_sleeping.iloc[:,1:27]\n",
    "only_sleeping['label'] = \"SLEEPING\"\n",
    "\n",
    "df = pd.concat([only_walking, only_sitting, only_sleeping], axis = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.003052, 0.139768, 0.115192, ..., 0.143926, -0.530033,\n",
       "        'WALKING'],\n",
       "       [1.005091, 0.232927, 0.190056, ..., 0.01635, 0.027738, 'WALKING'],\n",
       "       [0.988339, 0.025706, -0.012189, ..., 0.630796, -0.799983,\n",
       "        'WALKING'],\n",
       "       ...,\n",
       "       [1.000489, 0.001597, 0.000608, ..., -0.131635, 0.020957,\n",
       "        'SLEEPING'],\n",
       "       [1.000168, 0.003089, -0.0016879999999999998, ...,\n",
       "        -0.08845900000000001, -0.090014, 'SLEEPING'],\n",
       "       [1.019187, 0.143322, 0.24915, ..., 0.19007, -0.548567, 'SLEEPING']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,:-1] = interpolation(df.iloc[:,:-1]) #Interpolate acceleration columns\n",
    "df = df.values\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, ..., -0.03083, 0.143926, -0.530033],\n",
       "       [0.0, 0.0, 1.0, ..., 0.105712, 0.01635, 0.027738],\n",
       "       [0.0, 0.0, 1.0, ..., -0.603787, 0.630796, -0.799983],\n",
       "       ...,\n",
       "       [0.0, 1.0, 0.0, ..., -0.036237, -0.131635, 0.020957],\n",
       "       [0.0, 1.0, 0.0, ..., -0.153961, -0.08845900000000001, -0.090014],\n",
       "       [0.0, 1.0, 0.0, ..., -0.33720100000000003, 0.19007, -0.548567]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [-1])], remainder = 'passthrough')\n",
    "df = np.array(ct.fit_transform(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[:,3:]\n",
    "y = df[:,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15318826,  0.17868897, -0.57553311, ..., -0.328621  ,\n",
       "         3.56939516,  0.15235929],\n",
       "       [-0.41974312, -0.32761746, -0.30091875, ..., -0.14314379,\n",
       "         0.21891196,  0.06965899],\n",
       "       [-0.16352668, -0.32911315, -0.30147572, ..., -0.19254031,\n",
       "        -0.91191421,  0.48393729],\n",
       "       ...,\n",
       "       [-0.05276937, -0.33121933, -0.30076558, ..., -0.06681618,\n",
       "         0.1276062 ,  0.46506161],\n",
       "       [ 0.04300125, -0.31668977, -0.27504746, ...,  2.27662378,\n",
       "        -0.15462445,  1.23336846],\n",
       "       [-0.13050786, -0.32961681, -0.28820589, ..., -0.17347456,\n",
       "        -0.03012983,  0.22394108]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            classifier_block(10, 5),\n",
    "            nn.Linear(5, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        softmax = nn.Softmax(dim = 1)\n",
    "        return softmax(self.network(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 500\n",
    "batch_size = 50\n",
    "\n",
    "### 2000 epochs 75 batch size 0.001 learning rate\n",
    "criterion = FocalLoss(alpha = 0.2, gamma = 5)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7624682877212763, Final Batch Loss: 0.007937115617096424\n",
      "Epoch 2, Loss: 0.5324705438688397, Final Batch Loss: 0.006677409168332815\n",
      "Epoch 3, Loss: 0.4756819186732173, Final Batch Loss: 0.003642037510871887\n",
      "Epoch 4, Loss: 0.45176530512981117, Final Batch Loss: 0.004411949310451746\n",
      "Epoch 5, Loss: 0.42898940620943904, Final Batch Loss: 0.004724546335637569\n",
      "Epoch 6, Loss: 0.4168614714872092, Final Batch Loss: 0.004989458713680506\n",
      "Epoch 7, Loss: 0.4097594802733511, Final Batch Loss: 0.005821514874696732\n",
      "Epoch 8, Loss: 0.39625218766741455, Final Batch Loss: 0.004292347468435764\n",
      "Epoch 9, Loss: 0.39227309986017644, Final Batch Loss: 0.003743818961083889\n",
      "Epoch 10, Loss: 0.38318439829163253, Final Batch Loss: 0.003234102390706539\n",
      "Epoch 11, Loss: 0.3797982183750719, Final Batch Loss: 0.008116241544485092\n",
      "Epoch 12, Loss: 0.37111475644633174, Final Batch Loss: 0.0030514553654938936\n",
      "Epoch 13, Loss: 0.36384261841885746, Final Batch Loss: 0.006530009675770998\n",
      "Epoch 14, Loss: 0.35234665544703603, Final Batch Loss: 0.004221139941364527\n",
      "Epoch 15, Loss: 0.34420395107008517, Final Batch Loss: 0.0027669239789247513\n",
      "Epoch 16, Loss: 0.33339509926736355, Final Batch Loss: 0.003284603590145707\n",
      "Epoch 17, Loss: 0.3317545489408076, Final Batch Loss: 0.005507946014404297\n",
      "Epoch 18, Loss: 0.3242575190961361, Final Batch Loss: 0.0041904994286596775\n",
      "Epoch 19, Loss: 0.3224559968803078, Final Batch Loss: 0.002606462687253952\n",
      "Epoch 20, Loss: 0.3223899535369128, Final Batch Loss: 0.0044839344918727875\n",
      "Epoch 21, Loss: 0.32009319262579083, Final Batch Loss: 0.004610063042491674\n",
      "Epoch 22, Loss: 0.3139904800336808, Final Batch Loss: 0.0055336314253509045\n",
      "Epoch 23, Loss: 0.3142861893866211, Final Batch Loss: 0.001860740827396512\n",
      "Epoch 24, Loss: 0.3050935185747221, Final Batch Loss: 0.0017794434679672122\n",
      "Epoch 25, Loss: 0.3086546501144767, Final Batch Loss: 0.003622169839218259\n",
      "Epoch 26, Loss: 0.3111115172505379, Final Batch Loss: 0.0032344297505915165\n",
      "Epoch 27, Loss: 0.30599397572223097, Final Batch Loss: 0.0041246735490858555\n",
      "Epoch 28, Loss: 0.30166464776266366, Final Batch Loss: 0.004536227323114872\n",
      "Epoch 29, Loss: 0.3054617759771645, Final Batch Loss: 0.0024224030785262585\n",
      "Epoch 30, Loss: 0.2996959260199219, Final Batch Loss: 0.0027931726071983576\n",
      "Epoch 31, Loss: 0.29620119323953986, Final Batch Loss: 0.004205933306366205\n",
      "Epoch 32, Loss: 0.295183252543211, Final Batch Loss: 0.002921315608546138\n",
      "Epoch 33, Loss: 0.2935493856202811, Final Batch Loss: 0.00325443921610713\n",
      "Epoch 34, Loss: 0.29490454122424126, Final Batch Loss: 0.004407135769724846\n",
      "Epoch 35, Loss: 0.293243785854429, Final Batch Loss: 0.003064797492697835\n",
      "Epoch 36, Loss: 0.28952027147170156, Final Batch Loss: 0.0037853396497666836\n",
      "Epoch 37, Loss: 0.29398122639395297, Final Batch Loss: 0.002446981379762292\n",
      "Epoch 38, Loss: 0.293115476379171, Final Batch Loss: 0.0035213488154113293\n",
      "Epoch 39, Loss: 0.2869467814452946, Final Batch Loss: 0.003222548170015216\n",
      "Epoch 40, Loss: 0.285632562590763, Final Batch Loss: 0.0035652790684252977\n",
      "Epoch 41, Loss: 0.28636293520685285, Final Batch Loss: 0.0030810663010925055\n",
      "Epoch 42, Loss: 0.2819875117857009, Final Batch Loss: 0.002570384880527854\n",
      "Epoch 43, Loss: 0.2823421424254775, Final Batch Loss: 0.004199637100100517\n",
      "Epoch 44, Loss: 0.28569340822286904, Final Batch Loss: 0.0018236481118947268\n",
      "Epoch 45, Loss: 0.28490202932152897, Final Batch Loss: 0.0033249135594815016\n",
      "Epoch 46, Loss: 0.2856764330063015, Final Batch Loss: 0.002841239795088768\n",
      "Epoch 47, Loss: 0.28298898856155574, Final Batch Loss: 0.003048208076506853\n",
      "Epoch 48, Loss: 0.2813866783399135, Final Batch Loss: 0.00496860034763813\n",
      "Epoch 49, Loss: 0.28510452155023813, Final Batch Loss: 0.0013582464307546616\n",
      "Epoch 50, Loss: 0.2803216357715428, Final Batch Loss: 0.0029959590174257755\n",
      "Epoch 51, Loss: 0.27417547069489956, Final Batch Loss: 0.0018899200949817896\n",
      "Epoch 52, Loss: 0.2761407517828047, Final Batch Loss: 0.002290698466822505\n",
      "Epoch 53, Loss: 0.2752935627941042, Final Batch Loss: 0.00500686326995492\n",
      "Epoch 54, Loss: 0.27636178012471646, Final Batch Loss: 0.002354794181883335\n",
      "Epoch 55, Loss: 0.27637448522727937, Final Batch Loss: 0.002885984256863594\n",
      "Epoch 56, Loss: 0.2743454205337912, Final Batch Loss: 0.004071400500833988\n",
      "Epoch 57, Loss: 0.27514607226476073, Final Batch Loss: 0.0017819901695474982\n",
      "Epoch 58, Loss: 0.2691884752130136, Final Batch Loss: 0.002043568529188633\n",
      "Epoch 59, Loss: 0.27801291237119585, Final Batch Loss: 0.004148405976593494\n",
      "Epoch 60, Loss: 0.26817717019002885, Final Batch Loss: 0.003937289118766785\n",
      "Epoch 61, Loss: 0.2698756060563028, Final Batch Loss: 0.003449156181886792\n",
      "Epoch 62, Loss: 0.26866563339717686, Final Batch Loss: 0.001964473631232977\n",
      "Epoch 63, Loss: 0.2641971434932202, Final Batch Loss: 0.003971696365624666\n",
      "Epoch 64, Loss: 0.266048569814302, Final Batch Loss: 0.001843567006289959\n",
      "Epoch 65, Loss: 0.26730503619182855, Final Batch Loss: 0.0017126587918028235\n",
      "Epoch 66, Loss: 0.26754037383943796, Final Batch Loss: 0.0019381205784156919\n",
      "Epoch 67, Loss: 0.2592456251149997, Final Batch Loss: 0.0015684097306802869\n",
      "Epoch 68, Loss: 0.25908305577468127, Final Batch Loss: 0.00405455706641078\n",
      "Epoch 69, Loss: 0.29551915545016527, Final Batch Loss: 0.0011919698445126414\n",
      "Epoch 70, Loss: 0.27325706428382546, Final Batch Loss: 0.0024647286627441645\n",
      "Epoch 71, Loss: 0.26257484848611057, Final Batch Loss: 0.002151335822418332\n",
      "Epoch 72, Loss: 0.2730515361763537, Final Batch Loss: 0.0029233726672828197\n",
      "Epoch 73, Loss: 0.2694226116873324, Final Batch Loss: 0.0031523576471954584\n",
      "Epoch 74, Loss: 0.26415366993751377, Final Batch Loss: 0.003643276635557413\n",
      "Epoch 75, Loss: 0.26947996695525944, Final Batch Loss: 0.005809417460113764\n",
      "Epoch 76, Loss: 0.26559452060610056, Final Batch Loss: 0.00514124846085906\n",
      "Epoch 77, Loss: 0.2679370560217649, Final Batch Loss: 0.005078580230474472\n",
      "Epoch 78, Loss: 0.2788034558761865, Final Batch Loss: 0.002476323861628771\n",
      "Epoch 79, Loss: 0.2698096258100122, Final Batch Loss: 0.0028218307998031378\n",
      "Epoch 80, Loss: 0.2693161895731464, Final Batch Loss: 0.0068512712605297565\n",
      "Epoch 81, Loss: 0.25980937690474093, Final Batch Loss: 0.002409700769931078\n",
      "Epoch 82, Loss: 0.25425363588146865, Final Batch Loss: 0.0014127307804301381\n",
      "Epoch 83, Loss: 0.26001719397027045, Final Batch Loss: 0.0025024875067174435\n",
      "Epoch 84, Loss: 0.26064668665640056, Final Batch Loss: 0.004250358324497938\n",
      "Epoch 85, Loss: 0.2631826469441876, Final Batch Loss: 0.0020345987286418676\n",
      "Epoch 86, Loss: 0.26117168238852173, Final Batch Loss: 0.002186782890930772\n",
      "Epoch 87, Loss: 0.2594637190923095, Final Batch Loss: 0.003970933146774769\n",
      "Epoch 88, Loss: 0.2675986560061574, Final Batch Loss: 0.002674737712368369\n",
      "Epoch 89, Loss: 0.2559537545312196, Final Batch Loss: 0.003235241398215294\n",
      "Epoch 90, Loss: 0.2638549174880609, Final Batch Loss: 0.005864085629582405\n",
      "Epoch 91, Loss: 0.25321737420745194, Final Batch Loss: 0.0024877076502889395\n",
      "Epoch 92, Loss: 0.2555197353940457, Final Batch Loss: 0.002719070063903928\n",
      "Epoch 93, Loss: 0.26015308022033423, Final Batch Loss: 0.0014306476805359125\n",
      "Epoch 94, Loss: 0.26791327935643494, Final Batch Loss: 0.002152699278667569\n",
      "Epoch 95, Loss: 0.25334427400957793, Final Batch Loss: 0.0026843480300158262\n",
      "Epoch 96, Loss: 0.2619340217206627, Final Batch Loss: 0.0035443699453026056\n",
      "Epoch 97, Loss: 0.2677134064724669, Final Batch Loss: 0.004212751518934965\n",
      "Epoch 98, Loss: 0.26281598198693246, Final Batch Loss: 0.0029720482416450977\n",
      "Epoch 99, Loss: 0.26453800045419484, Final Batch Loss: 0.0020964876748621464\n",
      "Epoch 100, Loss: 0.2568349150242284, Final Batch Loss: 0.002318221377208829\n",
      "Epoch 101, Loss: 0.2568587037967518, Final Batch Loss: 0.003473903052508831\n",
      "Epoch 102, Loss: 0.2506876472616568, Final Batch Loss: 0.0016699526458978653\n",
      "Epoch 103, Loss: 0.2604732846375555, Final Batch Loss: 0.0034920834004878998\n",
      "Epoch 104, Loss: 0.2573327304562554, Final Batch Loss: 0.0022118196357041597\n",
      "Epoch 105, Loss: 0.25721539079677314, Final Batch Loss: 0.0009914074325934052\n",
      "Epoch 106, Loss: 0.2497652234742418, Final Batch Loss: 0.002192175481468439\n",
      "Epoch 107, Loss: 0.24685733392834663, Final Batch Loss: 0.002324580680578947\n",
      "Epoch 108, Loss: 0.2569248470244929, Final Batch Loss: 0.003800742095336318\n",
      "Epoch 109, Loss: 0.25999198225326836, Final Batch Loss: 0.005670398473739624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110, Loss: 0.2605456280289218, Final Batch Loss: 0.0017752861604094505\n",
      "Epoch 111, Loss: 0.2602618501987308, Final Batch Loss: 0.0028246373403817415\n",
      "Epoch 112, Loss: 0.2482633392792195, Final Batch Loss: 0.003582874545827508\n",
      "Epoch 113, Loss: 0.2556194810895249, Final Batch Loss: 0.007642595563083887\n",
      "Epoch 114, Loss: 0.25454450747929513, Final Batch Loss: 0.0023572903592139482\n",
      "Epoch 115, Loss: 0.24982838914729655, Final Batch Loss: 0.002959892386570573\n",
      "Epoch 116, Loss: 0.24532923137303442, Final Batch Loss: 0.0015633179573342204\n",
      "Epoch 117, Loss: 0.2505088694160804, Final Batch Loss: 0.0020705433562397957\n",
      "Epoch 118, Loss: 0.2602511664154008, Final Batch Loss: 0.008992914110422134\n",
      "Epoch 119, Loss: 0.25697991671040654, Final Batch Loss: 0.002706927014514804\n",
      "Epoch 120, Loss: 0.2491494407877326, Final Batch Loss: 0.0036182578187435865\n",
      "Epoch 121, Loss: 0.24952153849881142, Final Batch Loss: 0.002094357507303357\n",
      "Epoch 122, Loss: 0.24937488976866007, Final Batch Loss: 0.004922178573906422\n",
      "Epoch 123, Loss: 0.26372190949041396, Final Batch Loss: 0.0035822326317429543\n",
      "Epoch 124, Loss: 0.24298545543570071, Final Batch Loss: 0.003480640472844243\n",
      "Epoch 125, Loss: 0.24168080347590148, Final Batch Loss: 0.003024375531822443\n",
      "Epoch 126, Loss: 0.24970400845631957, Final Batch Loss: 0.0015188823454082012\n",
      "Epoch 127, Loss: 0.24757634161505848, Final Batch Loss: 0.002597247250378132\n",
      "Epoch 128, Loss: 0.23783774464391172, Final Batch Loss: 0.0015571853145956993\n",
      "Epoch 129, Loss: 0.24611086351796985, Final Batch Loss: 0.0037169521674513817\n",
      "Epoch 130, Loss: 0.2453546728938818, Final Batch Loss: 0.0018352018669247627\n",
      "Epoch 131, Loss: 0.23239953257143497, Final Batch Loss: 0.0021061154548078775\n",
      "Epoch 132, Loss: 0.26021914335433394, Final Batch Loss: 0.001527411979623139\n",
      "Epoch 133, Loss: 0.24279008770827204, Final Batch Loss: 0.001787000335752964\n",
      "Epoch 134, Loss: 0.24745017453096807, Final Batch Loss: 0.0032314083073288202\n",
      "Epoch 135, Loss: 0.23808351682964712, Final Batch Loss: 0.003817897755652666\n",
      "Epoch 136, Loss: 0.25002099107950926, Final Batch Loss: 0.0022139237262308598\n",
      "Epoch 137, Loss: 0.23384862288367003, Final Batch Loss: 0.00244440627284348\n",
      "Epoch 138, Loss: 0.2356951521942392, Final Batch Loss: 0.0029477709904313087\n",
      "Epoch 139, Loss: 0.24858422251418233, Final Batch Loss: 0.005189867224544287\n",
      "Epoch 140, Loss: 0.22976316302083433, Final Batch Loss: 0.0018206931417807937\n",
      "Epoch 141, Loss: 0.23431436077225953, Final Batch Loss: 0.003434099955484271\n",
      "Epoch 142, Loss: 0.2293658962007612, Final Batch Loss: 0.002355041680857539\n",
      "Epoch 143, Loss: 0.23077375115826726, Final Batch Loss: 0.006173347122967243\n",
      "Epoch 144, Loss: 0.22992954007349908, Final Batch Loss: 0.0034704774152487516\n",
      "Epoch 145, Loss: 0.24456758645828813, Final Batch Loss: 0.0033411788754165173\n",
      "Epoch 146, Loss: 0.23501954320818186, Final Batch Loss: 0.0014801195356994867\n",
      "Epoch 147, Loss: 0.23668528872076422, Final Batch Loss: 0.0028833874966949224\n",
      "Epoch 148, Loss: 0.23042911710217595, Final Batch Loss: 0.0013359527802094817\n",
      "Epoch 149, Loss: 0.22628902294673026, Final Batch Loss: 0.0014889497542753816\n",
      "Epoch 150, Loss: 0.23307000019121915, Final Batch Loss: 0.0038568293675780296\n",
      "Epoch 151, Loss: 0.23275226482655853, Final Batch Loss: 0.0020048548467457294\n",
      "Epoch 152, Loss: 0.22250133380293846, Final Batch Loss: 0.0015055444091558456\n",
      "Epoch 153, Loss: 0.22547802882036194, Final Batch Loss: 0.0005070262704975903\n",
      "Epoch 154, Loss: 0.23301295202691108, Final Batch Loss: 0.003951194696128368\n",
      "Epoch 155, Loss: 0.23954487359151244, Final Batch Loss: 0.0032691038213670254\n",
      "Epoch 156, Loss: 0.23005830310285091, Final Batch Loss: 0.0035855018068104982\n",
      "Epoch 157, Loss: 0.2475474454695359, Final Batch Loss: 0.003802053164690733\n",
      "Epoch 158, Loss: 0.2286831373348832, Final Batch Loss: 0.0026540495455265045\n",
      "Epoch 159, Loss: 0.22270603477954865, Final Batch Loss: 0.0026381087955087423\n",
      "Epoch 160, Loss: 0.22942783101461828, Final Batch Loss: 0.0014643478207290173\n",
      "Epoch 161, Loss: 0.2329413639381528, Final Batch Loss: 0.001360859489068389\n",
      "Epoch 162, Loss: 0.23872570495586842, Final Batch Loss: 0.003851362969726324\n",
      "Epoch 163, Loss: 0.2305428086547181, Final Batch Loss: 0.002569287782534957\n",
      "Epoch 164, Loss: 0.2310097716981545, Final Batch Loss: 0.00228758342564106\n",
      "Epoch 165, Loss: 0.22206086886581033, Final Batch Loss: 0.0012742197141051292\n",
      "Epoch 166, Loss: 0.22714916011318564, Final Batch Loss: 0.0013173995539546013\n",
      "Epoch 167, Loss: 0.2249748727772385, Final Batch Loss: 0.0015571275725960732\n",
      "Epoch 168, Loss: 0.22692329436540604, Final Batch Loss: 0.006617940496653318\n",
      "Epoch 169, Loss: 0.22463476727716625, Final Batch Loss: 0.0010265028104186058\n",
      "Epoch 170, Loss: 0.2276956964051351, Final Batch Loss: 0.0033058421686291695\n",
      "Epoch 171, Loss: 0.2220226563513279, Final Batch Loss: 0.0016653189668431878\n",
      "Epoch 172, Loss: 0.23176463355775923, Final Batch Loss: 0.0015905641485005617\n",
      "Epoch 173, Loss: 0.22088921593967825, Final Batch Loss: 0.0012268350692465901\n",
      "Epoch 174, Loss: 0.22076265327632427, Final Batch Loss: 0.0011094730580225587\n",
      "Epoch 175, Loss: 0.2110813597100787, Final Batch Loss: 0.0014870447339490056\n",
      "Epoch 176, Loss: 0.21694115141872317, Final Batch Loss: 0.0023106762673705816\n",
      "Epoch 177, Loss: 0.2335417738649994, Final Batch Loss: 0.0020705428905785084\n",
      "Epoch 178, Loss: 0.22855159983737394, Final Batch Loss: 0.002167655387893319\n",
      "Epoch 179, Loss: 0.21511904778890312, Final Batch Loss: 0.0016536007169634104\n",
      "Epoch 180, Loss: 0.21628882875666022, Final Batch Loss: 0.0026364028453826904\n",
      "Epoch 181, Loss: 0.21410517871845514, Final Batch Loss: 0.0009614655282348394\n",
      "Epoch 182, Loss: 0.2278114657383412, Final Batch Loss: 0.0012957716826349497\n",
      "Epoch 183, Loss: 0.22230839577969164, Final Batch Loss: 0.0023908461444079876\n",
      "Epoch 184, Loss: 0.22585747600533068, Final Batch Loss: 0.0019523004302754998\n",
      "Epoch 185, Loss: 0.23080333846155554, Final Batch Loss: 0.0038295406848192215\n",
      "Epoch 186, Loss: 0.22160113009158522, Final Batch Loss: 0.0011181312147527933\n",
      "Epoch 187, Loss: 0.22059476759750396, Final Batch Loss: 0.0019502223003655672\n",
      "Epoch 188, Loss: 0.21618807571940124, Final Batch Loss: 0.0014862766256555915\n",
      "Epoch 189, Loss: 0.22339527891017497, Final Batch Loss: 0.0013100487412884831\n",
      "Epoch 190, Loss: 0.2281158013502136, Final Batch Loss: 0.004259824752807617\n",
      "Epoch 191, Loss: 0.22840880311559886, Final Batch Loss: 0.002680406905710697\n",
      "Epoch 192, Loss: 0.21538363152649254, Final Batch Loss: 0.0009566289372742176\n",
      "Epoch 193, Loss: 0.2139384433394298, Final Batch Loss: 0.001752782496623695\n",
      "Epoch 194, Loss: 0.21453950693830848, Final Batch Loss: 0.0023619518615305424\n",
      "Epoch 195, Loss: 0.21729255991522223, Final Batch Loss: 0.0011981558054685593\n",
      "Epoch 196, Loss: 0.21083064621780068, Final Batch Loss: 0.001685194205492735\n",
      "Epoch 197, Loss: 0.21990711987018585, Final Batch Loss: 0.0018506774213165045\n",
      "Epoch 198, Loss: 0.22687809146009386, Final Batch Loss: 0.0014454929623752832\n",
      "Epoch 199, Loss: 0.2261020839214325, Final Batch Loss: 0.0040658386424183846\n",
      "Epoch 200, Loss: 0.20734618592541665, Final Batch Loss: 0.002342948690056801\n",
      "Epoch 201, Loss: 0.2151269526220858, Final Batch Loss: 0.0062020039185881615\n",
      "Epoch 202, Loss: 0.21908139763399959, Final Batch Loss: 0.004160065203905106\n",
      "Epoch 203, Loss: 0.22727088595274836, Final Batch Loss: 0.0015694806352257729\n",
      "Epoch 204, Loss: 0.21111138199921697, Final Batch Loss: 0.0016585716512054205\n",
      "Epoch 205, Loss: 0.22004763002041727, Final Batch Loss: 0.0031754462979733944\n",
      "Epoch 206, Loss: 0.2145898041781038, Final Batch Loss: 0.003691314486786723\n",
      "Epoch 207, Loss: 0.2158664318267256, Final Batch Loss: 0.0015783736016601324\n",
      "Epoch 208, Loss: 0.2163990251137875, Final Batch Loss: 0.0030906726606190205\n",
      "Epoch 209, Loss: 0.21237906371243298, Final Batch Loss: 0.0014915557112544775\n",
      "Epoch 210, Loss: 0.22202509955968708, Final Batch Loss: 0.004400508478283882\n",
      "Epoch 211, Loss: 0.20803826383780688, Final Batch Loss: 0.003651650156825781\n",
      "Epoch 212, Loss: 0.22026268090121448, Final Batch Loss: 0.003631405998021364\n",
      "Epoch 213, Loss: 0.21515041071688756, Final Batch Loss: 0.0021040020510554314\n",
      "Epoch 214, Loss: 0.21938600356224924, Final Batch Loss: 0.0012548031518235803\n",
      "Epoch 215, Loss: 0.20476594561478123, Final Batch Loss: 0.004032562952488661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216, Loss: 0.22143799415789545, Final Batch Loss: 0.002902839332818985\n",
      "Epoch 217, Loss: 0.23193302127765492, Final Batch Loss: 0.002312529133632779\n",
      "Epoch 218, Loss: 0.21259832801297307, Final Batch Loss: 0.0023833431769162416\n",
      "Epoch 219, Loss: 0.2348760743625462, Final Batch Loss: 0.0009951414540410042\n",
      "Epoch 220, Loss: 0.22511456755455583, Final Batch Loss: 0.0019230649340897799\n",
      "Epoch 221, Loss: 0.21682122815400362, Final Batch Loss: 0.002590067218989134\n",
      "Epoch 222, Loss: 0.21718913794029504, Final Batch Loss: 0.00374411279335618\n",
      "Epoch 223, Loss: 0.21865685458760709, Final Batch Loss: 0.003663575742393732\n",
      "Epoch 224, Loss: 0.21434394363313913, Final Batch Loss: 0.00455178739503026\n",
      "Epoch 225, Loss: 0.20873909606598318, Final Batch Loss: 0.002308481140062213\n",
      "Epoch 226, Loss: 0.2077296235365793, Final Batch Loss: 0.002978629432618618\n",
      "Epoch 227, Loss: 0.21335419651586562, Final Batch Loss: 0.003520447760820389\n",
      "Epoch 228, Loss: 0.20551384787540883, Final Batch Loss: 0.001485534361563623\n",
      "Epoch 229, Loss: 0.20701727061532438, Final Batch Loss: 0.0023877720814198256\n",
      "Epoch 230, Loss: 0.21064579812809825, Final Batch Loss: 0.0017438811482861638\n",
      "Epoch 231, Loss: 0.23882816859986633, Final Batch Loss: 0.0018647076794877648\n",
      "Epoch 232, Loss: 0.24408663576468825, Final Batch Loss: 0.002708244603127241\n",
      "Epoch 233, Loss: 0.2169346718583256, Final Batch Loss: 0.0018157127778977156\n",
      "Epoch 234, Loss: 0.21606582729145885, Final Batch Loss: 0.0031252384651452303\n",
      "Epoch 235, Loss: 0.22491141583304852, Final Batch Loss: 0.0037396415136754513\n",
      "Epoch 236, Loss: 0.211757602985017, Final Batch Loss: 0.0031789683271199465\n",
      "Epoch 237, Loss: 0.2101508853957057, Final Batch Loss: 0.0021368665620684624\n",
      "Epoch 238, Loss: 0.2788170105777681, Final Batch Loss: 0.0034965218510478735\n",
      "Epoch 239, Loss: 0.21163720032200217, Final Batch Loss: 0.002372471848502755\n",
      "Epoch 240, Loss: 0.21001792087918147, Final Batch Loss: 0.002861306071281433\n",
      "Epoch 241, Loss: 0.20367992826504633, Final Batch Loss: 0.0007388925296254456\n",
      "Epoch 242, Loss: 0.20136090292362496, Final Batch Loss: 0.0022891988046467304\n",
      "Epoch 243, Loss: 0.21220579731743783, Final Batch Loss: 0.0028171243611723185\n",
      "Epoch 244, Loss: 0.2100024683168158, Final Batch Loss: 0.001640841830521822\n",
      "Epoch 245, Loss: 0.20135710667818785, Final Batch Loss: 0.001962746726348996\n",
      "Epoch 246, Loss: 0.20734494482167065, Final Batch Loss: 0.0016248134197667241\n",
      "Epoch 247, Loss: 0.19956066785380244, Final Batch Loss: 0.002334506018087268\n",
      "Epoch 248, Loss: 0.2042850179132074, Final Batch Loss: 0.001836847746744752\n",
      "Epoch 249, Loss: 0.20486276596784592, Final Batch Loss: 0.002072095638141036\n",
      "Epoch 250, Loss: 0.2143130066106096, Final Batch Loss: 0.000988406827673316\n",
      "Epoch 251, Loss: 0.20827404782176018, Final Batch Loss: 0.0009826289024204016\n",
      "Epoch 252, Loss: 0.2136568286223337, Final Batch Loss: 0.0009894856484606862\n",
      "Epoch 253, Loss: 0.21122314827516675, Final Batch Loss: 0.002301298314705491\n",
      "Epoch 254, Loss: 0.21695384080521762, Final Batch Loss: 0.001474652555771172\n",
      "Epoch 255, Loss: 0.25151565682608634, Final Batch Loss: 0.004428587853908539\n",
      "Epoch 256, Loss: 0.21160107431933284, Final Batch Loss: 0.005199155304580927\n",
      "Epoch 257, Loss: 0.21110339229926467, Final Batch Loss: 0.0013399567687883973\n",
      "Epoch 258, Loss: 0.20526361803058535, Final Batch Loss: 0.0015575914876535535\n",
      "Epoch 259, Loss: 0.20311544951982796, Final Batch Loss: 0.0024155208375304937\n",
      "Epoch 260, Loss: 0.19940251886146143, Final Batch Loss: 0.0016981731168925762\n",
      "Epoch 261, Loss: 0.20501827471889555, Final Batch Loss: 0.004185961093753576\n",
      "Epoch 262, Loss: 0.20085323241073638, Final Batch Loss: 0.0014410638250410557\n",
      "Epoch 263, Loss: 0.21715174452401698, Final Batch Loss: 0.0022672417107969522\n",
      "Epoch 264, Loss: 0.2168609902728349, Final Batch Loss: 0.0021183977369219065\n",
      "Epoch 265, Loss: 0.20408144185785204, Final Batch Loss: 0.0017415524926036596\n",
      "Epoch 266, Loss: 0.20788910891860723, Final Batch Loss: 0.0017787098186090589\n",
      "Epoch 267, Loss: 0.20417865400668234, Final Batch Loss: 0.003103305585682392\n",
      "Epoch 268, Loss: 0.21689896780299023, Final Batch Loss: 0.0018395384540781379\n",
      "Epoch 269, Loss: 0.21519289282150567, Final Batch Loss: 0.0035781513433903456\n",
      "Epoch 270, Loss: 0.1996771020349115, Final Batch Loss: 0.0029635345563292503\n",
      "Epoch 271, Loss: 0.21387157693970948, Final Batch Loss: 0.002306358888745308\n",
      "Epoch 272, Loss: 0.20014702796470374, Final Batch Loss: 0.0020142211578786373\n",
      "Epoch 273, Loss: 0.206759326858446, Final Batch Loss: 0.0022385306656360626\n",
      "Epoch 274, Loss: 0.20409636746626347, Final Batch Loss: 0.001046448014676571\n",
      "Epoch 275, Loss: 0.21069809002801776, Final Batch Loss: 0.0030039821285754442\n",
      "Epoch 276, Loss: 0.20282379514537752, Final Batch Loss: 0.0011986829340457916\n",
      "Epoch 277, Loss: 0.206299441982992, Final Batch Loss: 0.002299975836649537\n",
      "Epoch 278, Loss: 0.2033819608623162, Final Batch Loss: 0.002020389772951603\n",
      "Epoch 279, Loss: 0.2054232171503827, Final Batch Loss: 0.002510606311261654\n",
      "Epoch 280, Loss: 0.1997489136410877, Final Batch Loss: 0.0016754315001890063\n",
      "Epoch 281, Loss: 0.208803144749254, Final Batch Loss: 0.001439618761651218\n",
      "Epoch 282, Loss: 0.20675229164771736, Final Batch Loss: 0.0017319803591817617\n",
      "Epoch 283, Loss: 0.20453505998011678, Final Batch Loss: 0.0010742279700934887\n",
      "Epoch 284, Loss: 0.1964660753728822, Final Batch Loss: 0.001537158153951168\n",
      "Epoch 285, Loss: 0.21643042168579996, Final Batch Loss: 0.0017879182705655694\n",
      "Epoch 286, Loss: 0.2095402335980907, Final Batch Loss: 0.005491526331752539\n",
      "Epoch 287, Loss: 0.21026524843182415, Final Batch Loss: 0.0031468889210373163\n",
      "Epoch 288, Loss: 0.2068030049558729, Final Batch Loss: 0.002733812667429447\n",
      "Epoch 289, Loss: 0.2021517682587728, Final Batch Loss: 0.003243433777242899\n",
      "Epoch 290, Loss: 0.20427487406414002, Final Batch Loss: 0.001944802817888558\n",
      "Epoch 291, Loss: 0.19452856271527708, Final Batch Loss: 0.0008043911075219512\n",
      "Epoch 292, Loss: 0.22331762791145593, Final Batch Loss: 0.002804424846544862\n",
      "Epoch 293, Loss: 0.19859270786400884, Final Batch Loss: 0.004476231522858143\n",
      "Epoch 294, Loss: 0.20228651160141453, Final Batch Loss: 0.0006363348220475018\n",
      "Epoch 295, Loss: 0.21112758934032172, Final Batch Loss: 0.0016213181661441922\n",
      "Epoch 296, Loss: 0.21046946197748184, Final Batch Loss: 0.0022609566804021597\n",
      "Epoch 297, Loss: 0.19861889840103686, Final Batch Loss: 0.0015791971236467361\n",
      "Epoch 298, Loss: 0.20757183490786701, Final Batch Loss: 0.0020757783204317093\n",
      "Epoch 299, Loss: 0.21856525912880898, Final Batch Loss: 0.0012820366537198424\n",
      "Epoch 300, Loss: 0.2052360416855663, Final Batch Loss: 0.003441519569605589\n",
      "Epoch 301, Loss: 0.207042045192793, Final Batch Loss: 0.0035461103543639183\n",
      "Epoch 302, Loss: 0.20150366763118654, Final Batch Loss: 0.0016780502628535032\n",
      "Epoch 303, Loss: 0.19941059802658856, Final Batch Loss: 0.002572959056124091\n",
      "Epoch 304, Loss: 0.20473463693633676, Final Batch Loss: 0.0010552521562203765\n",
      "Epoch 305, Loss: 0.22303315962199122, Final Batch Loss: 0.001333745545707643\n",
      "Epoch 306, Loss: 0.21298073744401336, Final Batch Loss: 0.0011601313017308712\n",
      "Epoch 307, Loss: 0.207588855875656, Final Batch Loss: 0.0025568229611963034\n",
      "Epoch 308, Loss: 0.20094387559220195, Final Batch Loss: 0.0005837079952470958\n",
      "Epoch 309, Loss: 0.1994214110309258, Final Batch Loss: 0.0017399012576788664\n",
      "Epoch 310, Loss: 0.1986522285733372, Final Batch Loss: 0.0020026934798806906\n",
      "Epoch 311, Loss: 0.21763636375544593, Final Batch Loss: 0.003146932227537036\n",
      "Epoch 312, Loss: 0.19530075683724135, Final Batch Loss: 0.0020792505238205194\n",
      "Epoch 313, Loss: 0.20258055807789788, Final Batch Loss: 0.002037414349615574\n",
      "Epoch 314, Loss: 0.19549686380196363, Final Batch Loss: 0.0015382897108793259\n",
      "Epoch 315, Loss: 0.20604858844308183, Final Batch Loss: 0.002876212587580085\n",
      "Epoch 316, Loss: 0.19627470226259902, Final Batch Loss: 0.0016857803566381335\n",
      "Epoch 317, Loss: 0.22292363794986159, Final Batch Loss: 0.00327979470603168\n",
      "Epoch 318, Loss: 0.2017421891214326, Final Batch Loss: 0.003325489815324545\n",
      "Epoch 319, Loss: 0.20370637346059084, Final Batch Loss: 0.0025196548085659742\n",
      "Epoch 320, Loss: 0.20585820160340518, Final Batch Loss: 0.002129166852682829\n",
      "Epoch 321, Loss: 0.20012072275858372, Final Batch Loss: 0.003170446492731571\n",
      "Epoch 322, Loss: 0.21638296521268785, Final Batch Loss: 0.003132052021101117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323, Loss: 0.20671535737346858, Final Batch Loss: 0.002301756991073489\n",
      "Epoch 324, Loss: 0.19528748176526278, Final Batch Loss: 0.0018511466914787889\n",
      "Epoch 325, Loss: 0.2003816991345957, Final Batch Loss: 0.0016221503028646111\n",
      "Epoch 326, Loss: 0.20049984788056463, Final Batch Loss: 0.003564555896446109\n",
      "Epoch 327, Loss: 0.19712448865175247, Final Batch Loss: 0.0037062629126012325\n",
      "Epoch 328, Loss: 0.2092071472434327, Final Batch Loss: 0.0023169792257249355\n",
      "Epoch 329, Loss: 0.20033253147266805, Final Batch Loss: 0.0031040948815643787\n",
      "Epoch 330, Loss: 0.20039840310346335, Final Batch Loss: 0.0019205068238079548\n",
      "Epoch 331, Loss: 0.20291962276678532, Final Batch Loss: 0.0020440544467419386\n",
      "Epoch 332, Loss: 0.19985499104950577, Final Batch Loss: 0.0026715027634054422\n",
      "Epoch 333, Loss: 0.2054916974157095, Final Batch Loss: 0.0044033280573785305\n",
      "Epoch 334, Loss: 0.19887482712510973, Final Batch Loss: 0.0006875409744679928\n",
      "Epoch 335, Loss: 0.22134141117567196, Final Batch Loss: 0.0009529714589007199\n",
      "Epoch 336, Loss: 0.20217934739775956, Final Batch Loss: 0.0019545445684343576\n",
      "Epoch 337, Loss: 0.202988134638872, Final Batch Loss: 0.0008774426532909274\n",
      "Epoch 338, Loss: 0.1959039052017033, Final Batch Loss: 0.005502687767148018\n",
      "Epoch 339, Loss: 0.1938885328709148, Final Batch Loss: 0.0009003370068967342\n",
      "Epoch 340, Loss: 0.2285963277099654, Final Batch Loss: 0.0030451142229139805\n",
      "Epoch 341, Loss: 0.22184018592815846, Final Batch Loss: 0.003561182878911495\n",
      "Epoch 342, Loss: 0.20639483665581793, Final Batch Loss: 0.002453228924423456\n",
      "Epoch 343, Loss: 0.20266389503376558, Final Batch Loss: 0.004217451438307762\n",
      "Epoch 344, Loss: 0.2037048552883789, Final Batch Loss: 0.0010977920610457659\n",
      "Epoch 345, Loss: 0.19622625468764454, Final Batch Loss: 0.002129851607605815\n",
      "Epoch 346, Loss: 0.21473877562675625, Final Batch Loss: 0.004698463715612888\n",
      "Epoch 347, Loss: 0.20659564342349768, Final Batch Loss: 0.010598461143672466\n",
      "Epoch 348, Loss: 0.19915344170294702, Final Batch Loss: 0.0017589613562449813\n",
      "Epoch 349, Loss: 0.20857226382941008, Final Batch Loss: 0.0015749757876619697\n",
      "Epoch 350, Loss: 0.21625192073406652, Final Batch Loss: 0.0014741597697138786\n",
      "Epoch 351, Loss: 0.20610893552657217, Final Batch Loss: 0.0012077377177774906\n",
      "Epoch 352, Loss: 0.20097523112781346, Final Batch Loss: 0.0013580964878201485\n",
      "Epoch 353, Loss: 0.20255249727051705, Final Batch Loss: 0.0037702550180256367\n",
      "Epoch 354, Loss: 0.20257322635734454, Final Batch Loss: 0.0008824661490507424\n",
      "Epoch 355, Loss: 0.19946860685013235, Final Batch Loss: 0.002792239421978593\n",
      "Epoch 356, Loss: 0.2026676778914407, Final Batch Loss: 0.0027664359658956528\n",
      "Epoch 357, Loss: 0.19759317918214947, Final Batch Loss: 0.002858293941244483\n",
      "Epoch 358, Loss: 0.20390063186641783, Final Batch Loss: 0.003686234587803483\n",
      "Epoch 359, Loss: 0.22094595502130687, Final Batch Loss: 0.001093051745556295\n",
      "Epoch 360, Loss: 0.20183156779967248, Final Batch Loss: 0.004353937692940235\n",
      "Epoch 361, Loss: 0.21472502243705094, Final Batch Loss: 0.0032464037649333477\n",
      "Epoch 362, Loss: 0.19466202822513878, Final Batch Loss: 0.0035144721623510122\n",
      "Epoch 363, Loss: 0.2012566216289997, Final Batch Loss: 0.0011529017938300967\n",
      "Epoch 364, Loss: 0.22852393536595628, Final Batch Loss: 0.0009122058399952948\n",
      "Epoch 365, Loss: 0.21066946035716683, Final Batch Loss: 0.001942357630468905\n",
      "Epoch 366, Loss: 0.19420823239488527, Final Batch Loss: 0.000537577026989311\n",
      "Epoch 367, Loss: 0.200100410846062, Final Batch Loss: 0.0011015188647434115\n",
      "Epoch 368, Loss: 0.2155640427954495, Final Batch Loss: 0.0014868017751723528\n",
      "Epoch 369, Loss: 0.19523777608992532, Final Batch Loss: 0.0006414453382603824\n",
      "Epoch 370, Loss: 0.1977401071926579, Final Batch Loss: 0.001722487504594028\n",
      "Epoch 371, Loss: 0.19650786241982132, Final Batch Loss: 0.0031312049832195044\n",
      "Epoch 372, Loss: 0.195778421824798, Final Batch Loss: 0.0018438007682561874\n",
      "Epoch 373, Loss: 0.2720326540293172, Final Batch Loss: 0.002113616792485118\n",
      "Epoch 374, Loss: 0.20057848491705954, Final Batch Loss: 0.002029742579907179\n",
      "Epoch 375, Loss: 0.19560326694045216, Final Batch Loss: 0.003407893003895879\n",
      "Epoch 376, Loss: 0.21176231978461146, Final Batch Loss: 0.0021949647925794125\n",
      "Epoch 377, Loss: 0.19673765916377306, Final Batch Loss: 0.0013293857919052243\n",
      "Epoch 378, Loss: 0.2046697923215106, Final Batch Loss: 0.0015200675697997212\n",
      "Epoch 379, Loss: 0.2876415327191353, Final Batch Loss: 0.0023217438720166683\n",
      "Epoch 380, Loss: 0.20440433628391474, Final Batch Loss: 0.0015691370936110616\n",
      "Epoch 381, Loss: 0.20741951477248222, Final Batch Loss: 0.0017854374600574374\n",
      "Epoch 382, Loss: 0.1890480675501749, Final Batch Loss: 0.002420977456495166\n",
      "Epoch 383, Loss: 0.20918022136902437, Final Batch Loss: 0.0037335287779569626\n",
      "Epoch 384, Loss: 0.19911391951609403, Final Batch Loss: 0.0011061734985560179\n",
      "Epoch 385, Loss: 0.19900598493404686, Final Batch Loss: 0.0028126754332333803\n",
      "Epoch 386, Loss: 0.2058800868690014, Final Batch Loss: 0.0013092253357172012\n",
      "Epoch 387, Loss: 0.20049670711159706, Final Batch Loss: 0.0012231034925207496\n",
      "Epoch 388, Loss: 0.2014785678475164, Final Batch Loss: 0.0009180305642075837\n",
      "Epoch 389, Loss: 0.2001159299397841, Final Batch Loss: 0.0015662368386983871\n",
      "Epoch 390, Loss: 0.19433801824925467, Final Batch Loss: 0.0008413208997808397\n",
      "Epoch 391, Loss: 0.19413239264395088, Final Batch Loss: 0.004695163108408451\n",
      "Epoch 392, Loss: 0.19385036872699857, Final Batch Loss: 0.001654363819397986\n",
      "Epoch 393, Loss: 0.23288627655711025, Final Batch Loss: 0.0029428203124552965\n",
      "Epoch 394, Loss: 0.20442308951169252, Final Batch Loss: 0.002034552861005068\n",
      "Epoch 395, Loss: 0.20811869483441114, Final Batch Loss: 0.005241216626018286\n",
      "Epoch 396, Loss: 0.2067079630214721, Final Batch Loss: 0.0012204718077555299\n",
      "Epoch 397, Loss: 0.2041447894880548, Final Batch Loss: 0.0012867288896813989\n",
      "Epoch 398, Loss: 0.2034140444593504, Final Batch Loss: 0.001135965809226036\n",
      "Epoch 399, Loss: 0.20596417132765055, Final Batch Loss: 0.003180831903591752\n",
      "Epoch 400, Loss: 0.2118175581563264, Final Batch Loss: 0.0016176312929019332\n",
      "Epoch 401, Loss: 0.2023394083371386, Final Batch Loss: 0.0023944354616105556\n",
      "Epoch 402, Loss: 0.19683955539949238, Final Batch Loss: 0.0029254842083901167\n",
      "Epoch 403, Loss: 0.19656890479382128, Final Batch Loss: 0.0006128688110038638\n",
      "Epoch 404, Loss: 0.2010187195846811, Final Batch Loss: 0.003452325239777565\n",
      "Epoch 405, Loss: 0.2019594560842961, Final Batch Loss: 0.002729564905166626\n",
      "Epoch 406, Loss: 0.18277090293122455, Final Batch Loss: 0.0006741786492057145\n",
      "Epoch 407, Loss: 0.1945450256462209, Final Batch Loss: 0.0021618239115923643\n",
      "Epoch 408, Loss: 0.19708673789864406, Final Batch Loss: 0.0018224987434223294\n",
      "Epoch 409, Loss: 0.1956945703132078, Final Batch Loss: 0.0015408194158226252\n",
      "Epoch 410, Loss: 0.2022140866611153, Final Batch Loss: 0.0025384481996297836\n",
      "Epoch 411, Loss: 0.22779150609858334, Final Batch Loss: 0.0017718117451295257\n",
      "Epoch 412, Loss: 0.2045058144722134, Final Batch Loss: 0.002249299781396985\n",
      "Epoch 413, Loss: 0.20770097960485145, Final Batch Loss: 0.0009489334188401699\n",
      "Epoch 414, Loss: 0.20799390616593882, Final Batch Loss: 0.003737053135409951\n",
      "Epoch 415, Loss: 0.20113359356764704, Final Batch Loss: 0.0014406584668904543\n",
      "Epoch 416, Loss: 0.19913281966000795, Final Batch Loss: 0.0019634529016911983\n",
      "Epoch 417, Loss: 0.2001459978055209, Final Batch Loss: 0.0024813651107251644\n",
      "Epoch 418, Loss: 0.2044866521609947, Final Batch Loss: 0.0008115585660561919\n",
      "Epoch 419, Loss: 0.19713020708877593, Final Batch Loss: 0.0020449364092200994\n",
      "Epoch 420, Loss: 0.20889390038792044, Final Batch Loss: 0.0029584835283458233\n",
      "Epoch 421, Loss: 0.2211656478466466, Final Batch Loss: 0.003213627729564905\n",
      "Epoch 422, Loss: 0.1968901890795678, Final Batch Loss: 0.001720556989312172\n",
      "Epoch 423, Loss: 0.20033473311923444, Final Batch Loss: 0.001355963060632348\n",
      "Epoch 424, Loss: 0.21090684458613396, Final Batch Loss: 0.001876942114904523\n",
      "Epoch 425, Loss: 0.2086822788696736, Final Batch Loss: 0.0009457709966227412\n",
      "Epoch 426, Loss: 0.19468541262904182, Final Batch Loss: 0.002955310745164752\n",
      "Epoch 427, Loss: 0.195755067281425, Final Batch Loss: 0.0008077387465164065\n",
      "Epoch 428, Loss: 0.19666185486130416, Final Batch Loss: 0.0013366867788136005\n",
      "Epoch 429, Loss: 0.19463241507764906, Final Batch Loss: 0.00232558511197567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430, Loss: 0.1964554728474468, Final Batch Loss: 0.0034637723583728075\n",
      "Epoch 431, Loss: 0.2021623527398333, Final Batch Loss: 0.00089608458802104\n",
      "Epoch 432, Loss: 0.20581460266839713, Final Batch Loss: 0.003685278119519353\n",
      "Epoch 433, Loss: 0.21167204604716972, Final Batch Loss: 0.000683001650031656\n",
      "Epoch 434, Loss: 0.21043368126265705, Final Batch Loss: 0.003914377186447382\n",
      "Epoch 435, Loss: 0.2267371986526996, Final Batch Loss: 0.002888280898332596\n",
      "Epoch 436, Loss: 0.20962845045141876, Final Batch Loss: 0.0024080348666757345\n",
      "Epoch 437, Loss: 0.2122871621977538, Final Batch Loss: 0.0022521463688462973\n",
      "Epoch 438, Loss: 0.2197145795216784, Final Batch Loss: 0.0029941832181066275\n",
      "Epoch 439, Loss: 0.19185004371684045, Final Batch Loss: 0.0030278675258159637\n",
      "Epoch 440, Loss: 0.20397809229325503, Final Batch Loss: 0.002391932299360633\n",
      "Epoch 441, Loss: 0.19436105445493013, Final Batch Loss: 0.001369359204545617\n",
      "Epoch 442, Loss: 0.20485488523263484, Final Batch Loss: 0.002406779443845153\n",
      "Epoch 443, Loss: 0.1930401388090104, Final Batch Loss: 0.0015731172170490026\n",
      "Epoch 444, Loss: 0.19444276473950595, Final Batch Loss: 0.002483825199306011\n",
      "Epoch 445, Loss: 0.19453464343678206, Final Batch Loss: 0.002434915630146861\n",
      "Epoch 446, Loss: 0.19691468821838498, Final Batch Loss: 0.0018151018302887678\n",
      "Epoch 447, Loss: 0.1975513330544345, Final Batch Loss: 0.003164310473948717\n",
      "Epoch 448, Loss: 0.19527076388476416, Final Batch Loss: 0.0010566932614892721\n",
      "Epoch 449, Loss: 0.19543592282570899, Final Batch Loss: 0.0013241006527096033\n",
      "Epoch 450, Loss: 0.1897395676933229, Final Batch Loss: 0.002295444021001458\n",
      "Epoch 451, Loss: 0.18967989698285237, Final Batch Loss: 0.0015377977397292852\n",
      "Epoch 452, Loss: 0.19279536651447415, Final Batch Loss: 0.0011137461988255382\n",
      "Epoch 453, Loss: 0.21720746310893446, Final Batch Loss: 0.0014315387234091759\n",
      "Epoch 454, Loss: 0.20282364659942687, Final Batch Loss: 0.003043108619749546\n",
      "Epoch 455, Loss: 0.20048366615083069, Final Batch Loss: 0.0008986332104541361\n",
      "Epoch 456, Loss: 0.21278710768092424, Final Batch Loss: 0.003368798643350601\n",
      "Epoch 457, Loss: 0.20219749363604933, Final Batch Loss: 0.005838395562022924\n",
      "Epoch 458, Loss: 0.19224616611609235, Final Batch Loss: 0.005163164343684912\n",
      "Epoch 459, Loss: 0.1905177814187482, Final Batch Loss: 0.001112046418711543\n",
      "Epoch 460, Loss: 0.2033069704193622, Final Batch Loss: 0.0022087765391916037\n",
      "Epoch 461, Loss: 0.19585197139531374, Final Batch Loss: 0.00223823101259768\n",
      "Epoch 462, Loss: 0.1907476985361427, Final Batch Loss: 0.0022764347959309816\n",
      "Epoch 463, Loss: 0.1975343098747544, Final Batch Loss: 0.0015252226730808616\n",
      "Epoch 464, Loss: 0.19356650486588478, Final Batch Loss: 0.0013776421546936035\n",
      "Epoch 465, Loss: 0.19149808876682073, Final Batch Loss: 0.0015860211569815874\n",
      "Epoch 466, Loss: 0.19965557765681297, Final Batch Loss: 0.003078701440244913\n",
      "Epoch 467, Loss: 0.19430576090235263, Final Batch Loss: 0.0022708347532898188\n",
      "Epoch 468, Loss: 0.18367120274342597, Final Batch Loss: 0.001279034768231213\n",
      "Epoch 469, Loss: 0.19927073596045375, Final Batch Loss: 0.007969710044562817\n",
      "Epoch 470, Loss: 0.1910573443165049, Final Batch Loss: 0.0014371693832799792\n",
      "Epoch 471, Loss: 0.19508319476153702, Final Batch Loss: 0.0008253423729911447\n",
      "Epoch 472, Loss: 0.19314687931910157, Final Batch Loss: 0.0013431168626993895\n",
      "Epoch 473, Loss: 0.19189859041944146, Final Batch Loss: 0.003666681470349431\n",
      "Epoch 474, Loss: 0.22125482570845634, Final Batch Loss: 0.0029222085140645504\n",
      "Epoch 475, Loss: 0.1996436263434589, Final Batch Loss: 0.002347823465242982\n",
      "Epoch 476, Loss: 0.18934200087096542, Final Batch Loss: 0.005779181607067585\n",
      "Epoch 477, Loss: 0.21016187442000955, Final Batch Loss: 0.0022433206904679537\n",
      "Epoch 478, Loss: 0.20720933529082686, Final Batch Loss: 0.0029085080605000257\n",
      "Epoch 479, Loss: 0.23871019668877125, Final Batch Loss: 0.0034129645209759474\n",
      "Epoch 480, Loss: 0.21367271384224296, Final Batch Loss: 0.0025303191505372524\n",
      "Epoch 481, Loss: 0.20653916022274643, Final Batch Loss: 0.0016903365030884743\n",
      "Epoch 482, Loss: 0.20391435222700238, Final Batch Loss: 0.003502505598589778\n",
      "Epoch 483, Loss: 0.1932605387410149, Final Batch Loss: 0.0010097844060510397\n",
      "Epoch 484, Loss: 0.1947217343840748, Final Batch Loss: 0.002242784481495619\n",
      "Epoch 485, Loss: 0.19156773365102708, Final Batch Loss: 0.0027091586962342262\n",
      "Epoch 486, Loss: 0.20219328289385885, Final Batch Loss: 0.003742282511666417\n",
      "Epoch 487, Loss: 0.19766912877094, Final Batch Loss: 0.001203708816319704\n",
      "Epoch 488, Loss: 0.19275703595485538, Final Batch Loss: 0.0025249726604670286\n",
      "Epoch 489, Loss: 0.2003458859398961, Final Batch Loss: 0.0022573613096028566\n",
      "Epoch 490, Loss: 0.19090426561888307, Final Batch Loss: 0.002078738994896412\n",
      "Epoch 491, Loss: 0.195667200780008, Final Batch Loss: 0.00338396686129272\n",
      "Epoch 492, Loss: 0.19931366806849837, Final Batch Loss: 0.0016507908003404737\n",
      "Epoch 493, Loss: 0.18661750620231032, Final Batch Loss: 0.0012063049944117665\n",
      "Epoch 494, Loss: 0.18780332908499986, Final Batch Loss: 0.001104591297917068\n",
      "Epoch 495, Loss: 0.18239795148838311, Final Batch Loss: 0.001398517400957644\n",
      "Epoch 496, Loss: 0.18781709566246718, Final Batch Loss: 0.002146334620192647\n",
      "Epoch 497, Loss: 0.1968023629160598, Final Batch Loss: 0.0010427596280351281\n",
      "Epoch 498, Loss: 0.1944467811845243, Final Batch Loss: 0.0017579362029209733\n",
      "Epoch 499, Loss: 0.18816957040689886, Final Batch Loss: 0.0009817925747483969\n",
      "Epoch 500, Loss: 0.19388413545675576, Final Batch Loss: 0.0010469231056049466\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        #_, encoded_labels = torch.max(labels, dim = 1) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')\n",
    "#     softmax = nn.Softmax(dim = 1)\n",
    "#     _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "#     _, answers = torch.max(labels, dim = 1)\n",
    "#     print(metrics.confusion_matrix(answers, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619  36  13]\n",
      " [ 22 310   0]\n",
      " [ 32   3  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.920     0.927     0.923       668\n",
      "           1      0.888     0.934     0.910       332\n",
      "           2      0.711     0.478     0.571        67\n",
      "\n",
      "    accuracy                          0.901      1067\n",
      "   macro avg      0.840     0.779     0.802      1067\n",
      "weighted avg      0.897     0.901     0.897      1067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    _, answers = torch.max(labels, dim = 1)\n",
    "    print(metrics.confusion_matrix(answers, preds))\n",
    "    print(metrics.classification_report(answers, preds, digits = 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
