{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>label:STAIRS_-_GOING_DOWN</th>\n",
       "      <th>label:ELEVATOR</th>\n",
       "      <th>label:OR_standing</th>\n",
       "      <th>label:AT_SCHOOL</th>\n",
       "      <th>label:PHONE_IN_HAND</th>\n",
       "      <th>label:PHONE_IN_BAG</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "      <th>label:WITH_CO-WORKERS</th>\n",
       "      <th>label:WITH_FRIENDS</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079161</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.995203</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.998502</td>\n",
       "      <td>1.748756</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079221</td>\n",
       "      <td>0.996864</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>1.935573</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079281</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.994797</td>\n",
       "      <td>0.996614</td>\n",
       "      <td>0.998704</td>\n",
       "      <td>2.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079341</td>\n",
       "      <td>0.996874</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>0.995050</td>\n",
       "      <td>0.996907</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>1.865318</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079431</td>\n",
       "      <td>0.997371</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.102332</td>\n",
       "      <td>0.995548</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.460806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   UUID   timestamp  \\\n",
       "0  00EABED2-271D-49D8-B599-1D4A09240601  1444079161   \n",
       "1  00EABED2-271D-49D8-B599-1D4A09240601  1444079221   \n",
       "2  00EABED2-271D-49D8-B599-1D4A09240601  1444079281   \n",
       "3  00EABED2-271D-49D8-B599-1D4A09240601  1444079341   \n",
       "4  00EABED2-271D-49D8-B599-1D4A09240601  1444079431   \n",
       "\n",
       "   raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0                      0.996815                     0.003529   \n",
       "1                      0.996864                     0.004172   \n",
       "2                      0.996825                     0.003667   \n",
       "3                      0.996874                     0.003541   \n",
       "4                      0.997371                     0.037653   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.002786                         0.006496   \n",
       "1                        -0.003110                         0.007050   \n",
       "2                         0.003094                         0.006076   \n",
       "3                         0.000626                         0.006059   \n",
       "4                         0.043389                         0.102332   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              0.995203                              0.996825   \n",
       "1                              0.994957                              0.996981   \n",
       "2                              0.994797                              0.996614   \n",
       "3                              0.995050                              0.996907   \n",
       "4                              0.995548                              0.996860   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              0.998502   \n",
       "1                              0.998766   \n",
       "2                              0.998704   \n",
       "3                              0.998690   \n",
       "4                              0.998205   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  ...  label:STAIRS_-_GOING_DOWN  \\\n",
       "0                               1.748756  ...                        NaN   \n",
       "1                               1.935573  ...                        NaN   \n",
       "2                               2.031780  ...                        NaN   \n",
       "3                               1.865318  ...                        NaN   \n",
       "4                               0.460806  ...                        NaN   \n",
       "\n",
       "   label:ELEVATOR  label:OR_standing  label:AT_SCHOOL  label:PHONE_IN_HAND  \\\n",
       "0             NaN                0.0              NaN                  NaN   \n",
       "1             NaN                0.0              NaN                  NaN   \n",
       "2             NaN                0.0              NaN                  NaN   \n",
       "3             NaN                0.0              NaN                  NaN   \n",
       "4             NaN                0.0              NaN                  NaN   \n",
       "\n",
       "   label:PHONE_IN_BAG  label:PHONE_ON_TABLE  label:WITH_CO-WORKERS  \\\n",
       "0                 NaN                   1.0                    1.0   \n",
       "1                 NaN                   1.0                    1.0   \n",
       "2                 NaN                   1.0                    1.0   \n",
       "3                 NaN                   1.0                    1.0   \n",
       "4                 NaN                   1.0                    1.0   \n",
       "\n",
       "   label:WITH_FRIENDS  label_source  \n",
       "0                 NaN             2  \n",
       "1                 NaN             2  \n",
       "2                 NaN             2  \n",
       "3                 NaN             2  \n",
       "4                 NaN             2  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data = pd.read_csv('Aggregated User Data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['UUID'] == '0BFC35E2-4817-4865-BFA7-764742302A2D') | (data['UUID'] == '0A986513-7828-4D53-AA1F-E02D6DF9561B') | (data['UUID'] == '00EABED2-271D-49D8-B599-1D4A09240601')] \n",
    "data.drop(columns = ['timestamp'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = ['label:FIX_walking', 'label:SITTING', 'label:SLEEPING', 'label:PHONE_ON_TABLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_data = interpolation(data[(data['label:FIX_walking'] == 1) | (data['label:SITTING'] == 1) | (data['label:SLEEPING'] == 1)]).reset_index()\n",
    "label_data = interpolation(activity_data[activities]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "activity_data = activity_data.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label:FIX_walking</th>\n",
       "      <th>label:SITTING</th>\n",
       "      <th>label:SLEEPING</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label:FIX_walking  label:SITTING  label:SLEEPING  label:PHONE_ON_TABLE\n",
       "0                0.0            1.0             0.0                   1.0\n",
       "1                0.0            1.0             0.0                   1.0\n",
       "2                0.0            1.0             0.0                   1.0\n",
       "3                0.0            1.0             0.0                   1.0\n",
       "4                0.0            1.0             0.0                   1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label:FIX_walking</th>\n",
       "      <th>label:SITTING</th>\n",
       "      <th>label:SLEEPING</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label:FIX_walking  label:SITTING  label:SLEEPING  label:PHONE_ON_TABLE  \\\n",
       "0                0.0            0.0             1.0                   0.0   \n",
       "1                0.0            0.0             1.0                   1.0   \n",
       "2                0.0            1.0             0.0                   0.0   \n",
       "3                0.0            1.0             0.0                   1.0   \n",
       "4                1.0            0.0             0.0                   0.0   \n",
       "\n",
       "   Count  \n",
       "0   1169  \n",
       "1    453  \n",
       "2   2896  \n",
       "3   1764  \n",
       "4    728  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labelpairs = label_data.groupby(activities).size().reset_index(name='Count')\n",
    "unique_labelpairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>raw_acc:magnitude_stats:time_entropy</th>\n",
       "      <th>raw_acc:magnitude_spectrum:log_energy_band0</th>\n",
       "      <th>...</th>\n",
       "      <th>raw_acc:magnitude_autocorrelation:normalized_ac</th>\n",
       "      <th>raw_acc:3d:mean_x</th>\n",
       "      <th>raw_acc:3d:mean_y</th>\n",
       "      <th>raw_acc:3d:mean_z</th>\n",
       "      <th>raw_acc:3d:std_x</th>\n",
       "      <th>raw_acc:3d:std_y</th>\n",
       "      <th>raw_acc:3d:std_z</th>\n",
       "      <th>raw_acc:3d:ro_xy</th>\n",
       "      <th>raw_acc:3d:ro_xz</th>\n",
       "      <th>raw_acc:3d:ro_yz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.996815</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.995203</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.998502</td>\n",
       "      <td>1.748756</td>\n",
       "      <td>6.684605</td>\n",
       "      <td>5.043970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148988</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.004614</td>\n",
       "      <td>-0.996790</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.106920</td>\n",
       "      <td>0.516842</td>\n",
       "      <td>0.255494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996864</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>1.935573</td>\n",
       "      <td>6.684603</td>\n",
       "      <td>5.043367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207046</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>-0.996832</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>-0.079483</td>\n",
       "      <td>0.357748</td>\n",
       "      <td>0.036252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.994797</td>\n",
       "      <td>0.996614</td>\n",
       "      <td>0.998704</td>\n",
       "      <td>2.031780</td>\n",
       "      <td>6.684605</td>\n",
       "      <td>5.043599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186961</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>-0.996785</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.122432</td>\n",
       "      <td>0.464881</td>\n",
       "      <td>0.222375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.996874</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>0.995050</td>\n",
       "      <td>0.996907</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>1.865318</td>\n",
       "      <td>6.684605</td>\n",
       "      <td>5.043263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479430</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>-0.996836</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.308841</td>\n",
       "      <td>0.773514</td>\n",
       "      <td>0.296194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.997371</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.102332</td>\n",
       "      <td>0.995548</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.460806</td>\n",
       "      <td>6.683904</td>\n",
       "      <td>5.042779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202305</td>\n",
       "      <td>-0.008383</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.994184</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>0.023784</td>\n",
       "      <td>0.039512</td>\n",
       "      <td>-0.187758</td>\n",
       "      <td>-0.204381</td>\n",
       "      <td>0.062696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0                      0.996815                     0.003529   \n",
       "1                      0.996864                     0.004172   \n",
       "2                      0.996825                     0.003667   \n",
       "3                      0.996874                     0.003541   \n",
       "4                      0.997371                     0.037653   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.002786                         0.006496   \n",
       "1                        -0.003110                         0.007050   \n",
       "2                         0.003094                         0.006076   \n",
       "3                         0.000626                         0.006059   \n",
       "4                         0.043389                         0.102332   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              0.995203                              0.996825   \n",
       "1                              0.994957                              0.996981   \n",
       "2                              0.994797                              0.996614   \n",
       "3                              0.995050                              0.996907   \n",
       "4                              0.995548                              0.996860   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              0.998502   \n",
       "1                              0.998766   \n",
       "2                              0.998704   \n",
       "3                              0.998690   \n",
       "4                              0.998205   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  \\\n",
       "0                               1.748756   \n",
       "1                               1.935573   \n",
       "2                               2.031780   \n",
       "3                               1.865318   \n",
       "4                               0.460806   \n",
       "\n",
       "   raw_acc:magnitude_stats:time_entropy  \\\n",
       "0                              6.684605   \n",
       "1                              6.684603   \n",
       "2                              6.684605   \n",
       "3                              6.684605   \n",
       "4                              6.683904   \n",
       "\n",
       "   raw_acc:magnitude_spectrum:log_energy_band0  ...  \\\n",
       "0                                     5.043970  ...   \n",
       "1                                     5.043367  ...   \n",
       "2                                     5.043599  ...   \n",
       "3                                     5.043263  ...   \n",
       "4                                     5.042779  ...   \n",
       "\n",
       "   raw_acc:magnitude_autocorrelation:normalized_ac  raw_acc:3d:mean_x  \\\n",
       "0                                         0.148988           0.002331   \n",
       "1                                         0.207046           0.003557   \n",
       "2                                         0.186961           0.004180   \n",
       "3                                         0.479430           0.004179   \n",
       "4                                         0.202305          -0.008383   \n",
       "\n",
       "   raw_acc:3d:mean_y  raw_acc:3d:mean_z  raw_acc:3d:std_x  raw_acc:3d:std_y  \\\n",
       "0           0.004614          -0.996790          0.003269          0.003521   \n",
       "1           0.005495          -0.996832          0.002489          0.003772   \n",
       "2           0.005776          -0.996785          0.003567          0.004051   \n",
       "3           0.005551          -0.996836          0.004202          0.002919   \n",
       "4          -0.000009          -0.994184          0.074612          0.023784   \n",
       "\n",
       "   raw_acc:3d:std_z  raw_acc:3d:ro_xy  raw_acc:3d:ro_xz  raw_acc:3d:ro_yz  \n",
       "0          0.003539          0.106920          0.516842          0.255494  \n",
       "1          0.004177         -0.079483          0.357748          0.036252  \n",
       "2          0.003681          0.122432          0.464881          0.222375  \n",
       "3          0.003559          0.308841          0.773514          0.296194  \n",
       "4          0.039512         -0.187758         -0.204381          0.062696  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = label_data.groupby(activities).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>raw_acc:magnitude_stats:time_entropy</th>\n",
       "      <th>raw_acc:magnitude_spectrum:log_energy_band0</th>\n",
       "      <th>...</th>\n",
       "      <th>raw_acc:3d:mean_x</th>\n",
       "      <th>raw_acc:3d:mean_y</th>\n",
       "      <th>raw_acc:3d:mean_z</th>\n",
       "      <th>raw_acc:3d:std_x</th>\n",
       "      <th>raw_acc:3d:std_y</th>\n",
       "      <th>raw_acc:3d:std_z</th>\n",
       "      <th>raw_acc:3d:ro_xy</th>\n",
       "      <th>raw_acc:3d:ro_xz</th>\n",
       "      <th>raw_acc:3d:ro_yz</th>\n",
       "      <th>Unique Label Pair Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.997478</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.996710</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>0.998350</td>\n",
       "      <td>1.800587</td>\n",
       "      <td>6.684611</td>\n",
       "      <td>5.043453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>-0.997354</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.130205</td>\n",
       "      <td>0.143692</td>\n",
       "      <td>0.183355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.997303</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.996528</td>\n",
       "      <td>0.997288</td>\n",
       "      <td>0.998111</td>\n",
       "      <td>2.556438</td>\n",
       "      <td>6.684611</td>\n",
       "      <td>5.043375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.015367</td>\n",
       "      <td>-0.997182</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.016236</td>\n",
       "      <td>-0.015922</td>\n",
       "      <td>-0.099538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.996976</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.996182</td>\n",
       "      <td>0.996953</td>\n",
       "      <td>0.997861</td>\n",
       "      <td>2.719104</td>\n",
       "      <td>6.684611</td>\n",
       "      <td>5.043517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>-0.996844</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>-0.051118</td>\n",
       "      <td>-0.017367</td>\n",
       "      <td>-0.133502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.996101</td>\n",
       "      <td>0.996910</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>2.538713</td>\n",
       "      <td>6.684611</td>\n",
       "      <td>5.043646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>-0.996766</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>-0.060276</td>\n",
       "      <td>-0.028098</td>\n",
       "      <td>-0.099209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.997187</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.996432</td>\n",
       "      <td>0.997121</td>\n",
       "      <td>0.997998</td>\n",
       "      <td>2.527216</td>\n",
       "      <td>6.684611</td>\n",
       "      <td>5.043647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.015887</td>\n",
       "      <td>-0.997058</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>-0.036610</td>\n",
       "      <td>-0.112272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>0.989871</td>\n",
       "      <td>0.067119</td>\n",
       "      <td>0.099246</td>\n",
       "      <td>0.150556</td>\n",
       "      <td>0.977105</td>\n",
       "      <td>0.981139</td>\n",
       "      <td>0.985729</td>\n",
       "      <td>0.974973</td>\n",
       "      <td>6.682444</td>\n",
       "      <td>5.036262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552374</td>\n",
       "      <td>-0.805412</td>\n",
       "      <td>0.038265</td>\n",
       "      <td>0.119152</td>\n",
       "      <td>0.090855</td>\n",
       "      <td>0.081419</td>\n",
       "      <td>0.689755</td>\n",
       "      <td>0.222105</td>\n",
       "      <td>0.380224</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>1.003197</td>\n",
       "      <td>0.087047</td>\n",
       "      <td>0.085149</td>\n",
       "      <td>0.153087</td>\n",
       "      <td>0.972808</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>1.025152</td>\n",
       "      <td>1.806637</td>\n",
       "      <td>6.680908</td>\n",
       "      <td>5.041197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502751</td>\n",
       "      <td>0.106469</td>\n",
       "      <td>0.844585</td>\n",
       "      <td>0.112669</td>\n",
       "      <td>0.112608</td>\n",
       "      <td>0.105780</td>\n",
       "      <td>0.052065</td>\n",
       "      <td>-0.319796</td>\n",
       "      <td>-0.185053</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6926</th>\n",
       "      <td>1.022369</td>\n",
       "      <td>0.057372</td>\n",
       "      <td>0.047823</td>\n",
       "      <td>0.106795</td>\n",
       "      <td>1.001885</td>\n",
       "      <td>1.019989</td>\n",
       "      <td>1.042060</td>\n",
       "      <td>1.760117</td>\n",
       "      <td>6.683045</td>\n",
       "      <td>5.042440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148705</td>\n",
       "      <td>0.469393</td>\n",
       "      <td>0.889832</td>\n",
       "      <td>0.049875</td>\n",
       "      <td>0.063585</td>\n",
       "      <td>0.088057</td>\n",
       "      <td>-0.533838</td>\n",
       "      <td>0.538437</td>\n",
       "      <td>-0.583549</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>1.028238</td>\n",
       "      <td>0.022944</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.057903</td>\n",
       "      <td>1.024304</td>\n",
       "      <td>1.027962</td>\n",
       "      <td>1.031794</td>\n",
       "      <td>1.055626</td>\n",
       "      <td>6.684366</td>\n",
       "      <td>5.041948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520750</td>\n",
       "      <td>0.057947</td>\n",
       "      <td>0.881174</td>\n",
       "      <td>0.045585</td>\n",
       "      <td>0.059979</td>\n",
       "      <td>0.033450</td>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.596598</td>\n",
       "      <td>0.229713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>1.001228</td>\n",
       "      <td>0.050090</td>\n",
       "      <td>0.115929</td>\n",
       "      <td>0.195423</td>\n",
       "      <td>0.989424</td>\n",
       "      <td>0.997288</td>\n",
       "      <td>1.006778</td>\n",
       "      <td>0.708429</td>\n",
       "      <td>6.683541</td>\n",
       "      <td>5.049378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083775</td>\n",
       "      <td>-0.989441</td>\n",
       "      <td>0.033190</td>\n",
       "      <td>0.083852</td>\n",
       "      <td>0.035236</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.192285</td>\n",
       "      <td>0.137674</td>\n",
       "      <td>-0.104514</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7010 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "478                       0.997478                     0.001418   \n",
       "479                       0.997303                     0.001148   \n",
       "480                       0.996976                     0.001247   \n",
       "481                       0.996895                     0.001172   \n",
       "482                       0.997187                     0.001197   \n",
       "...                            ...                          ...   \n",
       "6820                      0.989871                     0.067119   \n",
       "6861                      1.003197                     0.087047   \n",
       "6926                      1.022369                     0.057372   \n",
       "6988                      1.028238                     0.022944   \n",
       "6989                      1.001228                     0.050090   \n",
       "\n",
       "      raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "478                         -0.001431                         0.002539   \n",
       "479                          0.000560                         0.001507   \n",
       "480                         -0.000412                         0.001608   \n",
       "481                         -0.000609                         0.001543   \n",
       "482                          0.000329                         0.001570   \n",
       "...                               ...                              ...   \n",
       "6820                         0.099246                         0.150556   \n",
       "6861                         0.085149                         0.153087   \n",
       "6926                         0.047823                         0.106795   \n",
       "6988                         0.029560                         0.057903   \n",
       "6989                         0.115929                         0.195423   \n",
       "\n",
       "      raw_acc:magnitude_stats:percentile25  \\\n",
       "478                               0.996710   \n",
       "479                               0.996528   \n",
       "480                               0.996182   \n",
       "481                               0.996101   \n",
       "482                               0.996432   \n",
       "...                                    ...   \n",
       "6820                              0.977105   \n",
       "6861                              0.972808   \n",
       "6926                              1.001885   \n",
       "6988                              1.024304   \n",
       "6989                              0.989424   \n",
       "\n",
       "      raw_acc:magnitude_stats:percentile50  \\\n",
       "478                               0.997557   \n",
       "479                               0.997288   \n",
       "480                               0.996953   \n",
       "481                               0.996910   \n",
       "482                               0.997121   \n",
       "...                                    ...   \n",
       "6820                              0.981139   \n",
       "6861                              0.999512   \n",
       "6926                              1.019989   \n",
       "6988                              1.027962   \n",
       "6989                              0.997288   \n",
       "\n",
       "      raw_acc:magnitude_stats:percentile75  \\\n",
       "478                               0.998350   \n",
       "479                               0.998111   \n",
       "480                               0.997861   \n",
       "481                               0.997705   \n",
       "482                               0.997998   \n",
       "...                                    ...   \n",
       "6820                              0.985729   \n",
       "6861                              1.025152   \n",
       "6926                              1.042060   \n",
       "6988                              1.031794   \n",
       "6989                              1.006778   \n",
       "\n",
       "      raw_acc:magnitude_stats:value_entropy  \\\n",
       "478                                1.800587   \n",
       "479                                2.556438   \n",
       "480                                2.719104   \n",
       "481                                2.538713   \n",
       "482                                2.527216   \n",
       "...                                     ...   \n",
       "6820                               0.974973   \n",
       "6861                               1.806637   \n",
       "6926                               1.760117   \n",
       "6988                               1.055626   \n",
       "6989                               0.708429   \n",
       "\n",
       "      raw_acc:magnitude_stats:time_entropy  \\\n",
       "478                               6.684611   \n",
       "479                               6.684611   \n",
       "480                               6.684611   \n",
       "481                               6.684611   \n",
       "482                               6.684611   \n",
       "...                                    ...   \n",
       "6820                              6.682444   \n",
       "6861                              6.680908   \n",
       "6926                              6.683045   \n",
       "6988                              6.684366   \n",
       "6989                              6.683541   \n",
       "\n",
       "      raw_acc:magnitude_spectrum:log_energy_band0  ...  raw_acc:3d:mean_x  \\\n",
       "478                                      5.043453  ...           0.001369   \n",
       "479                                      5.043375  ...           0.001457   \n",
       "480                                      5.043517  ...           0.001267   \n",
       "481                                      5.043646  ...           0.001313   \n",
       "482                                      5.043647  ...           0.001311   \n",
       "...                                           ...  ...                ...   \n",
       "6820                                     5.036262  ...           0.552374   \n",
       "6861                                     5.041197  ...           0.502751   \n",
       "6926                                     5.042440  ...          -0.148705   \n",
       "6988                                     5.041948  ...          -0.520750   \n",
       "6989                                     5.049378  ...          -0.083775   \n",
       "\n",
       "      raw_acc:3d:mean_y  raw_acc:3d:mean_z  raw_acc:3d:std_x  \\\n",
       "478            0.015572          -0.997354          0.001038   \n",
       "479            0.015367          -0.997182          0.000987   \n",
       "480            0.016086          -0.996844          0.001021   \n",
       "481            0.015924          -0.996766          0.000991   \n",
       "482            0.015887          -0.997058          0.000979   \n",
       "...                 ...                ...               ...   \n",
       "6820          -0.805412           0.038265          0.119152   \n",
       "6861           0.106469           0.844585          0.112669   \n",
       "6926           0.469393           0.889832          0.049875   \n",
       "6988           0.057947           0.881174          0.045585   \n",
       "6989          -0.989441           0.033190          0.083852   \n",
       "\n",
       "      raw_acc:3d:std_y  raw_acc:3d:std_z  raw_acc:3d:ro_xy  raw_acc:3d:ro_xz  \\\n",
       "478           0.001313          0.001423          0.130205          0.143692   \n",
       "479           0.001127          0.001146          0.016236         -0.015922   \n",
       "480           0.001193          0.001245         -0.051118         -0.017367   \n",
       "481           0.001128          0.001171         -0.060276         -0.028098   \n",
       "482           0.001136          0.001195          0.004222         -0.036610   \n",
       "...                ...               ...               ...               ...   \n",
       "6820          0.090855          0.081419          0.689755          0.222105   \n",
       "6861          0.112608          0.105780          0.052065         -0.319796   \n",
       "6926          0.063585          0.088057         -0.533838          0.538437   \n",
       "6988          0.059979          0.033450          0.505155          0.596598   \n",
       "6989          0.035236          0.097875          0.192285          0.137674   \n",
       "\n",
       "      raw_acc:3d:ro_yz  Unique Label Pair Index  \n",
       "478           0.183355                        0  \n",
       "479          -0.099538                        0  \n",
       "480          -0.133502                        0  \n",
       "481          -0.099209                        0  \n",
       "482          -0.112272                        0  \n",
       "...                ...                      ...  \n",
       "6820          0.380224                        4  \n",
       "6861         -0.185053                        4  \n",
       "6926         -0.583549                        4  \n",
       "6988          0.229713                        4  \n",
       "6989         -0.104514                        4  \n",
       "\n",
       "[7010 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in range(len(unique_labelpairs)):\n",
    "    key = list(indices)[k]\n",
    "    val = list(indices.values())[k]\n",
    "    #print(key, val)\n",
    "    \n",
    "    if k == 0:\n",
    "        data_with_indices = activity_data.iloc[list(indices.values())[k]].copy(deep = True)\n",
    "        data_with_indices['Unique Label Pair Index'] = k\n",
    "    else:\n",
    "        current_data = activity_data.iloc[list(indices.values())[k]].copy(deep = True)\n",
    "        current_data['Unique Label Pair Index'] = k\n",
    "        \n",
    "        data_with_indices = pd.concat([data_with_indices, current_data])\n",
    "        \n",
    "data_with_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_with_indices.iloc[:,:-1].values\n",
    "y = data_with_indices.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2896\n"
     ]
    }
   ],
   "source": [
    "idx_of_largest = unique_labelpairs['Count'].idxmax()\n",
    "largest_volume = unique_labelpairs['Count'].max()\n",
    "print(idx_of_largest, largest_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2606, 1: 2606, 2: 2896, 3: 2606, 4: 2606}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in range(len(unique_labelpairs)):\n",
    "    if k == idx_of_largest:\n",
    "        volume_dictionary[k] = largest_volume\n",
    "    else:\n",
    "        volume_dictionary[k] = int(largest_volume * 0.9)\n",
    "        \n",
    "volume_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(sampling_strategy = volume_dictionary)\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3191037 , -0.41371654, -0.39955547, ..., -0.29360235,\n",
       "         0.19596913,  0.08113375],\n",
       "       [-0.24427294, -0.41323502, -0.38847629, ...,  0.25215588,\n",
       "         0.15447155,  0.49630476],\n",
       "       [-0.37525554,  0.09799538,  0.46529787, ..., -0.44855562,\n",
       "        -0.45653482,  0.9030356 ],\n",
       "       ...,\n",
       "       [-0.30994718, -0.40808372, -0.39869147, ..., -0.10191157,\n",
       "        -0.87063403,  0.06624984],\n",
       "       [-0.4965516 , -0.40089167, -0.41267482, ..., -0.29602238,\n",
       "         0.63268662, -0.06433178],\n",
       "       [-0.29688865, -0.41468138, -0.39521766, ..., -0.05294227,\n",
       "         0.1869491 , -0.02584584]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, len(unique_labelpairs))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier().to(device)\n",
    "\n",
    "n_epochs = 20000 #2000\n",
    "batch_size = 300 #200\n",
    "lr = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = 1e-5)\n",
    "\n",
    "train_features = torch.tensor(X_train).to(device)\n",
    "train_labels = torch.tensor(y_train).to(device)\n",
    "test_features = torch.tensor(X_test).to(device)\n",
    "test_labels = torch.tensor(y_test).to(device)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(y_test), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 59.361, Final Batch Loss: 1.677\n",
      "Epoch 2, Loss: 59.113, Final Batch Loss: 1.641\n",
      "Epoch 3, Loss: 58.868, Final Batch Loss: 1.611\n",
      "Epoch 4, Loss: 58.628, Final Batch Loss: 1.612\n",
      "Epoch 5, Loss: 58.399, Final Batch Loss: 1.625\n",
      "Epoch 6, Loss: 58.105, Final Batch Loss: 1.628\n",
      "Epoch 7, Loss: 57.754, Final Batch Loss: 1.594\n",
      "Epoch 8, Loss: 57.317, Final Batch Loss: 1.559\n",
      "Epoch 9, Loss: 56.771, Final Batch Loss: 1.588\n",
      "Epoch 10, Loss: 56.092, Final Batch Loss: 1.539\n",
      "Epoch 11, Loss: 55.438, Final Batch Loss: 1.560\n",
      "Epoch 12, Loss: 54.715, Final Batch Loss: 1.553\n",
      "Epoch 13, Loss: 53.986, Final Batch Loss: 1.498\n",
      "Epoch 14, Loss: 53.407, Final Batch Loss: 1.477\n",
      "Epoch 15, Loss: 52.854, Final Batch Loss: 1.428\n",
      "Epoch 16, Loss: 52.465, Final Batch Loss: 1.477\n",
      "Epoch 17, Loss: 52.023, Final Batch Loss: 1.446\n",
      "Epoch 18, Loss: 51.718, Final Batch Loss: 1.484\n",
      "Epoch 19, Loss: 51.427, Final Batch Loss: 1.430\n",
      "Epoch 20, Loss: 51.104, Final Batch Loss: 1.388\n",
      "Epoch 21, Loss: 50.810, Final Batch Loss: 1.416\n",
      "Epoch 22, Loss: 50.449, Final Batch Loss: 1.429\n",
      "Epoch 23, Loss: 50.031, Final Batch Loss: 1.331\n",
      "Epoch 24, Loss: 49.709, Final Batch Loss: 1.339\n",
      "Epoch 25, Loss: 49.360, Final Batch Loss: 1.393\n",
      "Epoch 26, Loss: 49.001, Final Batch Loss: 1.357\n",
      "Epoch 27, Loss: 48.663, Final Batch Loss: 1.328\n",
      "Epoch 28, Loss: 48.150, Final Batch Loss: 1.357\n",
      "Epoch 29, Loss: 47.752, Final Batch Loss: 1.324\n",
      "Epoch 30, Loss: 47.270, Final Batch Loss: 1.349\n",
      "Epoch 31, Loss: 46.794, Final Batch Loss: 1.369\n",
      "Epoch 32, Loss: 46.101, Final Batch Loss: 1.289\n",
      "Epoch 33, Loss: 45.548, Final Batch Loss: 1.272\n",
      "Epoch 34, Loss: 45.000, Final Batch Loss: 1.183\n",
      "Epoch 35, Loss: 44.410, Final Batch Loss: 1.142\n",
      "Epoch 36, Loss: 43.997, Final Batch Loss: 1.277\n",
      "Epoch 37, Loss: 43.454, Final Batch Loss: 1.262\n",
      "Epoch 38, Loss: 43.013, Final Batch Loss: 1.164\n",
      "Epoch 39, Loss: 42.570, Final Batch Loss: 1.187\n",
      "Epoch 40, Loss: 41.967, Final Batch Loss: 1.149\n",
      "Epoch 41, Loss: 41.567, Final Batch Loss: 1.080\n",
      "Epoch 42, Loss: 41.199, Final Batch Loss: 1.054\n",
      "Epoch 43, Loss: 40.797, Final Batch Loss: 1.110\n",
      "Epoch 44, Loss: 40.441, Final Batch Loss: 1.061\n",
      "Epoch 45, Loss: 40.137, Final Batch Loss: 1.120\n",
      "Epoch 46, Loss: 39.749, Final Batch Loss: 1.080\n",
      "Epoch 47, Loss: 39.320, Final Batch Loss: 1.121\n",
      "Epoch 48, Loss: 39.231, Final Batch Loss: 1.070\n",
      "Epoch 49, Loss: 38.738, Final Batch Loss: 1.088\n",
      "Epoch 50, Loss: 38.532, Final Batch Loss: 1.037\n",
      "Epoch 51, Loss: 38.251, Final Batch Loss: 1.059\n",
      "Epoch 52, Loss: 37.906, Final Batch Loss: 1.056\n",
      "Epoch 53, Loss: 37.798, Final Batch Loss: 1.068\n",
      "Epoch 54, Loss: 37.529, Final Batch Loss: 0.999\n",
      "Epoch 55, Loss: 37.508, Final Batch Loss: 1.063\n",
      "Epoch 56, Loss: 37.146, Final Batch Loss: 1.004\n",
      "Epoch 57, Loss: 36.978, Final Batch Loss: 0.915\n",
      "Epoch 58, Loss: 36.650, Final Batch Loss: 0.991\n",
      "Epoch 59, Loss: 36.550, Final Batch Loss: 0.881\n",
      "Epoch 60, Loss: 36.351, Final Batch Loss: 1.090\n",
      "Epoch 61, Loss: 36.238, Final Batch Loss: 0.976\n",
      "Epoch 62, Loss: 36.229, Final Batch Loss: 1.165\n",
      "Epoch 63, Loss: 35.950, Final Batch Loss: 1.015\n",
      "Epoch 64, Loss: 35.937, Final Batch Loss: 1.070\n",
      "Epoch 65, Loss: 35.625, Final Batch Loss: 0.952\n",
      "Epoch 66, Loss: 35.428, Final Batch Loss: 0.895\n",
      "Epoch 67, Loss: 35.345, Final Batch Loss: 1.054\n",
      "Epoch 68, Loss: 35.274, Final Batch Loss: 1.021\n",
      "Epoch 69, Loss: 35.030, Final Batch Loss: 1.034\n",
      "Epoch 70, Loss: 34.776, Final Batch Loss: 0.900\n",
      "Epoch 71, Loss: 34.893, Final Batch Loss: 0.898\n",
      "Epoch 72, Loss: 34.560, Final Batch Loss: 0.979\n",
      "Epoch 73, Loss: 34.590, Final Batch Loss: 0.917\n",
      "Epoch 74, Loss: 34.166, Final Batch Loss: 0.918\n",
      "Epoch 75, Loss: 34.281, Final Batch Loss: 1.000\n",
      "Epoch 76, Loss: 34.316, Final Batch Loss: 0.967\n",
      "Epoch 77, Loss: 34.064, Final Batch Loss: 1.092\n",
      "Epoch 78, Loss: 33.817, Final Batch Loss: 0.847\n",
      "Epoch 79, Loss: 33.841, Final Batch Loss: 0.865\n",
      "Epoch 80, Loss: 33.839, Final Batch Loss: 1.030\n",
      "Epoch 81, Loss: 33.485, Final Batch Loss: 0.972\n",
      "Epoch 82, Loss: 33.371, Final Batch Loss: 0.937\n",
      "Epoch 83, Loss: 33.322, Final Batch Loss: 1.017\n",
      "Epoch 84, Loss: 33.225, Final Batch Loss: 0.951\n",
      "Epoch 85, Loss: 33.029, Final Batch Loss: 0.860\n",
      "Epoch 86, Loss: 32.881, Final Batch Loss: 0.905\n",
      "Epoch 87, Loss: 32.890, Final Batch Loss: 0.887\n",
      "Epoch 88, Loss: 32.738, Final Batch Loss: 0.890\n",
      "Epoch 89, Loss: 32.779, Final Batch Loss: 0.858\n",
      "Epoch 90, Loss: 32.575, Final Batch Loss: 0.912\n",
      "Epoch 91, Loss: 32.576, Final Batch Loss: 0.883\n",
      "Epoch 92, Loss: 32.560, Final Batch Loss: 0.873\n",
      "Epoch 93, Loss: 32.327, Final Batch Loss: 0.936\n",
      "Epoch 94, Loss: 32.244, Final Batch Loss: 0.845\n",
      "Epoch 95, Loss: 32.261, Final Batch Loss: 1.053\n",
      "Epoch 96, Loss: 32.076, Final Batch Loss: 0.879\n",
      "Epoch 97, Loss: 32.064, Final Batch Loss: 0.928\n",
      "Epoch 98, Loss: 31.957, Final Batch Loss: 0.906\n",
      "Epoch 99, Loss: 31.852, Final Batch Loss: 0.922\n",
      "Epoch 100, Loss: 31.990, Final Batch Loss: 0.812\n",
      "Epoch 101, Loss: 31.723, Final Batch Loss: 0.948\n",
      "Epoch 102, Loss: 31.855, Final Batch Loss: 0.825\n",
      "Epoch 103, Loss: 31.474, Final Batch Loss: 0.840\n",
      "Epoch 104, Loss: 31.571, Final Batch Loss: 0.913\n",
      "Epoch 105, Loss: 31.430, Final Batch Loss: 0.838\n",
      "Epoch 106, Loss: 31.437, Final Batch Loss: 0.799\n",
      "Epoch 107, Loss: 31.077, Final Batch Loss: 0.809\n",
      "Epoch 108, Loss: 31.109, Final Batch Loss: 0.855\n",
      "Epoch 109, Loss: 31.405, Final Batch Loss: 0.948\n",
      "Epoch 110, Loss: 31.089, Final Batch Loss: 0.876\n",
      "Epoch 111, Loss: 30.925, Final Batch Loss: 0.870\n",
      "Epoch 112, Loss: 30.844, Final Batch Loss: 0.836\n",
      "Epoch 113, Loss: 30.945, Final Batch Loss: 0.903\n",
      "Epoch 114, Loss: 30.926, Final Batch Loss: 0.860\n",
      "Epoch 115, Loss: 30.669, Final Batch Loss: 0.853\n",
      "Epoch 116, Loss: 30.620, Final Batch Loss: 0.742\n",
      "Epoch 117, Loss: 30.530, Final Batch Loss: 0.768\n",
      "Epoch 118, Loss: 30.656, Final Batch Loss: 0.926\n",
      "Epoch 119, Loss: 30.509, Final Batch Loss: 0.657\n",
      "Epoch 120, Loss: 30.634, Final Batch Loss: 0.861\n",
      "Epoch 121, Loss: 30.496, Final Batch Loss: 0.763\n",
      "Epoch 122, Loss: 30.496, Final Batch Loss: 0.981\n",
      "Epoch 123, Loss: 30.474, Final Batch Loss: 0.835\n",
      "Epoch 124, Loss: 30.213, Final Batch Loss: 0.815\n",
      "Epoch 125, Loss: 30.139, Final Batch Loss: 0.803\n",
      "Epoch 126, Loss: 30.265, Final Batch Loss: 0.794\n",
      "Epoch 127, Loss: 30.004, Final Batch Loss: 0.854\n",
      "Epoch 128, Loss: 30.117, Final Batch Loss: 0.865\n",
      "Epoch 129, Loss: 29.875, Final Batch Loss: 0.816\n",
      "Epoch 130, Loss: 29.827, Final Batch Loss: 0.778\n",
      "Epoch 131, Loss: 29.673, Final Batch Loss: 0.763\n",
      "Epoch 132, Loss: 29.555, Final Batch Loss: 0.785\n",
      "Epoch 133, Loss: 29.665, Final Batch Loss: 0.806\n",
      "Epoch 134, Loss: 29.613, Final Batch Loss: 0.825\n",
      "Epoch 135, Loss: 29.703, Final Batch Loss: 0.916\n",
      "Epoch 136, Loss: 29.327, Final Batch Loss: 0.914\n",
      "Epoch 137, Loss: 29.410, Final Batch Loss: 0.823\n",
      "Epoch 138, Loss: 29.568, Final Batch Loss: 0.819\n",
      "Epoch 139, Loss: 29.322, Final Batch Loss: 0.779\n",
      "Epoch 140, Loss: 29.309, Final Batch Loss: 0.701\n",
      "Epoch 141, Loss: 29.441, Final Batch Loss: 0.805\n",
      "Epoch 142, Loss: 29.232, Final Batch Loss: 0.751\n",
      "Epoch 143, Loss: 29.307, Final Batch Loss: 0.891\n",
      "Epoch 144, Loss: 29.221, Final Batch Loss: 0.850\n",
      "Epoch 145, Loss: 29.189, Final Batch Loss: 0.733\n",
      "Epoch 146, Loss: 29.006, Final Batch Loss: 0.813\n",
      "Epoch 147, Loss: 28.775, Final Batch Loss: 0.650\n",
      "Epoch 148, Loss: 29.044, Final Batch Loss: 0.849\n",
      "Epoch 149, Loss: 28.824, Final Batch Loss: 0.727\n",
      "Epoch 150, Loss: 28.739, Final Batch Loss: 0.765\n",
      "Epoch 151, Loss: 28.968, Final Batch Loss: 0.809\n",
      "Epoch 152, Loss: 28.714, Final Batch Loss: 0.685\n",
      "Epoch 153, Loss: 28.834, Final Batch Loss: 0.755\n",
      "Epoch 154, Loss: 28.611, Final Batch Loss: 0.828\n",
      "Epoch 155, Loss: 28.615, Final Batch Loss: 0.896\n",
      "Epoch 156, Loss: 28.566, Final Batch Loss: 0.693\n",
      "Epoch 157, Loss: 28.530, Final Batch Loss: 0.759\n",
      "Epoch 158, Loss: 28.345, Final Batch Loss: 0.867\n",
      "Epoch 159, Loss: 28.535, Final Batch Loss: 0.738\n",
      "Epoch 160, Loss: 28.559, Final Batch Loss: 0.763\n",
      "Epoch 161, Loss: 28.414, Final Batch Loss: 0.839\n",
      "Epoch 162, Loss: 28.310, Final Batch Loss: 0.721\n",
      "Epoch 163, Loss: 28.156, Final Batch Loss: 0.696\n",
      "Epoch 164, Loss: 28.471, Final Batch Loss: 0.854\n",
      "Epoch 165, Loss: 28.166, Final Batch Loss: 0.787\n",
      "Epoch 166, Loss: 28.195, Final Batch Loss: 0.785\n",
      "Epoch 167, Loss: 28.148, Final Batch Loss: 0.761\n",
      "Epoch 168, Loss: 28.119, Final Batch Loss: 0.756\n",
      "Epoch 169, Loss: 27.979, Final Batch Loss: 0.863\n",
      "Epoch 170, Loss: 28.071, Final Batch Loss: 0.831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171, Loss: 28.050, Final Batch Loss: 0.772\n",
      "Epoch 172, Loss: 28.123, Final Batch Loss: 0.845\n",
      "Epoch 173, Loss: 27.959, Final Batch Loss: 0.824\n",
      "Epoch 174, Loss: 27.877, Final Batch Loss: 0.727\n",
      "Epoch 175, Loss: 27.771, Final Batch Loss: 0.752\n",
      "Epoch 176, Loss: 27.844, Final Batch Loss: 0.837\n",
      "Epoch 177, Loss: 27.469, Final Batch Loss: 0.712\n",
      "Epoch 178, Loss: 27.743, Final Batch Loss: 0.809\n",
      "Epoch 179, Loss: 27.629, Final Batch Loss: 0.881\n",
      "Epoch 180, Loss: 27.518, Final Batch Loss: 0.822\n",
      "Epoch 181, Loss: 27.594, Final Batch Loss: 0.823\n",
      "Epoch 182, Loss: 27.563, Final Batch Loss: 0.678\n",
      "Epoch 183, Loss: 27.418, Final Batch Loss: 0.710\n",
      "Epoch 184, Loss: 27.551, Final Batch Loss: 0.804\n",
      "Epoch 185, Loss: 27.526, Final Batch Loss: 0.768\n",
      "Epoch 186, Loss: 27.400, Final Batch Loss: 0.794\n",
      "Epoch 187, Loss: 27.563, Final Batch Loss: 0.798\n",
      "Epoch 188, Loss: 27.643, Final Batch Loss: 0.674\n",
      "Epoch 189, Loss: 27.310, Final Batch Loss: 0.749\n",
      "Epoch 190, Loss: 27.318, Final Batch Loss: 0.775\n",
      "Epoch 191, Loss: 27.182, Final Batch Loss: 0.809\n",
      "Epoch 192, Loss: 27.137, Final Batch Loss: 0.733\n",
      "Epoch 193, Loss: 27.253, Final Batch Loss: 0.733\n",
      "Epoch 194, Loss: 27.185, Final Batch Loss: 0.681\n",
      "Epoch 195, Loss: 27.102, Final Batch Loss: 0.725\n",
      "Epoch 196, Loss: 27.207, Final Batch Loss: 0.710\n",
      "Epoch 197, Loss: 27.010, Final Batch Loss: 0.776\n",
      "Epoch 198, Loss: 27.121, Final Batch Loss: 0.814\n",
      "Epoch 199, Loss: 27.240, Final Batch Loss: 0.761\n",
      "Epoch 200, Loss: 26.864, Final Batch Loss: 0.747\n",
      "Epoch 201, Loss: 26.942, Final Batch Loss: 0.761\n",
      "Epoch 202, Loss: 27.103, Final Batch Loss: 0.753\n",
      "Epoch 203, Loss: 27.022, Final Batch Loss: 0.737\n",
      "Epoch 204, Loss: 26.980, Final Batch Loss: 0.768\n",
      "Epoch 205, Loss: 26.901, Final Batch Loss: 0.741\n",
      "Epoch 206, Loss: 26.816, Final Batch Loss: 0.810\n",
      "Epoch 207, Loss: 26.819, Final Batch Loss: 0.777\n",
      "Epoch 208, Loss: 26.628, Final Batch Loss: 0.712\n",
      "Epoch 209, Loss: 26.617, Final Batch Loss: 0.659\n",
      "Epoch 210, Loss: 26.948, Final Batch Loss: 0.864\n",
      "Epoch 211, Loss: 26.722, Final Batch Loss: 0.761\n",
      "Epoch 212, Loss: 26.650, Final Batch Loss: 0.737\n",
      "Epoch 213, Loss: 26.570, Final Batch Loss: 0.786\n",
      "Epoch 214, Loss: 26.520, Final Batch Loss: 0.714\n",
      "Epoch 215, Loss: 26.599, Final Batch Loss: 0.690\n",
      "Epoch 216, Loss: 26.636, Final Batch Loss: 0.816\n",
      "Epoch 217, Loss: 26.634, Final Batch Loss: 0.754\n",
      "Epoch 218, Loss: 26.619, Final Batch Loss: 0.798\n",
      "Epoch 219, Loss: 26.604, Final Batch Loss: 0.859\n",
      "Epoch 220, Loss: 26.550, Final Batch Loss: 0.726\n",
      "Epoch 221, Loss: 26.373, Final Batch Loss: 0.706\n",
      "Epoch 222, Loss: 26.624, Final Batch Loss: 0.830\n",
      "Epoch 223, Loss: 26.541, Final Batch Loss: 0.736\n",
      "Epoch 224, Loss: 26.409, Final Batch Loss: 0.788\n",
      "Epoch 225, Loss: 26.566, Final Batch Loss: 0.683\n",
      "Epoch 226, Loss: 26.250, Final Batch Loss: 0.654\n",
      "Epoch 227, Loss: 26.472, Final Batch Loss: 0.720\n",
      "Epoch 228, Loss: 26.358, Final Batch Loss: 0.682\n",
      "Epoch 229, Loss: 26.298, Final Batch Loss: 0.675\n",
      "Epoch 230, Loss: 26.136, Final Batch Loss: 0.654\n",
      "Epoch 231, Loss: 26.326, Final Batch Loss: 0.693\n",
      "Epoch 232, Loss: 26.464, Final Batch Loss: 0.772\n",
      "Epoch 233, Loss: 26.156, Final Batch Loss: 0.768\n",
      "Epoch 234, Loss: 26.157, Final Batch Loss: 0.700\n",
      "Epoch 235, Loss: 26.278, Final Batch Loss: 0.794\n",
      "Epoch 236, Loss: 26.121, Final Batch Loss: 0.706\n",
      "Epoch 237, Loss: 26.222, Final Batch Loss: 0.777\n",
      "Epoch 238, Loss: 26.190, Final Batch Loss: 0.730\n",
      "Epoch 239, Loss: 26.098, Final Batch Loss: 0.527\n",
      "Epoch 240, Loss: 25.955, Final Batch Loss: 0.689\n",
      "Epoch 241, Loss: 26.073, Final Batch Loss: 0.641\n",
      "Epoch 242, Loss: 26.020, Final Batch Loss: 0.781\n",
      "Epoch 243, Loss: 26.137, Final Batch Loss: 0.722\n",
      "Epoch 244, Loss: 25.896, Final Batch Loss: 0.746\n",
      "Epoch 245, Loss: 26.043, Final Batch Loss: 0.739\n",
      "Epoch 246, Loss: 26.024, Final Batch Loss: 0.695\n",
      "Epoch 247, Loss: 25.906, Final Batch Loss: 0.649\n",
      "Epoch 248, Loss: 26.069, Final Batch Loss: 0.738\n",
      "Epoch 249, Loss: 25.864, Final Batch Loss: 0.766\n",
      "Epoch 250, Loss: 25.884, Final Batch Loss: 0.748\n",
      "Epoch 251, Loss: 25.962, Final Batch Loss: 0.704\n",
      "Epoch 252, Loss: 25.797, Final Batch Loss: 0.678\n",
      "Epoch 253, Loss: 25.983, Final Batch Loss: 0.770\n",
      "Epoch 254, Loss: 25.665, Final Batch Loss: 0.706\n",
      "Epoch 255, Loss: 25.730, Final Batch Loss: 0.758\n",
      "Epoch 256, Loss: 25.779, Final Batch Loss: 0.876\n",
      "Epoch 257, Loss: 25.891, Final Batch Loss: 0.809\n",
      "Epoch 258, Loss: 25.747, Final Batch Loss: 0.694\n",
      "Epoch 259, Loss: 25.769, Final Batch Loss: 0.713\n",
      "Epoch 260, Loss: 25.568, Final Batch Loss: 0.542\n",
      "Epoch 261, Loss: 25.718, Final Batch Loss: 0.815\n",
      "Epoch 262, Loss: 25.654, Final Batch Loss: 0.627\n",
      "Epoch 263, Loss: 25.409, Final Batch Loss: 0.820\n",
      "Epoch 264, Loss: 25.862, Final Batch Loss: 0.649\n",
      "Epoch 265, Loss: 25.614, Final Batch Loss: 0.697\n",
      "Epoch 266, Loss: 25.463, Final Batch Loss: 0.770\n",
      "Epoch 267, Loss: 25.561, Final Batch Loss: 0.680\n",
      "Epoch 268, Loss: 25.624, Final Batch Loss: 0.716\n",
      "Epoch 269, Loss: 25.418, Final Batch Loss: 0.595\n",
      "Epoch 270, Loss: 25.580, Final Batch Loss: 0.713\n",
      "Epoch 271, Loss: 25.561, Final Batch Loss: 0.758\n",
      "Epoch 272, Loss: 25.807, Final Batch Loss: 0.784\n",
      "Epoch 273, Loss: 25.460, Final Batch Loss: 0.677\n",
      "Epoch 274, Loss: 25.639, Final Batch Loss: 0.755\n",
      "Epoch 275, Loss: 25.434, Final Batch Loss: 0.699\n",
      "Epoch 276, Loss: 25.547, Final Batch Loss: 0.719\n",
      "Epoch 277, Loss: 25.601, Final Batch Loss: 0.744\n",
      "Epoch 278, Loss: 25.490, Final Batch Loss: 0.709\n",
      "Epoch 279, Loss: 25.365, Final Batch Loss: 0.664\n",
      "Epoch 280, Loss: 25.569, Final Batch Loss: 0.839\n",
      "Epoch 281, Loss: 25.424, Final Batch Loss: 0.603\n",
      "Epoch 282, Loss: 25.379, Final Batch Loss: 0.660\n",
      "Epoch 283, Loss: 25.421, Final Batch Loss: 0.653\n",
      "Epoch 284, Loss: 25.507, Final Batch Loss: 0.730\n",
      "Epoch 285, Loss: 25.539, Final Batch Loss: 0.652\n",
      "Epoch 286, Loss: 25.487, Final Batch Loss: 0.718\n",
      "Epoch 287, Loss: 25.394, Final Batch Loss: 0.687\n",
      "Epoch 288, Loss: 25.417, Final Batch Loss: 0.669\n",
      "Epoch 289, Loss: 25.293, Final Batch Loss: 0.756\n",
      "Epoch 290, Loss: 25.150, Final Batch Loss: 0.757\n",
      "Epoch 291, Loss: 25.348, Final Batch Loss: 0.628\n",
      "Epoch 292, Loss: 25.370, Final Batch Loss: 0.804\n",
      "Epoch 293, Loss: 25.449, Final Batch Loss: 0.681\n",
      "Epoch 294, Loss: 25.360, Final Batch Loss: 0.754\n",
      "Epoch 295, Loss: 25.230, Final Batch Loss: 0.739\n",
      "Epoch 296, Loss: 25.268, Final Batch Loss: 0.621\n",
      "Epoch 297, Loss: 24.991, Final Batch Loss: 0.703\n",
      "Epoch 298, Loss: 25.107, Final Batch Loss: 0.729\n",
      "Epoch 299, Loss: 24.998, Final Batch Loss: 0.741\n",
      "Epoch 300, Loss: 25.341, Final Batch Loss: 0.681\n",
      "Epoch 301, Loss: 25.278, Final Batch Loss: 0.698\n",
      "Epoch 302, Loss: 25.339, Final Batch Loss: 0.941\n",
      "Epoch 303, Loss: 25.119, Final Batch Loss: 0.773\n",
      "Epoch 304, Loss: 25.278, Final Batch Loss: 0.784\n",
      "Epoch 305, Loss: 25.122, Final Batch Loss: 0.685\n",
      "Epoch 306, Loss: 24.904, Final Batch Loss: 0.662\n",
      "Epoch 307, Loss: 25.031, Final Batch Loss: 0.790\n",
      "Epoch 308, Loss: 25.053, Final Batch Loss: 0.727\n",
      "Epoch 309, Loss: 25.035, Final Batch Loss: 0.585\n",
      "Epoch 310, Loss: 24.962, Final Batch Loss: 0.656\n",
      "Epoch 311, Loss: 25.065, Final Batch Loss: 0.611\n",
      "Epoch 312, Loss: 24.959, Final Batch Loss: 0.595\n",
      "Epoch 313, Loss: 25.273, Final Batch Loss: 0.800\n",
      "Epoch 314, Loss: 25.019, Final Batch Loss: 0.753\n",
      "Epoch 315, Loss: 24.911, Final Batch Loss: 0.562\n",
      "Epoch 316, Loss: 25.148, Final Batch Loss: 0.721\n",
      "Epoch 317, Loss: 25.043, Final Batch Loss: 0.664\n",
      "Epoch 318, Loss: 25.103, Final Batch Loss: 0.926\n",
      "Epoch 319, Loss: 24.922, Final Batch Loss: 0.747\n",
      "Epoch 320, Loss: 25.182, Final Batch Loss: 0.708\n",
      "Epoch 321, Loss: 24.915, Final Batch Loss: 0.716\n",
      "Epoch 322, Loss: 24.891, Final Batch Loss: 0.645\n",
      "Epoch 323, Loss: 24.954, Final Batch Loss: 0.680\n",
      "Epoch 324, Loss: 24.974, Final Batch Loss: 0.758\n",
      "Epoch 325, Loss: 24.600, Final Batch Loss: 0.644\n",
      "Epoch 326, Loss: 24.808, Final Batch Loss: 0.571\n",
      "Epoch 327, Loss: 24.773, Final Batch Loss: 0.671\n",
      "Epoch 328, Loss: 24.655, Final Batch Loss: 0.575\n",
      "Epoch 329, Loss: 25.031, Final Batch Loss: 0.730\n",
      "Epoch 330, Loss: 24.668, Final Batch Loss: 0.739\n",
      "Epoch 331, Loss: 24.893, Final Batch Loss: 0.644\n",
      "Epoch 332, Loss: 24.809, Final Batch Loss: 0.698\n",
      "Epoch 333, Loss: 24.856, Final Batch Loss: 0.769\n",
      "Epoch 334, Loss: 24.769, Final Batch Loss: 0.701\n",
      "Epoch 335, Loss: 24.568, Final Batch Loss: 0.641\n",
      "Epoch 336, Loss: 24.671, Final Batch Loss: 0.706\n",
      "Epoch 337, Loss: 24.658, Final Batch Loss: 0.563\n",
      "Epoch 338, Loss: 24.844, Final Batch Loss: 0.620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339, Loss: 24.900, Final Batch Loss: 0.719\n",
      "Epoch 340, Loss: 24.768, Final Batch Loss: 0.578\n",
      "Epoch 341, Loss: 24.761, Final Batch Loss: 0.828\n",
      "Epoch 342, Loss: 24.532, Final Batch Loss: 0.671\n",
      "Epoch 343, Loss: 24.765, Final Batch Loss: 0.647\n",
      "Epoch 344, Loss: 24.643, Final Batch Loss: 0.721\n",
      "Epoch 345, Loss: 24.923, Final Batch Loss: 0.750\n",
      "Epoch 346, Loss: 24.574, Final Batch Loss: 0.637\n",
      "Epoch 347, Loss: 24.518, Final Batch Loss: 0.773\n",
      "Epoch 348, Loss: 24.786, Final Batch Loss: 0.653\n",
      "Epoch 349, Loss: 24.496, Final Batch Loss: 0.532\n",
      "Epoch 350, Loss: 24.782, Final Batch Loss: 0.754\n",
      "Epoch 351, Loss: 24.531, Final Batch Loss: 0.593\n",
      "Epoch 352, Loss: 24.805, Final Batch Loss: 0.677\n",
      "Epoch 353, Loss: 24.681, Final Batch Loss: 0.596\n",
      "Epoch 354, Loss: 24.523, Final Batch Loss: 0.684\n",
      "Epoch 355, Loss: 24.693, Final Batch Loss: 0.731\n",
      "Epoch 356, Loss: 24.578, Final Batch Loss: 0.721\n",
      "Epoch 357, Loss: 24.501, Final Batch Loss: 0.670\n",
      "Epoch 358, Loss: 24.639, Final Batch Loss: 0.725\n",
      "Epoch 359, Loss: 24.574, Final Batch Loss: 0.569\n",
      "Epoch 360, Loss: 24.521, Final Batch Loss: 0.585\n",
      "Epoch 361, Loss: 24.789, Final Batch Loss: 0.676\n",
      "Epoch 362, Loss: 24.488, Final Batch Loss: 0.738\n",
      "Epoch 363, Loss: 24.595, Final Batch Loss: 0.843\n",
      "Epoch 364, Loss: 24.631, Final Batch Loss: 0.719\n",
      "Epoch 365, Loss: 24.621, Final Batch Loss: 0.798\n",
      "Epoch 366, Loss: 24.478, Final Batch Loss: 0.589\n",
      "Epoch 367, Loss: 24.763, Final Batch Loss: 0.728\n",
      "Epoch 368, Loss: 24.607, Final Batch Loss: 0.634\n",
      "Epoch 369, Loss: 24.566, Final Batch Loss: 0.652\n",
      "Epoch 370, Loss: 24.432, Final Batch Loss: 0.647\n",
      "Epoch 371, Loss: 24.415, Final Batch Loss: 0.591\n",
      "Epoch 372, Loss: 24.471, Final Batch Loss: 0.773\n",
      "Epoch 373, Loss: 24.377, Final Batch Loss: 0.635\n",
      "Epoch 374, Loss: 24.512, Final Batch Loss: 0.716\n",
      "Epoch 375, Loss: 24.748, Final Batch Loss: 0.759\n",
      "Epoch 376, Loss: 24.559, Final Batch Loss: 0.711\n",
      "Epoch 377, Loss: 24.415, Final Batch Loss: 0.602\n",
      "Epoch 378, Loss: 24.357, Final Batch Loss: 0.612\n",
      "Epoch 379, Loss: 24.587, Final Batch Loss: 0.708\n",
      "Epoch 380, Loss: 24.342, Final Batch Loss: 0.637\n",
      "Epoch 381, Loss: 24.238, Final Batch Loss: 0.628\n",
      "Epoch 382, Loss: 24.295, Final Batch Loss: 0.605\n",
      "Epoch 383, Loss: 24.256, Final Batch Loss: 0.647\n",
      "Epoch 384, Loss: 24.314, Final Batch Loss: 0.672\n",
      "Epoch 385, Loss: 24.409, Final Batch Loss: 0.678\n",
      "Epoch 386, Loss: 24.488, Final Batch Loss: 0.688\n",
      "Epoch 387, Loss: 24.247, Final Batch Loss: 0.672\n",
      "Epoch 388, Loss: 24.488, Final Batch Loss: 0.693\n",
      "Epoch 389, Loss: 24.211, Final Batch Loss: 0.727\n",
      "Epoch 390, Loss: 24.320, Final Batch Loss: 0.721\n",
      "Epoch 391, Loss: 24.200, Final Batch Loss: 0.637\n",
      "Epoch 392, Loss: 24.123, Final Batch Loss: 0.619\n",
      "Epoch 393, Loss: 24.326, Final Batch Loss: 0.733\n",
      "Epoch 394, Loss: 24.313, Final Batch Loss: 0.673\n",
      "Epoch 395, Loss: 24.178, Final Batch Loss: 0.648\n",
      "Epoch 396, Loss: 24.213, Final Batch Loss: 0.748\n",
      "Epoch 397, Loss: 24.174, Final Batch Loss: 0.562\n",
      "Epoch 398, Loss: 24.424, Final Batch Loss: 0.660\n",
      "Epoch 399, Loss: 24.132, Final Batch Loss: 0.693\n",
      "Epoch 400, Loss: 24.141, Final Batch Loss: 0.627\n",
      "Epoch 401, Loss: 24.077, Final Batch Loss: 0.574\n",
      "Epoch 402, Loss: 24.217, Final Batch Loss: 0.656\n",
      "Epoch 403, Loss: 24.124, Final Batch Loss: 0.669\n",
      "Epoch 404, Loss: 24.405, Final Batch Loss: 0.829\n",
      "Epoch 405, Loss: 24.128, Final Batch Loss: 0.711\n",
      "Epoch 406, Loss: 24.425, Final Batch Loss: 0.675\n",
      "Epoch 407, Loss: 24.431, Final Batch Loss: 0.767\n",
      "Epoch 408, Loss: 24.454, Final Batch Loss: 0.718\n",
      "Epoch 409, Loss: 24.217, Final Batch Loss: 0.638\n",
      "Epoch 410, Loss: 23.979, Final Batch Loss: 0.697\n",
      "Epoch 411, Loss: 24.320, Final Batch Loss: 0.600\n",
      "Epoch 412, Loss: 24.104, Final Batch Loss: 0.646\n",
      "Epoch 413, Loss: 23.929, Final Batch Loss: 0.649\n",
      "Epoch 414, Loss: 24.225, Final Batch Loss: 0.711\n",
      "Epoch 415, Loss: 24.208, Final Batch Loss: 0.608\n",
      "Epoch 416, Loss: 24.205, Final Batch Loss: 0.725\n",
      "Epoch 417, Loss: 24.200, Final Batch Loss: 0.671\n",
      "Epoch 418, Loss: 24.091, Final Batch Loss: 0.642\n",
      "Epoch 419, Loss: 24.285, Final Batch Loss: 0.711\n",
      "Epoch 420, Loss: 24.168, Final Batch Loss: 0.595\n",
      "Epoch 421, Loss: 24.030, Final Batch Loss: 0.639\n",
      "Epoch 422, Loss: 24.106, Final Batch Loss: 0.662\n",
      "Epoch 423, Loss: 24.111, Final Batch Loss: 0.641\n",
      "Epoch 424, Loss: 24.062, Final Batch Loss: 0.598\n",
      "Epoch 425, Loss: 23.943, Final Batch Loss: 0.727\n",
      "Epoch 426, Loss: 24.044, Final Batch Loss: 0.614\n",
      "Epoch 427, Loss: 23.966, Final Batch Loss: 0.754\n",
      "Epoch 428, Loss: 24.052, Final Batch Loss: 0.734\n",
      "Epoch 429, Loss: 24.078, Final Batch Loss: 0.656\n",
      "Epoch 430, Loss: 23.891, Final Batch Loss: 0.712\n",
      "Epoch 431, Loss: 24.088, Final Batch Loss: 0.738\n",
      "Epoch 432, Loss: 23.888, Final Batch Loss: 0.639\n",
      "Epoch 433, Loss: 23.993, Final Batch Loss: 0.759\n",
      "Epoch 434, Loss: 24.166, Final Batch Loss: 0.680\n",
      "Epoch 435, Loss: 24.052, Final Batch Loss: 0.605\n",
      "Epoch 436, Loss: 23.979, Final Batch Loss: 0.615\n",
      "Epoch 437, Loss: 23.991, Final Batch Loss: 0.634\n",
      "Epoch 438, Loss: 23.930, Final Batch Loss: 0.581\n",
      "Epoch 439, Loss: 24.076, Final Batch Loss: 0.599\n",
      "Epoch 440, Loss: 23.919, Final Batch Loss: 0.601\n",
      "Epoch 441, Loss: 23.956, Final Batch Loss: 0.744\n",
      "Epoch 442, Loss: 23.988, Final Batch Loss: 0.732\n",
      "Epoch 443, Loss: 24.078, Final Batch Loss: 0.621\n",
      "Epoch 444, Loss: 23.977, Final Batch Loss: 0.830\n",
      "Epoch 445, Loss: 23.840, Final Batch Loss: 0.566\n",
      "Epoch 446, Loss: 24.094, Final Batch Loss: 0.778\n",
      "Epoch 447, Loss: 23.855, Final Batch Loss: 0.717\n",
      "Epoch 448, Loss: 24.160, Final Batch Loss: 0.736\n",
      "Epoch 449, Loss: 23.923, Final Batch Loss: 0.699\n",
      "Epoch 450, Loss: 23.957, Final Batch Loss: 0.634\n",
      "Epoch 451, Loss: 23.925, Final Batch Loss: 0.695\n",
      "Epoch 452, Loss: 23.943, Final Batch Loss: 0.617\n",
      "Epoch 453, Loss: 23.943, Final Batch Loss: 0.731\n",
      "Epoch 454, Loss: 23.838, Final Batch Loss: 0.686\n",
      "Epoch 455, Loss: 23.904, Final Batch Loss: 0.573\n",
      "Epoch 456, Loss: 23.783, Final Batch Loss: 0.632\n",
      "Epoch 457, Loss: 23.767, Final Batch Loss: 0.673\n",
      "Epoch 458, Loss: 23.878, Final Batch Loss: 0.656\n",
      "Epoch 459, Loss: 23.804, Final Batch Loss: 0.688\n",
      "Epoch 460, Loss: 23.869, Final Batch Loss: 0.803\n",
      "Epoch 461, Loss: 23.814, Final Batch Loss: 0.774\n",
      "Epoch 462, Loss: 23.813, Final Batch Loss: 0.661\n",
      "Epoch 463, Loss: 23.843, Final Batch Loss: 0.569\n",
      "Epoch 464, Loss: 23.895, Final Batch Loss: 0.681\n",
      "Epoch 465, Loss: 23.988, Final Batch Loss: 0.653\n",
      "Epoch 466, Loss: 23.793, Final Batch Loss: 0.702\n",
      "Epoch 467, Loss: 23.672, Final Batch Loss: 0.584\n",
      "Epoch 468, Loss: 23.745, Final Batch Loss: 0.578\n",
      "Epoch 469, Loss: 23.917, Final Batch Loss: 0.717\n",
      "Epoch 470, Loss: 23.782, Final Batch Loss: 0.800\n",
      "Epoch 471, Loss: 23.839, Final Batch Loss: 0.592\n",
      "Epoch 472, Loss: 23.880, Final Batch Loss: 0.687\n",
      "Epoch 473, Loss: 23.832, Final Batch Loss: 0.642\n",
      "Epoch 474, Loss: 23.672, Final Batch Loss: 0.624\n",
      "Epoch 475, Loss: 23.756, Final Batch Loss: 0.676\n",
      "Epoch 476, Loss: 23.798, Final Batch Loss: 0.702\n",
      "Epoch 477, Loss: 23.887, Final Batch Loss: 0.812\n",
      "Epoch 478, Loss: 23.589, Final Batch Loss: 0.652\n",
      "Epoch 479, Loss: 23.602, Final Batch Loss: 0.672\n",
      "Epoch 480, Loss: 23.899, Final Batch Loss: 0.720\n",
      "Epoch 481, Loss: 23.727, Final Batch Loss: 0.622\n",
      "Epoch 482, Loss: 23.658, Final Batch Loss: 0.637\n",
      "Epoch 483, Loss: 23.590, Final Batch Loss: 0.692\n",
      "Epoch 484, Loss: 23.513, Final Batch Loss: 0.640\n",
      "Epoch 485, Loss: 23.725, Final Batch Loss: 0.716\n",
      "Epoch 486, Loss: 23.797, Final Batch Loss: 0.600\n",
      "Epoch 487, Loss: 23.863, Final Batch Loss: 0.601\n",
      "Epoch 488, Loss: 23.716, Final Batch Loss: 0.710\n",
      "Epoch 489, Loss: 23.768, Final Batch Loss: 0.635\n",
      "Epoch 490, Loss: 23.873, Final Batch Loss: 0.686\n",
      "Epoch 491, Loss: 23.650, Final Batch Loss: 0.695\n",
      "Epoch 492, Loss: 23.645, Final Batch Loss: 0.587\n",
      "Epoch 493, Loss: 23.729, Final Batch Loss: 0.644\n",
      "Epoch 494, Loss: 23.727, Final Batch Loss: 0.607\n",
      "Epoch 495, Loss: 23.577, Final Batch Loss: 0.583\n",
      "Epoch 496, Loss: 23.493, Final Batch Loss: 0.553\n",
      "Epoch 497, Loss: 23.706, Final Batch Loss: 0.670\n",
      "Epoch 498, Loss: 23.736, Final Batch Loss: 0.665\n",
      "Epoch 499, Loss: 23.603, Final Batch Loss: 0.578\n",
      "Epoch 500, Loss: 23.673, Final Batch Loss: 0.685\n",
      "Epoch 501, Loss: 23.668, Final Batch Loss: 0.579\n",
      "Epoch 502, Loss: 23.656, Final Batch Loss: 0.716\n",
      "Epoch 503, Loss: 23.753, Final Batch Loss: 0.632\n",
      "Epoch 504, Loss: 23.601, Final Batch Loss: 0.743\n",
      "Epoch 505, Loss: 23.655, Final Batch Loss: 0.675\n",
      "Epoch 506, Loss: 23.469, Final Batch Loss: 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 507, Loss: 23.570, Final Batch Loss: 0.715\n",
      "Epoch 508, Loss: 23.718, Final Batch Loss: 0.691\n",
      "Epoch 509, Loss: 23.539, Final Batch Loss: 0.665\n",
      "Epoch 510, Loss: 23.584, Final Batch Loss: 0.607\n",
      "Epoch 511, Loss: 23.636, Final Batch Loss: 0.697\n",
      "Epoch 512, Loss: 23.477, Final Batch Loss: 0.631\n",
      "Epoch 513, Loss: 23.673, Final Batch Loss: 0.622\n",
      "Epoch 514, Loss: 23.531, Final Batch Loss: 0.684\n",
      "Epoch 515, Loss: 23.608, Final Batch Loss: 0.639\n",
      "Epoch 516, Loss: 23.646, Final Batch Loss: 0.557\n",
      "Epoch 517, Loss: 23.496, Final Batch Loss: 0.691\n",
      "Epoch 518, Loss: 23.525, Final Batch Loss: 0.560\n",
      "Epoch 519, Loss: 23.586, Final Batch Loss: 0.582\n",
      "Epoch 520, Loss: 23.469, Final Batch Loss: 0.666\n",
      "Epoch 521, Loss: 23.605, Final Batch Loss: 0.629\n",
      "Epoch 522, Loss: 23.590, Final Batch Loss: 0.680\n",
      "Epoch 523, Loss: 23.443, Final Batch Loss: 0.602\n",
      "Epoch 524, Loss: 23.429, Final Batch Loss: 0.695\n",
      "Epoch 525, Loss: 23.619, Final Batch Loss: 0.658\n",
      "Epoch 526, Loss: 23.414, Final Batch Loss: 0.669\n",
      "Epoch 527, Loss: 23.748, Final Batch Loss: 0.711\n",
      "Epoch 528, Loss: 23.642, Final Batch Loss: 0.649\n",
      "Epoch 529, Loss: 23.614, Final Batch Loss: 0.686\n",
      "Epoch 530, Loss: 23.411, Final Batch Loss: 0.691\n",
      "Epoch 531, Loss: 23.350, Final Batch Loss: 0.561\n",
      "Epoch 532, Loss: 23.581, Final Batch Loss: 0.689\n",
      "Epoch 533, Loss: 23.524, Final Batch Loss: 0.683\n",
      "Epoch 534, Loss: 23.349, Final Batch Loss: 0.688\n",
      "Epoch 535, Loss: 23.451, Final Batch Loss: 0.653\n",
      "Epoch 536, Loss: 23.321, Final Batch Loss: 0.596\n",
      "Epoch 537, Loss: 23.587, Final Batch Loss: 0.631\n",
      "Epoch 538, Loss: 23.328, Final Batch Loss: 0.551\n",
      "Epoch 539, Loss: 23.573, Final Batch Loss: 0.705\n",
      "Epoch 540, Loss: 23.458, Final Batch Loss: 0.627\n",
      "Epoch 541, Loss: 23.497, Final Batch Loss: 0.533\n",
      "Epoch 542, Loss: 23.382, Final Batch Loss: 0.716\n",
      "Epoch 543, Loss: 23.416, Final Batch Loss: 0.638\n",
      "Epoch 544, Loss: 23.388, Final Batch Loss: 0.541\n",
      "Epoch 545, Loss: 23.412, Final Batch Loss: 0.671\n",
      "Epoch 546, Loss: 23.389, Final Batch Loss: 0.647\n",
      "Epoch 547, Loss: 23.459, Final Batch Loss: 0.695\n",
      "Epoch 548, Loss: 23.324, Final Batch Loss: 0.697\n",
      "Epoch 549, Loss: 23.342, Final Batch Loss: 0.565\n",
      "Epoch 550, Loss: 23.436, Final Batch Loss: 0.628\n",
      "Epoch 551, Loss: 23.406, Final Batch Loss: 0.732\n",
      "Epoch 552, Loss: 23.334, Final Batch Loss: 0.554\n",
      "Epoch 553, Loss: 23.472, Final Batch Loss: 0.650\n",
      "Epoch 554, Loss: 23.424, Final Batch Loss: 0.655\n",
      "Epoch 555, Loss: 23.435, Final Batch Loss: 0.716\n",
      "Epoch 556, Loss: 23.245, Final Batch Loss: 0.584\n",
      "Epoch 557, Loss: 23.128, Final Batch Loss: 0.661\n",
      "Epoch 558, Loss: 23.280, Final Batch Loss: 0.548\n",
      "Epoch 559, Loss: 23.264, Final Batch Loss: 0.618\n",
      "Epoch 560, Loss: 23.518, Final Batch Loss: 0.689\n",
      "Epoch 561, Loss: 23.398, Final Batch Loss: 0.752\n",
      "Epoch 562, Loss: 23.328, Final Batch Loss: 0.606\n",
      "Epoch 563, Loss: 23.318, Final Batch Loss: 0.577\n",
      "Epoch 564, Loss: 23.579, Final Batch Loss: 0.692\n",
      "Epoch 565, Loss: 23.411, Final Batch Loss: 0.634\n",
      "Epoch 566, Loss: 23.144, Final Batch Loss: 0.657\n",
      "Epoch 567, Loss: 23.364, Final Batch Loss: 0.715\n",
      "Epoch 568, Loss: 23.277, Final Batch Loss: 0.653\n",
      "Epoch 569, Loss: 23.275, Final Batch Loss: 0.702\n",
      "Epoch 570, Loss: 23.344, Final Batch Loss: 0.653\n",
      "Epoch 571, Loss: 23.580, Final Batch Loss: 0.649\n",
      "Epoch 572, Loss: 23.274, Final Batch Loss: 0.611\n",
      "Epoch 573, Loss: 23.278, Final Batch Loss: 0.570\n",
      "Epoch 574, Loss: 23.301, Final Batch Loss: 0.604\n",
      "Epoch 575, Loss: 23.506, Final Batch Loss: 0.665\n",
      "Epoch 576, Loss: 23.420, Final Batch Loss: 0.633\n",
      "Epoch 577, Loss: 23.332, Final Batch Loss: 0.599\n",
      "Epoch 578, Loss: 23.104, Final Batch Loss: 0.677\n",
      "Epoch 579, Loss: 23.314, Final Batch Loss: 0.592\n",
      "Epoch 580, Loss: 23.577, Final Batch Loss: 0.759\n",
      "Epoch 581, Loss: 23.140, Final Batch Loss: 0.557\n",
      "Epoch 582, Loss: 23.286, Final Batch Loss: 0.730\n",
      "Epoch 583, Loss: 23.321, Final Batch Loss: 0.674\n",
      "Epoch 584, Loss: 23.183, Final Batch Loss: 0.629\n",
      "Epoch 585, Loss: 23.043, Final Batch Loss: 0.579\n",
      "Epoch 586, Loss: 23.346, Final Batch Loss: 0.693\n",
      "Epoch 587, Loss: 23.144, Final Batch Loss: 0.524\n",
      "Epoch 588, Loss: 23.269, Final Batch Loss: 0.609\n",
      "Epoch 589, Loss: 23.224, Final Batch Loss: 0.671\n",
      "Epoch 590, Loss: 23.276, Final Batch Loss: 0.656\n",
      "Epoch 591, Loss: 23.194, Final Batch Loss: 0.511\n",
      "Epoch 592, Loss: 23.223, Final Batch Loss: 0.578\n",
      "Epoch 593, Loss: 23.083, Final Batch Loss: 0.540\n",
      "Epoch 594, Loss: 23.246, Final Batch Loss: 0.586\n",
      "Epoch 595, Loss: 23.201, Final Batch Loss: 0.732\n",
      "Epoch 596, Loss: 23.248, Final Batch Loss: 0.656\n",
      "Epoch 597, Loss: 23.178, Final Batch Loss: 0.700\n",
      "Epoch 598, Loss: 23.362, Final Batch Loss: 0.676\n",
      "Epoch 599, Loss: 23.281, Final Batch Loss: 0.641\n",
      "Epoch 600, Loss: 23.311, Final Batch Loss: 0.608\n",
      "Epoch 601, Loss: 23.344, Final Batch Loss: 0.687\n",
      "Epoch 602, Loss: 23.097, Final Batch Loss: 0.636\n",
      "Epoch 603, Loss: 23.271, Final Batch Loss: 0.717\n",
      "Epoch 604, Loss: 23.017, Final Batch Loss: 0.750\n",
      "Epoch 605, Loss: 23.075, Final Batch Loss: 0.685\n",
      "Epoch 606, Loss: 23.303, Final Batch Loss: 0.679\n",
      "Epoch 607, Loss: 23.178, Final Batch Loss: 0.700\n",
      "Epoch 608, Loss: 23.077, Final Batch Loss: 0.661\n",
      "Epoch 609, Loss: 23.283, Final Batch Loss: 0.810\n",
      "Epoch 610, Loss: 23.108, Final Batch Loss: 0.605\n",
      "Epoch 611, Loss: 23.162, Final Batch Loss: 0.676\n",
      "Epoch 612, Loss: 23.119, Final Batch Loss: 0.661\n",
      "Epoch 613, Loss: 23.186, Final Batch Loss: 0.670\n",
      "Epoch 614, Loss: 23.291, Final Batch Loss: 0.703\n",
      "Epoch 615, Loss: 23.213, Final Batch Loss: 0.674\n",
      "Epoch 616, Loss: 22.840, Final Batch Loss: 0.558\n",
      "Epoch 617, Loss: 22.995, Final Batch Loss: 0.625\n",
      "Epoch 618, Loss: 23.151, Final Batch Loss: 0.553\n",
      "Epoch 619, Loss: 23.264, Final Batch Loss: 0.616\n",
      "Epoch 620, Loss: 23.232, Final Batch Loss: 0.642\n",
      "Epoch 621, Loss: 23.064, Final Batch Loss: 0.622\n",
      "Epoch 622, Loss: 23.154, Final Batch Loss: 0.600\n",
      "Epoch 623, Loss: 23.135, Final Batch Loss: 0.675\n",
      "Epoch 624, Loss: 23.140, Final Batch Loss: 0.628\n",
      "Epoch 625, Loss: 23.027, Final Batch Loss: 0.530\n",
      "Epoch 626, Loss: 23.249, Final Batch Loss: 0.700\n",
      "Epoch 627, Loss: 23.030, Final Batch Loss: 0.706\n",
      "Epoch 628, Loss: 23.014, Final Batch Loss: 0.634\n",
      "Epoch 629, Loss: 23.023, Final Batch Loss: 0.613\n",
      "Epoch 630, Loss: 22.844, Final Batch Loss: 0.647\n",
      "Epoch 631, Loss: 22.902, Final Batch Loss: 0.503\n",
      "Epoch 632, Loss: 23.105, Final Batch Loss: 0.644\n",
      "Epoch 633, Loss: 22.834, Final Batch Loss: 0.653\n",
      "Epoch 634, Loss: 23.258, Final Batch Loss: 0.603\n",
      "Epoch 635, Loss: 22.909, Final Batch Loss: 0.640\n",
      "Epoch 636, Loss: 23.192, Final Batch Loss: 0.572\n",
      "Epoch 637, Loss: 22.994, Final Batch Loss: 0.589\n",
      "Epoch 638, Loss: 22.967, Final Batch Loss: 0.644\n",
      "Epoch 639, Loss: 23.090, Final Batch Loss: 0.590\n",
      "Epoch 640, Loss: 22.899, Final Batch Loss: 0.607\n",
      "Epoch 641, Loss: 23.178, Final Batch Loss: 0.724\n",
      "Epoch 642, Loss: 23.139, Final Batch Loss: 0.697\n",
      "Epoch 643, Loss: 23.092, Final Batch Loss: 0.609\n",
      "Epoch 644, Loss: 22.886, Final Batch Loss: 0.654\n",
      "Epoch 645, Loss: 23.158, Final Batch Loss: 0.685\n",
      "Epoch 646, Loss: 23.135, Final Batch Loss: 0.658\n",
      "Epoch 647, Loss: 22.994, Final Batch Loss: 0.656\n",
      "Epoch 648, Loss: 23.010, Final Batch Loss: 0.677\n",
      "Epoch 649, Loss: 23.095, Final Batch Loss: 0.703\n",
      "Epoch 650, Loss: 23.079, Final Batch Loss: 0.563\n",
      "Epoch 651, Loss: 22.847, Final Batch Loss: 0.618\n",
      "Epoch 652, Loss: 22.884, Final Batch Loss: 0.685\n",
      "Epoch 653, Loss: 22.807, Final Batch Loss: 0.614\n",
      "Epoch 654, Loss: 22.938, Final Batch Loss: 0.545\n",
      "Epoch 655, Loss: 23.036, Final Batch Loss: 0.591\n",
      "Epoch 656, Loss: 23.059, Final Batch Loss: 0.621\n",
      "Epoch 657, Loss: 22.715, Final Batch Loss: 0.609\n",
      "Epoch 658, Loss: 22.876, Final Batch Loss: 0.607\n",
      "Epoch 659, Loss: 23.046, Final Batch Loss: 0.602\n",
      "Epoch 660, Loss: 22.773, Final Batch Loss: 0.533\n",
      "Epoch 661, Loss: 22.833, Final Batch Loss: 0.646\n",
      "Epoch 662, Loss: 23.219, Final Batch Loss: 0.635\n",
      "Epoch 663, Loss: 22.942, Final Batch Loss: 0.643\n",
      "Epoch 664, Loss: 22.889, Final Batch Loss: 0.601\n",
      "Epoch 665, Loss: 22.921, Final Batch Loss: 0.599\n",
      "Epoch 666, Loss: 22.967, Final Batch Loss: 0.651\n",
      "Epoch 667, Loss: 23.027, Final Batch Loss: 0.620\n",
      "Epoch 668, Loss: 22.888, Final Batch Loss: 0.618\n",
      "Epoch 669, Loss: 22.810, Final Batch Loss: 0.690\n",
      "Epoch 670, Loss: 23.009, Final Batch Loss: 0.634\n",
      "Epoch 671, Loss: 22.886, Final Batch Loss: 0.652\n",
      "Epoch 672, Loss: 22.775, Final Batch Loss: 0.580\n",
      "Epoch 673, Loss: 22.799, Final Batch Loss: 0.609\n",
      "Epoch 674, Loss: 22.992, Final Batch Loss: 0.601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 675, Loss: 22.772, Final Batch Loss: 0.589\n",
      "Epoch 676, Loss: 22.958, Final Batch Loss: 0.773\n",
      "Epoch 677, Loss: 22.942, Final Batch Loss: 0.686\n",
      "Epoch 678, Loss: 22.933, Final Batch Loss: 0.633\n",
      "Epoch 679, Loss: 22.924, Final Batch Loss: 0.574\n",
      "Epoch 680, Loss: 22.970, Final Batch Loss: 0.583\n",
      "Epoch 681, Loss: 22.925, Final Batch Loss: 0.563\n",
      "Epoch 682, Loss: 23.060, Final Batch Loss: 0.745\n",
      "Epoch 683, Loss: 23.113, Final Batch Loss: 0.714\n",
      "Epoch 684, Loss: 22.929, Final Batch Loss: 0.575\n",
      "Epoch 685, Loss: 22.984, Final Batch Loss: 0.574\n",
      "Epoch 686, Loss: 22.820, Final Batch Loss: 0.622\n",
      "Epoch 687, Loss: 22.876, Final Batch Loss: 0.632\n",
      "Epoch 688, Loss: 23.004, Final Batch Loss: 0.634\n",
      "Epoch 689, Loss: 22.937, Final Batch Loss: 0.539\n",
      "Epoch 690, Loss: 22.890, Final Batch Loss: 0.744\n",
      "Epoch 691, Loss: 22.780, Final Batch Loss: 0.604\n",
      "Epoch 692, Loss: 22.853, Final Batch Loss: 0.622\n",
      "Epoch 693, Loss: 22.683, Final Batch Loss: 0.567\n",
      "Epoch 694, Loss: 22.968, Final Batch Loss: 0.519\n",
      "Epoch 695, Loss: 22.734, Final Batch Loss: 0.690\n",
      "Epoch 696, Loss: 22.850, Final Batch Loss: 0.584\n",
      "Epoch 697, Loss: 22.790, Final Batch Loss: 0.593\n",
      "Epoch 698, Loss: 22.829, Final Batch Loss: 0.631\n",
      "Epoch 699, Loss: 22.459, Final Batch Loss: 0.491\n",
      "Epoch 700, Loss: 22.977, Final Batch Loss: 0.566\n",
      "Epoch 701, Loss: 22.864, Final Batch Loss: 0.744\n",
      "Epoch 702, Loss: 22.981, Final Batch Loss: 0.660\n",
      "Epoch 703, Loss: 22.916, Final Batch Loss: 0.622\n",
      "Epoch 704, Loss: 22.835, Final Batch Loss: 0.649\n",
      "Epoch 705, Loss: 22.762, Final Batch Loss: 0.622\n",
      "Epoch 706, Loss: 22.905, Final Batch Loss: 0.774\n",
      "Epoch 707, Loss: 22.837, Final Batch Loss: 0.570\n",
      "Epoch 708, Loss: 23.019, Final Batch Loss: 0.557\n",
      "Epoch 709, Loss: 22.668, Final Batch Loss: 0.533\n",
      "Epoch 710, Loss: 22.816, Final Batch Loss: 0.684\n",
      "Epoch 711, Loss: 22.848, Final Batch Loss: 0.701\n",
      "Epoch 712, Loss: 22.709, Final Batch Loss: 0.662\n",
      "Epoch 713, Loss: 23.029, Final Batch Loss: 0.633\n",
      "Epoch 714, Loss: 22.677, Final Batch Loss: 0.514\n",
      "Epoch 715, Loss: 22.835, Final Batch Loss: 0.611\n",
      "Epoch 716, Loss: 22.697, Final Batch Loss: 0.625\n",
      "Epoch 717, Loss: 22.806, Final Batch Loss: 0.589\n",
      "Epoch 718, Loss: 22.722, Final Batch Loss: 0.680\n",
      "Epoch 719, Loss: 22.552, Final Batch Loss: 0.520\n",
      "Epoch 720, Loss: 22.657, Final Batch Loss: 0.652\n",
      "Epoch 721, Loss: 22.697, Final Batch Loss: 0.664\n",
      "Epoch 722, Loss: 22.682, Final Batch Loss: 0.671\n",
      "Epoch 723, Loss: 22.823, Final Batch Loss: 0.766\n",
      "Epoch 724, Loss: 22.686, Final Batch Loss: 0.596\n",
      "Epoch 725, Loss: 22.823, Final Batch Loss: 0.664\n",
      "Epoch 726, Loss: 22.690, Final Batch Loss: 0.595\n",
      "Epoch 727, Loss: 22.923, Final Batch Loss: 0.729\n",
      "Epoch 728, Loss: 22.816, Final Batch Loss: 0.546\n",
      "Epoch 729, Loss: 22.732, Final Batch Loss: 0.535\n",
      "Epoch 730, Loss: 22.831, Final Batch Loss: 0.735\n",
      "Epoch 731, Loss: 22.685, Final Batch Loss: 0.568\n",
      "Epoch 732, Loss: 22.540, Final Batch Loss: 0.603\n",
      "Epoch 733, Loss: 23.033, Final Batch Loss: 0.756\n",
      "Epoch 734, Loss: 22.870, Final Batch Loss: 0.542\n",
      "Epoch 735, Loss: 22.610, Final Batch Loss: 0.687\n",
      "Epoch 736, Loss: 22.723, Final Batch Loss: 0.580\n",
      "Epoch 737, Loss: 22.875, Final Batch Loss: 0.715\n",
      "Epoch 738, Loss: 22.679, Final Batch Loss: 0.633\n",
      "Epoch 739, Loss: 22.732, Final Batch Loss: 0.679\n",
      "Epoch 740, Loss: 22.760, Final Batch Loss: 0.545\n",
      "Epoch 741, Loss: 22.442, Final Batch Loss: 0.556\n",
      "Epoch 742, Loss: 22.629, Final Batch Loss: 0.632\n",
      "Epoch 743, Loss: 22.676, Final Batch Loss: 0.594\n",
      "Epoch 744, Loss: 22.733, Final Batch Loss: 0.628\n",
      "Epoch 745, Loss: 22.830, Final Batch Loss: 0.679\n",
      "Epoch 746, Loss: 22.669, Final Batch Loss: 0.582\n",
      "Epoch 747, Loss: 22.609, Final Batch Loss: 0.654\n",
      "Epoch 748, Loss: 22.677, Final Batch Loss: 0.616\n",
      "Epoch 749, Loss: 22.773, Final Batch Loss: 0.569\n",
      "Epoch 750, Loss: 22.765, Final Batch Loss: 0.669\n",
      "Epoch 751, Loss: 22.714, Final Batch Loss: 0.558\n",
      "Epoch 752, Loss: 22.594, Final Batch Loss: 0.578\n",
      "Epoch 753, Loss: 22.760, Final Batch Loss: 0.610\n",
      "Epoch 754, Loss: 22.801, Final Batch Loss: 0.644\n",
      "Epoch 755, Loss: 22.699, Final Batch Loss: 0.630\n",
      "Epoch 756, Loss: 22.606, Final Batch Loss: 0.630\n",
      "Epoch 757, Loss: 22.877, Final Batch Loss: 0.868\n",
      "Epoch 758, Loss: 22.477, Final Batch Loss: 0.522\n",
      "Epoch 759, Loss: 22.605, Final Batch Loss: 0.592\n",
      "Epoch 760, Loss: 22.575, Final Batch Loss: 0.681\n",
      "Epoch 761, Loss: 22.578, Final Batch Loss: 0.513\n",
      "Epoch 762, Loss: 22.471, Final Batch Loss: 0.702\n",
      "Epoch 763, Loss: 22.806, Final Batch Loss: 0.585\n",
      "Epoch 764, Loss: 22.553, Final Batch Loss: 0.667\n",
      "Epoch 765, Loss: 22.697, Final Batch Loss: 0.495\n",
      "Epoch 766, Loss: 22.690, Final Batch Loss: 0.712\n",
      "Epoch 767, Loss: 22.406, Final Batch Loss: 0.590\n",
      "Epoch 768, Loss: 22.639, Final Batch Loss: 0.544\n",
      "Epoch 769, Loss: 22.656, Final Batch Loss: 0.656\n",
      "Epoch 770, Loss: 22.667, Final Batch Loss: 0.652\n",
      "Epoch 771, Loss: 22.583, Final Batch Loss: 0.519\n",
      "Epoch 772, Loss: 22.383, Final Batch Loss: 0.544\n",
      "Epoch 773, Loss: 22.546, Final Batch Loss: 0.636\n",
      "Epoch 774, Loss: 22.637, Final Batch Loss: 0.624\n",
      "Epoch 775, Loss: 22.717, Final Batch Loss: 0.574\n",
      "Epoch 776, Loss: 22.587, Final Batch Loss: 0.643\n",
      "Epoch 777, Loss: 22.478, Final Batch Loss: 0.495\n",
      "Epoch 778, Loss: 22.845, Final Batch Loss: 0.641\n",
      "Epoch 779, Loss: 22.594, Final Batch Loss: 0.542\n",
      "Epoch 780, Loss: 22.496, Final Batch Loss: 0.602\n",
      "Epoch 781, Loss: 22.678, Final Batch Loss: 0.586\n",
      "Epoch 782, Loss: 22.578, Final Batch Loss: 0.722\n",
      "Epoch 783, Loss: 22.519, Final Batch Loss: 0.645\n",
      "Epoch 784, Loss: 22.494, Final Batch Loss: 0.513\n",
      "Epoch 785, Loss: 22.620, Final Batch Loss: 0.675\n",
      "Epoch 786, Loss: 22.764, Final Batch Loss: 0.686\n",
      "Epoch 787, Loss: 22.548, Final Batch Loss: 0.725\n",
      "Epoch 788, Loss: 22.521, Final Batch Loss: 0.614\n",
      "Epoch 789, Loss: 22.590, Final Batch Loss: 0.559\n",
      "Epoch 790, Loss: 22.676, Final Batch Loss: 0.693\n",
      "Epoch 791, Loss: 22.498, Final Batch Loss: 0.669\n",
      "Epoch 792, Loss: 22.631, Final Batch Loss: 0.575\n",
      "Epoch 793, Loss: 22.751, Final Batch Loss: 0.595\n",
      "Epoch 794, Loss: 22.547, Final Batch Loss: 0.633\n",
      "Epoch 795, Loss: 22.449, Final Batch Loss: 0.662\n",
      "Epoch 796, Loss: 22.539, Final Batch Loss: 0.507\n",
      "Epoch 797, Loss: 22.413, Final Batch Loss: 0.668\n",
      "Epoch 798, Loss: 22.551, Final Batch Loss: 0.530\n",
      "Epoch 799, Loss: 22.509, Final Batch Loss: 0.600\n",
      "Epoch 800, Loss: 22.639, Final Batch Loss: 0.685\n",
      "Epoch 801, Loss: 22.644, Final Batch Loss: 0.602\n",
      "Epoch 802, Loss: 22.433, Final Batch Loss: 0.778\n",
      "Epoch 803, Loss: 22.529, Final Batch Loss: 0.656\n",
      "Epoch 804, Loss: 22.598, Final Batch Loss: 0.652\n",
      "Epoch 805, Loss: 22.553, Final Batch Loss: 0.601\n",
      "Epoch 806, Loss: 22.621, Final Batch Loss: 0.661\n",
      "Epoch 807, Loss: 22.474, Final Batch Loss: 0.667\n",
      "Epoch 808, Loss: 22.410, Final Batch Loss: 0.508\n",
      "Epoch 809, Loss: 22.625, Final Batch Loss: 0.658\n",
      "Epoch 810, Loss: 22.342, Final Batch Loss: 0.589\n",
      "Epoch 811, Loss: 22.462, Final Batch Loss: 0.557\n",
      "Epoch 812, Loss: 22.461, Final Batch Loss: 0.710\n",
      "Epoch 813, Loss: 22.488, Final Batch Loss: 0.707\n",
      "Epoch 814, Loss: 22.479, Final Batch Loss: 0.623\n",
      "Epoch 815, Loss: 22.467, Final Batch Loss: 0.590\n",
      "Epoch 816, Loss: 22.419, Final Batch Loss: 0.672\n",
      "Epoch 817, Loss: 22.307, Final Batch Loss: 0.684\n",
      "Epoch 818, Loss: 22.327, Final Batch Loss: 0.641\n",
      "Epoch 819, Loss: 22.404, Final Batch Loss: 0.775\n",
      "Epoch 820, Loss: 22.499, Final Batch Loss: 0.636\n",
      "Epoch 821, Loss: 22.416, Final Batch Loss: 0.602\n",
      "Epoch 822, Loss: 22.565, Final Batch Loss: 0.621\n",
      "Epoch 823, Loss: 22.229, Final Batch Loss: 0.551\n",
      "Epoch 824, Loss: 22.185, Final Batch Loss: 0.541\n",
      "Epoch 825, Loss: 22.360, Final Batch Loss: 0.636\n",
      "Epoch 826, Loss: 22.404, Final Batch Loss: 0.516\n",
      "Epoch 827, Loss: 22.530, Final Batch Loss: 0.702\n",
      "Epoch 828, Loss: 22.341, Final Batch Loss: 0.653\n",
      "Epoch 829, Loss: 22.628, Final Batch Loss: 0.755\n",
      "Epoch 830, Loss: 22.424, Final Batch Loss: 0.656\n",
      "Epoch 831, Loss: 22.288, Final Batch Loss: 0.646\n",
      "Epoch 832, Loss: 22.496, Final Batch Loss: 0.649\n",
      "Epoch 833, Loss: 22.262, Final Batch Loss: 0.542\n",
      "Epoch 834, Loss: 22.375, Final Batch Loss: 0.579\n",
      "Epoch 835, Loss: 22.407, Final Batch Loss: 0.648\n",
      "Epoch 836, Loss: 22.323, Final Batch Loss: 0.608\n",
      "Epoch 837, Loss: 22.506, Final Batch Loss: 0.534\n",
      "Epoch 838, Loss: 22.338, Final Batch Loss: 0.733\n",
      "Epoch 839, Loss: 22.311, Final Batch Loss: 0.680\n",
      "Epoch 840, Loss: 22.288, Final Batch Loss: 0.593\n",
      "Epoch 841, Loss: 22.152, Final Batch Loss: 0.602\n",
      "Epoch 842, Loss: 22.275, Final Batch Loss: 0.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 843, Loss: 22.269, Final Batch Loss: 0.540\n",
      "Epoch 844, Loss: 22.372, Final Batch Loss: 0.616\n",
      "Epoch 845, Loss: 22.506, Final Batch Loss: 0.602\n",
      "Epoch 846, Loss: 22.413, Final Batch Loss: 0.615\n",
      "Epoch 847, Loss: 22.194, Final Batch Loss: 0.608\n",
      "Epoch 848, Loss: 22.343, Final Batch Loss: 0.646\n",
      "Epoch 849, Loss: 22.539, Final Batch Loss: 0.679\n",
      "Epoch 850, Loss: 22.409, Final Batch Loss: 0.644\n",
      "Epoch 851, Loss: 22.302, Final Batch Loss: 0.579\n",
      "Epoch 852, Loss: 22.480, Final Batch Loss: 0.625\n",
      "Epoch 853, Loss: 22.571, Final Batch Loss: 0.764\n",
      "Epoch 854, Loss: 22.278, Final Batch Loss: 0.669\n",
      "Epoch 855, Loss: 22.419, Final Batch Loss: 0.633\n",
      "Epoch 856, Loss: 22.373, Final Batch Loss: 0.610\n",
      "Epoch 857, Loss: 22.551, Final Batch Loss: 0.593\n",
      "Epoch 858, Loss: 22.188, Final Batch Loss: 0.563\n",
      "Epoch 859, Loss: 22.350, Final Batch Loss: 0.690\n",
      "Epoch 860, Loss: 22.107, Final Batch Loss: 0.525\n",
      "Epoch 861, Loss: 22.414, Final Batch Loss: 0.554\n",
      "Epoch 862, Loss: 22.260, Final Batch Loss: 0.682\n",
      "Epoch 863, Loss: 22.326, Final Batch Loss: 0.580\n",
      "Epoch 864, Loss: 22.245, Final Batch Loss: 0.628\n",
      "Epoch 865, Loss: 22.418, Final Batch Loss: 0.655\n",
      "Epoch 866, Loss: 22.200, Final Batch Loss: 0.709\n",
      "Epoch 867, Loss: 22.183, Final Batch Loss: 0.604\n",
      "Epoch 868, Loss: 22.420, Final Batch Loss: 0.712\n",
      "Epoch 869, Loss: 22.164, Final Batch Loss: 0.579\n",
      "Epoch 870, Loss: 22.431, Final Batch Loss: 0.723\n",
      "Epoch 871, Loss: 22.271, Final Batch Loss: 0.723\n",
      "Epoch 872, Loss: 22.196, Final Batch Loss: 0.623\n",
      "Epoch 873, Loss: 22.470, Final Batch Loss: 0.610\n",
      "Epoch 874, Loss: 22.364, Final Batch Loss: 0.627\n",
      "Epoch 875, Loss: 22.331, Final Batch Loss: 0.693\n",
      "Epoch 876, Loss: 22.542, Final Batch Loss: 0.656\n",
      "Epoch 877, Loss: 22.192, Final Batch Loss: 0.630\n",
      "Epoch 878, Loss: 22.214, Final Batch Loss: 0.654\n",
      "Epoch 879, Loss: 22.345, Final Batch Loss: 0.737\n",
      "Epoch 880, Loss: 22.089, Final Batch Loss: 0.591\n",
      "Epoch 881, Loss: 22.092, Final Batch Loss: 0.602\n",
      "Epoch 882, Loss: 22.570, Final Batch Loss: 0.611\n",
      "Epoch 883, Loss: 22.247, Final Batch Loss: 0.632\n",
      "Epoch 884, Loss: 22.154, Final Batch Loss: 0.544\n",
      "Epoch 885, Loss: 22.254, Final Batch Loss: 0.589\n",
      "Epoch 886, Loss: 22.106, Final Batch Loss: 0.621\n",
      "Epoch 887, Loss: 22.036, Final Batch Loss: 0.510\n",
      "Epoch 888, Loss: 22.346, Final Batch Loss: 0.675\n",
      "Epoch 889, Loss: 22.058, Final Batch Loss: 0.648\n",
      "Epoch 890, Loss: 22.202, Final Batch Loss: 0.513\n",
      "Epoch 891, Loss: 22.415, Final Batch Loss: 0.777\n",
      "Epoch 892, Loss: 22.223, Final Batch Loss: 0.638\n",
      "Epoch 893, Loss: 22.161, Final Batch Loss: 0.552\n",
      "Epoch 894, Loss: 22.269, Final Batch Loss: 0.567\n",
      "Epoch 895, Loss: 22.254, Final Batch Loss: 0.674\n",
      "Epoch 896, Loss: 22.267, Final Batch Loss: 0.571\n",
      "Epoch 897, Loss: 22.340, Final Batch Loss: 0.631\n",
      "Epoch 898, Loss: 22.139, Final Batch Loss: 0.716\n",
      "Epoch 899, Loss: 22.451, Final Batch Loss: 0.707\n",
      "Epoch 900, Loss: 22.207, Final Batch Loss: 0.596\n",
      "Epoch 901, Loss: 22.239, Final Batch Loss: 0.530\n",
      "Epoch 902, Loss: 22.276, Final Batch Loss: 0.604\n",
      "Epoch 903, Loss: 22.380, Final Batch Loss: 0.598\n",
      "Epoch 904, Loss: 22.346, Final Batch Loss: 0.566\n",
      "Epoch 905, Loss: 22.321, Final Batch Loss: 0.685\n",
      "Epoch 906, Loss: 22.385, Final Batch Loss: 0.664\n",
      "Epoch 907, Loss: 22.373, Final Batch Loss: 0.634\n",
      "Epoch 908, Loss: 22.278, Final Batch Loss: 0.658\n",
      "Epoch 909, Loss: 22.227, Final Batch Loss: 0.683\n",
      "Epoch 910, Loss: 22.140, Final Batch Loss: 0.694\n",
      "Epoch 911, Loss: 22.319, Final Batch Loss: 0.591\n",
      "Epoch 912, Loss: 22.205, Final Batch Loss: 0.620\n",
      "Epoch 913, Loss: 22.319, Final Batch Loss: 0.660\n",
      "Epoch 914, Loss: 22.006, Final Batch Loss: 0.557\n",
      "Epoch 915, Loss: 22.049, Final Batch Loss: 0.539\n",
      "Epoch 916, Loss: 22.260, Final Batch Loss: 0.576\n",
      "Epoch 917, Loss: 22.321, Final Batch Loss: 0.609\n",
      "Epoch 918, Loss: 22.038, Final Batch Loss: 0.640\n",
      "Epoch 919, Loss: 22.216, Final Batch Loss: 0.543\n",
      "Epoch 920, Loss: 21.985, Final Batch Loss: 0.641\n",
      "Epoch 921, Loss: 21.956, Final Batch Loss: 0.640\n",
      "Epoch 922, Loss: 22.296, Final Batch Loss: 0.647\n",
      "Epoch 923, Loss: 22.223, Final Batch Loss: 0.673\n",
      "Epoch 924, Loss: 22.255, Final Batch Loss: 0.599\n",
      "Epoch 925, Loss: 22.313, Final Batch Loss: 0.553\n",
      "Epoch 926, Loss: 22.294, Final Batch Loss: 0.607\n",
      "Epoch 927, Loss: 22.271, Final Batch Loss: 0.659\n",
      "Epoch 928, Loss: 22.060, Final Batch Loss: 0.488\n",
      "Epoch 929, Loss: 22.222, Final Batch Loss: 0.646\n",
      "Epoch 930, Loss: 22.075, Final Batch Loss: 0.561\n",
      "Epoch 931, Loss: 22.305, Final Batch Loss: 0.644\n",
      "Epoch 932, Loss: 22.316, Final Batch Loss: 0.730\n",
      "Epoch 933, Loss: 22.224, Final Batch Loss: 0.644\n",
      "Epoch 934, Loss: 22.224, Final Batch Loss: 0.606\n",
      "Epoch 935, Loss: 22.417, Final Batch Loss: 0.651\n",
      "Epoch 936, Loss: 22.235, Final Batch Loss: 0.687\n",
      "Epoch 937, Loss: 22.083, Final Batch Loss: 0.654\n",
      "Epoch 938, Loss: 22.164, Final Batch Loss: 0.558\n",
      "Epoch 939, Loss: 22.111, Final Batch Loss: 0.682\n",
      "Epoch 940, Loss: 22.249, Final Batch Loss: 0.655\n",
      "Epoch 941, Loss: 22.132, Final Batch Loss: 0.696\n",
      "Epoch 942, Loss: 22.232, Final Batch Loss: 0.587\n",
      "Epoch 943, Loss: 22.114, Final Batch Loss: 0.565\n",
      "Epoch 944, Loss: 22.252, Final Batch Loss: 0.782\n",
      "Epoch 945, Loss: 22.043, Final Batch Loss: 0.679\n",
      "Epoch 946, Loss: 22.165, Final Batch Loss: 0.611\n",
      "Epoch 947, Loss: 22.095, Final Batch Loss: 0.609\n",
      "Epoch 948, Loss: 21.911, Final Batch Loss: 0.528\n",
      "Epoch 949, Loss: 22.148, Final Batch Loss: 0.676\n",
      "Epoch 950, Loss: 22.061, Final Batch Loss: 0.611\n",
      "Epoch 951, Loss: 22.011, Final Batch Loss: 0.592\n",
      "Epoch 952, Loss: 22.139, Final Batch Loss: 0.790\n",
      "Epoch 953, Loss: 22.053, Final Batch Loss: 0.579\n",
      "Epoch 954, Loss: 22.167, Final Batch Loss: 0.684\n",
      "Epoch 955, Loss: 22.024, Final Batch Loss: 0.575\n",
      "Epoch 956, Loss: 22.088, Final Batch Loss: 0.481\n",
      "Epoch 957, Loss: 21.986, Final Batch Loss: 0.603\n",
      "Epoch 958, Loss: 22.095, Final Batch Loss: 0.726\n",
      "Epoch 959, Loss: 22.290, Final Batch Loss: 0.567\n",
      "Epoch 960, Loss: 21.910, Final Batch Loss: 0.509\n",
      "Epoch 961, Loss: 22.170, Final Batch Loss: 0.643\n",
      "Epoch 962, Loss: 22.176, Final Batch Loss: 0.601\n",
      "Epoch 963, Loss: 22.135, Final Batch Loss: 0.608\n",
      "Epoch 964, Loss: 22.091, Final Batch Loss: 0.499\n",
      "Epoch 965, Loss: 22.071, Final Batch Loss: 0.590\n",
      "Epoch 966, Loss: 22.179, Final Batch Loss: 0.546\n",
      "Epoch 967, Loss: 21.762, Final Batch Loss: 0.632\n",
      "Epoch 968, Loss: 22.029, Final Batch Loss: 0.608\n",
      "Epoch 969, Loss: 22.044, Final Batch Loss: 0.498\n",
      "Epoch 970, Loss: 22.048, Final Batch Loss: 0.576\n",
      "Epoch 971, Loss: 21.938, Final Batch Loss: 0.517\n",
      "Epoch 972, Loss: 21.964, Final Batch Loss: 0.586\n",
      "Epoch 973, Loss: 22.172, Final Batch Loss: 0.592\n",
      "Epoch 974, Loss: 22.143, Final Batch Loss: 0.655\n",
      "Epoch 975, Loss: 22.101, Final Batch Loss: 0.637\n",
      "Epoch 976, Loss: 22.039, Final Batch Loss: 0.592\n",
      "Epoch 977, Loss: 22.136, Final Batch Loss: 0.660\n",
      "Epoch 978, Loss: 22.243, Final Batch Loss: 0.641\n",
      "Epoch 979, Loss: 22.086, Final Batch Loss: 0.680\n",
      "Epoch 980, Loss: 21.991, Final Batch Loss: 0.616\n",
      "Epoch 981, Loss: 22.201, Final Batch Loss: 0.683\n",
      "Epoch 982, Loss: 22.114, Final Batch Loss: 0.590\n",
      "Epoch 983, Loss: 22.061, Final Batch Loss: 0.693\n",
      "Epoch 984, Loss: 21.840, Final Batch Loss: 0.678\n",
      "Epoch 985, Loss: 21.933, Final Batch Loss: 0.538\n",
      "Epoch 986, Loss: 22.087, Final Batch Loss: 0.546\n",
      "Epoch 987, Loss: 22.046, Final Batch Loss: 0.618\n",
      "Epoch 988, Loss: 21.928, Final Batch Loss: 0.540\n",
      "Epoch 989, Loss: 22.090, Final Batch Loss: 0.717\n",
      "Epoch 990, Loss: 22.117, Final Batch Loss: 0.627\n",
      "Epoch 991, Loss: 21.999, Final Batch Loss: 0.551\n",
      "Epoch 992, Loss: 22.125, Final Batch Loss: 0.674\n",
      "Epoch 993, Loss: 21.996, Final Batch Loss: 0.548\n",
      "Epoch 994, Loss: 21.902, Final Batch Loss: 0.631\n",
      "Epoch 995, Loss: 22.230, Final Batch Loss: 0.635\n",
      "Epoch 996, Loss: 21.877, Final Batch Loss: 0.513\n",
      "Epoch 997, Loss: 21.944, Final Batch Loss: 0.658\n",
      "Epoch 998, Loss: 22.028, Final Batch Loss: 0.558\n",
      "Epoch 999, Loss: 21.909, Final Batch Loss: 0.634\n",
      "Epoch 1000, Loss: 21.962, Final Batch Loss: 0.629\n",
      "Epoch 1001, Loss: 21.979, Final Batch Loss: 0.622\n",
      "Epoch 1002, Loss: 22.101, Final Batch Loss: 0.683\n",
      "Epoch 1003, Loss: 22.116, Final Batch Loss: 0.568\n",
      "Epoch 1004, Loss: 21.923, Final Batch Loss: 0.632\n",
      "Epoch 1005, Loss: 21.858, Final Batch Loss: 0.684\n",
      "Epoch 1006, Loss: 22.127, Final Batch Loss: 0.586\n",
      "Epoch 1007, Loss: 21.924, Final Batch Loss: 0.547\n",
      "Epoch 1008, Loss: 21.920, Final Batch Loss: 0.498\n",
      "Epoch 1009, Loss: 22.126, Final Batch Loss: 0.641\n",
      "Epoch 1010, Loss: 21.822, Final Batch Loss: 0.593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1011, Loss: 21.809, Final Batch Loss: 0.556\n",
      "Epoch 1012, Loss: 21.983, Final Batch Loss: 0.682\n",
      "Epoch 1013, Loss: 21.856, Final Batch Loss: 0.552\n",
      "Epoch 1014, Loss: 21.706, Final Batch Loss: 0.594\n",
      "Epoch 1015, Loss: 21.884, Final Batch Loss: 0.590\n",
      "Epoch 1016, Loss: 21.842, Final Batch Loss: 0.447\n",
      "Epoch 1017, Loss: 22.019, Final Batch Loss: 0.610\n",
      "Epoch 1018, Loss: 21.822, Final Batch Loss: 0.657\n",
      "Epoch 1019, Loss: 22.010, Final Batch Loss: 0.576\n",
      "Epoch 1020, Loss: 21.912, Final Batch Loss: 0.596\n",
      "Epoch 1021, Loss: 21.783, Final Batch Loss: 0.585\n",
      "Epoch 1022, Loss: 22.037, Final Batch Loss: 0.572\n",
      "Epoch 1023, Loss: 21.881, Final Batch Loss: 0.567\n",
      "Epoch 1024, Loss: 21.853, Final Batch Loss: 0.514\n",
      "Epoch 1025, Loss: 22.190, Final Batch Loss: 0.659\n",
      "Epoch 1026, Loss: 21.966, Final Batch Loss: 0.648\n",
      "Epoch 1027, Loss: 21.958, Final Batch Loss: 0.632\n",
      "Epoch 1028, Loss: 21.962, Final Batch Loss: 0.623\n",
      "Epoch 1029, Loss: 21.707, Final Batch Loss: 0.635\n",
      "Epoch 1030, Loss: 22.043, Final Batch Loss: 0.625\n",
      "Epoch 1031, Loss: 21.998, Final Batch Loss: 0.713\n",
      "Epoch 1032, Loss: 21.846, Final Batch Loss: 0.578\n",
      "Epoch 1033, Loss: 21.715, Final Batch Loss: 0.598\n",
      "Epoch 1034, Loss: 21.983, Final Batch Loss: 0.553\n",
      "Epoch 1035, Loss: 21.949, Final Batch Loss: 0.565\n",
      "Epoch 1036, Loss: 21.847, Final Batch Loss: 0.678\n",
      "Epoch 1037, Loss: 22.021, Final Batch Loss: 0.676\n",
      "Epoch 1038, Loss: 21.708, Final Batch Loss: 0.561\n",
      "Epoch 1039, Loss: 22.031, Final Batch Loss: 0.656\n",
      "Epoch 1040, Loss: 21.821, Final Batch Loss: 0.551\n",
      "Epoch 1041, Loss: 22.008, Final Batch Loss: 0.591\n",
      "Epoch 1042, Loss: 21.999, Final Batch Loss: 0.749\n",
      "Epoch 1043, Loss: 22.013, Final Batch Loss: 0.658\n",
      "Epoch 1044, Loss: 21.801, Final Batch Loss: 0.562\n",
      "Epoch 1045, Loss: 21.854, Final Batch Loss: 0.637\n",
      "Epoch 1046, Loss: 21.811, Final Batch Loss: 0.686\n",
      "Epoch 1047, Loss: 21.972, Final Batch Loss: 0.651\n",
      "Epoch 1048, Loss: 21.838, Final Batch Loss: 0.589\n",
      "Epoch 1049, Loss: 21.895, Final Batch Loss: 0.628\n",
      "Epoch 1050, Loss: 21.707, Final Batch Loss: 0.548\n",
      "Epoch 1051, Loss: 21.866, Final Batch Loss: 0.448\n",
      "Epoch 1052, Loss: 21.789, Final Batch Loss: 0.607\n",
      "Epoch 1053, Loss: 21.919, Final Batch Loss: 0.572\n",
      "Epoch 1054, Loss: 21.864, Final Batch Loss: 0.669\n",
      "Epoch 1055, Loss: 22.004, Final Batch Loss: 0.606\n",
      "Epoch 1056, Loss: 21.830, Final Batch Loss: 0.605\n",
      "Epoch 1057, Loss: 21.713, Final Batch Loss: 0.560\n",
      "Epoch 1058, Loss: 21.637, Final Batch Loss: 0.515\n",
      "Epoch 1059, Loss: 21.657, Final Batch Loss: 0.639\n",
      "Epoch 1060, Loss: 21.771, Final Batch Loss: 0.588\n",
      "Epoch 1061, Loss: 22.047, Final Batch Loss: 0.590\n",
      "Epoch 1062, Loss: 21.648, Final Batch Loss: 0.554\n",
      "Epoch 1063, Loss: 21.574, Final Batch Loss: 0.618\n",
      "Epoch 1064, Loss: 21.729, Final Batch Loss: 0.585\n",
      "Epoch 1065, Loss: 21.796, Final Batch Loss: 0.600\n",
      "Epoch 1066, Loss: 21.588, Final Batch Loss: 0.568\n",
      "Epoch 1067, Loss: 21.965, Final Batch Loss: 0.620\n",
      "Epoch 1068, Loss: 21.893, Final Batch Loss: 0.576\n",
      "Epoch 1069, Loss: 21.776, Final Batch Loss: 0.711\n",
      "Epoch 1070, Loss: 21.693, Final Batch Loss: 0.531\n",
      "Epoch 1071, Loss: 21.990, Final Batch Loss: 0.616\n",
      "Epoch 1072, Loss: 21.928, Final Batch Loss: 0.592\n",
      "Epoch 1073, Loss: 21.767, Final Batch Loss: 0.559\n",
      "Epoch 1074, Loss: 21.663, Final Batch Loss: 0.571\n",
      "Epoch 1075, Loss: 21.796, Final Batch Loss: 0.674\n",
      "Epoch 1076, Loss: 21.720, Final Batch Loss: 0.707\n",
      "Epoch 1077, Loss: 21.815, Final Batch Loss: 0.695\n",
      "Epoch 1078, Loss: 21.705, Final Batch Loss: 0.509\n",
      "Epoch 1079, Loss: 21.777, Final Batch Loss: 0.623\n",
      "Epoch 1080, Loss: 21.983, Final Batch Loss: 0.670\n",
      "Epoch 1081, Loss: 21.734, Final Batch Loss: 0.605\n",
      "Epoch 1082, Loss: 21.854, Final Batch Loss: 0.595\n",
      "Epoch 1083, Loss: 21.868, Final Batch Loss: 0.583\n",
      "Epoch 1084, Loss: 21.933, Final Batch Loss: 0.749\n",
      "Epoch 1085, Loss: 21.996, Final Batch Loss: 0.653\n",
      "Epoch 1086, Loss: 21.594, Final Batch Loss: 0.594\n",
      "Epoch 1087, Loss: 21.746, Final Batch Loss: 0.584\n",
      "Epoch 1088, Loss: 21.634, Final Batch Loss: 0.486\n",
      "Epoch 1089, Loss: 21.598, Final Batch Loss: 0.608\n",
      "Epoch 1090, Loss: 21.641, Final Batch Loss: 0.639\n",
      "Epoch 1091, Loss: 21.798, Final Batch Loss: 0.568\n",
      "Epoch 1092, Loss: 21.725, Final Batch Loss: 0.605\n",
      "Epoch 1093, Loss: 21.690, Final Batch Loss: 0.589\n",
      "Epoch 1094, Loss: 21.691, Final Batch Loss: 0.587\n",
      "Epoch 1095, Loss: 21.711, Final Batch Loss: 0.612\n",
      "Epoch 1096, Loss: 21.746, Final Batch Loss: 0.613\n",
      "Epoch 1097, Loss: 21.822, Final Batch Loss: 0.618\n",
      "Epoch 1098, Loss: 21.848, Final Batch Loss: 0.549\n",
      "Epoch 1099, Loss: 21.842, Final Batch Loss: 0.624\n",
      "Epoch 1100, Loss: 21.801, Final Batch Loss: 0.458\n",
      "Epoch 1101, Loss: 21.807, Final Batch Loss: 0.559\n",
      "Epoch 1102, Loss: 21.704, Final Batch Loss: 0.585\n",
      "Epoch 1103, Loss: 21.784, Final Batch Loss: 0.629\n",
      "Epoch 1104, Loss: 21.623, Final Batch Loss: 0.484\n",
      "Epoch 1105, Loss: 22.004, Final Batch Loss: 0.659\n",
      "Epoch 1106, Loss: 21.748, Final Batch Loss: 0.708\n",
      "Epoch 1107, Loss: 21.651, Final Batch Loss: 0.510\n",
      "Epoch 1108, Loss: 21.778, Final Batch Loss: 0.689\n",
      "Epoch 1109, Loss: 21.822, Final Batch Loss: 0.633\n",
      "Epoch 1110, Loss: 21.582, Final Batch Loss: 0.534\n",
      "Epoch 1111, Loss: 21.637, Final Batch Loss: 0.664\n",
      "Epoch 1112, Loss: 21.878, Final Batch Loss: 0.582\n",
      "Epoch 1113, Loss: 21.546, Final Batch Loss: 0.640\n",
      "Epoch 1114, Loss: 21.767, Final Batch Loss: 0.535\n",
      "Epoch 1115, Loss: 21.762, Final Batch Loss: 0.485\n",
      "Epoch 1116, Loss: 21.835, Final Batch Loss: 0.674\n",
      "Epoch 1117, Loss: 21.528, Final Batch Loss: 0.488\n",
      "Epoch 1118, Loss: 21.591, Final Batch Loss: 0.490\n",
      "Epoch 1119, Loss: 21.635, Final Batch Loss: 0.552\n",
      "Epoch 1120, Loss: 21.748, Final Batch Loss: 0.663\n",
      "Epoch 1121, Loss: 21.667, Final Batch Loss: 0.592\n",
      "Epoch 1122, Loss: 21.826, Final Batch Loss: 0.569\n",
      "Epoch 1123, Loss: 21.693, Final Batch Loss: 0.486\n",
      "Epoch 1124, Loss: 21.799, Final Batch Loss: 0.610\n",
      "Epoch 1125, Loss: 21.744, Final Batch Loss: 0.524\n",
      "Epoch 1126, Loss: 21.761, Final Batch Loss: 0.615\n",
      "Epoch 1127, Loss: 21.815, Final Batch Loss: 0.647\n",
      "Epoch 1128, Loss: 21.772, Final Batch Loss: 0.629\n",
      "Epoch 1129, Loss: 21.666, Final Batch Loss: 0.616\n",
      "Epoch 1130, Loss: 21.674, Final Batch Loss: 0.633\n",
      "Epoch 1131, Loss: 21.770, Final Batch Loss: 0.525\n",
      "Epoch 1132, Loss: 21.655, Final Batch Loss: 0.688\n",
      "Epoch 1133, Loss: 21.770, Final Batch Loss: 0.519\n",
      "Epoch 1134, Loss: 21.751, Final Batch Loss: 0.539\n",
      "Epoch 1135, Loss: 21.767, Final Batch Loss: 0.667\n",
      "Epoch 1136, Loss: 21.575, Final Batch Loss: 0.558\n",
      "Epoch 1137, Loss: 21.768, Final Batch Loss: 0.648\n",
      "Epoch 1138, Loss: 21.794, Final Batch Loss: 0.553\n",
      "Epoch 1139, Loss: 21.780, Final Batch Loss: 0.620\n",
      "Epoch 1140, Loss: 21.557, Final Batch Loss: 0.512\n",
      "Epoch 1141, Loss: 21.949, Final Batch Loss: 0.746\n",
      "Epoch 1142, Loss: 21.548, Final Batch Loss: 0.551\n",
      "Epoch 1143, Loss: 21.590, Final Batch Loss: 0.706\n",
      "Epoch 1144, Loss: 21.227, Final Batch Loss: 0.591\n",
      "Epoch 1145, Loss: 21.680, Final Batch Loss: 0.681\n",
      "Epoch 1146, Loss: 21.491, Final Batch Loss: 0.502\n",
      "Epoch 1147, Loss: 21.429, Final Batch Loss: 0.592\n",
      "Epoch 1148, Loss: 21.768, Final Batch Loss: 0.574\n",
      "Epoch 1149, Loss: 21.537, Final Batch Loss: 0.687\n",
      "Epoch 1150, Loss: 21.525, Final Batch Loss: 0.593\n",
      "Epoch 1151, Loss: 21.631, Final Batch Loss: 0.513\n",
      "Epoch 1152, Loss: 21.649, Final Batch Loss: 0.556\n",
      "Epoch 1153, Loss: 21.314, Final Batch Loss: 0.559\n",
      "Epoch 1154, Loss: 21.411, Final Batch Loss: 0.598\n",
      "Epoch 1155, Loss: 21.500, Final Batch Loss: 0.589\n",
      "Epoch 1156, Loss: 21.487, Final Batch Loss: 0.599\n",
      "Epoch 1157, Loss: 21.645, Final Batch Loss: 0.509\n",
      "Epoch 1158, Loss: 21.627, Final Batch Loss: 0.625\n",
      "Epoch 1159, Loss: 21.462, Final Batch Loss: 0.618\n",
      "Epoch 1160, Loss: 21.303, Final Batch Loss: 0.617\n",
      "Epoch 1161, Loss: 21.537, Final Batch Loss: 0.541\n",
      "Epoch 1162, Loss: 21.673, Final Batch Loss: 0.665\n",
      "Epoch 1163, Loss: 21.666, Final Batch Loss: 0.654\n",
      "Epoch 1164, Loss: 21.915, Final Batch Loss: 0.627\n",
      "Epoch 1165, Loss: 21.457, Final Batch Loss: 0.507\n",
      "Epoch 1166, Loss: 21.522, Final Batch Loss: 0.578\n",
      "Epoch 1167, Loss: 21.465, Final Batch Loss: 0.565\n",
      "Epoch 1168, Loss: 21.605, Final Batch Loss: 0.603\n",
      "Epoch 1169, Loss: 21.670, Final Batch Loss: 0.614\n",
      "Epoch 1170, Loss: 21.719, Final Batch Loss: 0.571\n",
      "Epoch 1171, Loss: 21.565, Final Batch Loss: 0.683\n",
      "Epoch 1172, Loss: 21.493, Final Batch Loss: 0.528\n",
      "Epoch 1173, Loss: 21.500, Final Batch Loss: 0.678\n",
      "Epoch 1174, Loss: 21.404, Final Batch Loss: 0.600\n",
      "Epoch 1175, Loss: 21.582, Final Batch Loss: 0.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1176, Loss: 21.471, Final Batch Loss: 0.560\n",
      "Epoch 1177, Loss: 21.506, Final Batch Loss: 0.607\n",
      "Epoch 1178, Loss: 21.563, Final Batch Loss: 0.678\n",
      "Epoch 1179, Loss: 21.348, Final Batch Loss: 0.591\n",
      "Epoch 1180, Loss: 21.604, Final Batch Loss: 0.644\n",
      "Epoch 1181, Loss: 21.465, Final Batch Loss: 0.528\n",
      "Epoch 1182, Loss: 21.763, Final Batch Loss: 0.636\n",
      "Epoch 1183, Loss: 21.691, Final Batch Loss: 0.606\n",
      "Epoch 1184, Loss: 21.311, Final Batch Loss: 0.578\n",
      "Epoch 1185, Loss: 21.615, Final Batch Loss: 0.555\n",
      "Epoch 1186, Loss: 21.651, Final Batch Loss: 0.675\n",
      "Epoch 1187, Loss: 21.490, Final Batch Loss: 0.621\n",
      "Epoch 1188, Loss: 21.847, Final Batch Loss: 0.647\n",
      "Epoch 1189, Loss: 21.819, Final Batch Loss: 0.565\n",
      "Epoch 1190, Loss: 21.429, Final Batch Loss: 0.654\n",
      "Epoch 1191, Loss: 21.586, Final Batch Loss: 0.564\n",
      "Epoch 1192, Loss: 21.438, Final Batch Loss: 0.597\n",
      "Epoch 1193, Loss: 21.627, Final Batch Loss: 0.562\n",
      "Epoch 1194, Loss: 21.397, Final Batch Loss: 0.620\n",
      "Epoch 1195, Loss: 21.272, Final Batch Loss: 0.564\n",
      "Epoch 1196, Loss: 21.702, Final Batch Loss: 0.594\n",
      "Epoch 1197, Loss: 21.567, Final Batch Loss: 0.584\n",
      "Epoch 1198, Loss: 21.492, Final Batch Loss: 0.593\n",
      "Epoch 1199, Loss: 21.926, Final Batch Loss: 0.713\n",
      "Epoch 1200, Loss: 21.660, Final Batch Loss: 0.609\n",
      "Epoch 1201, Loss: 21.783, Final Batch Loss: 0.595\n",
      "Epoch 1202, Loss: 21.621, Final Batch Loss: 0.533\n",
      "Epoch 1203, Loss: 21.639, Final Batch Loss: 0.556\n",
      "Epoch 1204, Loss: 21.571, Final Batch Loss: 0.673\n",
      "Epoch 1205, Loss: 21.418, Final Batch Loss: 0.605\n",
      "Epoch 1206, Loss: 21.352, Final Batch Loss: 0.706\n",
      "Epoch 1207, Loss: 21.322, Final Batch Loss: 0.600\n",
      "Epoch 1208, Loss: 21.585, Final Batch Loss: 0.706\n",
      "Epoch 1209, Loss: 21.293, Final Batch Loss: 0.552\n",
      "Epoch 1210, Loss: 21.588, Final Batch Loss: 0.572\n",
      "Epoch 1211, Loss: 21.608, Final Batch Loss: 0.691\n",
      "Epoch 1212, Loss: 21.417, Final Batch Loss: 0.570\n",
      "Epoch 1213, Loss: 21.553, Final Batch Loss: 0.548\n",
      "Epoch 1214, Loss: 21.520, Final Batch Loss: 0.622\n",
      "Epoch 1215, Loss: 21.517, Final Batch Loss: 0.652\n",
      "Epoch 1216, Loss: 21.368, Final Batch Loss: 0.591\n",
      "Epoch 1217, Loss: 21.599, Final Batch Loss: 0.517\n",
      "Epoch 1218, Loss: 21.562, Final Batch Loss: 0.509\n",
      "Epoch 1219, Loss: 21.270, Final Batch Loss: 0.608\n",
      "Epoch 1220, Loss: 21.539, Final Batch Loss: 0.663\n",
      "Epoch 1221, Loss: 21.597, Final Batch Loss: 0.606\n",
      "Epoch 1222, Loss: 21.323, Final Batch Loss: 0.562\n",
      "Epoch 1223, Loss: 21.359, Final Batch Loss: 0.544\n",
      "Epoch 1224, Loss: 21.392, Final Batch Loss: 0.673\n",
      "Epoch 1225, Loss: 21.565, Final Batch Loss: 0.680\n",
      "Epoch 1226, Loss: 21.638, Final Batch Loss: 0.591\n",
      "Epoch 1227, Loss: 21.440, Final Batch Loss: 0.540\n",
      "Epoch 1228, Loss: 21.166, Final Batch Loss: 0.567\n",
      "Epoch 1229, Loss: 21.407, Final Batch Loss: 0.583\n",
      "Epoch 1230, Loss: 21.184, Final Batch Loss: 0.554\n",
      "Epoch 1231, Loss: 21.583, Final Batch Loss: 0.608\n",
      "Epoch 1232, Loss: 21.666, Final Batch Loss: 0.588\n",
      "Epoch 1233, Loss: 21.586, Final Batch Loss: 0.677\n",
      "Epoch 1234, Loss: 21.402, Final Batch Loss: 0.611\n",
      "Epoch 1235, Loss: 21.379, Final Batch Loss: 0.666\n",
      "Epoch 1236, Loss: 21.439, Final Batch Loss: 0.770\n",
      "Epoch 1237, Loss: 21.330, Final Batch Loss: 0.570\n",
      "Epoch 1238, Loss: 21.466, Final Batch Loss: 0.539\n",
      "Epoch 1239, Loss: 21.331, Final Batch Loss: 0.596\n",
      "Epoch 1240, Loss: 21.523, Final Batch Loss: 0.619\n",
      "Epoch 1241, Loss: 21.387, Final Batch Loss: 0.478\n",
      "Epoch 1242, Loss: 21.225, Final Batch Loss: 0.586\n",
      "Epoch 1243, Loss: 21.338, Final Batch Loss: 0.597\n",
      "Epoch 1244, Loss: 21.408, Final Batch Loss: 0.670\n",
      "Epoch 1245, Loss: 21.466, Final Batch Loss: 0.650\n",
      "Epoch 1246, Loss: 21.575, Final Batch Loss: 0.667\n",
      "Epoch 1247, Loss: 21.337, Final Batch Loss: 0.575\n",
      "Epoch 1248, Loss: 21.269, Final Batch Loss: 0.601\n",
      "Epoch 1249, Loss: 21.600, Final Batch Loss: 0.601\n",
      "Epoch 1250, Loss: 21.526, Final Batch Loss: 0.624\n",
      "Epoch 1251, Loss: 21.214, Final Batch Loss: 0.573\n",
      "Epoch 1252, Loss: 21.447, Final Batch Loss: 0.650\n",
      "Epoch 1253, Loss: 21.444, Final Batch Loss: 0.657\n",
      "Epoch 1254, Loss: 21.324, Final Batch Loss: 0.556\n",
      "Epoch 1255, Loss: 21.503, Final Batch Loss: 0.575\n",
      "Epoch 1256, Loss: 21.346, Final Batch Loss: 0.611\n",
      "Epoch 1257, Loss: 21.462, Final Batch Loss: 0.585\n",
      "Epoch 1258, Loss: 21.722, Final Batch Loss: 0.691\n",
      "Epoch 1259, Loss: 21.400, Final Batch Loss: 0.746\n",
      "Epoch 1260, Loss: 21.281, Final Batch Loss: 0.619\n",
      "Epoch 1261, Loss: 21.362, Final Batch Loss: 0.652\n",
      "Epoch 1262, Loss: 21.266, Final Batch Loss: 0.526\n",
      "Epoch 1263, Loss: 21.284, Final Batch Loss: 0.636\n",
      "Epoch 1264, Loss: 21.362, Final Batch Loss: 0.628\n",
      "Epoch 1265, Loss: 21.583, Final Batch Loss: 0.703\n",
      "Epoch 1266, Loss: 21.136, Final Batch Loss: 0.613\n",
      "Epoch 1267, Loss: 21.467, Final Batch Loss: 0.561\n",
      "Epoch 1268, Loss: 21.328, Final Batch Loss: 0.613\n",
      "Epoch 1269, Loss: 21.428, Final Batch Loss: 0.587\n",
      "Epoch 1270, Loss: 21.388, Final Batch Loss: 0.578\n",
      "Epoch 1271, Loss: 21.300, Final Batch Loss: 0.519\n",
      "Epoch 1272, Loss: 21.493, Final Batch Loss: 0.520\n",
      "Epoch 1273, Loss: 21.429, Final Batch Loss: 0.612\n",
      "Epoch 1274, Loss: 21.531, Final Batch Loss: 0.501\n",
      "Epoch 1275, Loss: 21.251, Final Batch Loss: 0.652\n",
      "Epoch 1276, Loss: 21.392, Final Batch Loss: 0.605\n",
      "Epoch 1277, Loss: 21.344, Final Batch Loss: 0.528\n",
      "Epoch 1278, Loss: 21.390, Final Batch Loss: 0.567\n",
      "Epoch 1279, Loss: 21.496, Final Batch Loss: 0.540\n",
      "Epoch 1280, Loss: 21.438, Final Batch Loss: 0.645\n",
      "Epoch 1281, Loss: 21.177, Final Batch Loss: 0.530\n",
      "Epoch 1282, Loss: 21.388, Final Batch Loss: 0.617\n",
      "Epoch 1283, Loss: 21.285, Final Batch Loss: 0.582\n",
      "Epoch 1284, Loss: 21.525, Final Batch Loss: 0.613\n",
      "Epoch 1285, Loss: 21.288, Final Batch Loss: 0.721\n",
      "Epoch 1286, Loss: 21.177, Final Batch Loss: 0.566\n",
      "Epoch 1287, Loss: 21.311, Final Batch Loss: 0.575\n",
      "Epoch 1288, Loss: 21.342, Final Batch Loss: 0.673\n",
      "Epoch 1289, Loss: 21.484, Final Batch Loss: 0.607\n",
      "Epoch 1290, Loss: 21.502, Final Batch Loss: 0.541\n",
      "Epoch 1291, Loss: 21.424, Final Batch Loss: 0.564\n",
      "Epoch 1292, Loss: 21.743, Final Batch Loss: 0.605\n",
      "Epoch 1293, Loss: 21.455, Final Batch Loss: 0.634\n",
      "Epoch 1294, Loss: 21.307, Final Batch Loss: 0.462\n",
      "Epoch 1295, Loss: 21.346, Final Batch Loss: 0.603\n",
      "Epoch 1296, Loss: 21.193, Final Batch Loss: 0.552\n",
      "Epoch 1297, Loss: 21.441, Final Batch Loss: 0.681\n",
      "Epoch 1298, Loss: 21.525, Final Batch Loss: 0.579\n",
      "Epoch 1299, Loss: 21.156, Final Batch Loss: 0.598\n",
      "Epoch 1300, Loss: 21.210, Final Batch Loss: 0.626\n",
      "Epoch 1301, Loss: 21.557, Final Batch Loss: 0.644\n",
      "Epoch 1302, Loss: 21.330, Final Batch Loss: 0.568\n",
      "Epoch 1303, Loss: 21.301, Final Batch Loss: 0.608\n",
      "Epoch 1304, Loss: 21.289, Final Batch Loss: 0.551\n",
      "Epoch 1305, Loss: 21.202, Final Batch Loss: 0.546\n",
      "Epoch 1306, Loss: 21.335, Final Batch Loss: 0.599\n",
      "Epoch 1307, Loss: 21.301, Final Batch Loss: 0.627\n",
      "Epoch 1308, Loss: 21.385, Final Batch Loss: 0.585\n",
      "Epoch 1309, Loss: 21.304, Final Batch Loss: 0.571\n",
      "Epoch 1310, Loss: 21.222, Final Batch Loss: 0.545\n",
      "Epoch 1311, Loss: 21.287, Final Batch Loss: 0.538\n",
      "Epoch 1312, Loss: 21.377, Final Batch Loss: 0.613\n",
      "Epoch 1313, Loss: 21.209, Final Batch Loss: 0.629\n",
      "Epoch 1314, Loss: 21.178, Final Batch Loss: 0.633\n",
      "Epoch 1315, Loss: 21.268, Final Batch Loss: 0.644\n",
      "Epoch 1316, Loss: 21.189, Final Batch Loss: 0.580\n",
      "Epoch 1317, Loss: 21.469, Final Batch Loss: 0.493\n",
      "Epoch 1318, Loss: 21.234, Final Batch Loss: 0.615\n",
      "Epoch 1319, Loss: 21.385, Final Batch Loss: 0.648\n",
      "Epoch 1320, Loss: 21.334, Final Batch Loss: 0.642\n",
      "Epoch 1321, Loss: 21.391, Final Batch Loss: 0.596\n",
      "Epoch 1322, Loss: 21.457, Final Batch Loss: 0.539\n",
      "Epoch 1323, Loss: 21.406, Final Batch Loss: 0.650\n",
      "Epoch 1324, Loss: 21.225, Final Batch Loss: 0.560\n",
      "Epoch 1325, Loss: 21.379, Final Batch Loss: 0.677\n",
      "Epoch 1326, Loss: 21.126, Final Batch Loss: 0.586\n",
      "Epoch 1327, Loss: 21.377, Final Batch Loss: 0.650\n",
      "Epoch 1328, Loss: 21.278, Final Batch Loss: 0.662\n",
      "Epoch 1329, Loss: 21.074, Final Batch Loss: 0.571\n",
      "Epoch 1330, Loss: 21.347, Final Batch Loss: 0.579\n",
      "Epoch 1331, Loss: 21.252, Final Batch Loss: 0.631\n",
      "Epoch 1332, Loss: 21.298, Final Batch Loss: 0.615\n",
      "Epoch 1333, Loss: 21.059, Final Batch Loss: 0.591\n",
      "Epoch 1334, Loss: 21.350, Final Batch Loss: 0.657\n",
      "Epoch 1335, Loss: 21.241, Final Batch Loss: 0.544\n",
      "Epoch 1336, Loss: 21.341, Final Batch Loss: 0.621\n",
      "Epoch 1337, Loss: 21.466, Final Batch Loss: 0.646\n",
      "Epoch 1338, Loss: 21.500, Final Batch Loss: 0.608\n",
      "Epoch 1339, Loss: 21.232, Final Batch Loss: 0.616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1340, Loss: 21.435, Final Batch Loss: 0.626\n",
      "Epoch 1341, Loss: 21.199, Final Batch Loss: 0.501\n",
      "Epoch 1342, Loss: 21.273, Final Batch Loss: 0.645\n",
      "Epoch 1343, Loss: 21.324, Final Batch Loss: 0.716\n",
      "Epoch 1344, Loss: 21.136, Final Batch Loss: 0.621\n",
      "Epoch 1345, Loss: 21.213, Final Batch Loss: 0.534\n",
      "Epoch 1346, Loss: 21.265, Final Batch Loss: 0.621\n",
      "Epoch 1347, Loss: 21.255, Final Batch Loss: 0.643\n",
      "Epoch 1348, Loss: 21.142, Final Batch Loss: 0.531\n",
      "Epoch 1349, Loss: 21.341, Final Batch Loss: 0.606\n",
      "Epoch 1350, Loss: 21.188, Final Batch Loss: 0.682\n",
      "Epoch 1351, Loss: 21.149, Final Batch Loss: 0.688\n",
      "Epoch 1352, Loss: 21.241, Final Batch Loss: 0.728\n",
      "Epoch 1353, Loss: 21.274, Final Batch Loss: 0.496\n",
      "Epoch 1354, Loss: 21.291, Final Batch Loss: 0.631\n",
      "Epoch 1355, Loss: 21.223, Final Batch Loss: 0.655\n",
      "Epoch 1356, Loss: 21.313, Final Batch Loss: 0.484\n",
      "Epoch 1357, Loss: 21.255, Final Batch Loss: 0.512\n",
      "Epoch 1358, Loss: 21.090, Final Batch Loss: 0.647\n",
      "Epoch 1359, Loss: 21.262, Final Batch Loss: 0.596\n",
      "Epoch 1360, Loss: 21.233, Final Batch Loss: 0.571\n",
      "Epoch 1361, Loss: 21.295, Final Batch Loss: 0.618\n",
      "Epoch 1362, Loss: 21.424, Final Batch Loss: 0.626\n",
      "Epoch 1363, Loss: 21.168, Final Batch Loss: 0.442\n",
      "Epoch 1364, Loss: 21.470, Final Batch Loss: 0.750\n",
      "Epoch 1365, Loss: 21.238, Final Batch Loss: 0.582\n",
      "Epoch 1366, Loss: 21.539, Final Batch Loss: 0.840\n",
      "Epoch 1367, Loss: 21.393, Final Batch Loss: 0.628\n",
      "Epoch 1368, Loss: 21.167, Final Batch Loss: 0.564\n",
      "Epoch 1369, Loss: 21.259, Final Batch Loss: 0.539\n",
      "Epoch 1370, Loss: 21.345, Final Batch Loss: 0.659\n",
      "Epoch 1371, Loss: 21.196, Final Batch Loss: 0.619\n",
      "Epoch 1372, Loss: 21.429, Final Batch Loss: 0.657\n",
      "Epoch 1373, Loss: 21.417, Final Batch Loss: 0.631\n",
      "Epoch 1374, Loss: 21.367, Final Batch Loss: 0.648\n",
      "Epoch 1375, Loss: 21.219, Final Batch Loss: 0.591\n",
      "Epoch 1376, Loss: 21.341, Final Batch Loss: 0.503\n",
      "Epoch 1377, Loss: 21.200, Final Batch Loss: 0.644\n",
      "Epoch 1378, Loss: 21.319, Final Batch Loss: 0.600\n",
      "Epoch 1379, Loss: 21.192, Final Batch Loss: 0.420\n",
      "Epoch 1380, Loss: 21.278, Final Batch Loss: 0.700\n",
      "Epoch 1381, Loss: 21.332, Final Batch Loss: 0.648\n",
      "Epoch 1382, Loss: 21.462, Final Batch Loss: 0.615\n",
      "Epoch 1383, Loss: 21.217, Final Batch Loss: 0.581\n",
      "Epoch 1384, Loss: 21.382, Final Batch Loss: 0.609\n",
      "Epoch 1385, Loss: 21.273, Final Batch Loss: 0.686\n",
      "Epoch 1386, Loss: 21.245, Final Batch Loss: 0.623\n",
      "Epoch 1387, Loss: 21.261, Final Batch Loss: 0.660\n",
      "Epoch 1388, Loss: 21.434, Final Batch Loss: 0.528\n",
      "Epoch 1389, Loss: 21.305, Final Batch Loss: 0.569\n",
      "Epoch 1390, Loss: 21.174, Final Batch Loss: 0.527\n",
      "Epoch 1391, Loss: 21.399, Final Batch Loss: 0.728\n",
      "Epoch 1392, Loss: 21.315, Final Batch Loss: 0.595\n",
      "Epoch 1393, Loss: 21.266, Final Batch Loss: 0.611\n",
      "Epoch 1394, Loss: 21.112, Final Batch Loss: 0.635\n",
      "Epoch 1395, Loss: 21.117, Final Batch Loss: 0.549\n",
      "Epoch 1396, Loss: 21.232, Final Batch Loss: 0.644\n",
      "Epoch 1397, Loss: 21.131, Final Batch Loss: 0.498\n",
      "Epoch 1398, Loss: 21.172, Final Batch Loss: 0.532\n",
      "Epoch 1399, Loss: 21.171, Final Batch Loss: 0.546\n",
      "Epoch 1400, Loss: 21.237, Final Batch Loss: 0.749\n",
      "Epoch 1401, Loss: 21.109, Final Batch Loss: 0.567\n",
      "Epoch 1402, Loss: 21.198, Final Batch Loss: 0.653\n",
      "Epoch 1403, Loss: 21.216, Final Batch Loss: 0.618\n",
      "Epoch 1404, Loss: 21.060, Final Batch Loss: 0.641\n",
      "Epoch 1405, Loss: 21.380, Final Batch Loss: 0.567\n",
      "Epoch 1406, Loss: 21.162, Final Batch Loss: 0.632\n",
      "Epoch 1407, Loss: 21.049, Final Batch Loss: 0.602\n",
      "Epoch 1408, Loss: 21.134, Final Batch Loss: 0.521\n",
      "Epoch 1409, Loss: 21.248, Final Batch Loss: 0.607\n",
      "Epoch 1410, Loss: 21.204, Final Batch Loss: 0.586\n",
      "Epoch 1411, Loss: 21.008, Final Batch Loss: 0.502\n",
      "Epoch 1412, Loss: 21.363, Final Batch Loss: 0.581\n",
      "Epoch 1413, Loss: 21.032, Final Batch Loss: 0.493\n",
      "Epoch 1414, Loss: 21.132, Final Batch Loss: 0.623\n",
      "Epoch 1415, Loss: 21.298, Final Batch Loss: 0.733\n",
      "Epoch 1416, Loss: 21.128, Final Batch Loss: 0.529\n",
      "Epoch 1417, Loss: 21.196, Final Batch Loss: 0.612\n",
      "Epoch 1418, Loss: 21.096, Final Batch Loss: 0.535\n",
      "Epoch 1419, Loss: 21.136, Final Batch Loss: 0.566\n",
      "Epoch 1420, Loss: 21.271, Final Batch Loss: 0.614\n",
      "Epoch 1421, Loss: 21.110, Final Batch Loss: 0.569\n",
      "Epoch 1422, Loss: 21.217, Final Batch Loss: 0.545\n",
      "Epoch 1423, Loss: 21.245, Final Batch Loss: 0.646\n",
      "Epoch 1424, Loss: 20.983, Final Batch Loss: 0.587\n",
      "Epoch 1425, Loss: 21.205, Final Batch Loss: 0.658\n",
      "Epoch 1426, Loss: 21.273, Final Batch Loss: 0.733\n",
      "Epoch 1427, Loss: 21.051, Final Batch Loss: 0.570\n",
      "Epoch 1428, Loss: 21.286, Final Batch Loss: 0.681\n",
      "Epoch 1429, Loss: 21.099, Final Batch Loss: 0.547\n",
      "Epoch 1430, Loss: 21.074, Final Batch Loss: 0.564\n",
      "Epoch 1431, Loss: 21.451, Final Batch Loss: 0.577\n",
      "Epoch 1432, Loss: 21.395, Final Batch Loss: 0.684\n",
      "Epoch 1433, Loss: 21.273, Final Batch Loss: 0.599\n",
      "Epoch 1434, Loss: 21.061, Final Batch Loss: 0.553\n",
      "Epoch 1435, Loss: 21.123, Final Batch Loss: 0.566\n",
      "Epoch 1436, Loss: 20.842, Final Batch Loss: 0.495\n",
      "Epoch 1437, Loss: 21.062, Final Batch Loss: 0.484\n",
      "Epoch 1438, Loss: 20.957, Final Batch Loss: 0.623\n",
      "Epoch 1439, Loss: 21.396, Final Batch Loss: 0.475\n",
      "Epoch 1440, Loss: 21.169, Final Batch Loss: 0.549\n",
      "Epoch 1441, Loss: 21.074, Final Batch Loss: 0.557\n",
      "Epoch 1442, Loss: 21.127, Final Batch Loss: 0.585\n",
      "Epoch 1443, Loss: 21.008, Final Batch Loss: 0.639\n",
      "Epoch 1444, Loss: 21.260, Final Batch Loss: 0.549\n",
      "Epoch 1445, Loss: 21.049, Final Batch Loss: 0.535\n",
      "Epoch 1446, Loss: 21.136, Final Batch Loss: 0.503\n",
      "Epoch 1447, Loss: 21.157, Final Batch Loss: 0.615\n",
      "Epoch 1448, Loss: 21.006, Final Batch Loss: 0.554\n",
      "Epoch 1449, Loss: 21.158, Final Batch Loss: 0.632\n",
      "Epoch 1450, Loss: 21.027, Final Batch Loss: 0.507\n",
      "Epoch 1451, Loss: 21.240, Final Batch Loss: 0.709\n",
      "Epoch 1452, Loss: 20.985, Final Batch Loss: 0.555\n",
      "Epoch 1453, Loss: 20.943, Final Batch Loss: 0.589\n",
      "Epoch 1454, Loss: 21.042, Final Batch Loss: 0.489\n",
      "Epoch 1455, Loss: 20.823, Final Batch Loss: 0.550\n",
      "Epoch 1456, Loss: 21.175, Final Batch Loss: 0.687\n",
      "Epoch 1457, Loss: 21.180, Final Batch Loss: 0.600\n",
      "Epoch 1458, Loss: 21.347, Final Batch Loss: 0.704\n",
      "Epoch 1459, Loss: 21.114, Final Batch Loss: 0.553\n",
      "Epoch 1460, Loss: 21.322, Final Batch Loss: 0.685\n",
      "Epoch 1461, Loss: 21.068, Final Batch Loss: 0.685\n",
      "Epoch 1462, Loss: 21.075, Final Batch Loss: 0.687\n",
      "Epoch 1463, Loss: 20.938, Final Batch Loss: 0.543\n",
      "Epoch 1464, Loss: 21.106, Final Batch Loss: 0.568\n",
      "Epoch 1465, Loss: 21.198, Final Batch Loss: 0.586\n",
      "Epoch 1466, Loss: 21.234, Final Batch Loss: 0.534\n",
      "Epoch 1467, Loss: 21.223, Final Batch Loss: 0.622\n",
      "Epoch 1468, Loss: 21.226, Final Batch Loss: 0.587\n",
      "Epoch 1469, Loss: 20.993, Final Batch Loss: 0.551\n",
      "Epoch 1470, Loss: 21.116, Final Batch Loss: 0.667\n",
      "Epoch 1471, Loss: 21.168, Final Batch Loss: 0.557\n",
      "Epoch 1472, Loss: 21.355, Final Batch Loss: 0.617\n",
      "Epoch 1473, Loss: 21.054, Final Batch Loss: 0.565\n",
      "Epoch 1474, Loss: 20.907, Final Batch Loss: 0.440\n",
      "Epoch 1475, Loss: 21.129, Final Batch Loss: 0.617\n",
      "Epoch 1476, Loss: 20.989, Final Batch Loss: 0.547\n",
      "Epoch 1477, Loss: 21.117, Final Batch Loss: 0.577\n",
      "Epoch 1478, Loss: 21.068, Final Batch Loss: 0.581\n",
      "Epoch 1479, Loss: 21.060, Final Batch Loss: 0.528\n",
      "Epoch 1480, Loss: 20.673, Final Batch Loss: 0.490\n",
      "Epoch 1481, Loss: 21.182, Final Batch Loss: 0.526\n",
      "Epoch 1482, Loss: 21.171, Final Batch Loss: 0.514\n",
      "Epoch 1483, Loss: 21.296, Final Batch Loss: 0.653\n",
      "Epoch 1484, Loss: 20.921, Final Batch Loss: 0.635\n",
      "Epoch 1485, Loss: 21.193, Final Batch Loss: 0.611\n",
      "Epoch 1486, Loss: 21.006, Final Batch Loss: 0.546\n",
      "Epoch 1487, Loss: 21.000, Final Batch Loss: 0.572\n",
      "Epoch 1488, Loss: 21.221, Final Batch Loss: 0.690\n",
      "Epoch 1489, Loss: 21.035, Final Batch Loss: 0.555\n",
      "Epoch 1490, Loss: 20.891, Final Batch Loss: 0.554\n",
      "Epoch 1491, Loss: 20.991, Final Batch Loss: 0.474\n",
      "Epoch 1492, Loss: 21.016, Final Batch Loss: 0.695\n",
      "Epoch 1493, Loss: 21.162, Final Batch Loss: 0.698\n",
      "Epoch 1494, Loss: 21.166, Final Batch Loss: 0.603\n",
      "Epoch 1495, Loss: 21.045, Final Batch Loss: 0.575\n",
      "Epoch 1496, Loss: 21.175, Final Batch Loss: 0.581\n",
      "Epoch 1497, Loss: 21.229, Final Batch Loss: 0.520\n",
      "Epoch 1498, Loss: 20.878, Final Batch Loss: 0.618\n",
      "Epoch 1499, Loss: 21.104, Final Batch Loss: 0.482\n",
      "Epoch 1500, Loss: 21.020, Final Batch Loss: 0.627\n",
      "Epoch 1501, Loss: 21.304, Final Batch Loss: 0.668\n",
      "Epoch 1502, Loss: 21.000, Final Batch Loss: 0.626\n",
      "Epoch 1503, Loss: 21.012, Final Batch Loss: 0.474\n",
      "Epoch 1504, Loss: 21.067, Final Batch Loss: 0.693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1505, Loss: 21.140, Final Batch Loss: 0.632\n",
      "Epoch 1506, Loss: 20.914, Final Batch Loss: 0.520\n",
      "Epoch 1507, Loss: 20.949, Final Batch Loss: 0.651\n",
      "Epoch 1508, Loss: 21.019, Final Batch Loss: 0.542\n",
      "Epoch 1509, Loss: 21.113, Final Batch Loss: 0.533\n",
      "Epoch 1510, Loss: 21.011, Final Batch Loss: 0.652\n",
      "Epoch 1511, Loss: 20.946, Final Batch Loss: 0.571\n",
      "Epoch 1512, Loss: 21.001, Final Batch Loss: 0.505\n",
      "Epoch 1513, Loss: 21.391, Final Batch Loss: 0.648\n",
      "Epoch 1514, Loss: 20.986, Final Batch Loss: 0.626\n",
      "Epoch 1515, Loss: 21.016, Final Batch Loss: 0.597\n",
      "Epoch 1516, Loss: 20.851, Final Batch Loss: 0.515\n",
      "Epoch 1517, Loss: 20.821, Final Batch Loss: 0.625\n",
      "Epoch 1518, Loss: 20.825, Final Batch Loss: 0.607\n",
      "Epoch 1519, Loss: 20.856, Final Batch Loss: 0.515\n",
      "Epoch 1520, Loss: 21.317, Final Batch Loss: 0.652\n",
      "Epoch 1521, Loss: 21.078, Final Batch Loss: 0.653\n",
      "Epoch 1522, Loss: 21.167, Final Batch Loss: 0.666\n",
      "Epoch 1523, Loss: 21.069, Final Batch Loss: 0.491\n",
      "Epoch 1524, Loss: 21.010, Final Batch Loss: 0.723\n",
      "Epoch 1525, Loss: 21.128, Final Batch Loss: 0.665\n",
      "Epoch 1526, Loss: 21.215, Final Batch Loss: 0.676\n",
      "Epoch 1527, Loss: 21.231, Final Batch Loss: 0.612\n",
      "Epoch 1528, Loss: 21.040, Final Batch Loss: 0.514\n",
      "Epoch 1529, Loss: 21.168, Final Batch Loss: 0.587\n",
      "Epoch 1530, Loss: 21.070, Final Batch Loss: 0.627\n",
      "Epoch 1531, Loss: 21.121, Final Batch Loss: 0.588\n",
      "Epoch 1532, Loss: 20.937, Final Batch Loss: 0.673\n",
      "Epoch 1533, Loss: 21.117, Final Batch Loss: 0.589\n",
      "Epoch 1534, Loss: 20.958, Final Batch Loss: 0.677\n",
      "Epoch 1535, Loss: 21.084, Final Batch Loss: 0.503\n",
      "Epoch 1536, Loss: 21.032, Final Batch Loss: 0.522\n",
      "Epoch 1537, Loss: 21.081, Final Batch Loss: 0.619\n",
      "Epoch 1538, Loss: 21.076, Final Batch Loss: 0.688\n",
      "Epoch 1539, Loss: 20.887, Final Batch Loss: 0.591\n",
      "Epoch 1540, Loss: 20.891, Final Batch Loss: 0.542\n",
      "Epoch 1541, Loss: 20.959, Final Batch Loss: 0.577\n",
      "Epoch 1542, Loss: 21.008, Final Batch Loss: 0.644\n",
      "Epoch 1543, Loss: 20.642, Final Batch Loss: 0.582\n",
      "Epoch 1544, Loss: 21.075, Final Batch Loss: 0.535\n",
      "Epoch 1545, Loss: 21.059, Final Batch Loss: 0.513\n",
      "Epoch 1546, Loss: 20.839, Final Batch Loss: 0.595\n",
      "Epoch 1547, Loss: 21.090, Final Batch Loss: 0.626\n",
      "Epoch 1548, Loss: 21.054, Final Batch Loss: 0.637\n",
      "Epoch 1549, Loss: 21.097, Final Batch Loss: 0.674\n",
      "Epoch 1550, Loss: 20.928, Final Batch Loss: 0.605\n",
      "Epoch 1551, Loss: 21.107, Final Batch Loss: 0.594\n",
      "Epoch 1552, Loss: 20.719, Final Batch Loss: 0.625\n",
      "Epoch 1553, Loss: 21.006, Final Batch Loss: 0.511\n",
      "Epoch 1554, Loss: 20.891, Final Batch Loss: 0.555\n",
      "Epoch 1555, Loss: 20.871, Final Batch Loss: 0.612\n",
      "Epoch 1556, Loss: 21.011, Final Batch Loss: 0.623\n",
      "Epoch 1557, Loss: 21.289, Final Batch Loss: 0.624\n",
      "Epoch 1558, Loss: 20.913, Final Batch Loss: 0.651\n",
      "Epoch 1559, Loss: 21.182, Final Batch Loss: 0.633\n",
      "Epoch 1560, Loss: 21.022, Final Batch Loss: 0.624\n",
      "Epoch 1561, Loss: 20.812, Final Batch Loss: 0.624\n",
      "Epoch 1562, Loss: 21.051, Final Batch Loss: 0.590\n",
      "Epoch 1563, Loss: 20.935, Final Batch Loss: 0.518\n",
      "Epoch 1564, Loss: 21.098, Final Batch Loss: 0.500\n",
      "Epoch 1565, Loss: 21.154, Final Batch Loss: 0.613\n",
      "Epoch 1566, Loss: 20.623, Final Batch Loss: 0.436\n",
      "Epoch 1567, Loss: 20.862, Final Batch Loss: 0.748\n",
      "Epoch 1568, Loss: 20.803, Final Batch Loss: 0.566\n",
      "Epoch 1569, Loss: 21.023, Final Batch Loss: 0.564\n",
      "Epoch 1570, Loss: 20.955, Final Batch Loss: 0.539\n",
      "Epoch 1571, Loss: 20.960, Final Batch Loss: 0.482\n",
      "Epoch 1572, Loss: 20.949, Final Batch Loss: 0.477\n",
      "Epoch 1573, Loss: 21.014, Final Batch Loss: 0.645\n",
      "Epoch 1574, Loss: 21.029, Final Batch Loss: 0.568\n",
      "Epoch 1575, Loss: 20.692, Final Batch Loss: 0.583\n",
      "Epoch 1576, Loss: 20.957, Final Batch Loss: 0.547\n",
      "Epoch 1577, Loss: 20.998, Final Batch Loss: 0.563\n",
      "Epoch 1578, Loss: 20.977, Final Batch Loss: 0.610\n",
      "Epoch 1579, Loss: 20.976, Final Batch Loss: 0.569\n",
      "Epoch 1580, Loss: 21.104, Final Batch Loss: 0.585\n",
      "Epoch 1581, Loss: 20.921, Final Batch Loss: 0.678\n",
      "Epoch 1582, Loss: 21.098, Final Batch Loss: 0.608\n",
      "Epoch 1583, Loss: 20.761, Final Batch Loss: 0.556\n",
      "Epoch 1584, Loss: 20.987, Final Batch Loss: 0.678\n",
      "Epoch 1585, Loss: 21.095, Final Batch Loss: 0.624\n",
      "Epoch 1586, Loss: 20.988, Final Batch Loss: 0.642\n",
      "Epoch 1587, Loss: 20.814, Final Batch Loss: 0.498\n",
      "Epoch 1588, Loss: 21.030, Final Batch Loss: 0.705\n",
      "Epoch 1589, Loss: 20.791, Final Batch Loss: 0.592\n",
      "Epoch 1590, Loss: 21.011, Final Batch Loss: 0.619\n",
      "Epoch 1591, Loss: 20.949, Final Batch Loss: 0.595\n",
      "Epoch 1592, Loss: 21.287, Final Batch Loss: 0.506\n",
      "Epoch 1593, Loss: 20.900, Final Batch Loss: 0.605\n",
      "Epoch 1594, Loss: 20.800, Final Batch Loss: 0.592\n",
      "Epoch 1595, Loss: 20.680, Final Batch Loss: 0.620\n",
      "Epoch 1596, Loss: 20.989, Final Batch Loss: 0.632\n",
      "Epoch 1597, Loss: 20.885, Final Batch Loss: 0.479\n",
      "Epoch 1598, Loss: 20.903, Final Batch Loss: 0.617\n",
      "Epoch 1599, Loss: 20.678, Final Batch Loss: 0.493\n",
      "Epoch 1600, Loss: 21.086, Final Batch Loss: 0.548\n",
      "Epoch 1601, Loss: 20.668, Final Batch Loss: 0.595\n",
      "Epoch 1602, Loss: 20.795, Final Batch Loss: 0.543\n",
      "Epoch 1603, Loss: 20.719, Final Batch Loss: 0.484\n",
      "Epoch 1604, Loss: 20.925, Final Batch Loss: 0.568\n",
      "Epoch 1605, Loss: 21.110, Final Batch Loss: 0.607\n",
      "Epoch 1606, Loss: 20.835, Final Batch Loss: 0.514\n",
      "Epoch 1607, Loss: 20.895, Final Batch Loss: 0.557\n",
      "Epoch 1608, Loss: 20.933, Final Batch Loss: 0.530\n",
      "Epoch 1609, Loss: 20.779, Final Batch Loss: 0.503\n",
      "Epoch 1610, Loss: 20.932, Final Batch Loss: 0.586\n",
      "Epoch 1611, Loss: 20.848, Final Batch Loss: 0.667\n",
      "Epoch 1612, Loss: 20.957, Final Batch Loss: 0.476\n",
      "Epoch 1613, Loss: 21.005, Final Batch Loss: 0.613\n",
      "Epoch 1614, Loss: 20.890, Final Batch Loss: 0.562\n",
      "Epoch 1615, Loss: 20.717, Final Batch Loss: 0.577\n",
      "Epoch 1616, Loss: 20.975, Final Batch Loss: 0.497\n",
      "Epoch 1617, Loss: 20.973, Final Batch Loss: 0.563\n",
      "Epoch 1618, Loss: 21.112, Final Batch Loss: 0.588\n",
      "Epoch 1619, Loss: 20.729, Final Batch Loss: 0.545\n",
      "Epoch 1620, Loss: 20.847, Final Batch Loss: 0.516\n",
      "Epoch 1621, Loss: 20.902, Final Batch Loss: 0.562\n",
      "Epoch 1622, Loss: 20.739, Final Batch Loss: 0.679\n",
      "Epoch 1623, Loss: 20.598, Final Batch Loss: 0.564\n",
      "Epoch 1624, Loss: 20.881, Final Batch Loss: 0.585\n",
      "Epoch 1625, Loss: 20.901, Final Batch Loss: 0.519\n",
      "Epoch 1626, Loss: 20.795, Final Batch Loss: 0.581\n",
      "Epoch 1627, Loss: 20.871, Final Batch Loss: 0.539\n",
      "Epoch 1628, Loss: 20.919, Final Batch Loss: 0.636\n",
      "Epoch 1629, Loss: 20.949, Final Batch Loss: 0.583\n",
      "Epoch 1630, Loss: 20.923, Final Batch Loss: 0.516\n",
      "Epoch 1631, Loss: 20.816, Final Batch Loss: 0.475\n",
      "Epoch 1632, Loss: 20.876, Final Batch Loss: 0.674\n",
      "Epoch 1633, Loss: 21.039, Final Batch Loss: 0.652\n",
      "Epoch 1634, Loss: 20.668, Final Batch Loss: 0.553\n",
      "Epoch 1635, Loss: 20.835, Final Batch Loss: 0.683\n",
      "Epoch 1636, Loss: 21.046, Final Batch Loss: 0.647\n",
      "Epoch 1637, Loss: 20.976, Final Batch Loss: 0.521\n",
      "Epoch 1638, Loss: 20.884, Final Batch Loss: 0.500\n",
      "Epoch 1639, Loss: 20.899, Final Batch Loss: 0.404\n",
      "Epoch 1640, Loss: 20.748, Final Batch Loss: 0.606\n",
      "Epoch 1641, Loss: 20.928, Final Batch Loss: 0.607\n",
      "Epoch 1642, Loss: 20.893, Final Batch Loss: 0.611\n",
      "Epoch 1643, Loss: 20.951, Final Batch Loss: 0.657\n",
      "Epoch 1644, Loss: 20.989, Final Batch Loss: 0.604\n",
      "Epoch 1645, Loss: 20.947, Final Batch Loss: 0.672\n",
      "Epoch 1646, Loss: 20.871, Final Batch Loss: 0.604\n",
      "Epoch 1647, Loss: 21.040, Final Batch Loss: 0.816\n",
      "Epoch 1648, Loss: 20.733, Final Batch Loss: 0.508\n",
      "Epoch 1649, Loss: 20.719, Final Batch Loss: 0.560\n",
      "Epoch 1650, Loss: 20.906, Final Batch Loss: 0.583\n",
      "Epoch 1651, Loss: 20.848, Final Batch Loss: 0.652\n",
      "Epoch 1652, Loss: 20.782, Final Batch Loss: 0.564\n",
      "Epoch 1653, Loss: 21.037, Final Batch Loss: 0.611\n",
      "Epoch 1654, Loss: 20.933, Final Batch Loss: 0.688\n",
      "Epoch 1655, Loss: 20.834, Final Batch Loss: 0.527\n",
      "Epoch 1656, Loss: 20.855, Final Batch Loss: 0.608\n",
      "Epoch 1657, Loss: 20.756, Final Batch Loss: 0.553\n",
      "Epoch 1658, Loss: 20.780, Final Batch Loss: 0.580\n",
      "Epoch 1659, Loss: 20.899, Final Batch Loss: 0.617\n",
      "Epoch 1660, Loss: 20.730, Final Batch Loss: 0.493\n",
      "Epoch 1661, Loss: 20.694, Final Batch Loss: 0.490\n",
      "Epoch 1662, Loss: 20.820, Final Batch Loss: 0.562\n",
      "Epoch 1663, Loss: 20.800, Final Batch Loss: 0.580\n",
      "Epoch 1664, Loss: 20.848, Final Batch Loss: 0.551\n",
      "Epoch 1665, Loss: 20.831, Final Batch Loss: 0.536\n",
      "Epoch 1666, Loss: 21.001, Final Batch Loss: 0.631\n",
      "Epoch 1667, Loss: 20.938, Final Batch Loss: 0.612\n",
      "Epoch 1668, Loss: 20.762, Final Batch Loss: 0.588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1669, Loss: 20.786, Final Batch Loss: 0.520\n",
      "Epoch 1670, Loss: 20.738, Final Batch Loss: 0.516\n",
      "Epoch 1671, Loss: 20.694, Final Batch Loss: 0.571\n",
      "Epoch 1672, Loss: 20.759, Final Batch Loss: 0.457\n",
      "Epoch 1673, Loss: 20.797, Final Batch Loss: 0.528\n",
      "Epoch 1674, Loss: 20.832, Final Batch Loss: 0.556\n",
      "Epoch 1675, Loss: 20.720, Final Batch Loss: 0.506\n",
      "Epoch 1676, Loss: 21.008, Final Batch Loss: 0.649\n",
      "Epoch 1677, Loss: 20.791, Final Batch Loss: 0.491\n",
      "Epoch 1678, Loss: 20.864, Final Batch Loss: 0.533\n",
      "Epoch 1679, Loss: 20.869, Final Batch Loss: 0.523\n",
      "Epoch 1680, Loss: 20.801, Final Batch Loss: 0.549\n",
      "Epoch 1681, Loss: 20.995, Final Batch Loss: 0.539\n",
      "Epoch 1682, Loss: 20.967, Final Batch Loss: 0.639\n",
      "Epoch 1683, Loss: 20.987, Final Batch Loss: 0.563\n",
      "Epoch 1684, Loss: 20.647, Final Batch Loss: 0.560\n",
      "Epoch 1685, Loss: 20.874, Final Batch Loss: 0.483\n",
      "Epoch 1686, Loss: 20.854, Final Batch Loss: 0.575\n",
      "Epoch 1687, Loss: 21.066, Final Batch Loss: 0.599\n",
      "Epoch 1688, Loss: 20.710, Final Batch Loss: 0.471\n",
      "Epoch 1689, Loss: 20.712, Final Batch Loss: 0.510\n",
      "Epoch 1690, Loss: 21.072, Final Batch Loss: 0.534\n",
      "Epoch 1691, Loss: 20.886, Final Batch Loss: 0.510\n",
      "Epoch 1692, Loss: 20.846, Final Batch Loss: 0.583\n",
      "Epoch 1693, Loss: 20.953, Final Batch Loss: 0.457\n",
      "Epoch 1694, Loss: 20.701, Final Batch Loss: 0.628\n",
      "Epoch 1695, Loss: 20.719, Final Batch Loss: 0.599\n",
      "Epoch 1696, Loss: 20.678, Final Batch Loss: 0.717\n",
      "Epoch 1697, Loss: 20.909, Final Batch Loss: 0.576\n",
      "Epoch 1698, Loss: 20.668, Final Batch Loss: 0.549\n",
      "Epoch 1699, Loss: 20.860, Final Batch Loss: 0.516\n",
      "Epoch 1700, Loss: 20.661, Final Batch Loss: 0.468\n",
      "Epoch 1701, Loss: 20.858, Final Batch Loss: 0.637\n",
      "Epoch 1702, Loss: 20.951, Final Batch Loss: 0.595\n",
      "Epoch 1703, Loss: 20.739, Final Batch Loss: 0.602\n",
      "Epoch 1704, Loss: 20.768, Final Batch Loss: 0.520\n",
      "Epoch 1705, Loss: 20.685, Final Batch Loss: 0.502\n",
      "Epoch 1706, Loss: 20.728, Final Batch Loss: 0.495\n",
      "Epoch 1707, Loss: 20.546, Final Batch Loss: 0.537\n",
      "Epoch 1708, Loss: 20.659, Final Batch Loss: 0.479\n",
      "Epoch 1709, Loss: 21.001, Final Batch Loss: 0.750\n",
      "Epoch 1710, Loss: 20.949, Final Batch Loss: 0.547\n",
      "Epoch 1711, Loss: 20.708, Final Batch Loss: 0.517\n",
      "Epoch 1712, Loss: 20.581, Final Batch Loss: 0.461\n",
      "Epoch 1713, Loss: 20.661, Final Batch Loss: 0.628\n",
      "Epoch 1714, Loss: 20.640, Final Batch Loss: 0.552\n",
      "Epoch 1715, Loss: 20.683, Final Batch Loss: 0.546\n",
      "Epoch 1716, Loss: 20.614, Final Batch Loss: 0.589\n",
      "Epoch 1717, Loss: 20.694, Final Batch Loss: 0.461\n",
      "Epoch 1718, Loss: 20.640, Final Batch Loss: 0.620\n",
      "Epoch 1719, Loss: 20.926, Final Batch Loss: 0.508\n",
      "Epoch 1720, Loss: 20.946, Final Batch Loss: 0.699\n",
      "Epoch 1721, Loss: 20.890, Final Batch Loss: 0.567\n",
      "Epoch 1722, Loss: 20.793, Final Batch Loss: 0.441\n",
      "Epoch 1723, Loss: 20.710, Final Batch Loss: 0.588\n",
      "Epoch 1724, Loss: 20.957, Final Batch Loss: 0.551\n",
      "Epoch 1725, Loss: 20.847, Final Batch Loss: 0.517\n",
      "Epoch 1726, Loss: 20.884, Final Batch Loss: 0.595\n",
      "Epoch 1727, Loss: 20.880, Final Batch Loss: 0.650\n",
      "Epoch 1728, Loss: 20.831, Final Batch Loss: 0.564\n",
      "Epoch 1729, Loss: 20.708, Final Batch Loss: 0.453\n",
      "Epoch 1730, Loss: 20.827, Final Batch Loss: 0.642\n",
      "Epoch 1731, Loss: 20.946, Final Batch Loss: 0.584\n",
      "Epoch 1732, Loss: 21.071, Final Batch Loss: 0.608\n",
      "Epoch 1733, Loss: 20.978, Final Batch Loss: 0.608\n",
      "Epoch 1734, Loss: 20.807, Final Batch Loss: 0.544\n",
      "Epoch 1735, Loss: 20.813, Final Batch Loss: 0.570\n",
      "Epoch 1736, Loss: 20.636, Final Batch Loss: 0.486\n",
      "Epoch 1737, Loss: 20.876, Final Batch Loss: 0.612\n",
      "Epoch 1738, Loss: 20.837, Final Batch Loss: 0.433\n",
      "Epoch 1739, Loss: 20.846, Final Batch Loss: 0.610\n",
      "Epoch 1740, Loss: 20.814, Final Batch Loss: 0.531\n",
      "Epoch 1741, Loss: 20.817, Final Batch Loss: 0.440\n",
      "Epoch 1742, Loss: 20.625, Final Batch Loss: 0.637\n",
      "Epoch 1743, Loss: 20.816, Final Batch Loss: 0.630\n",
      "Epoch 1744, Loss: 20.766, Final Batch Loss: 0.539\n",
      "Epoch 1745, Loss: 20.704, Final Batch Loss: 0.604\n",
      "Epoch 1746, Loss: 20.731, Final Batch Loss: 0.622\n",
      "Epoch 1747, Loss: 20.817, Final Batch Loss: 0.619\n",
      "Epoch 1748, Loss: 20.424, Final Batch Loss: 0.545\n",
      "Epoch 1749, Loss: 20.920, Final Batch Loss: 0.721\n",
      "Epoch 1750, Loss: 20.622, Final Batch Loss: 0.544\n",
      "Epoch 1751, Loss: 20.355, Final Batch Loss: 0.588\n",
      "Epoch 1752, Loss: 20.736, Final Batch Loss: 0.597\n",
      "Epoch 1753, Loss: 20.770, Final Batch Loss: 0.479\n",
      "Epoch 1754, Loss: 20.668, Final Batch Loss: 0.546\n",
      "Epoch 1755, Loss: 20.623, Final Batch Loss: 0.501\n",
      "Epoch 1756, Loss: 21.039, Final Batch Loss: 0.615\n",
      "Epoch 1757, Loss: 20.629, Final Batch Loss: 0.603\n",
      "Epoch 1758, Loss: 20.656, Final Batch Loss: 0.550\n",
      "Epoch 1759, Loss: 20.699, Final Batch Loss: 0.624\n",
      "Epoch 1760, Loss: 20.771, Final Batch Loss: 0.650\n",
      "Epoch 1761, Loss: 20.823, Final Batch Loss: 0.550\n",
      "Epoch 1762, Loss: 20.801, Final Batch Loss: 0.554\n",
      "Epoch 1763, Loss: 20.853, Final Batch Loss: 0.558\n",
      "Epoch 1764, Loss: 20.686, Final Batch Loss: 0.611\n",
      "Epoch 1765, Loss: 20.892, Final Batch Loss: 0.618\n",
      "Epoch 1766, Loss: 20.687, Final Batch Loss: 0.479\n",
      "Epoch 1767, Loss: 20.656, Final Batch Loss: 0.556\n",
      "Epoch 1768, Loss: 20.591, Final Batch Loss: 0.598\n",
      "Epoch 1769, Loss: 20.688, Final Batch Loss: 0.671\n",
      "Epoch 1770, Loss: 20.774, Final Batch Loss: 0.771\n",
      "Epoch 1771, Loss: 20.774, Final Batch Loss: 0.607\n",
      "Epoch 1772, Loss: 20.723, Final Batch Loss: 0.512\n",
      "Epoch 1773, Loss: 20.642, Final Batch Loss: 0.602\n",
      "Epoch 1774, Loss: 20.635, Final Batch Loss: 0.589\n",
      "Epoch 1775, Loss: 20.894, Final Batch Loss: 0.539\n",
      "Epoch 1776, Loss: 20.826, Final Batch Loss: 0.592\n",
      "Epoch 1777, Loss: 20.667, Final Batch Loss: 0.518\n",
      "Epoch 1778, Loss: 20.556, Final Batch Loss: 0.535\n",
      "Epoch 1779, Loss: 20.563, Final Batch Loss: 0.628\n",
      "Epoch 1780, Loss: 20.737, Final Batch Loss: 0.579\n",
      "Epoch 1781, Loss: 20.754, Final Batch Loss: 0.658\n",
      "Epoch 1782, Loss: 20.554, Final Batch Loss: 0.493\n",
      "Epoch 1783, Loss: 20.818, Final Batch Loss: 0.560\n",
      "Epoch 1784, Loss: 20.582, Final Batch Loss: 0.488\n",
      "Epoch 1785, Loss: 20.803, Final Batch Loss: 0.526\n",
      "Epoch 1786, Loss: 20.970, Final Batch Loss: 0.669\n",
      "Epoch 1787, Loss: 21.051, Final Batch Loss: 0.489\n",
      "Epoch 1788, Loss: 20.719, Final Batch Loss: 0.631\n",
      "Epoch 1789, Loss: 20.716, Final Batch Loss: 0.577\n",
      "Epoch 1790, Loss: 20.517, Final Batch Loss: 0.544\n",
      "Epoch 1791, Loss: 20.678, Final Batch Loss: 0.588\n",
      "Epoch 1792, Loss: 20.771, Final Batch Loss: 0.489\n",
      "Epoch 1793, Loss: 20.785, Final Batch Loss: 0.656\n",
      "Epoch 1794, Loss: 21.015, Final Batch Loss: 0.500\n",
      "Epoch 1795, Loss: 20.590, Final Batch Loss: 0.676\n",
      "Epoch 1796, Loss: 20.900, Final Batch Loss: 0.627\n",
      "Epoch 1797, Loss: 20.425, Final Batch Loss: 0.488\n",
      "Epoch 1798, Loss: 20.649, Final Batch Loss: 0.620\n",
      "Epoch 1799, Loss: 20.827, Final Batch Loss: 0.618\n",
      "Epoch 1800, Loss: 20.739, Final Batch Loss: 0.509\n",
      "Epoch 1801, Loss: 20.591, Final Batch Loss: 0.624\n",
      "Epoch 1802, Loss: 20.813, Final Batch Loss: 0.579\n",
      "Epoch 1803, Loss: 20.707, Final Batch Loss: 0.620\n",
      "Epoch 1804, Loss: 20.746, Final Batch Loss: 0.591\n",
      "Epoch 1805, Loss: 20.558, Final Batch Loss: 0.498\n",
      "Epoch 1806, Loss: 20.743, Final Batch Loss: 0.634\n",
      "Epoch 1807, Loss: 20.632, Final Batch Loss: 0.586\n",
      "Epoch 1808, Loss: 20.656, Final Batch Loss: 0.486\n",
      "Epoch 1809, Loss: 20.774, Final Batch Loss: 0.624\n",
      "Epoch 1810, Loss: 20.682, Final Batch Loss: 0.555\n",
      "Epoch 1811, Loss: 20.685, Final Batch Loss: 0.627\n",
      "Epoch 1812, Loss: 20.742, Final Batch Loss: 0.538\n",
      "Epoch 1813, Loss: 20.473, Final Batch Loss: 0.545\n",
      "Epoch 1814, Loss: 20.627, Final Batch Loss: 0.571\n",
      "Epoch 1815, Loss: 20.663, Final Batch Loss: 0.550\n",
      "Epoch 1816, Loss: 20.791, Final Batch Loss: 0.493\n",
      "Epoch 1817, Loss: 20.494, Final Batch Loss: 0.584\n",
      "Epoch 1818, Loss: 20.569, Final Batch Loss: 0.509\n",
      "Epoch 1819, Loss: 20.822, Final Batch Loss: 0.649\n",
      "Epoch 1820, Loss: 20.810, Final Batch Loss: 0.558\n",
      "Epoch 1821, Loss: 20.577, Final Batch Loss: 0.602\n",
      "Epoch 1822, Loss: 20.772, Final Batch Loss: 0.647\n",
      "Epoch 1823, Loss: 20.791, Final Batch Loss: 0.594\n",
      "Epoch 1824, Loss: 20.575, Final Batch Loss: 0.525\n",
      "Epoch 1825, Loss: 20.670, Final Batch Loss: 0.520\n",
      "Epoch 1826, Loss: 20.713, Final Batch Loss: 0.609\n",
      "Epoch 1827, Loss: 20.527, Final Batch Loss: 0.551\n",
      "Epoch 1828, Loss: 20.720, Final Batch Loss: 0.633\n",
      "Epoch 1829, Loss: 20.534, Final Batch Loss: 0.525\n",
      "Epoch 1830, Loss: 20.708, Final Batch Loss: 0.492\n",
      "Epoch 1831, Loss: 20.633, Final Batch Loss: 0.563\n",
      "Epoch 1832, Loss: 20.448, Final Batch Loss: 0.481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1833, Loss: 20.842, Final Batch Loss: 0.754\n",
      "Epoch 1834, Loss: 20.753, Final Batch Loss: 0.620\n",
      "Epoch 1835, Loss: 20.661, Final Batch Loss: 0.539\n",
      "Epoch 1836, Loss: 20.782, Final Batch Loss: 0.573\n",
      "Epoch 1837, Loss: 20.430, Final Batch Loss: 0.594\n",
      "Epoch 1838, Loss: 20.704, Final Batch Loss: 0.557\n",
      "Epoch 1839, Loss: 20.583, Final Batch Loss: 0.530\n",
      "Epoch 1840, Loss: 20.643, Final Batch Loss: 0.673\n",
      "Epoch 1841, Loss: 20.934, Final Batch Loss: 0.611\n",
      "Epoch 1842, Loss: 20.751, Final Batch Loss: 0.605\n",
      "Epoch 1843, Loss: 20.597, Final Batch Loss: 0.700\n",
      "Epoch 1844, Loss: 20.318, Final Batch Loss: 0.606\n",
      "Epoch 1845, Loss: 20.954, Final Batch Loss: 0.603\n",
      "Epoch 1846, Loss: 20.779, Final Batch Loss: 0.552\n",
      "Epoch 1847, Loss: 20.299, Final Batch Loss: 0.495\n",
      "Epoch 1848, Loss: 20.548, Final Batch Loss: 0.479\n",
      "Epoch 1849, Loss: 20.743, Final Batch Loss: 0.492\n",
      "Epoch 1850, Loss: 20.772, Final Batch Loss: 0.531\n",
      "Epoch 1851, Loss: 20.689, Final Batch Loss: 0.583\n",
      "Epoch 1852, Loss: 20.646, Final Batch Loss: 0.549\n",
      "Epoch 1853, Loss: 20.633, Final Batch Loss: 0.590\n",
      "Epoch 1854, Loss: 20.440, Final Batch Loss: 0.606\n",
      "Epoch 1855, Loss: 20.524, Final Batch Loss: 0.537\n",
      "Epoch 1856, Loss: 20.911, Final Batch Loss: 0.665\n",
      "Epoch 1857, Loss: 20.789, Final Batch Loss: 0.587\n",
      "Epoch 1858, Loss: 20.555, Final Batch Loss: 0.485\n",
      "Epoch 1859, Loss: 20.788, Final Batch Loss: 0.500\n",
      "Epoch 1860, Loss: 20.596, Final Batch Loss: 0.453\n",
      "Epoch 1861, Loss: 20.659, Final Batch Loss: 0.627\n",
      "Epoch 1862, Loss: 20.789, Final Batch Loss: 0.581\n",
      "Epoch 1863, Loss: 20.464, Final Batch Loss: 0.565\n",
      "Epoch 1864, Loss: 20.611, Final Batch Loss: 0.595\n",
      "Epoch 1865, Loss: 20.655, Final Batch Loss: 0.633\n",
      "Epoch 1866, Loss: 20.648, Final Batch Loss: 0.631\n",
      "Epoch 1867, Loss: 20.612, Final Batch Loss: 0.550\n",
      "Epoch 1868, Loss: 20.604, Final Batch Loss: 0.604\n",
      "Epoch 1869, Loss: 20.750, Final Batch Loss: 0.540\n",
      "Epoch 1870, Loss: 20.773, Final Batch Loss: 0.569\n",
      "Epoch 1871, Loss: 20.836, Final Batch Loss: 0.566\n",
      "Epoch 1872, Loss: 20.564, Final Batch Loss: 0.558\n",
      "Epoch 1873, Loss: 20.446, Final Batch Loss: 0.597\n",
      "Epoch 1874, Loss: 20.643, Final Batch Loss: 0.522\n",
      "Epoch 1875, Loss: 20.855, Final Batch Loss: 0.585\n",
      "Epoch 1876, Loss: 20.457, Final Batch Loss: 0.529\n",
      "Epoch 1877, Loss: 20.688, Final Batch Loss: 0.567\n",
      "Epoch 1878, Loss: 20.488, Final Batch Loss: 0.569\n",
      "Epoch 1879, Loss: 20.636, Final Batch Loss: 0.590\n",
      "Epoch 1880, Loss: 20.527, Final Batch Loss: 0.508\n",
      "Epoch 1881, Loss: 20.629, Final Batch Loss: 0.499\n",
      "Epoch 1882, Loss: 20.447, Final Batch Loss: 0.502\n",
      "Epoch 1883, Loss: 20.588, Final Batch Loss: 0.524\n",
      "Epoch 1884, Loss: 20.467, Final Batch Loss: 0.527\n",
      "Epoch 1885, Loss: 20.790, Final Batch Loss: 0.728\n",
      "Epoch 1886, Loss: 20.400, Final Batch Loss: 0.493\n",
      "Epoch 1887, Loss: 20.757, Final Batch Loss: 0.602\n",
      "Epoch 1888, Loss: 20.526, Final Batch Loss: 0.646\n",
      "Epoch 1889, Loss: 20.537, Final Batch Loss: 0.498\n",
      "Epoch 1890, Loss: 20.738, Final Batch Loss: 0.573\n",
      "Epoch 1891, Loss: 20.538, Final Batch Loss: 0.625\n",
      "Epoch 1892, Loss: 20.586, Final Batch Loss: 0.461\n",
      "Epoch 1893, Loss: 20.745, Final Batch Loss: 0.616\n",
      "Epoch 1894, Loss: 20.343, Final Batch Loss: 0.594\n",
      "Epoch 1895, Loss: 20.688, Final Batch Loss: 0.630\n",
      "Epoch 1896, Loss: 20.570, Final Batch Loss: 0.546\n",
      "Epoch 1897, Loss: 20.505, Final Batch Loss: 0.575\n",
      "Epoch 1898, Loss: 20.640, Final Batch Loss: 0.658\n",
      "Epoch 1899, Loss: 20.485, Final Batch Loss: 0.598\n",
      "Epoch 1900, Loss: 20.317, Final Batch Loss: 0.545\n",
      "Epoch 1901, Loss: 20.662, Final Batch Loss: 0.622\n",
      "Epoch 1902, Loss: 20.616, Final Batch Loss: 0.513\n",
      "Epoch 1903, Loss: 20.685, Final Batch Loss: 0.491\n",
      "Epoch 1904, Loss: 20.675, Final Batch Loss: 0.533\n",
      "Epoch 1905, Loss: 20.609, Final Batch Loss: 0.511\n",
      "Epoch 1906, Loss: 20.479, Final Batch Loss: 0.585\n",
      "Epoch 1907, Loss: 20.783, Final Batch Loss: 0.543\n",
      "Epoch 1908, Loss: 20.577, Final Batch Loss: 0.575\n",
      "Epoch 1909, Loss: 20.664, Final Batch Loss: 0.548\n",
      "Epoch 1910, Loss: 20.661, Final Batch Loss: 0.444\n",
      "Epoch 1911, Loss: 20.503, Final Batch Loss: 0.544\n",
      "Epoch 1912, Loss: 20.566, Final Batch Loss: 0.616\n",
      "Epoch 1913, Loss: 20.673, Final Batch Loss: 0.546\n",
      "Epoch 1914, Loss: 20.388, Final Batch Loss: 0.590\n",
      "Epoch 1915, Loss: 20.606, Final Batch Loss: 0.588\n",
      "Epoch 1916, Loss: 20.759, Final Batch Loss: 0.556\n",
      "Epoch 1917, Loss: 20.782, Final Batch Loss: 0.569\n",
      "Epoch 1918, Loss: 20.250, Final Batch Loss: 0.641\n",
      "Epoch 1919, Loss: 20.717, Final Batch Loss: 0.576\n",
      "Epoch 1920, Loss: 20.764, Final Batch Loss: 0.740\n",
      "Epoch 1921, Loss: 20.479, Final Batch Loss: 0.500\n",
      "Epoch 1922, Loss: 20.750, Final Batch Loss: 0.565\n",
      "Epoch 1923, Loss: 20.458, Final Batch Loss: 0.559\n",
      "Epoch 1924, Loss: 20.726, Final Batch Loss: 0.516\n",
      "Epoch 1925, Loss: 20.644, Final Batch Loss: 0.509\n",
      "Epoch 1926, Loss: 20.538, Final Batch Loss: 0.545\n",
      "Epoch 1927, Loss: 20.503, Final Batch Loss: 0.487\n",
      "Epoch 1928, Loss: 20.590, Final Batch Loss: 0.704\n",
      "Epoch 1929, Loss: 20.436, Final Batch Loss: 0.512\n",
      "Epoch 1930, Loss: 20.565, Final Batch Loss: 0.637\n",
      "Epoch 1931, Loss: 20.391, Final Batch Loss: 0.540\n",
      "Epoch 1932, Loss: 20.601, Final Batch Loss: 0.564\n",
      "Epoch 1933, Loss: 20.280, Final Batch Loss: 0.599\n",
      "Epoch 1934, Loss: 20.576, Final Batch Loss: 0.616\n",
      "Epoch 1935, Loss: 20.726, Final Batch Loss: 0.677\n",
      "Epoch 1936, Loss: 20.533, Final Batch Loss: 0.395\n",
      "Epoch 1937, Loss: 20.415, Final Batch Loss: 0.484\n",
      "Epoch 1938, Loss: 20.655, Final Batch Loss: 0.571\n",
      "Epoch 1939, Loss: 20.820, Final Batch Loss: 0.638\n",
      "Epoch 1940, Loss: 20.299, Final Batch Loss: 0.556\n",
      "Epoch 1941, Loss: 20.447, Final Batch Loss: 0.614\n",
      "Epoch 1942, Loss: 20.621, Final Batch Loss: 0.539\n",
      "Epoch 1943, Loss: 20.687, Final Batch Loss: 0.506\n",
      "Epoch 1944, Loss: 20.320, Final Batch Loss: 0.563\n",
      "Epoch 1945, Loss: 20.460, Final Batch Loss: 0.585\n",
      "Epoch 1946, Loss: 20.467, Final Batch Loss: 0.636\n",
      "Epoch 1947, Loss: 20.555, Final Batch Loss: 0.541\n",
      "Epoch 1948, Loss: 20.370, Final Batch Loss: 0.454\n",
      "Epoch 1949, Loss: 20.674, Final Batch Loss: 0.574\n",
      "Epoch 1950, Loss: 20.471, Final Batch Loss: 0.598\n",
      "Epoch 1951, Loss: 20.709, Final Batch Loss: 0.483\n",
      "Epoch 1952, Loss: 20.746, Final Batch Loss: 0.628\n",
      "Epoch 1953, Loss: 20.746, Final Batch Loss: 0.597\n",
      "Epoch 1954, Loss: 20.619, Final Batch Loss: 0.549\n",
      "Epoch 1955, Loss: 20.500, Final Batch Loss: 0.569\n",
      "Epoch 1956, Loss: 20.594, Final Batch Loss: 0.566\n",
      "Epoch 1957, Loss: 20.313, Final Batch Loss: 0.564\n",
      "Epoch 1958, Loss: 20.480, Final Batch Loss: 0.545\n",
      "Epoch 1959, Loss: 20.610, Final Batch Loss: 0.684\n",
      "Epoch 1960, Loss: 20.486, Final Batch Loss: 0.659\n",
      "Epoch 1961, Loss: 20.541, Final Batch Loss: 0.529\n",
      "Epoch 1962, Loss: 20.405, Final Batch Loss: 0.601\n",
      "Epoch 1963, Loss: 20.505, Final Batch Loss: 0.603\n",
      "Epoch 1964, Loss: 20.484, Final Batch Loss: 0.564\n",
      "Epoch 1965, Loss: 20.532, Final Batch Loss: 0.544\n",
      "Epoch 1966, Loss: 20.584, Final Batch Loss: 0.543\n",
      "Epoch 1967, Loss: 20.361, Final Batch Loss: 0.453\n",
      "Epoch 1968, Loss: 20.614, Final Batch Loss: 0.486\n",
      "Epoch 1969, Loss: 20.608, Final Batch Loss: 0.527\n",
      "Epoch 1970, Loss: 20.626, Final Batch Loss: 0.529\n",
      "Epoch 1971, Loss: 20.644, Final Batch Loss: 0.546\n",
      "Epoch 1972, Loss: 20.626, Final Batch Loss: 0.543\n",
      "Epoch 1973, Loss: 20.535, Final Batch Loss: 0.562\n",
      "Epoch 1974, Loss: 20.454, Final Batch Loss: 0.618\n",
      "Epoch 1975, Loss: 20.631, Final Batch Loss: 0.495\n",
      "Epoch 1976, Loss: 20.533, Final Batch Loss: 0.506\n",
      "Epoch 1977, Loss: 20.774, Final Batch Loss: 0.606\n",
      "Epoch 1978, Loss: 20.322, Final Batch Loss: 0.528\n",
      "Epoch 1979, Loss: 20.596, Final Batch Loss: 0.689\n",
      "Epoch 1980, Loss: 20.426, Final Batch Loss: 0.608\n",
      "Epoch 1981, Loss: 20.487, Final Batch Loss: 0.476\n",
      "Epoch 1982, Loss: 20.608, Final Batch Loss: 0.644\n",
      "Epoch 1983, Loss: 20.615, Final Batch Loss: 0.674\n",
      "Epoch 1984, Loss: 20.431, Final Batch Loss: 0.622\n",
      "Epoch 1985, Loss: 20.447, Final Batch Loss: 0.537\n",
      "Epoch 1986, Loss: 20.382, Final Batch Loss: 0.496\n",
      "Epoch 1987, Loss: 20.485, Final Batch Loss: 0.554\n",
      "Epoch 1988, Loss: 20.554, Final Batch Loss: 0.531\n",
      "Epoch 1989, Loss: 20.738, Final Batch Loss: 0.709\n",
      "Epoch 1990, Loss: 20.580, Final Batch Loss: 0.566\n",
      "Epoch 1991, Loss: 20.510, Final Batch Loss: 0.508\n",
      "Epoch 1992, Loss: 20.613, Final Batch Loss: 0.619\n",
      "Epoch 1993, Loss: 20.697, Final Batch Loss: 0.560\n",
      "Epoch 1994, Loss: 20.740, Final Batch Loss: 0.795\n",
      "Epoch 1995, Loss: 20.590, Final Batch Loss: 0.549\n",
      "Epoch 1996, Loss: 20.550, Final Batch Loss: 0.548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1997, Loss: 20.529, Final Batch Loss: 0.664\n",
      "Epoch 1998, Loss: 20.627, Final Batch Loss: 0.540\n",
      "Epoch 1999, Loss: 20.403, Final Batch Loss: 0.489\n",
      "Epoch 2000, Loss: 20.385, Final Batch Loss: 0.730\n",
      "Epoch 2001, Loss: 20.415, Final Batch Loss: 0.646\n",
      "Epoch 2002, Loss: 20.521, Final Batch Loss: 0.656\n",
      "Epoch 2003, Loss: 20.420, Final Batch Loss: 0.524\n",
      "Epoch 2004, Loss: 20.475, Final Batch Loss: 0.504\n",
      "Epoch 2005, Loss: 20.456, Final Batch Loss: 0.562\n",
      "Epoch 2006, Loss: 20.635, Final Batch Loss: 0.537\n",
      "Epoch 2007, Loss: 20.760, Final Batch Loss: 0.633\n",
      "Epoch 2008, Loss: 20.592, Final Batch Loss: 0.662\n",
      "Epoch 2009, Loss: 20.369, Final Batch Loss: 0.497\n",
      "Epoch 2010, Loss: 20.526, Final Batch Loss: 0.611\n",
      "Epoch 2011, Loss: 20.879, Final Batch Loss: 0.509\n",
      "Epoch 2012, Loss: 20.404, Final Batch Loss: 0.565\n",
      "Epoch 2013, Loss: 20.195, Final Batch Loss: 0.548\n",
      "Epoch 2014, Loss: 20.529, Final Batch Loss: 0.575\n",
      "Epoch 2015, Loss: 20.575, Final Batch Loss: 0.634\n",
      "Epoch 2016, Loss: 20.642, Final Batch Loss: 0.732\n",
      "Epoch 2017, Loss: 20.450, Final Batch Loss: 0.580\n",
      "Epoch 2018, Loss: 20.362, Final Batch Loss: 0.421\n",
      "Epoch 2019, Loss: 20.457, Final Batch Loss: 0.484\n",
      "Epoch 2020, Loss: 20.485, Final Batch Loss: 0.625\n",
      "Epoch 2021, Loss: 20.713, Final Batch Loss: 0.613\n",
      "Epoch 2022, Loss: 20.432, Final Batch Loss: 0.608\n",
      "Epoch 2023, Loss: 20.290, Final Batch Loss: 0.605\n",
      "Epoch 2024, Loss: 20.282, Final Batch Loss: 0.509\n",
      "Epoch 2025, Loss: 20.480, Final Batch Loss: 0.536\n",
      "Epoch 2026, Loss: 20.496, Final Batch Loss: 0.579\n",
      "Epoch 2027, Loss: 20.498, Final Batch Loss: 0.545\n",
      "Epoch 2028, Loss: 20.675, Final Batch Loss: 0.589\n",
      "Epoch 2029, Loss: 20.501, Final Batch Loss: 0.619\n",
      "Epoch 2030, Loss: 20.319, Final Batch Loss: 0.502\n",
      "Epoch 2031, Loss: 20.396, Final Batch Loss: 0.526\n",
      "Epoch 2032, Loss: 20.369, Final Batch Loss: 0.542\n",
      "Epoch 2033, Loss: 20.528, Final Batch Loss: 0.519\n",
      "Epoch 2034, Loss: 20.419, Final Batch Loss: 0.587\n",
      "Epoch 2035, Loss: 20.429, Final Batch Loss: 0.497\n",
      "Epoch 2036, Loss: 20.310, Final Batch Loss: 0.545\n",
      "Epoch 2037, Loss: 20.591, Final Batch Loss: 0.401\n",
      "Epoch 2038, Loss: 20.279, Final Batch Loss: 0.506\n",
      "Epoch 2039, Loss: 20.768, Final Batch Loss: 0.639\n",
      "Epoch 2040, Loss: 20.456, Final Batch Loss: 0.644\n",
      "Epoch 2041, Loss: 20.455, Final Batch Loss: 0.612\n",
      "Epoch 2042, Loss: 20.107, Final Batch Loss: 0.650\n",
      "Epoch 2043, Loss: 20.460, Final Batch Loss: 0.732\n",
      "Epoch 2044, Loss: 20.608, Final Batch Loss: 0.584\n",
      "Epoch 2045, Loss: 20.520, Final Batch Loss: 0.551\n",
      "Epoch 2046, Loss: 20.590, Final Batch Loss: 0.504\n",
      "Epoch 2047, Loss: 20.564, Final Batch Loss: 0.597\n",
      "Epoch 2048, Loss: 20.452, Final Batch Loss: 0.576\n",
      "Epoch 2049, Loss: 20.386, Final Batch Loss: 0.633\n",
      "Epoch 2050, Loss: 20.544, Final Batch Loss: 0.525\n",
      "Epoch 2051, Loss: 20.416, Final Batch Loss: 0.572\n",
      "Epoch 2052, Loss: 20.510, Final Batch Loss: 0.534\n",
      "Epoch 2053, Loss: 20.338, Final Batch Loss: 0.543\n",
      "Epoch 2054, Loss: 20.584, Final Batch Loss: 0.501\n",
      "Epoch 2055, Loss: 20.465, Final Batch Loss: 0.450\n",
      "Epoch 2056, Loss: 20.393, Final Batch Loss: 0.649\n",
      "Epoch 2057, Loss: 20.509, Final Batch Loss: 0.555\n",
      "Epoch 2058, Loss: 20.310, Final Batch Loss: 0.633\n",
      "Epoch 2059, Loss: 20.375, Final Batch Loss: 0.608\n",
      "Epoch 2060, Loss: 20.228, Final Batch Loss: 0.633\n",
      "Epoch 2061, Loss: 20.323, Final Batch Loss: 0.692\n",
      "Epoch 2062, Loss: 20.425, Final Batch Loss: 0.503\n",
      "Epoch 2063, Loss: 20.471, Final Batch Loss: 0.596\n",
      "Epoch 2064, Loss: 20.439, Final Batch Loss: 0.613\n",
      "Epoch 2065, Loss: 20.489, Final Batch Loss: 0.608\n",
      "Epoch 2066, Loss: 20.406, Final Batch Loss: 0.606\n",
      "Epoch 2067, Loss: 20.505, Final Batch Loss: 0.584\n",
      "Epoch 2068, Loss: 20.440, Final Batch Loss: 0.500\n",
      "Epoch 2069, Loss: 20.611, Final Batch Loss: 0.562\n",
      "Epoch 2070, Loss: 20.547, Final Batch Loss: 0.529\n",
      "Epoch 2071, Loss: 20.362, Final Batch Loss: 0.649\n",
      "Epoch 2072, Loss: 20.235, Final Batch Loss: 0.550\n",
      "Epoch 2073, Loss: 20.381, Final Batch Loss: 0.559\n",
      "Epoch 2074, Loss: 20.740, Final Batch Loss: 0.587\n",
      "Epoch 2075, Loss: 20.539, Final Batch Loss: 0.554\n",
      "Epoch 2076, Loss: 20.450, Final Batch Loss: 0.609\n",
      "Epoch 2077, Loss: 20.304, Final Batch Loss: 0.526\n",
      "Epoch 2078, Loss: 20.662, Final Batch Loss: 0.617\n",
      "Epoch 2079, Loss: 20.435, Final Batch Loss: 0.622\n",
      "Epoch 2080, Loss: 20.544, Final Batch Loss: 0.576\n",
      "Epoch 2081, Loss: 20.539, Final Batch Loss: 0.521\n",
      "Epoch 2082, Loss: 20.617, Final Batch Loss: 0.644\n",
      "Epoch 2083, Loss: 20.204, Final Batch Loss: 0.614\n",
      "Epoch 2084, Loss: 20.767, Final Batch Loss: 0.561\n",
      "Epoch 2085, Loss: 20.219, Final Batch Loss: 0.445\n",
      "Epoch 2086, Loss: 20.348, Final Batch Loss: 0.572\n",
      "Epoch 2087, Loss: 20.488, Final Batch Loss: 0.610\n",
      "Epoch 2088, Loss: 20.206, Final Batch Loss: 0.639\n",
      "Epoch 2089, Loss: 20.446, Final Batch Loss: 0.566\n",
      "Epoch 2090, Loss: 20.458, Final Batch Loss: 0.590\n",
      "Epoch 2091, Loss: 20.392, Final Batch Loss: 0.557\n",
      "Epoch 2092, Loss: 20.559, Final Batch Loss: 0.524\n",
      "Epoch 2093, Loss: 20.564, Final Batch Loss: 0.640\n",
      "Epoch 2094, Loss: 20.525, Final Batch Loss: 0.582\n",
      "Epoch 2095, Loss: 20.405, Final Batch Loss: 0.531\n",
      "Epoch 2096, Loss: 20.228, Final Batch Loss: 0.494\n",
      "Epoch 2097, Loss: 20.605, Final Batch Loss: 0.652\n",
      "Epoch 2098, Loss: 20.651, Final Batch Loss: 0.631\n",
      "Epoch 2099, Loss: 20.489, Final Batch Loss: 0.522\n",
      "Epoch 2100, Loss: 20.413, Final Batch Loss: 0.541\n",
      "Epoch 2101, Loss: 20.233, Final Batch Loss: 0.660\n",
      "Epoch 2102, Loss: 20.527, Final Batch Loss: 0.570\n",
      "Epoch 2103, Loss: 20.444, Final Batch Loss: 0.477\n",
      "Epoch 2104, Loss: 20.410, Final Batch Loss: 0.595\n",
      "Epoch 2105, Loss: 20.476, Final Batch Loss: 0.645\n",
      "Epoch 2106, Loss: 20.487, Final Batch Loss: 0.554\n",
      "Epoch 2107, Loss: 20.594, Final Batch Loss: 0.596\n",
      "Epoch 2108, Loss: 20.371, Final Batch Loss: 0.558\n",
      "Epoch 2109, Loss: 20.258, Final Batch Loss: 0.520\n",
      "Epoch 2110, Loss: 20.506, Final Batch Loss: 0.577\n",
      "Epoch 2111, Loss: 20.483, Final Batch Loss: 0.474\n",
      "Epoch 2112, Loss: 20.389, Final Batch Loss: 0.482\n",
      "Epoch 2113, Loss: 20.484, Final Batch Loss: 0.734\n",
      "Epoch 2114, Loss: 20.388, Final Batch Loss: 0.569\n",
      "Epoch 2115, Loss: 20.412, Final Batch Loss: 0.561\n",
      "Epoch 2116, Loss: 20.339, Final Batch Loss: 0.462\n",
      "Epoch 2117, Loss: 20.638, Final Batch Loss: 0.487\n",
      "Epoch 2118, Loss: 20.175, Final Batch Loss: 0.495\n",
      "Epoch 2119, Loss: 20.459, Final Batch Loss: 0.477\n",
      "Epoch 2120, Loss: 20.240, Final Batch Loss: 0.465\n",
      "Epoch 2121, Loss: 20.279, Final Batch Loss: 0.568\n",
      "Epoch 2122, Loss: 20.492, Final Batch Loss: 0.598\n",
      "Epoch 2123, Loss: 20.334, Final Batch Loss: 0.515\n",
      "Epoch 2124, Loss: 20.410, Final Batch Loss: 0.582\n",
      "Epoch 2125, Loss: 20.548, Final Batch Loss: 0.564\n",
      "Epoch 2126, Loss: 20.469, Final Batch Loss: 0.637\n",
      "Epoch 2127, Loss: 20.225, Final Batch Loss: 0.522\n",
      "Epoch 2128, Loss: 20.242, Final Batch Loss: 0.569\n",
      "Epoch 2129, Loss: 20.548, Final Batch Loss: 0.560\n",
      "Epoch 2130, Loss: 20.492, Final Batch Loss: 0.640\n",
      "Epoch 2131, Loss: 20.295, Final Batch Loss: 0.702\n",
      "Epoch 2132, Loss: 20.552, Final Batch Loss: 0.571\n",
      "Epoch 2133, Loss: 20.430, Final Batch Loss: 0.555\n",
      "Epoch 2134, Loss: 20.316, Final Batch Loss: 0.622\n",
      "Epoch 2135, Loss: 20.307, Final Batch Loss: 0.573\n",
      "Epoch 2136, Loss: 20.382, Final Batch Loss: 0.681\n",
      "Epoch 2137, Loss: 20.474, Final Batch Loss: 0.662\n",
      "Epoch 2138, Loss: 20.381, Final Batch Loss: 0.508\n",
      "Epoch 2139, Loss: 20.450, Final Batch Loss: 0.663\n",
      "Epoch 2140, Loss: 20.281, Final Batch Loss: 0.606\n",
      "Epoch 2141, Loss: 20.357, Final Batch Loss: 0.500\n",
      "Epoch 2142, Loss: 20.325, Final Batch Loss: 0.679\n",
      "Epoch 2143, Loss: 20.323, Final Batch Loss: 0.674\n",
      "Epoch 2144, Loss: 20.413, Final Batch Loss: 0.564\n",
      "Epoch 2145, Loss: 20.172, Final Batch Loss: 0.544\n",
      "Epoch 2146, Loss: 20.200, Final Batch Loss: 0.528\n",
      "Epoch 2147, Loss: 20.623, Final Batch Loss: 0.575\n",
      "Epoch 2148, Loss: 20.436, Final Batch Loss: 0.547\n",
      "Epoch 2149, Loss: 20.419, Final Batch Loss: 0.523\n",
      "Epoch 2150, Loss: 20.442, Final Batch Loss: 0.577\n",
      "Epoch 2151, Loss: 20.284, Final Batch Loss: 0.534\n",
      "Epoch 2152, Loss: 20.389, Final Batch Loss: 0.627\n",
      "Epoch 2153, Loss: 20.449, Final Batch Loss: 0.687\n",
      "Epoch 2154, Loss: 20.343, Final Batch Loss: 0.636\n",
      "Epoch 2155, Loss: 20.180, Final Batch Loss: 0.559\n",
      "Epoch 2156, Loss: 20.509, Final Batch Loss: 0.609\n",
      "Epoch 2157, Loss: 20.355, Final Batch Loss: 0.421\n",
      "Epoch 2158, Loss: 20.356, Final Batch Loss: 0.561\n",
      "Epoch 2159, Loss: 20.381, Final Batch Loss: 0.582\n",
      "Epoch 2160, Loss: 20.361, Final Batch Loss: 0.533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2161, Loss: 20.219, Final Batch Loss: 0.544\n",
      "Epoch 2162, Loss: 20.344, Final Batch Loss: 0.513\n",
      "Epoch 2163, Loss: 20.370, Final Batch Loss: 0.511\n",
      "Epoch 2164, Loss: 20.544, Final Batch Loss: 0.492\n",
      "Epoch 2165, Loss: 20.596, Final Batch Loss: 0.496\n",
      "Epoch 2166, Loss: 20.323, Final Batch Loss: 0.631\n",
      "Epoch 2167, Loss: 20.501, Final Batch Loss: 0.553\n",
      "Epoch 2168, Loss: 20.212, Final Batch Loss: 0.613\n",
      "Epoch 2169, Loss: 20.294, Final Batch Loss: 0.596\n",
      "Epoch 2170, Loss: 20.313, Final Batch Loss: 0.540\n",
      "Epoch 2171, Loss: 20.392, Final Batch Loss: 0.610\n",
      "Epoch 2172, Loss: 20.077, Final Batch Loss: 0.591\n",
      "Epoch 2173, Loss: 20.097, Final Batch Loss: 0.452\n",
      "Epoch 2174, Loss: 20.428, Final Batch Loss: 0.611\n",
      "Epoch 2175, Loss: 20.195, Final Batch Loss: 0.479\n",
      "Epoch 2176, Loss: 20.378, Final Batch Loss: 0.505\n",
      "Epoch 2177, Loss: 20.210, Final Batch Loss: 0.460\n",
      "Epoch 2178, Loss: 20.413, Final Batch Loss: 0.529\n",
      "Epoch 2179, Loss: 20.234, Final Batch Loss: 0.593\n",
      "Epoch 2180, Loss: 20.194, Final Batch Loss: 0.554\n",
      "Epoch 2181, Loss: 20.275, Final Batch Loss: 0.530\n",
      "Epoch 2182, Loss: 20.235, Final Batch Loss: 0.503\n",
      "Epoch 2183, Loss: 20.280, Final Batch Loss: 0.539\n",
      "Epoch 2184, Loss: 20.388, Final Batch Loss: 0.533\n",
      "Epoch 2185, Loss: 20.359, Final Batch Loss: 0.452\n",
      "Epoch 2186, Loss: 20.376, Final Batch Loss: 0.491\n",
      "Epoch 2187, Loss: 20.313, Final Batch Loss: 0.604\n",
      "Epoch 2188, Loss: 20.364, Final Batch Loss: 0.537\n",
      "Epoch 2189, Loss: 20.337, Final Batch Loss: 0.511\n",
      "Epoch 2190, Loss: 20.262, Final Batch Loss: 0.556\n",
      "Epoch 2191, Loss: 20.334, Final Batch Loss: 0.488\n",
      "Epoch 2192, Loss: 20.404, Final Batch Loss: 0.496\n",
      "Epoch 2193, Loss: 20.096, Final Batch Loss: 0.451\n",
      "Epoch 2194, Loss: 20.433, Final Batch Loss: 0.547\n",
      "Epoch 2195, Loss: 20.575, Final Batch Loss: 0.667\n",
      "Epoch 2196, Loss: 20.153, Final Batch Loss: 0.577\n",
      "Epoch 2197, Loss: 20.459, Final Batch Loss: 0.624\n",
      "Epoch 2198, Loss: 20.311, Final Batch Loss: 0.694\n",
      "Epoch 2199, Loss: 20.340, Final Batch Loss: 0.576\n",
      "Epoch 2200, Loss: 20.398, Final Batch Loss: 0.551\n",
      "Epoch 2201, Loss: 20.387, Final Batch Loss: 0.526\n",
      "Epoch 2202, Loss: 20.215, Final Batch Loss: 0.634\n",
      "Epoch 2203, Loss: 20.474, Final Batch Loss: 0.689\n",
      "Epoch 2204, Loss: 20.385, Final Batch Loss: 0.547\n",
      "Epoch 2205, Loss: 20.583, Final Batch Loss: 0.566\n",
      "Epoch 2206, Loss: 20.223, Final Batch Loss: 0.437\n",
      "Epoch 2207, Loss: 20.205, Final Batch Loss: 0.543\n",
      "Epoch 2208, Loss: 20.050, Final Batch Loss: 0.504\n",
      "Epoch 2209, Loss: 20.364, Final Batch Loss: 0.609\n",
      "Epoch 2210, Loss: 20.242, Final Batch Loss: 0.615\n",
      "Epoch 2211, Loss: 20.411, Final Batch Loss: 0.656\n",
      "Epoch 2212, Loss: 20.343, Final Batch Loss: 0.481\n",
      "Epoch 2213, Loss: 20.286, Final Batch Loss: 0.607\n",
      "Epoch 2214, Loss: 20.496, Final Batch Loss: 0.490\n",
      "Epoch 2215, Loss: 20.294, Final Batch Loss: 0.575\n",
      "Epoch 2216, Loss: 20.288, Final Batch Loss: 0.554\n",
      "Epoch 2217, Loss: 20.289, Final Batch Loss: 0.531\n",
      "Epoch 2218, Loss: 20.218, Final Batch Loss: 0.534\n",
      "Epoch 2219, Loss: 20.361, Final Batch Loss: 0.575\n",
      "Epoch 2220, Loss: 20.443, Final Batch Loss: 0.522\n",
      "Epoch 2221, Loss: 20.161, Final Batch Loss: 0.714\n",
      "Epoch 2222, Loss: 20.327, Final Batch Loss: 0.525\n",
      "Epoch 2223, Loss: 20.395, Final Batch Loss: 0.614\n",
      "Epoch 2224, Loss: 20.354, Final Batch Loss: 0.511\n",
      "Epoch 2225, Loss: 20.362, Final Batch Loss: 0.542\n",
      "Epoch 2226, Loss: 20.568, Final Batch Loss: 0.537\n",
      "Epoch 2227, Loss: 20.284, Final Batch Loss: 0.581\n",
      "Epoch 2228, Loss: 20.322, Final Batch Loss: 0.493\n",
      "Epoch 2229, Loss: 20.516, Final Batch Loss: 0.613\n",
      "Epoch 2230, Loss: 20.323, Final Batch Loss: 0.577\n",
      "Epoch 2231, Loss: 20.478, Final Batch Loss: 0.621\n",
      "Epoch 2232, Loss: 20.508, Final Batch Loss: 0.512\n",
      "Epoch 2233, Loss: 20.375, Final Batch Loss: 0.591\n",
      "Epoch 2234, Loss: 20.248, Final Batch Loss: 0.613\n",
      "Epoch 2235, Loss: 20.414, Final Batch Loss: 0.478\n",
      "Epoch 2236, Loss: 20.253, Final Batch Loss: 0.530\n",
      "Epoch 2237, Loss: 20.198, Final Batch Loss: 0.508\n",
      "Epoch 2238, Loss: 20.178, Final Batch Loss: 0.542\n",
      "Epoch 2239, Loss: 20.214, Final Batch Loss: 0.607\n",
      "Epoch 2240, Loss: 20.197, Final Batch Loss: 0.507\n",
      "Epoch 2241, Loss: 20.411, Final Batch Loss: 0.490\n",
      "Epoch 2242, Loss: 20.502, Final Batch Loss: 0.580\n",
      "Epoch 2243, Loss: 20.401, Final Batch Loss: 0.529\n",
      "Epoch 2244, Loss: 20.351, Final Batch Loss: 0.582\n",
      "Epoch 2245, Loss: 20.289, Final Batch Loss: 0.575\n",
      "Epoch 2246, Loss: 20.430, Final Batch Loss: 0.524\n",
      "Epoch 2247, Loss: 20.378, Final Batch Loss: 0.531\n",
      "Epoch 2248, Loss: 20.349, Final Batch Loss: 0.489\n",
      "Epoch 2249, Loss: 20.196, Final Batch Loss: 0.496\n",
      "Epoch 2250, Loss: 20.282, Final Batch Loss: 0.499\n",
      "Epoch 2251, Loss: 20.261, Final Batch Loss: 0.506\n",
      "Epoch 2252, Loss: 20.305, Final Batch Loss: 0.579\n",
      "Epoch 2253, Loss: 20.431, Final Batch Loss: 0.518\n",
      "Epoch 2254, Loss: 20.118, Final Batch Loss: 0.480\n",
      "Epoch 2255, Loss: 20.269, Final Batch Loss: 0.579\n",
      "Epoch 2256, Loss: 20.244, Final Batch Loss: 0.506\n",
      "Epoch 2257, Loss: 20.239, Final Batch Loss: 0.461\n",
      "Epoch 2258, Loss: 20.489, Final Batch Loss: 0.669\n",
      "Epoch 2259, Loss: 20.595, Final Batch Loss: 0.556\n",
      "Epoch 2260, Loss: 20.452, Final Batch Loss: 0.588\n",
      "Epoch 2261, Loss: 20.312, Final Batch Loss: 0.562\n",
      "Epoch 2262, Loss: 20.406, Final Batch Loss: 0.557\n",
      "Epoch 2263, Loss: 20.319, Final Batch Loss: 0.555\n",
      "Epoch 2264, Loss: 20.433, Final Batch Loss: 0.508\n",
      "Epoch 2265, Loss: 20.326, Final Batch Loss: 0.474\n",
      "Epoch 2266, Loss: 20.476, Final Batch Loss: 0.475\n",
      "Epoch 2267, Loss: 20.379, Final Batch Loss: 0.645\n",
      "Epoch 2268, Loss: 20.248, Final Batch Loss: 0.570\n",
      "Epoch 2269, Loss: 20.358, Final Batch Loss: 0.568\n",
      "Epoch 2270, Loss: 20.415, Final Batch Loss: 0.581\n",
      "Epoch 2271, Loss: 20.301, Final Batch Loss: 0.504\n",
      "Epoch 2272, Loss: 20.309, Final Batch Loss: 0.553\n",
      "Epoch 2273, Loss: 20.349, Final Batch Loss: 0.602\n",
      "Epoch 2274, Loss: 20.293, Final Batch Loss: 0.628\n",
      "Epoch 2275, Loss: 20.346, Final Batch Loss: 0.601\n",
      "Epoch 2276, Loss: 20.325, Final Batch Loss: 0.544\n",
      "Epoch 2277, Loss: 20.236, Final Batch Loss: 0.445\n",
      "Epoch 2278, Loss: 20.215, Final Batch Loss: 0.562\n",
      "Epoch 2279, Loss: 20.610, Final Batch Loss: 0.746\n",
      "Epoch 2280, Loss: 20.062, Final Batch Loss: 0.541\n",
      "Epoch 2281, Loss: 20.157, Final Batch Loss: 0.577\n",
      "Epoch 2282, Loss: 20.340, Final Batch Loss: 0.522\n",
      "Epoch 2283, Loss: 20.162, Final Batch Loss: 0.451\n",
      "Epoch 2284, Loss: 20.190, Final Batch Loss: 0.555\n",
      "Epoch 2285, Loss: 20.217, Final Batch Loss: 0.528\n",
      "Epoch 2286, Loss: 20.186, Final Batch Loss: 0.590\n",
      "Epoch 2287, Loss: 20.547, Final Batch Loss: 0.611\n",
      "Epoch 2288, Loss: 20.361, Final Batch Loss: 0.568\n",
      "Epoch 2289, Loss: 20.244, Final Batch Loss: 0.440\n",
      "Epoch 2290, Loss: 20.349, Final Batch Loss: 0.607\n",
      "Epoch 2291, Loss: 20.016, Final Batch Loss: 0.613\n",
      "Epoch 2292, Loss: 20.273, Final Batch Loss: 0.616\n",
      "Epoch 2293, Loss: 20.410, Final Batch Loss: 0.717\n",
      "Epoch 2294, Loss: 20.219, Final Batch Loss: 0.569\n",
      "Epoch 2295, Loss: 20.451, Final Batch Loss: 0.669\n",
      "Epoch 2296, Loss: 20.254, Final Batch Loss: 0.615\n",
      "Epoch 2297, Loss: 20.344, Final Batch Loss: 0.678\n",
      "Epoch 2298, Loss: 20.515, Final Batch Loss: 0.658\n",
      "Epoch 2299, Loss: 20.436, Final Batch Loss: 0.561\n",
      "Epoch 2300, Loss: 20.063, Final Batch Loss: 0.575\n",
      "Epoch 2301, Loss: 20.161, Final Batch Loss: 0.618\n",
      "Epoch 2302, Loss: 20.360, Final Batch Loss: 0.605\n",
      "Epoch 2303, Loss: 20.106, Final Batch Loss: 0.543\n",
      "Epoch 2304, Loss: 20.368, Final Batch Loss: 0.561\n",
      "Epoch 2305, Loss: 20.276, Final Batch Loss: 0.627\n",
      "Epoch 2306, Loss: 20.278, Final Batch Loss: 0.476\n",
      "Epoch 2307, Loss: 20.377, Final Batch Loss: 0.487\n",
      "Epoch 2308, Loss: 20.224, Final Batch Loss: 0.611\n",
      "Epoch 2309, Loss: 20.372, Final Batch Loss: 0.638\n",
      "Epoch 2310, Loss: 19.968, Final Batch Loss: 0.474\n",
      "Epoch 2311, Loss: 20.261, Final Batch Loss: 0.462\n",
      "Epoch 2312, Loss: 20.211, Final Batch Loss: 0.519\n",
      "Epoch 2313, Loss: 20.186, Final Batch Loss: 0.550\n",
      "Epoch 2314, Loss: 19.996, Final Batch Loss: 0.549\n",
      "Epoch 2315, Loss: 20.335, Final Batch Loss: 0.475\n",
      "Epoch 2316, Loss: 20.151, Final Batch Loss: 0.608\n",
      "Epoch 2317, Loss: 20.237, Final Batch Loss: 0.450\n",
      "Epoch 2318, Loss: 20.116, Final Batch Loss: 0.470\n",
      "Epoch 2319, Loss: 20.374, Final Batch Loss: 0.469\n",
      "Epoch 2320, Loss: 20.382, Final Batch Loss: 0.597\n",
      "Epoch 2321, Loss: 20.408, Final Batch Loss: 0.542\n",
      "Epoch 2322, Loss: 20.164, Final Batch Loss: 0.510\n",
      "Epoch 2323, Loss: 20.064, Final Batch Loss: 0.559\n",
      "Epoch 2324, Loss: 20.327, Final Batch Loss: 0.608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2325, Loss: 20.214, Final Batch Loss: 0.581\n",
      "Epoch 2326, Loss: 20.261, Final Batch Loss: 0.532\n",
      "Epoch 2327, Loss: 20.132, Final Batch Loss: 0.554\n",
      "Epoch 2328, Loss: 20.196, Final Batch Loss: 0.669\n",
      "Epoch 2329, Loss: 20.264, Final Batch Loss: 0.694\n",
      "Epoch 2330, Loss: 20.198, Final Batch Loss: 0.526\n",
      "Epoch 2331, Loss: 20.115, Final Batch Loss: 0.470\n",
      "Epoch 2332, Loss: 20.048, Final Batch Loss: 0.526\n",
      "Epoch 2333, Loss: 20.171, Final Batch Loss: 0.638\n",
      "Epoch 2334, Loss: 20.052, Final Batch Loss: 0.452\n",
      "Epoch 2335, Loss: 20.411, Final Batch Loss: 0.694\n",
      "Epoch 2336, Loss: 20.345, Final Batch Loss: 0.602\n",
      "Epoch 2337, Loss: 20.270, Final Batch Loss: 0.592\n",
      "Epoch 2338, Loss: 20.193, Final Batch Loss: 0.511\n",
      "Epoch 2339, Loss: 20.418, Final Batch Loss: 0.692\n",
      "Epoch 2340, Loss: 20.108, Final Batch Loss: 0.659\n",
      "Epoch 2341, Loss: 20.230, Final Batch Loss: 0.500\n",
      "Epoch 2342, Loss: 20.300, Final Batch Loss: 0.517\n",
      "Epoch 2343, Loss: 20.051, Final Batch Loss: 0.549\n",
      "Epoch 2344, Loss: 20.269, Final Batch Loss: 0.505\n",
      "Epoch 2345, Loss: 20.089, Final Batch Loss: 0.541\n",
      "Epoch 2346, Loss: 20.235, Final Batch Loss: 0.640\n",
      "Epoch 2347, Loss: 20.192, Final Batch Loss: 0.517\n",
      "Epoch 2348, Loss: 20.171, Final Batch Loss: 0.529\n",
      "Epoch 2349, Loss: 20.283, Final Batch Loss: 0.526\n",
      "Epoch 2350, Loss: 20.250, Final Batch Loss: 0.548\n",
      "Epoch 2351, Loss: 20.192, Final Batch Loss: 0.572\n",
      "Epoch 2352, Loss: 20.260, Final Batch Loss: 0.597\n",
      "Epoch 2353, Loss: 20.231, Final Batch Loss: 0.649\n",
      "Epoch 2354, Loss: 20.194, Final Batch Loss: 0.499\n",
      "Epoch 2355, Loss: 20.269, Final Batch Loss: 0.592\n",
      "Epoch 2356, Loss: 20.291, Final Batch Loss: 0.477\n",
      "Epoch 2357, Loss: 20.027, Final Batch Loss: 0.516\n",
      "Epoch 2358, Loss: 20.201, Final Batch Loss: 0.601\n",
      "Epoch 2359, Loss: 20.250, Final Batch Loss: 0.559\n",
      "Epoch 2360, Loss: 20.225, Final Batch Loss: 0.738\n",
      "Epoch 2361, Loss: 20.101, Final Batch Loss: 0.458\n",
      "Epoch 2362, Loss: 20.275, Final Batch Loss: 0.615\n",
      "Epoch 2363, Loss: 20.416, Final Batch Loss: 0.555\n",
      "Epoch 2364, Loss: 20.342, Final Batch Loss: 0.534\n",
      "Epoch 2365, Loss: 20.068, Final Batch Loss: 0.624\n",
      "Epoch 2366, Loss: 20.292, Final Batch Loss: 0.485\n",
      "Epoch 2367, Loss: 20.091, Final Batch Loss: 0.512\n",
      "Epoch 2368, Loss: 20.219, Final Batch Loss: 0.730\n",
      "Epoch 2369, Loss: 20.133, Final Batch Loss: 0.564\n",
      "Epoch 2370, Loss: 20.307, Final Batch Loss: 0.649\n",
      "Epoch 2371, Loss: 20.399, Final Batch Loss: 0.638\n",
      "Epoch 2372, Loss: 20.196, Final Batch Loss: 0.523\n",
      "Epoch 2373, Loss: 20.139, Final Batch Loss: 0.554\n",
      "Epoch 2374, Loss: 20.172, Final Batch Loss: 0.644\n",
      "Epoch 2375, Loss: 20.404, Final Batch Loss: 0.562\n",
      "Epoch 2376, Loss: 20.189, Final Batch Loss: 0.521\n",
      "Epoch 2377, Loss: 20.041, Final Batch Loss: 0.520\n",
      "Epoch 2378, Loss: 20.357, Final Batch Loss: 0.501\n",
      "Epoch 2379, Loss: 20.193, Final Batch Loss: 0.563\n",
      "Epoch 2380, Loss: 20.070, Final Batch Loss: 0.545\n",
      "Epoch 2381, Loss: 20.434, Final Batch Loss: 0.574\n",
      "Epoch 2382, Loss: 20.276, Final Batch Loss: 0.566\n",
      "Epoch 2383, Loss: 20.120, Final Batch Loss: 0.630\n",
      "Epoch 2384, Loss: 20.239, Final Batch Loss: 0.627\n",
      "Epoch 2385, Loss: 20.001, Final Batch Loss: 0.522\n",
      "Epoch 2386, Loss: 20.198, Final Batch Loss: 0.608\n",
      "Epoch 2387, Loss: 20.004, Final Batch Loss: 0.532\n",
      "Epoch 2388, Loss: 20.060, Final Batch Loss: 0.657\n",
      "Epoch 2389, Loss: 20.210, Final Batch Loss: 0.544\n",
      "Epoch 2390, Loss: 20.029, Final Batch Loss: 0.586\n",
      "Epoch 2391, Loss: 20.166, Final Batch Loss: 0.624\n",
      "Epoch 2392, Loss: 20.141, Final Batch Loss: 0.613\n",
      "Epoch 2393, Loss: 20.085, Final Batch Loss: 0.588\n",
      "Epoch 2394, Loss: 20.145, Final Batch Loss: 0.518\n",
      "Epoch 2395, Loss: 20.119, Final Batch Loss: 0.606\n",
      "Epoch 2396, Loss: 20.190, Final Batch Loss: 0.595\n",
      "Epoch 2397, Loss: 20.303, Final Batch Loss: 0.561\n",
      "Epoch 2398, Loss: 20.207, Final Batch Loss: 0.395\n",
      "Epoch 2399, Loss: 20.164, Final Batch Loss: 0.512\n",
      "Epoch 2400, Loss: 20.216, Final Batch Loss: 0.503\n",
      "Epoch 2401, Loss: 20.002, Final Batch Loss: 0.612\n",
      "Epoch 2402, Loss: 20.167, Final Batch Loss: 0.519\n",
      "Epoch 2403, Loss: 20.322, Final Batch Loss: 0.560\n",
      "Epoch 2404, Loss: 20.106, Final Batch Loss: 0.570\n",
      "Epoch 2405, Loss: 20.294, Final Batch Loss: 0.579\n",
      "Epoch 2406, Loss: 20.182, Final Batch Loss: 0.558\n",
      "Epoch 2407, Loss: 20.085, Final Batch Loss: 0.635\n",
      "Epoch 2408, Loss: 20.312, Final Batch Loss: 0.586\n",
      "Epoch 2409, Loss: 20.109, Final Batch Loss: 0.582\n",
      "Epoch 2410, Loss: 20.270, Final Batch Loss: 0.508\n",
      "Epoch 2411, Loss: 20.038, Final Batch Loss: 0.617\n",
      "Epoch 2412, Loss: 20.195, Final Batch Loss: 0.612\n",
      "Epoch 2413, Loss: 20.075, Final Batch Loss: 0.522\n",
      "Epoch 2414, Loss: 20.044, Final Batch Loss: 0.662\n",
      "Epoch 2415, Loss: 20.384, Final Batch Loss: 0.624\n",
      "Epoch 2416, Loss: 20.134, Final Batch Loss: 0.546\n",
      "Epoch 2417, Loss: 20.407, Final Batch Loss: 0.639\n",
      "Epoch 2418, Loss: 20.366, Final Batch Loss: 0.470\n",
      "Epoch 2419, Loss: 20.223, Final Batch Loss: 0.564\n",
      "Epoch 2420, Loss: 20.164, Final Batch Loss: 0.605\n",
      "Epoch 2421, Loss: 20.109, Final Batch Loss: 0.668\n",
      "Epoch 2422, Loss: 19.968, Final Batch Loss: 0.656\n",
      "Epoch 2423, Loss: 20.077, Final Batch Loss: 0.512\n",
      "Epoch 2424, Loss: 20.312, Final Batch Loss: 0.549\n",
      "Epoch 2425, Loss: 20.327, Final Batch Loss: 0.535\n",
      "Epoch 2426, Loss: 20.181, Final Batch Loss: 0.524\n",
      "Epoch 2427, Loss: 20.035, Final Batch Loss: 0.562\n",
      "Epoch 2428, Loss: 20.260, Final Batch Loss: 0.528\n",
      "Epoch 2429, Loss: 20.069, Final Batch Loss: 0.575\n",
      "Epoch 2430, Loss: 20.214, Final Batch Loss: 0.642\n",
      "Epoch 2431, Loss: 20.198, Final Batch Loss: 0.610\n",
      "Epoch 2432, Loss: 20.164, Final Batch Loss: 0.578\n",
      "Epoch 2433, Loss: 20.293, Final Batch Loss: 0.582\n",
      "Epoch 2434, Loss: 20.081, Final Batch Loss: 0.496\n",
      "Epoch 2435, Loss: 19.989, Final Batch Loss: 0.469\n",
      "Epoch 2436, Loss: 20.119, Final Batch Loss: 0.600\n",
      "Epoch 2437, Loss: 20.173, Final Batch Loss: 0.557\n",
      "Epoch 2438, Loss: 20.208, Final Batch Loss: 0.532\n",
      "Epoch 2439, Loss: 20.486, Final Batch Loss: 0.675\n",
      "Epoch 2440, Loss: 19.959, Final Batch Loss: 0.614\n",
      "Epoch 2441, Loss: 20.237, Final Batch Loss: 0.674\n",
      "Epoch 2442, Loss: 20.338, Final Batch Loss: 0.588\n",
      "Epoch 2443, Loss: 20.105, Final Batch Loss: 0.640\n",
      "Epoch 2444, Loss: 20.138, Final Batch Loss: 0.572\n",
      "Epoch 2445, Loss: 20.119, Final Batch Loss: 0.565\n",
      "Epoch 2446, Loss: 20.113, Final Batch Loss: 0.588\n",
      "Epoch 2447, Loss: 20.327, Final Batch Loss: 0.505\n",
      "Epoch 2448, Loss: 20.209, Final Batch Loss: 0.512\n",
      "Epoch 2449, Loss: 20.135, Final Batch Loss: 0.562\n",
      "Epoch 2450, Loss: 20.272, Final Batch Loss: 0.706\n",
      "Epoch 2451, Loss: 20.265, Final Batch Loss: 0.738\n",
      "Epoch 2452, Loss: 20.144, Final Batch Loss: 0.534\n",
      "Epoch 2453, Loss: 20.294, Final Batch Loss: 0.740\n",
      "Epoch 2454, Loss: 19.943, Final Batch Loss: 0.566\n",
      "Epoch 2455, Loss: 20.160, Final Batch Loss: 0.532\n",
      "Epoch 2456, Loss: 20.333, Final Batch Loss: 0.631\n",
      "Epoch 2457, Loss: 19.963, Final Batch Loss: 0.559\n",
      "Epoch 2458, Loss: 20.047, Final Batch Loss: 0.602\n",
      "Epoch 2459, Loss: 20.189, Final Batch Loss: 0.471\n",
      "Epoch 2460, Loss: 20.349, Final Batch Loss: 0.567\n",
      "Epoch 2461, Loss: 20.010, Final Batch Loss: 0.572\n",
      "Epoch 2462, Loss: 20.137, Final Batch Loss: 0.592\n",
      "Epoch 2463, Loss: 20.113, Final Batch Loss: 0.563\n",
      "Epoch 2464, Loss: 19.906, Final Batch Loss: 0.498\n",
      "Epoch 2465, Loss: 20.211, Final Batch Loss: 0.606\n",
      "Epoch 2466, Loss: 20.012, Final Batch Loss: 0.602\n",
      "Epoch 2467, Loss: 20.108, Final Batch Loss: 0.629\n",
      "Epoch 2468, Loss: 20.153, Final Batch Loss: 0.603\n",
      "Epoch 2469, Loss: 19.948, Final Batch Loss: 0.594\n",
      "Epoch 2470, Loss: 20.050, Final Batch Loss: 0.582\n",
      "Epoch 2471, Loss: 19.930, Final Batch Loss: 0.437\n",
      "Epoch 2472, Loss: 20.013, Final Batch Loss: 0.585\n",
      "Epoch 2473, Loss: 20.100, Final Batch Loss: 0.654\n",
      "Epoch 2474, Loss: 20.122, Final Batch Loss: 0.555\n",
      "Epoch 2475, Loss: 19.888, Final Batch Loss: 0.489\n",
      "Epoch 2476, Loss: 20.046, Final Batch Loss: 0.493\n",
      "Epoch 2477, Loss: 19.946, Final Batch Loss: 0.596\n",
      "Epoch 2478, Loss: 20.174, Final Batch Loss: 0.607\n",
      "Epoch 2479, Loss: 20.233, Final Batch Loss: 0.520\n",
      "Epoch 2480, Loss: 20.231, Final Batch Loss: 0.625\n",
      "Epoch 2481, Loss: 20.074, Final Batch Loss: 0.579\n",
      "Epoch 2482, Loss: 20.237, Final Batch Loss: 0.665\n",
      "Epoch 2483, Loss: 19.797, Final Batch Loss: 0.652\n",
      "Epoch 2484, Loss: 20.052, Final Batch Loss: 0.530\n",
      "Epoch 2485, Loss: 19.805, Final Batch Loss: 0.580\n",
      "Epoch 2486, Loss: 20.242, Final Batch Loss: 0.671\n",
      "Epoch 2487, Loss: 20.306, Final Batch Loss: 0.586\n",
      "Epoch 2488, Loss: 20.262, Final Batch Loss: 0.579\n",
      "Epoch 2489, Loss: 20.057, Final Batch Loss: 0.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2490, Loss: 20.117, Final Batch Loss: 0.400\n",
      "Epoch 2491, Loss: 20.203, Final Batch Loss: 0.573\n",
      "Epoch 2492, Loss: 20.294, Final Batch Loss: 0.624\n",
      "Epoch 2493, Loss: 20.178, Final Batch Loss: 0.501\n",
      "Epoch 2494, Loss: 20.162, Final Batch Loss: 0.648\n",
      "Epoch 2495, Loss: 20.160, Final Batch Loss: 0.573\n",
      "Epoch 2496, Loss: 20.081, Final Batch Loss: 0.622\n",
      "Epoch 2497, Loss: 20.128, Final Batch Loss: 0.655\n",
      "Epoch 2498, Loss: 20.144, Final Batch Loss: 0.559\n",
      "Epoch 2499, Loss: 20.087, Final Batch Loss: 0.504\n",
      "Epoch 2500, Loss: 20.247, Final Batch Loss: 0.516\n",
      "Epoch 2501, Loss: 19.940, Final Batch Loss: 0.524\n",
      "Epoch 2502, Loss: 20.039, Final Batch Loss: 0.496\n",
      "Epoch 2503, Loss: 20.115, Final Batch Loss: 0.523\n",
      "Epoch 2504, Loss: 20.248, Final Batch Loss: 0.629\n",
      "Epoch 2505, Loss: 20.058, Final Batch Loss: 0.560\n",
      "Epoch 2506, Loss: 20.117, Final Batch Loss: 0.512\n",
      "Epoch 2507, Loss: 20.207, Final Batch Loss: 0.581\n",
      "Epoch 2508, Loss: 20.092, Final Batch Loss: 0.515\n",
      "Epoch 2509, Loss: 19.913, Final Batch Loss: 0.556\n",
      "Epoch 2510, Loss: 20.059, Final Batch Loss: 0.459\n",
      "Epoch 2511, Loss: 19.959, Final Batch Loss: 0.479\n",
      "Epoch 2512, Loss: 20.114, Final Batch Loss: 0.487\n",
      "Epoch 2513, Loss: 20.142, Final Batch Loss: 0.417\n",
      "Epoch 2514, Loss: 20.276, Final Batch Loss: 0.636\n",
      "Epoch 2515, Loss: 20.195, Final Batch Loss: 0.581\n",
      "Epoch 2516, Loss: 20.319, Final Batch Loss: 0.607\n",
      "Epoch 2517, Loss: 20.143, Final Batch Loss: 0.525\n",
      "Epoch 2518, Loss: 20.046, Final Batch Loss: 0.569\n",
      "Epoch 2519, Loss: 20.282, Final Batch Loss: 0.570\n",
      "Epoch 2520, Loss: 20.162, Final Batch Loss: 0.537\n",
      "Epoch 2521, Loss: 20.188, Final Batch Loss: 0.585\n",
      "Epoch 2522, Loss: 20.520, Final Batch Loss: 0.589\n",
      "Epoch 2523, Loss: 20.182, Final Batch Loss: 0.604\n",
      "Epoch 2524, Loss: 20.375, Final Batch Loss: 0.522\n",
      "Epoch 2525, Loss: 20.185, Final Batch Loss: 0.580\n",
      "Epoch 2526, Loss: 20.118, Final Batch Loss: 0.534\n",
      "Epoch 2527, Loss: 20.114, Final Batch Loss: 0.663\n",
      "Epoch 2528, Loss: 20.231, Final Batch Loss: 0.609\n",
      "Epoch 2529, Loss: 19.975, Final Batch Loss: 0.539\n",
      "Epoch 2530, Loss: 19.924, Final Batch Loss: 0.527\n",
      "Epoch 2531, Loss: 19.987, Final Batch Loss: 0.525\n",
      "Epoch 2532, Loss: 20.042, Final Batch Loss: 0.584\n",
      "Epoch 2533, Loss: 19.874, Final Batch Loss: 0.570\n",
      "Epoch 2534, Loss: 19.950, Final Batch Loss: 0.546\n",
      "Epoch 2535, Loss: 20.160, Final Batch Loss: 0.551\n",
      "Epoch 2536, Loss: 19.866, Final Batch Loss: 0.602\n",
      "Epoch 2537, Loss: 19.947, Final Batch Loss: 0.530\n",
      "Epoch 2538, Loss: 20.032, Final Batch Loss: 0.577\n",
      "Epoch 2539, Loss: 19.868, Final Batch Loss: 0.570\n",
      "Epoch 2540, Loss: 19.796, Final Batch Loss: 0.606\n",
      "Epoch 2541, Loss: 19.919, Final Batch Loss: 0.442\n",
      "Epoch 2542, Loss: 20.215, Final Batch Loss: 0.532\n",
      "Epoch 2543, Loss: 20.189, Final Batch Loss: 0.598\n",
      "Epoch 2544, Loss: 20.021, Final Batch Loss: 0.483\n",
      "Epoch 2545, Loss: 20.065, Final Batch Loss: 0.516\n",
      "Epoch 2546, Loss: 20.159, Final Batch Loss: 0.601\n",
      "Epoch 2547, Loss: 20.145, Final Batch Loss: 0.574\n",
      "Epoch 2548, Loss: 20.131, Final Batch Loss: 0.517\n",
      "Epoch 2549, Loss: 20.048, Final Batch Loss: 0.529\n",
      "Epoch 2550, Loss: 19.991, Final Batch Loss: 0.616\n",
      "Epoch 2551, Loss: 19.947, Final Batch Loss: 0.492\n",
      "Epoch 2552, Loss: 20.097, Final Batch Loss: 0.539\n",
      "Epoch 2553, Loss: 20.208, Final Batch Loss: 0.543\n",
      "Epoch 2554, Loss: 20.177, Final Batch Loss: 0.551\n",
      "Epoch 2555, Loss: 20.317, Final Batch Loss: 0.612\n",
      "Epoch 2556, Loss: 20.238, Final Batch Loss: 0.513\n",
      "Epoch 2557, Loss: 20.065, Final Batch Loss: 0.569\n",
      "Epoch 2558, Loss: 19.760, Final Batch Loss: 0.425\n",
      "Epoch 2559, Loss: 20.166, Final Batch Loss: 0.544\n",
      "Epoch 2560, Loss: 19.935, Final Batch Loss: 0.533\n",
      "Epoch 2561, Loss: 20.000, Final Batch Loss: 0.510\n",
      "Epoch 2562, Loss: 20.223, Final Batch Loss: 0.529\n",
      "Epoch 2563, Loss: 19.966, Final Batch Loss: 0.569\n",
      "Epoch 2564, Loss: 19.985, Final Batch Loss: 0.591\n",
      "Epoch 2565, Loss: 20.296, Final Batch Loss: 0.572\n",
      "Epoch 2566, Loss: 20.170, Final Batch Loss: 0.516\n",
      "Epoch 2567, Loss: 19.968, Final Batch Loss: 0.497\n",
      "Epoch 2568, Loss: 20.187, Final Batch Loss: 0.542\n",
      "Epoch 2569, Loss: 19.930, Final Batch Loss: 0.564\n",
      "Epoch 2570, Loss: 19.962, Final Batch Loss: 0.524\n",
      "Epoch 2571, Loss: 19.981, Final Batch Loss: 0.582\n",
      "Epoch 2572, Loss: 20.045, Final Batch Loss: 0.548\n",
      "Epoch 2573, Loss: 20.258, Final Batch Loss: 0.616\n",
      "Epoch 2574, Loss: 20.066, Final Batch Loss: 0.539\n",
      "Epoch 2575, Loss: 20.049, Final Batch Loss: 0.531\n",
      "Epoch 2576, Loss: 20.156, Final Batch Loss: 0.532\n",
      "Epoch 2577, Loss: 20.187, Final Batch Loss: 0.503\n",
      "Epoch 2578, Loss: 20.037, Final Batch Loss: 0.488\n",
      "Epoch 2579, Loss: 19.909, Final Batch Loss: 0.591\n",
      "Epoch 2580, Loss: 20.279, Final Batch Loss: 0.758\n",
      "Epoch 2581, Loss: 19.979, Final Batch Loss: 0.550\n",
      "Epoch 2582, Loss: 20.103, Final Batch Loss: 0.572\n",
      "Epoch 2583, Loss: 20.302, Final Batch Loss: 0.678\n",
      "Epoch 2584, Loss: 20.049, Final Batch Loss: 0.614\n",
      "Epoch 2585, Loss: 20.032, Final Batch Loss: 0.564\n",
      "Epoch 2586, Loss: 20.034, Final Batch Loss: 0.604\n",
      "Epoch 2587, Loss: 20.007, Final Batch Loss: 0.518\n",
      "Epoch 2588, Loss: 19.953, Final Batch Loss: 0.620\n",
      "Epoch 2589, Loss: 19.816, Final Batch Loss: 0.536\n",
      "Epoch 2590, Loss: 20.352, Final Batch Loss: 0.621\n",
      "Epoch 2591, Loss: 20.076, Final Batch Loss: 0.500\n",
      "Epoch 2592, Loss: 19.837, Final Batch Loss: 0.602\n",
      "Epoch 2593, Loss: 20.001, Final Batch Loss: 0.584\n",
      "Epoch 2594, Loss: 20.104, Final Batch Loss: 0.589\n",
      "Epoch 2595, Loss: 20.300, Final Batch Loss: 0.563\n",
      "Epoch 2596, Loss: 20.018, Final Batch Loss: 0.512\n",
      "Epoch 2597, Loss: 20.059, Final Batch Loss: 0.552\n",
      "Epoch 2598, Loss: 20.110, Final Batch Loss: 0.598\n",
      "Epoch 2599, Loss: 20.037, Final Batch Loss: 0.606\n",
      "Epoch 2600, Loss: 20.082, Final Batch Loss: 0.478\n",
      "Epoch 2601, Loss: 20.085, Final Batch Loss: 0.526\n",
      "Epoch 2602, Loss: 20.124, Final Batch Loss: 0.487\n",
      "Epoch 2603, Loss: 20.060, Final Batch Loss: 0.565\n",
      "Epoch 2604, Loss: 20.131, Final Batch Loss: 0.609\n",
      "Epoch 2605, Loss: 19.854, Final Batch Loss: 0.534\n",
      "Epoch 2606, Loss: 19.993, Final Batch Loss: 0.594\n",
      "Epoch 2607, Loss: 20.075, Final Batch Loss: 0.569\n",
      "Epoch 2608, Loss: 19.899, Final Batch Loss: 0.527\n",
      "Epoch 2609, Loss: 19.828, Final Batch Loss: 0.604\n",
      "Epoch 2610, Loss: 20.107, Final Batch Loss: 0.525\n",
      "Epoch 2611, Loss: 19.930, Final Batch Loss: 0.630\n",
      "Epoch 2612, Loss: 20.238, Final Batch Loss: 0.700\n",
      "Epoch 2613, Loss: 20.069, Final Batch Loss: 0.619\n",
      "Epoch 2614, Loss: 20.131, Final Batch Loss: 0.657\n",
      "Epoch 2615, Loss: 19.864, Final Batch Loss: 0.612\n",
      "Epoch 2616, Loss: 20.085, Final Batch Loss: 0.563\n",
      "Epoch 2617, Loss: 19.893, Final Batch Loss: 0.615\n",
      "Epoch 2618, Loss: 20.282, Final Batch Loss: 0.596\n",
      "Epoch 2619, Loss: 19.862, Final Batch Loss: 0.631\n",
      "Epoch 2620, Loss: 20.210, Final Batch Loss: 0.546\n",
      "Epoch 2621, Loss: 19.911, Final Batch Loss: 0.489\n",
      "Epoch 2622, Loss: 19.939, Final Batch Loss: 0.533\n",
      "Epoch 2623, Loss: 20.095, Final Batch Loss: 0.501\n",
      "Epoch 2624, Loss: 20.296, Final Batch Loss: 0.618\n",
      "Epoch 2625, Loss: 20.116, Final Batch Loss: 0.653\n",
      "Epoch 2626, Loss: 20.154, Final Batch Loss: 0.547\n",
      "Epoch 2627, Loss: 20.208, Final Batch Loss: 0.545\n",
      "Epoch 2628, Loss: 20.057, Final Batch Loss: 0.646\n",
      "Epoch 2629, Loss: 20.142, Final Batch Loss: 0.475\n",
      "Epoch 2630, Loss: 19.877, Final Batch Loss: 0.613\n",
      "Epoch 2631, Loss: 20.072, Final Batch Loss: 0.637\n",
      "Epoch 2632, Loss: 20.116, Final Batch Loss: 0.665\n",
      "Epoch 2633, Loss: 20.101, Final Batch Loss: 0.553\n",
      "Epoch 2634, Loss: 20.225, Final Batch Loss: 0.612\n",
      "Epoch 2635, Loss: 19.873, Final Batch Loss: 0.627\n",
      "Epoch 2636, Loss: 20.176, Final Batch Loss: 0.591\n",
      "Epoch 2637, Loss: 20.141, Final Batch Loss: 0.499\n",
      "Epoch 2638, Loss: 19.928, Final Batch Loss: 0.543\n",
      "Epoch 2639, Loss: 20.193, Final Batch Loss: 0.668\n",
      "Epoch 2640, Loss: 19.741, Final Batch Loss: 0.506\n",
      "Epoch 2641, Loss: 19.669, Final Batch Loss: 0.453\n",
      "Epoch 2642, Loss: 19.893, Final Batch Loss: 0.505\n",
      "Epoch 2643, Loss: 19.895, Final Batch Loss: 0.470\n",
      "Epoch 2644, Loss: 20.205, Final Batch Loss: 0.536\n",
      "Epoch 2645, Loss: 19.946, Final Batch Loss: 0.529\n",
      "Epoch 2646, Loss: 19.825, Final Batch Loss: 0.475\n",
      "Epoch 2647, Loss: 19.998, Final Batch Loss: 0.605\n",
      "Epoch 2648, Loss: 20.163, Final Batch Loss: 0.579\n",
      "Epoch 2649, Loss: 20.024, Final Batch Loss: 0.525\n",
      "Epoch 2650, Loss: 20.095, Final Batch Loss: 0.585\n",
      "Epoch 2651, Loss: 20.015, Final Batch Loss: 0.537\n",
      "Epoch 2652, Loss: 20.223, Final Batch Loss: 0.568\n",
      "Epoch 2653, Loss: 20.060, Final Batch Loss: 0.537\n",
      "Epoch 2654, Loss: 20.246, Final Batch Loss: 0.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2655, Loss: 19.902, Final Batch Loss: 0.458\n",
      "Epoch 2656, Loss: 19.868, Final Batch Loss: 0.599\n",
      "Epoch 2657, Loss: 19.991, Final Batch Loss: 0.479\n",
      "Epoch 2658, Loss: 19.998, Final Batch Loss: 0.617\n",
      "Epoch 2659, Loss: 19.989, Final Batch Loss: 0.569\n",
      "Epoch 2660, Loss: 20.169, Final Batch Loss: 0.607\n",
      "Epoch 2661, Loss: 20.128, Final Batch Loss: 0.637\n",
      "Epoch 2662, Loss: 20.229, Final Batch Loss: 0.747\n",
      "Epoch 2663, Loss: 19.905, Final Batch Loss: 0.541\n",
      "Epoch 2664, Loss: 20.061, Final Batch Loss: 0.646\n",
      "Epoch 2665, Loss: 20.078, Final Batch Loss: 0.540\n",
      "Epoch 2666, Loss: 20.022, Final Batch Loss: 0.548\n",
      "Epoch 2667, Loss: 20.131, Final Batch Loss: 0.473\n",
      "Epoch 2668, Loss: 19.945, Final Batch Loss: 0.508\n",
      "Epoch 2669, Loss: 20.012, Final Batch Loss: 0.500\n",
      "Epoch 2670, Loss: 19.925, Final Batch Loss: 0.470\n",
      "Epoch 2671, Loss: 20.068, Final Batch Loss: 0.542\n",
      "Epoch 2672, Loss: 19.964, Final Batch Loss: 0.445\n",
      "Epoch 2673, Loss: 20.028, Final Batch Loss: 0.541\n",
      "Epoch 2674, Loss: 19.979, Final Batch Loss: 0.612\n",
      "Epoch 2675, Loss: 20.088, Final Batch Loss: 0.472\n",
      "Epoch 2676, Loss: 19.889, Final Batch Loss: 0.615\n",
      "Epoch 2677, Loss: 20.038, Final Batch Loss: 0.519\n",
      "Epoch 2678, Loss: 19.851, Final Batch Loss: 0.585\n",
      "Epoch 2679, Loss: 19.844, Final Batch Loss: 0.393\n",
      "Epoch 2680, Loss: 20.116, Final Batch Loss: 0.483\n",
      "Epoch 2681, Loss: 19.949, Final Batch Loss: 0.567\n",
      "Epoch 2682, Loss: 19.950, Final Batch Loss: 0.570\n",
      "Epoch 2683, Loss: 19.983, Final Batch Loss: 0.721\n",
      "Epoch 2684, Loss: 19.934, Final Batch Loss: 0.619\n",
      "Epoch 2685, Loss: 20.059, Final Batch Loss: 0.610\n",
      "Epoch 2686, Loss: 20.071, Final Batch Loss: 0.533\n",
      "Epoch 2687, Loss: 20.087, Final Batch Loss: 0.720\n",
      "Epoch 2688, Loss: 19.832, Final Batch Loss: 0.506\n",
      "Epoch 2689, Loss: 20.024, Final Batch Loss: 0.485\n",
      "Epoch 2690, Loss: 19.985, Final Batch Loss: 0.565\n",
      "Epoch 2691, Loss: 20.210, Final Batch Loss: 0.612\n",
      "Epoch 2692, Loss: 20.164, Final Batch Loss: 0.536\n",
      "Epoch 2693, Loss: 19.934, Final Batch Loss: 0.642\n",
      "Epoch 2694, Loss: 19.709, Final Batch Loss: 0.506\n",
      "Epoch 2695, Loss: 19.961, Final Batch Loss: 0.472\n",
      "Epoch 2696, Loss: 19.905, Final Batch Loss: 0.543\n",
      "Epoch 2697, Loss: 19.893, Final Batch Loss: 0.512\n",
      "Epoch 2698, Loss: 19.997, Final Batch Loss: 0.610\n",
      "Epoch 2699, Loss: 20.077, Final Batch Loss: 0.632\n",
      "Epoch 2700, Loss: 19.902, Final Batch Loss: 0.554\n",
      "Epoch 2701, Loss: 19.949, Final Batch Loss: 0.414\n",
      "Epoch 2702, Loss: 19.822, Final Batch Loss: 0.485\n",
      "Epoch 2703, Loss: 20.077, Final Batch Loss: 0.536\n",
      "Epoch 2704, Loss: 20.074, Final Batch Loss: 0.638\n",
      "Epoch 2705, Loss: 20.030, Final Batch Loss: 0.600\n",
      "Epoch 2706, Loss: 19.961, Final Batch Loss: 0.685\n",
      "Epoch 2707, Loss: 20.079, Final Batch Loss: 0.475\n",
      "Epoch 2708, Loss: 19.926, Final Batch Loss: 0.579\n",
      "Epoch 2709, Loss: 20.002, Final Batch Loss: 0.576\n",
      "Epoch 2710, Loss: 20.076, Final Batch Loss: 0.613\n",
      "Epoch 2711, Loss: 20.135, Final Batch Loss: 0.584\n",
      "Epoch 2712, Loss: 20.033, Final Batch Loss: 0.517\n",
      "Epoch 2713, Loss: 19.728, Final Batch Loss: 0.465\n",
      "Epoch 2714, Loss: 19.882, Final Batch Loss: 0.570\n",
      "Epoch 2715, Loss: 19.993, Final Batch Loss: 0.456\n",
      "Epoch 2716, Loss: 19.886, Final Batch Loss: 0.541\n",
      "Epoch 2717, Loss: 20.026, Final Batch Loss: 0.485\n",
      "Epoch 2718, Loss: 20.022, Final Batch Loss: 0.565\n",
      "Epoch 2719, Loss: 19.959, Final Batch Loss: 0.570\n",
      "Epoch 2720, Loss: 19.913, Final Batch Loss: 0.577\n",
      "Epoch 2721, Loss: 20.037, Final Batch Loss: 0.605\n",
      "Epoch 2722, Loss: 19.950, Final Batch Loss: 0.477\n",
      "Epoch 2723, Loss: 20.069, Final Batch Loss: 0.560\n",
      "Epoch 2724, Loss: 19.932, Final Batch Loss: 0.510\n",
      "Epoch 2725, Loss: 20.101, Final Batch Loss: 0.563\n",
      "Epoch 2726, Loss: 19.889, Final Batch Loss: 0.511\n",
      "Epoch 2727, Loss: 20.004, Final Batch Loss: 0.546\n",
      "Epoch 2728, Loss: 19.841, Final Batch Loss: 0.565\n",
      "Epoch 2729, Loss: 20.155, Final Batch Loss: 0.559\n",
      "Epoch 2730, Loss: 20.023, Final Batch Loss: 0.502\n",
      "Epoch 2731, Loss: 20.018, Final Batch Loss: 0.602\n",
      "Epoch 2732, Loss: 20.159, Final Batch Loss: 0.579\n",
      "Epoch 2733, Loss: 19.910, Final Batch Loss: 0.532\n",
      "Epoch 2734, Loss: 19.930, Final Batch Loss: 0.487\n",
      "Epoch 2735, Loss: 19.686, Final Batch Loss: 0.511\n",
      "Epoch 2736, Loss: 20.098, Final Batch Loss: 0.545\n",
      "Epoch 2737, Loss: 20.062, Final Batch Loss: 0.663\n",
      "Epoch 2738, Loss: 20.003, Final Batch Loss: 0.708\n",
      "Epoch 2739, Loss: 19.688, Final Batch Loss: 0.503\n",
      "Epoch 2740, Loss: 19.991, Final Batch Loss: 0.587\n",
      "Epoch 2741, Loss: 19.850, Final Batch Loss: 0.542\n",
      "Epoch 2742, Loss: 19.998, Final Batch Loss: 0.467\n",
      "Epoch 2743, Loss: 19.989, Final Batch Loss: 0.629\n",
      "Epoch 2744, Loss: 19.748, Final Batch Loss: 0.538\n",
      "Epoch 2745, Loss: 20.116, Final Batch Loss: 0.612\n",
      "Epoch 2746, Loss: 20.070, Final Batch Loss: 0.683\n",
      "Epoch 2747, Loss: 20.016, Final Batch Loss: 0.624\n",
      "Epoch 2748, Loss: 19.933, Final Batch Loss: 0.549\n",
      "Epoch 2749, Loss: 20.092, Final Batch Loss: 0.710\n",
      "Epoch 2750, Loss: 19.875, Final Batch Loss: 0.525\n",
      "Epoch 2751, Loss: 20.091, Final Batch Loss: 0.578\n",
      "Epoch 2752, Loss: 19.990, Final Batch Loss: 0.471\n",
      "Epoch 2753, Loss: 19.946, Final Batch Loss: 0.607\n",
      "Epoch 2754, Loss: 20.036, Final Batch Loss: 0.507\n",
      "Epoch 2755, Loss: 20.154, Final Batch Loss: 0.565\n",
      "Epoch 2756, Loss: 19.949, Final Batch Loss: 0.505\n",
      "Epoch 2757, Loss: 20.144, Final Batch Loss: 0.611\n",
      "Epoch 2758, Loss: 20.028, Final Batch Loss: 0.616\n",
      "Epoch 2759, Loss: 19.884, Final Batch Loss: 0.522\n",
      "Epoch 2760, Loss: 19.913, Final Batch Loss: 0.479\n",
      "Epoch 2761, Loss: 19.844, Final Batch Loss: 0.579\n",
      "Epoch 2762, Loss: 20.218, Final Batch Loss: 0.727\n",
      "Epoch 2763, Loss: 19.650, Final Batch Loss: 0.597\n",
      "Epoch 2764, Loss: 20.123, Final Batch Loss: 0.478\n",
      "Epoch 2765, Loss: 20.037, Final Batch Loss: 0.698\n",
      "Epoch 2766, Loss: 19.842, Final Batch Loss: 0.452\n",
      "Epoch 2767, Loss: 20.035, Final Batch Loss: 0.526\n",
      "Epoch 2768, Loss: 19.978, Final Batch Loss: 0.527\n",
      "Epoch 2769, Loss: 19.912, Final Batch Loss: 0.536\n",
      "Epoch 2770, Loss: 19.874, Final Batch Loss: 0.611\n",
      "Epoch 2771, Loss: 19.886, Final Batch Loss: 0.477\n",
      "Epoch 2772, Loss: 20.083, Final Batch Loss: 0.627\n",
      "Epoch 2773, Loss: 19.922, Final Batch Loss: 0.513\n",
      "Epoch 2774, Loss: 19.938, Final Batch Loss: 0.485\n",
      "Epoch 2775, Loss: 19.873, Final Batch Loss: 0.527\n",
      "Epoch 2776, Loss: 20.070, Final Batch Loss: 0.482\n",
      "Epoch 2777, Loss: 19.921, Final Batch Loss: 0.465\n",
      "Epoch 2778, Loss: 20.073, Final Batch Loss: 0.649\n",
      "Epoch 2779, Loss: 19.734, Final Batch Loss: 0.542\n",
      "Epoch 2780, Loss: 19.896, Final Batch Loss: 0.485\n",
      "Epoch 2781, Loss: 19.893, Final Batch Loss: 0.641\n",
      "Epoch 2782, Loss: 19.711, Final Batch Loss: 0.513\n",
      "Epoch 2783, Loss: 19.728, Final Batch Loss: 0.510\n",
      "Epoch 2784, Loss: 19.867, Final Batch Loss: 0.581\n",
      "Epoch 2785, Loss: 20.052, Final Batch Loss: 0.523\n",
      "Epoch 2786, Loss: 20.051, Final Batch Loss: 0.524\n",
      "Epoch 2787, Loss: 19.815, Final Batch Loss: 0.497\n",
      "Epoch 2788, Loss: 20.067, Final Batch Loss: 0.606\n",
      "Epoch 2789, Loss: 19.834, Final Batch Loss: 0.556\n",
      "Epoch 2790, Loss: 19.968, Final Batch Loss: 0.497\n",
      "Epoch 2791, Loss: 19.592, Final Batch Loss: 0.524\n",
      "Epoch 2792, Loss: 19.896, Final Batch Loss: 0.513\n",
      "Epoch 2793, Loss: 19.889, Final Batch Loss: 0.431\n",
      "Epoch 2794, Loss: 19.983, Final Batch Loss: 0.510\n",
      "Epoch 2795, Loss: 20.114, Final Batch Loss: 0.605\n",
      "Epoch 2796, Loss: 20.001, Final Batch Loss: 0.569\n",
      "Epoch 2797, Loss: 19.990, Final Batch Loss: 0.682\n",
      "Epoch 2798, Loss: 19.699, Final Batch Loss: 0.474\n",
      "Epoch 2799, Loss: 20.097, Final Batch Loss: 0.663\n",
      "Epoch 2800, Loss: 19.802, Final Batch Loss: 0.496\n",
      "Epoch 2801, Loss: 19.804, Final Batch Loss: 0.532\n",
      "Epoch 2802, Loss: 19.942, Final Batch Loss: 0.576\n",
      "Epoch 2803, Loss: 19.931, Final Batch Loss: 0.582\n",
      "Epoch 2804, Loss: 20.059, Final Batch Loss: 0.493\n",
      "Epoch 2805, Loss: 19.954, Final Batch Loss: 0.580\n",
      "Epoch 2806, Loss: 19.971, Final Batch Loss: 0.495\n",
      "Epoch 2807, Loss: 19.754, Final Batch Loss: 0.555\n",
      "Epoch 2808, Loss: 19.933, Final Batch Loss: 0.580\n",
      "Epoch 2809, Loss: 19.942, Final Batch Loss: 0.628\n",
      "Epoch 2810, Loss: 19.714, Final Batch Loss: 0.487\n",
      "Epoch 2811, Loss: 19.796, Final Batch Loss: 0.523\n",
      "Epoch 2812, Loss: 19.918, Final Batch Loss: 0.546\n",
      "Epoch 2813, Loss: 19.815, Final Batch Loss: 0.511\n",
      "Epoch 2814, Loss: 19.650, Final Batch Loss: 0.554\n",
      "Epoch 2815, Loss: 19.814, Final Batch Loss: 0.524\n",
      "Epoch 2816, Loss: 19.894, Final Batch Loss: 0.571\n",
      "Epoch 2817, Loss: 19.717, Final Batch Loss: 0.543\n",
      "Epoch 2818, Loss: 19.950, Final Batch Loss: 0.548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2819, Loss: 19.779, Final Batch Loss: 0.546\n",
      "Epoch 2820, Loss: 19.913, Final Batch Loss: 0.498\n",
      "Epoch 2821, Loss: 20.147, Final Batch Loss: 0.657\n",
      "Epoch 2822, Loss: 20.033, Final Batch Loss: 0.513\n",
      "Epoch 2823, Loss: 19.921, Final Batch Loss: 0.645\n",
      "Epoch 2824, Loss: 20.057, Final Batch Loss: 0.514\n",
      "Epoch 2825, Loss: 19.885, Final Batch Loss: 0.562\n",
      "Epoch 2826, Loss: 19.959, Final Batch Loss: 0.525\n",
      "Epoch 2827, Loss: 20.029, Final Batch Loss: 0.586\n",
      "Epoch 2828, Loss: 19.871, Final Batch Loss: 0.481\n",
      "Epoch 2829, Loss: 19.916, Final Batch Loss: 0.509\n",
      "Epoch 2830, Loss: 19.897, Final Batch Loss: 0.562\n",
      "Epoch 2831, Loss: 20.033, Final Batch Loss: 0.509\n",
      "Epoch 2832, Loss: 20.087, Final Batch Loss: 0.592\n",
      "Epoch 2833, Loss: 20.003, Final Batch Loss: 0.527\n",
      "Epoch 2834, Loss: 19.957, Final Batch Loss: 0.507\n",
      "Epoch 2835, Loss: 19.983, Final Batch Loss: 0.592\n",
      "Epoch 2836, Loss: 19.878, Final Batch Loss: 0.590\n",
      "Epoch 2837, Loss: 20.126, Final Batch Loss: 0.666\n",
      "Epoch 2838, Loss: 19.936, Final Batch Loss: 0.553\n",
      "Epoch 2839, Loss: 19.801, Final Batch Loss: 0.520\n",
      "Epoch 2840, Loss: 19.821, Final Batch Loss: 0.563\n",
      "Epoch 2841, Loss: 19.945, Final Batch Loss: 0.398\n",
      "Epoch 2842, Loss: 20.061, Final Batch Loss: 0.640\n",
      "Epoch 2843, Loss: 20.055, Final Batch Loss: 0.580\n",
      "Epoch 2844, Loss: 20.080, Final Batch Loss: 0.483\n",
      "Epoch 2845, Loss: 19.731, Final Batch Loss: 0.491\n",
      "Epoch 2846, Loss: 19.728, Final Batch Loss: 0.593\n",
      "Epoch 2847, Loss: 19.880, Final Batch Loss: 0.600\n",
      "Epoch 2848, Loss: 19.757, Final Batch Loss: 0.540\n",
      "Epoch 2849, Loss: 20.022, Final Batch Loss: 0.576\n",
      "Epoch 2850, Loss: 19.843, Final Batch Loss: 0.611\n",
      "Epoch 2851, Loss: 19.726, Final Batch Loss: 0.453\n",
      "Epoch 2852, Loss: 19.889, Final Batch Loss: 0.511\n",
      "Epoch 2853, Loss: 19.827, Final Batch Loss: 0.621\n",
      "Epoch 2854, Loss: 19.957, Final Batch Loss: 0.635\n",
      "Epoch 2855, Loss: 19.895, Final Batch Loss: 0.579\n",
      "Epoch 2856, Loss: 19.910, Final Batch Loss: 0.651\n",
      "Epoch 2857, Loss: 19.879, Final Batch Loss: 0.567\n",
      "Epoch 2858, Loss: 19.879, Final Batch Loss: 0.537\n",
      "Epoch 2859, Loss: 19.979, Final Batch Loss: 0.595\n",
      "Epoch 2860, Loss: 19.921, Final Batch Loss: 0.546\n",
      "Epoch 2861, Loss: 19.927, Final Batch Loss: 0.527\n",
      "Epoch 2862, Loss: 19.828, Final Batch Loss: 0.444\n",
      "Epoch 2863, Loss: 19.987, Final Batch Loss: 0.549\n",
      "Epoch 2864, Loss: 19.852, Final Batch Loss: 0.533\n",
      "Epoch 2865, Loss: 19.840, Final Batch Loss: 0.473\n",
      "Epoch 2866, Loss: 19.889, Final Batch Loss: 0.618\n",
      "Epoch 2867, Loss: 19.828, Final Batch Loss: 0.478\n",
      "Epoch 2868, Loss: 19.905, Final Batch Loss: 0.481\n",
      "Epoch 2869, Loss: 19.753, Final Batch Loss: 0.478\n",
      "Epoch 2870, Loss: 20.019, Final Batch Loss: 0.536\n",
      "Epoch 2871, Loss: 19.951, Final Batch Loss: 0.583\n",
      "Epoch 2872, Loss: 19.812, Final Batch Loss: 0.552\n",
      "Epoch 2873, Loss: 19.857, Final Batch Loss: 0.574\n",
      "Epoch 2874, Loss: 19.900, Final Batch Loss: 0.597\n",
      "Epoch 2875, Loss: 19.965, Final Batch Loss: 0.503\n",
      "Epoch 2876, Loss: 19.818, Final Batch Loss: 0.574\n",
      "Epoch 2877, Loss: 20.110, Final Batch Loss: 0.593\n",
      "Epoch 2878, Loss: 20.031, Final Batch Loss: 0.519\n",
      "Epoch 2879, Loss: 20.054, Final Batch Loss: 0.498\n",
      "Epoch 2880, Loss: 19.968, Final Batch Loss: 0.570\n",
      "Epoch 2881, Loss: 19.882, Final Batch Loss: 0.561\n",
      "Epoch 2882, Loss: 19.911, Final Batch Loss: 0.453\n",
      "Epoch 2883, Loss: 20.051, Final Batch Loss: 0.599\n",
      "Epoch 2884, Loss: 20.117, Final Batch Loss: 0.578\n",
      "Epoch 2885, Loss: 19.854, Final Batch Loss: 0.569\n",
      "Epoch 2886, Loss: 19.947, Final Batch Loss: 0.598\n",
      "Epoch 2887, Loss: 19.795, Final Batch Loss: 0.526\n",
      "Epoch 2888, Loss: 19.862, Final Batch Loss: 0.544\n",
      "Epoch 2889, Loss: 20.029, Final Batch Loss: 0.533\n",
      "Epoch 2890, Loss: 19.863, Final Batch Loss: 0.680\n",
      "Epoch 2891, Loss: 19.921, Final Batch Loss: 0.510\n",
      "Epoch 2892, Loss: 19.738, Final Batch Loss: 0.666\n",
      "Epoch 2893, Loss: 20.123, Final Batch Loss: 0.566\n",
      "Epoch 2894, Loss: 19.834, Final Batch Loss: 0.574\n",
      "Epoch 2895, Loss: 20.056, Final Batch Loss: 0.585\n",
      "Epoch 2896, Loss: 19.858, Final Batch Loss: 0.592\n",
      "Epoch 2897, Loss: 20.294, Final Batch Loss: 0.810\n",
      "Epoch 2898, Loss: 20.069, Final Batch Loss: 0.610\n",
      "Epoch 2899, Loss: 19.685, Final Batch Loss: 0.533\n",
      "Epoch 2900, Loss: 19.941, Final Batch Loss: 0.553\n",
      "Epoch 2901, Loss: 19.796, Final Batch Loss: 0.525\n",
      "Epoch 2902, Loss: 20.099, Final Batch Loss: 0.632\n",
      "Epoch 2903, Loss: 20.092, Final Batch Loss: 0.627\n",
      "Epoch 2904, Loss: 19.771, Final Batch Loss: 0.459\n",
      "Epoch 2905, Loss: 19.835, Final Batch Loss: 0.537\n",
      "Epoch 2906, Loss: 19.812, Final Batch Loss: 0.501\n",
      "Epoch 2907, Loss: 20.107, Final Batch Loss: 0.567\n",
      "Epoch 2908, Loss: 19.973, Final Batch Loss: 0.617\n",
      "Epoch 2909, Loss: 19.948, Final Batch Loss: 0.576\n",
      "Epoch 2910, Loss: 19.867, Final Batch Loss: 0.528\n",
      "Epoch 2911, Loss: 19.846, Final Batch Loss: 0.519\n",
      "Epoch 2912, Loss: 19.854, Final Batch Loss: 0.484\n",
      "Epoch 2913, Loss: 19.912, Final Batch Loss: 0.480\n",
      "Epoch 2914, Loss: 19.498, Final Batch Loss: 0.506\n",
      "Epoch 2915, Loss: 19.924, Final Batch Loss: 0.538\n",
      "Epoch 2916, Loss: 19.843, Final Batch Loss: 0.650\n",
      "Epoch 2917, Loss: 19.942, Final Batch Loss: 0.591\n",
      "Epoch 2918, Loss: 20.117, Final Batch Loss: 0.489\n",
      "Epoch 2919, Loss: 19.912, Final Batch Loss: 0.636\n",
      "Epoch 2920, Loss: 19.684, Final Batch Loss: 0.495\n",
      "Epoch 2921, Loss: 19.902, Final Batch Loss: 0.574\n",
      "Epoch 2922, Loss: 19.776, Final Batch Loss: 0.648\n",
      "Epoch 2923, Loss: 19.777, Final Batch Loss: 0.525\n",
      "Epoch 2924, Loss: 19.935, Final Batch Loss: 0.651\n",
      "Epoch 2925, Loss: 19.730, Final Batch Loss: 0.557\n",
      "Epoch 2926, Loss: 19.756, Final Batch Loss: 0.548\n",
      "Epoch 2927, Loss: 19.916, Final Batch Loss: 0.548\n",
      "Epoch 2928, Loss: 19.641, Final Batch Loss: 0.527\n",
      "Epoch 2929, Loss: 19.816, Final Batch Loss: 0.579\n",
      "Epoch 2930, Loss: 19.640, Final Batch Loss: 0.439\n",
      "Epoch 2931, Loss: 20.013, Final Batch Loss: 0.531\n",
      "Epoch 2932, Loss: 19.865, Final Batch Loss: 0.557\n",
      "Epoch 2933, Loss: 19.988, Final Batch Loss: 0.642\n",
      "Epoch 2934, Loss: 19.812, Final Batch Loss: 0.615\n",
      "Epoch 2935, Loss: 19.822, Final Batch Loss: 0.592\n",
      "Epoch 2936, Loss: 19.968, Final Batch Loss: 0.467\n",
      "Epoch 2937, Loss: 19.734, Final Batch Loss: 0.565\n",
      "Epoch 2938, Loss: 19.849, Final Batch Loss: 0.551\n",
      "Epoch 2939, Loss: 20.095, Final Batch Loss: 0.530\n",
      "Epoch 2940, Loss: 19.760, Final Batch Loss: 0.462\n",
      "Epoch 2941, Loss: 19.996, Final Batch Loss: 0.529\n",
      "Epoch 2942, Loss: 19.725, Final Batch Loss: 0.617\n",
      "Epoch 2943, Loss: 19.837, Final Batch Loss: 0.570\n",
      "Epoch 2944, Loss: 20.053, Final Batch Loss: 0.654\n",
      "Epoch 2945, Loss: 19.856, Final Batch Loss: 0.646\n",
      "Epoch 2946, Loss: 19.890, Final Batch Loss: 0.497\n",
      "Epoch 2947, Loss: 19.967, Final Batch Loss: 0.583\n",
      "Epoch 2948, Loss: 19.900, Final Batch Loss: 0.612\n",
      "Epoch 2949, Loss: 19.719, Final Batch Loss: 0.627\n",
      "Epoch 2950, Loss: 19.935, Final Batch Loss: 0.620\n",
      "Epoch 2951, Loss: 19.993, Final Batch Loss: 0.600\n",
      "Epoch 2952, Loss: 19.825, Final Batch Loss: 0.562\n",
      "Epoch 2953, Loss: 20.094, Final Batch Loss: 0.575\n",
      "Epoch 2954, Loss: 20.080, Final Batch Loss: 0.625\n",
      "Epoch 2955, Loss: 19.860, Final Batch Loss: 0.547\n",
      "Epoch 2956, Loss: 19.816, Final Batch Loss: 0.506\n",
      "Epoch 2957, Loss: 19.825, Final Batch Loss: 0.545\n",
      "Epoch 2958, Loss: 19.847, Final Batch Loss: 0.636\n",
      "Epoch 2959, Loss: 19.803, Final Batch Loss: 0.469\n",
      "Epoch 2960, Loss: 20.046, Final Batch Loss: 0.517\n",
      "Epoch 2961, Loss: 19.829, Final Batch Loss: 0.524\n",
      "Epoch 2962, Loss: 19.856, Final Batch Loss: 0.463\n",
      "Epoch 2963, Loss: 19.659, Final Batch Loss: 0.553\n",
      "Epoch 2964, Loss: 19.674, Final Batch Loss: 0.557\n",
      "Epoch 2965, Loss: 20.070, Final Batch Loss: 0.528\n",
      "Epoch 2966, Loss: 19.947, Final Batch Loss: 0.524\n",
      "Epoch 2967, Loss: 19.960, Final Batch Loss: 0.564\n",
      "Epoch 2968, Loss: 19.893, Final Batch Loss: 0.697\n",
      "Epoch 2969, Loss: 19.887, Final Batch Loss: 0.611\n",
      "Epoch 2970, Loss: 19.964, Final Batch Loss: 0.569\n",
      "Epoch 2971, Loss: 19.699, Final Batch Loss: 0.474\n",
      "Epoch 2972, Loss: 20.060, Final Batch Loss: 0.621\n",
      "Epoch 2973, Loss: 19.739, Final Batch Loss: 0.498\n",
      "Epoch 2974, Loss: 19.867, Final Batch Loss: 0.540\n",
      "Epoch 2975, Loss: 19.781, Final Batch Loss: 0.608\n",
      "Epoch 2976, Loss: 19.716, Final Batch Loss: 0.522\n",
      "Epoch 2977, Loss: 19.715, Final Batch Loss: 0.515\n",
      "Epoch 2978, Loss: 19.605, Final Batch Loss: 0.540\n",
      "Epoch 2979, Loss: 19.793, Final Batch Loss: 0.478\n",
      "Epoch 2980, Loss: 19.780, Final Batch Loss: 0.581\n",
      "Epoch 2981, Loss: 20.090, Final Batch Loss: 0.516\n",
      "Epoch 2982, Loss: 19.855, Final Batch Loss: 0.474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2983, Loss: 19.836, Final Batch Loss: 0.480\n",
      "Epoch 2984, Loss: 19.868, Final Batch Loss: 0.699\n",
      "Epoch 2985, Loss: 19.909, Final Batch Loss: 0.567\n",
      "Epoch 2986, Loss: 19.945, Final Batch Loss: 0.537\n",
      "Epoch 2987, Loss: 19.770, Final Batch Loss: 0.476\n",
      "Epoch 2988, Loss: 19.948, Final Batch Loss: 0.595\n",
      "Epoch 2989, Loss: 19.995, Final Batch Loss: 0.628\n",
      "Epoch 2990, Loss: 19.604, Final Batch Loss: 0.601\n",
      "Epoch 2991, Loss: 19.531, Final Batch Loss: 0.513\n",
      "Epoch 2992, Loss: 19.756, Final Batch Loss: 0.565\n",
      "Epoch 2993, Loss: 19.876, Final Batch Loss: 0.539\n",
      "Epoch 2994, Loss: 19.822, Final Batch Loss: 0.500\n",
      "Epoch 2995, Loss: 19.799, Final Batch Loss: 0.598\n",
      "Epoch 2996, Loss: 20.121, Final Batch Loss: 0.631\n",
      "Epoch 2997, Loss: 19.744, Final Batch Loss: 0.537\n",
      "Epoch 2998, Loss: 19.581, Final Batch Loss: 0.569\n",
      "Epoch 2999, Loss: 19.867, Final Batch Loss: 0.501\n",
      "Epoch 3000, Loss: 19.850, Final Batch Loss: 0.495\n",
      "Epoch 3001, Loss: 19.613, Final Batch Loss: 0.459\n",
      "Epoch 3002, Loss: 19.803, Final Batch Loss: 0.547\n",
      "Epoch 3003, Loss: 19.718, Final Batch Loss: 0.430\n",
      "Epoch 3004, Loss: 19.815, Final Batch Loss: 0.531\n",
      "Epoch 3005, Loss: 19.811, Final Batch Loss: 0.489\n",
      "Epoch 3006, Loss: 19.758, Final Batch Loss: 0.599\n",
      "Epoch 3007, Loss: 19.747, Final Batch Loss: 0.556\n",
      "Epoch 3008, Loss: 19.855, Final Batch Loss: 0.468\n",
      "Epoch 3009, Loss: 19.596, Final Batch Loss: 0.543\n",
      "Epoch 3010, Loss: 20.032, Final Batch Loss: 0.569\n",
      "Epoch 3011, Loss: 20.149, Final Batch Loss: 0.625\n",
      "Epoch 3012, Loss: 19.718, Final Batch Loss: 0.536\n",
      "Epoch 3013, Loss: 19.560, Final Batch Loss: 0.530\n",
      "Epoch 3014, Loss: 19.853, Final Batch Loss: 0.567\n",
      "Epoch 3015, Loss: 19.827, Final Batch Loss: 0.531\n",
      "Epoch 3016, Loss: 19.670, Final Batch Loss: 0.626\n",
      "Epoch 3017, Loss: 19.654, Final Batch Loss: 0.544\n",
      "Epoch 3018, Loss: 19.780, Final Batch Loss: 0.547\n",
      "Epoch 3019, Loss: 19.608, Final Batch Loss: 0.586\n",
      "Epoch 3020, Loss: 19.823, Final Batch Loss: 0.685\n",
      "Epoch 3021, Loss: 19.887, Final Batch Loss: 0.523\n",
      "Epoch 3022, Loss: 19.837, Final Batch Loss: 0.621\n",
      "Epoch 3023, Loss: 19.804, Final Batch Loss: 0.463\n",
      "Epoch 3024, Loss: 19.770, Final Batch Loss: 0.483\n",
      "Epoch 3025, Loss: 19.679, Final Batch Loss: 0.756\n",
      "Epoch 3026, Loss: 19.901, Final Batch Loss: 0.507\n",
      "Epoch 3027, Loss: 19.708, Final Batch Loss: 0.546\n",
      "Epoch 3028, Loss: 19.946, Final Batch Loss: 0.506\n",
      "Epoch 3029, Loss: 19.574, Final Batch Loss: 0.439\n",
      "Epoch 3030, Loss: 19.818, Final Batch Loss: 0.591\n",
      "Epoch 3031, Loss: 19.748, Final Batch Loss: 0.550\n",
      "Epoch 3032, Loss: 19.650, Final Batch Loss: 0.570\n",
      "Epoch 3033, Loss: 19.932, Final Batch Loss: 0.590\n",
      "Epoch 3034, Loss: 19.545, Final Batch Loss: 0.614\n",
      "Epoch 3035, Loss: 19.824, Final Batch Loss: 0.531\n",
      "Epoch 3036, Loss: 19.896, Final Batch Loss: 0.663\n",
      "Epoch 3037, Loss: 19.698, Final Batch Loss: 0.586\n",
      "Epoch 3038, Loss: 19.487, Final Batch Loss: 0.467\n",
      "Epoch 3039, Loss: 19.486, Final Batch Loss: 0.522\n",
      "Epoch 3040, Loss: 19.813, Final Batch Loss: 0.704\n",
      "Epoch 3041, Loss: 19.801, Final Batch Loss: 0.472\n",
      "Epoch 3042, Loss: 19.878, Final Batch Loss: 0.536\n",
      "Epoch 3043, Loss: 19.815, Final Batch Loss: 0.463\n",
      "Epoch 3044, Loss: 19.901, Final Batch Loss: 0.628\n",
      "Epoch 3045, Loss: 19.925, Final Batch Loss: 0.537\n",
      "Epoch 3046, Loss: 19.742, Final Batch Loss: 0.507\n",
      "Epoch 3047, Loss: 19.689, Final Batch Loss: 0.488\n",
      "Epoch 3048, Loss: 19.716, Final Batch Loss: 0.595\n",
      "Epoch 3049, Loss: 19.630, Final Batch Loss: 0.503\n",
      "Epoch 3050, Loss: 19.730, Final Batch Loss: 0.480\n",
      "Epoch 3051, Loss: 19.786, Final Batch Loss: 0.539\n",
      "Epoch 3052, Loss: 19.953, Final Batch Loss: 0.543\n",
      "Epoch 3053, Loss: 19.965, Final Batch Loss: 0.540\n",
      "Epoch 3054, Loss: 19.623, Final Batch Loss: 0.494\n",
      "Epoch 3055, Loss: 19.507, Final Batch Loss: 0.624\n",
      "Epoch 3056, Loss: 19.625, Final Batch Loss: 0.513\n",
      "Epoch 3057, Loss: 19.694, Final Batch Loss: 0.492\n",
      "Epoch 3058, Loss: 19.768, Final Batch Loss: 0.545\n",
      "Epoch 3059, Loss: 19.813, Final Batch Loss: 0.529\n",
      "Epoch 3060, Loss: 19.978, Final Batch Loss: 0.522\n",
      "Epoch 3061, Loss: 19.835, Final Batch Loss: 0.537\n",
      "Epoch 3062, Loss: 19.830, Final Batch Loss: 0.451\n",
      "Epoch 3063, Loss: 19.887, Final Batch Loss: 0.520\n",
      "Epoch 3064, Loss: 19.389, Final Batch Loss: 0.576\n",
      "Epoch 3065, Loss: 20.046, Final Batch Loss: 0.589\n",
      "Epoch 3066, Loss: 19.576, Final Batch Loss: 0.480\n",
      "Epoch 3067, Loss: 19.660, Final Batch Loss: 0.598\n",
      "Epoch 3068, Loss: 19.722, Final Batch Loss: 0.496\n",
      "Epoch 3069, Loss: 19.835, Final Batch Loss: 0.639\n",
      "Epoch 3070, Loss: 19.693, Final Batch Loss: 0.473\n",
      "Epoch 3071, Loss: 20.175, Final Batch Loss: 0.643\n",
      "Epoch 3072, Loss: 19.590, Final Batch Loss: 0.494\n",
      "Epoch 3073, Loss: 19.843, Final Batch Loss: 0.615\n",
      "Epoch 3074, Loss: 19.801, Final Batch Loss: 0.614\n",
      "Epoch 3075, Loss: 19.676, Final Batch Loss: 0.683\n",
      "Epoch 3076, Loss: 19.896, Final Batch Loss: 0.440\n",
      "Epoch 3077, Loss: 19.766, Final Batch Loss: 0.486\n",
      "Epoch 3078, Loss: 20.036, Final Batch Loss: 0.701\n",
      "Epoch 3079, Loss: 19.646, Final Batch Loss: 0.524\n",
      "Epoch 3080, Loss: 19.608, Final Batch Loss: 0.565\n",
      "Epoch 3081, Loss: 19.559, Final Batch Loss: 0.558\n",
      "Epoch 3082, Loss: 19.825, Final Batch Loss: 0.594\n",
      "Epoch 3083, Loss: 19.581, Final Batch Loss: 0.533\n",
      "Epoch 3084, Loss: 20.080, Final Batch Loss: 0.736\n",
      "Epoch 3085, Loss: 19.623, Final Batch Loss: 0.516\n",
      "Epoch 3086, Loss: 19.949, Final Batch Loss: 0.588\n",
      "Epoch 3087, Loss: 19.507, Final Batch Loss: 0.501\n",
      "Epoch 3088, Loss: 19.765, Final Batch Loss: 0.581\n",
      "Epoch 3089, Loss: 19.815, Final Batch Loss: 0.623\n",
      "Epoch 3090, Loss: 19.795, Final Batch Loss: 0.607\n",
      "Epoch 3091, Loss: 19.629, Final Batch Loss: 0.502\n",
      "Epoch 3092, Loss: 19.735, Final Batch Loss: 0.648\n",
      "Epoch 3093, Loss: 19.722, Final Batch Loss: 0.544\n",
      "Epoch 3094, Loss: 19.629, Final Batch Loss: 0.553\n",
      "Epoch 3095, Loss: 19.776, Final Batch Loss: 0.568\n",
      "Epoch 3096, Loss: 19.758, Final Batch Loss: 0.441\n",
      "Epoch 3097, Loss: 19.902, Final Batch Loss: 0.583\n",
      "Epoch 3098, Loss: 19.841, Final Batch Loss: 0.538\n",
      "Epoch 3099, Loss: 19.769, Final Batch Loss: 0.499\n",
      "Epoch 3100, Loss: 19.945, Final Batch Loss: 0.702\n",
      "Epoch 3101, Loss: 19.662, Final Batch Loss: 0.536\n",
      "Epoch 3102, Loss: 19.918, Final Batch Loss: 0.587\n",
      "Epoch 3103, Loss: 19.809, Final Batch Loss: 0.605\n",
      "Epoch 3104, Loss: 19.703, Final Batch Loss: 0.609\n",
      "Epoch 3105, Loss: 19.686, Final Batch Loss: 0.519\n",
      "Epoch 3106, Loss: 19.671, Final Batch Loss: 0.677\n",
      "Epoch 3107, Loss: 19.989, Final Batch Loss: 0.549\n",
      "Epoch 3108, Loss: 19.814, Final Batch Loss: 0.510\n",
      "Epoch 3109, Loss: 19.752, Final Batch Loss: 0.523\n",
      "Epoch 3110, Loss: 20.032, Final Batch Loss: 0.545\n",
      "Epoch 3111, Loss: 19.594, Final Batch Loss: 0.523\n",
      "Epoch 3112, Loss: 19.602, Final Batch Loss: 0.557\n",
      "Epoch 3113, Loss: 19.801, Final Batch Loss: 0.525\n",
      "Epoch 3114, Loss: 19.793, Final Batch Loss: 0.522\n",
      "Epoch 3115, Loss: 19.791, Final Batch Loss: 0.560\n",
      "Epoch 3116, Loss: 19.631, Final Batch Loss: 0.508\n",
      "Epoch 3117, Loss: 19.653, Final Batch Loss: 0.618\n",
      "Epoch 3118, Loss: 19.654, Final Batch Loss: 0.567\n",
      "Epoch 3119, Loss: 19.871, Final Batch Loss: 0.660\n",
      "Epoch 3120, Loss: 19.620, Final Batch Loss: 0.574\n",
      "Epoch 3121, Loss: 19.625, Final Batch Loss: 0.457\n",
      "Epoch 3122, Loss: 19.701, Final Batch Loss: 0.669\n",
      "Epoch 3123, Loss: 20.034, Final Batch Loss: 0.540\n",
      "Epoch 3124, Loss: 19.547, Final Batch Loss: 0.576\n",
      "Epoch 3125, Loss: 19.944, Final Batch Loss: 0.502\n",
      "Epoch 3126, Loss: 19.746, Final Batch Loss: 0.626\n",
      "Epoch 3127, Loss: 19.688, Final Batch Loss: 0.517\n",
      "Epoch 3128, Loss: 19.702, Final Batch Loss: 0.712\n",
      "Epoch 3129, Loss: 19.828, Final Batch Loss: 0.541\n",
      "Epoch 3130, Loss: 19.679, Final Batch Loss: 0.641\n",
      "Epoch 3131, Loss: 19.607, Final Batch Loss: 0.505\n",
      "Epoch 3132, Loss: 19.730, Final Batch Loss: 0.495\n",
      "Epoch 3133, Loss: 19.755, Final Batch Loss: 0.561\n",
      "Epoch 3134, Loss: 19.904, Final Batch Loss: 0.555\n",
      "Epoch 3135, Loss: 19.586, Final Batch Loss: 0.566\n",
      "Epoch 3136, Loss: 19.583, Final Batch Loss: 0.519\n",
      "Epoch 3137, Loss: 19.679, Final Batch Loss: 0.547\n",
      "Epoch 3138, Loss: 19.781, Final Batch Loss: 0.599\n",
      "Epoch 3139, Loss: 19.538, Final Batch Loss: 0.498\n",
      "Epoch 3140, Loss: 19.883, Final Batch Loss: 0.651\n",
      "Epoch 3141, Loss: 19.884, Final Batch Loss: 0.536\n",
      "Epoch 3142, Loss: 19.592, Final Batch Loss: 0.547\n",
      "Epoch 3143, Loss: 19.720, Final Batch Loss: 0.475\n",
      "Epoch 3144, Loss: 19.578, Final Batch Loss: 0.459\n",
      "Epoch 3145, Loss: 19.805, Final Batch Loss: 0.619\n",
      "Epoch 3146, Loss: 19.676, Final Batch Loss: 0.473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3147, Loss: 19.587, Final Batch Loss: 0.452\n",
      "Epoch 3148, Loss: 19.463, Final Batch Loss: 0.493\n",
      "Epoch 3149, Loss: 19.738, Final Batch Loss: 0.455\n",
      "Epoch 3150, Loss: 19.720, Final Batch Loss: 0.514\n",
      "Epoch 3151, Loss: 19.889, Final Batch Loss: 0.610\n",
      "Epoch 3152, Loss: 19.696, Final Batch Loss: 0.563\n",
      "Epoch 3153, Loss: 19.497, Final Batch Loss: 0.516\n",
      "Epoch 3154, Loss: 19.734, Final Batch Loss: 0.573\n",
      "Epoch 3155, Loss: 19.784, Final Batch Loss: 0.611\n",
      "Epoch 3156, Loss: 19.620, Final Batch Loss: 0.489\n",
      "Epoch 3157, Loss: 19.870, Final Batch Loss: 0.665\n",
      "Epoch 3158, Loss: 19.657, Final Batch Loss: 0.608\n",
      "Epoch 3159, Loss: 19.761, Final Batch Loss: 0.497\n",
      "Epoch 3160, Loss: 19.795, Final Batch Loss: 0.663\n",
      "Epoch 3161, Loss: 19.465, Final Batch Loss: 0.532\n",
      "Epoch 3162, Loss: 19.677, Final Batch Loss: 0.514\n",
      "Epoch 3163, Loss: 19.773, Final Batch Loss: 0.648\n",
      "Epoch 3164, Loss: 19.841, Final Batch Loss: 0.488\n",
      "Epoch 3165, Loss: 19.788, Final Batch Loss: 0.492\n",
      "Epoch 3166, Loss: 19.840, Final Batch Loss: 0.568\n",
      "Epoch 3167, Loss: 19.835, Final Batch Loss: 0.542\n",
      "Epoch 3168, Loss: 19.574, Final Batch Loss: 0.540\n",
      "Epoch 3169, Loss: 19.779, Final Batch Loss: 0.471\n",
      "Epoch 3170, Loss: 19.724, Final Batch Loss: 0.499\n",
      "Epoch 3171, Loss: 19.608, Final Batch Loss: 0.490\n",
      "Epoch 3172, Loss: 19.863, Final Batch Loss: 0.536\n",
      "Epoch 3173, Loss: 20.018, Final Batch Loss: 0.634\n",
      "Epoch 3174, Loss: 19.534, Final Batch Loss: 0.529\n",
      "Epoch 3175, Loss: 19.647, Final Batch Loss: 0.520\n",
      "Epoch 3176, Loss: 19.961, Final Batch Loss: 0.506\n",
      "Epoch 3177, Loss: 19.666, Final Batch Loss: 0.486\n",
      "Epoch 3178, Loss: 19.622, Final Batch Loss: 0.610\n",
      "Epoch 3179, Loss: 19.784, Final Batch Loss: 0.610\n",
      "Epoch 3180, Loss: 19.875, Final Batch Loss: 0.615\n",
      "Epoch 3181, Loss: 19.822, Final Batch Loss: 0.642\n",
      "Epoch 3182, Loss: 19.611, Final Batch Loss: 0.537\n",
      "Epoch 3183, Loss: 19.769, Final Batch Loss: 0.486\n",
      "Epoch 3184, Loss: 19.783, Final Batch Loss: 0.580\n",
      "Epoch 3185, Loss: 19.702, Final Batch Loss: 0.519\n",
      "Epoch 3186, Loss: 19.588, Final Batch Loss: 0.434\n",
      "Epoch 3187, Loss: 19.644, Final Batch Loss: 0.545\n",
      "Epoch 3188, Loss: 19.581, Final Batch Loss: 0.486\n",
      "Epoch 3189, Loss: 19.440, Final Batch Loss: 0.494\n",
      "Epoch 3190, Loss: 19.713, Final Batch Loss: 0.535\n",
      "Epoch 3191, Loss: 19.708, Final Batch Loss: 0.536\n",
      "Epoch 3192, Loss: 19.924, Final Batch Loss: 0.592\n",
      "Epoch 3193, Loss: 19.575, Final Batch Loss: 0.559\n",
      "Epoch 3194, Loss: 19.552, Final Batch Loss: 0.415\n",
      "Epoch 3195, Loss: 19.606, Final Batch Loss: 0.578\n",
      "Epoch 3196, Loss: 19.748, Final Batch Loss: 0.573\n",
      "Epoch 3197, Loss: 19.835, Final Batch Loss: 0.573\n",
      "Epoch 3198, Loss: 19.591, Final Batch Loss: 0.500\n",
      "Epoch 3199, Loss: 19.627, Final Batch Loss: 0.539\n",
      "Epoch 3200, Loss: 19.712, Final Batch Loss: 0.508\n",
      "Epoch 3201, Loss: 20.049, Final Batch Loss: 0.506\n",
      "Epoch 3202, Loss: 19.566, Final Batch Loss: 0.432\n",
      "Epoch 3203, Loss: 19.968, Final Batch Loss: 0.521\n",
      "Epoch 3204, Loss: 19.834, Final Batch Loss: 0.598\n",
      "Epoch 3205, Loss: 19.649, Final Batch Loss: 0.559\n",
      "Epoch 3206, Loss: 19.423, Final Batch Loss: 0.499\n",
      "Epoch 3207, Loss: 19.652, Final Batch Loss: 0.496\n",
      "Epoch 3208, Loss: 19.888, Final Batch Loss: 0.649\n",
      "Epoch 3209, Loss: 19.538, Final Batch Loss: 0.545\n",
      "Epoch 3210, Loss: 19.664, Final Batch Loss: 0.592\n",
      "Epoch 3211, Loss: 19.647, Final Batch Loss: 0.475\n",
      "Epoch 3212, Loss: 19.702, Final Batch Loss: 0.517\n",
      "Epoch 3213, Loss: 19.678, Final Batch Loss: 0.614\n",
      "Epoch 3214, Loss: 19.689, Final Batch Loss: 0.619\n",
      "Epoch 3215, Loss: 19.680, Final Batch Loss: 0.415\n",
      "Epoch 3216, Loss: 19.616, Final Batch Loss: 0.552\n",
      "Epoch 3217, Loss: 19.500, Final Batch Loss: 0.412\n",
      "Epoch 3218, Loss: 19.904, Final Batch Loss: 0.546\n",
      "Epoch 3219, Loss: 19.593, Final Batch Loss: 0.494\n",
      "Epoch 3220, Loss: 19.681, Final Batch Loss: 0.482\n",
      "Epoch 3221, Loss: 19.782, Final Batch Loss: 0.476\n",
      "Epoch 3222, Loss: 19.572, Final Batch Loss: 0.454\n",
      "Epoch 3223, Loss: 19.760, Final Batch Loss: 0.513\n",
      "Epoch 3224, Loss: 19.763, Final Batch Loss: 0.510\n",
      "Epoch 3225, Loss: 19.834, Final Batch Loss: 0.531\n",
      "Epoch 3226, Loss: 19.897, Final Batch Loss: 0.569\n",
      "Epoch 3227, Loss: 19.892, Final Batch Loss: 0.621\n",
      "Epoch 3228, Loss: 19.918, Final Batch Loss: 0.545\n",
      "Epoch 3229, Loss: 19.629, Final Batch Loss: 0.445\n",
      "Epoch 3230, Loss: 19.774, Final Batch Loss: 0.580\n",
      "Epoch 3231, Loss: 19.716, Final Batch Loss: 0.602\n",
      "Epoch 3232, Loss: 19.807, Final Batch Loss: 0.536\n",
      "Epoch 3233, Loss: 19.576, Final Batch Loss: 0.580\n",
      "Epoch 3234, Loss: 19.557, Final Batch Loss: 0.554\n",
      "Epoch 3235, Loss: 19.835, Final Batch Loss: 0.422\n",
      "Epoch 3236, Loss: 19.672, Final Batch Loss: 0.473\n",
      "Epoch 3237, Loss: 19.590, Final Batch Loss: 0.603\n",
      "Epoch 3238, Loss: 19.617, Final Batch Loss: 0.600\n",
      "Epoch 3239, Loss: 19.611, Final Batch Loss: 0.595\n",
      "Epoch 3240, Loss: 19.570, Final Batch Loss: 0.479\n",
      "Epoch 3241, Loss: 19.675, Final Batch Loss: 0.550\n",
      "Epoch 3242, Loss: 19.827, Final Batch Loss: 0.586\n",
      "Epoch 3243, Loss: 19.641, Final Batch Loss: 0.444\n",
      "Epoch 3244, Loss: 19.443, Final Batch Loss: 0.523\n",
      "Epoch 3245, Loss: 19.726, Final Batch Loss: 0.628\n",
      "Epoch 3246, Loss: 19.680, Final Batch Loss: 0.683\n",
      "Epoch 3247, Loss: 19.552, Final Batch Loss: 0.526\n",
      "Epoch 3248, Loss: 19.799, Final Batch Loss: 0.552\n",
      "Epoch 3249, Loss: 19.807, Final Batch Loss: 0.555\n",
      "Epoch 3250, Loss: 19.705, Final Batch Loss: 0.571\n",
      "Epoch 3251, Loss: 19.353, Final Batch Loss: 0.586\n",
      "Epoch 3252, Loss: 19.649, Final Batch Loss: 0.517\n",
      "Epoch 3253, Loss: 19.885, Final Batch Loss: 0.513\n",
      "Epoch 3254, Loss: 19.566, Final Batch Loss: 0.504\n",
      "Epoch 3255, Loss: 19.671, Final Batch Loss: 0.611\n",
      "Epoch 3256, Loss: 19.467, Final Batch Loss: 0.598\n",
      "Epoch 3257, Loss: 19.714, Final Batch Loss: 0.591\n",
      "Epoch 3258, Loss: 19.579, Final Batch Loss: 0.506\n",
      "Epoch 3259, Loss: 19.647, Final Batch Loss: 0.519\n",
      "Epoch 3260, Loss: 19.695, Final Batch Loss: 0.523\n",
      "Epoch 3261, Loss: 19.733, Final Batch Loss: 0.507\n",
      "Epoch 3262, Loss: 19.456, Final Batch Loss: 0.513\n",
      "Epoch 3263, Loss: 19.801, Final Batch Loss: 0.638\n",
      "Epoch 3264, Loss: 19.869, Final Batch Loss: 0.655\n",
      "Epoch 3265, Loss: 19.649, Final Batch Loss: 0.532\n",
      "Epoch 3266, Loss: 19.438, Final Batch Loss: 0.493\n",
      "Epoch 3267, Loss: 19.746, Final Batch Loss: 0.533\n",
      "Epoch 3268, Loss: 19.526, Final Batch Loss: 0.528\n",
      "Epoch 3269, Loss: 19.700, Final Batch Loss: 0.647\n",
      "Epoch 3270, Loss: 19.522, Final Batch Loss: 0.504\n",
      "Epoch 3271, Loss: 19.595, Final Batch Loss: 0.562\n",
      "Epoch 3272, Loss: 19.701, Final Batch Loss: 0.620\n",
      "Epoch 3273, Loss: 19.676, Final Batch Loss: 0.618\n",
      "Epoch 3274, Loss: 19.537, Final Batch Loss: 0.487\n",
      "Epoch 3275, Loss: 19.452, Final Batch Loss: 0.553\n",
      "Epoch 3276, Loss: 19.741, Final Batch Loss: 0.534\n",
      "Epoch 3277, Loss: 19.609, Final Batch Loss: 0.503\n",
      "Epoch 3278, Loss: 19.474, Final Batch Loss: 0.529\n",
      "Epoch 3279, Loss: 19.732, Final Batch Loss: 0.679\n",
      "Epoch 3280, Loss: 19.739, Final Batch Loss: 0.438\n",
      "Epoch 3281, Loss: 19.436, Final Batch Loss: 0.461\n",
      "Epoch 3282, Loss: 19.516, Final Batch Loss: 0.536\n",
      "Epoch 3283, Loss: 19.794, Final Batch Loss: 0.575\n",
      "Epoch 3284, Loss: 19.649, Final Batch Loss: 0.588\n",
      "Epoch 3285, Loss: 19.314, Final Batch Loss: 0.446\n",
      "Epoch 3286, Loss: 19.591, Final Batch Loss: 0.518\n",
      "Epoch 3287, Loss: 19.715, Final Batch Loss: 0.508\n",
      "Epoch 3288, Loss: 19.733, Final Batch Loss: 0.645\n",
      "Epoch 3289, Loss: 19.491, Final Batch Loss: 0.607\n",
      "Epoch 3290, Loss: 19.575, Final Batch Loss: 0.496\n",
      "Epoch 3291, Loss: 19.772, Final Batch Loss: 0.512\n",
      "Epoch 3292, Loss: 19.720, Final Batch Loss: 0.549\n",
      "Epoch 3293, Loss: 19.755, Final Batch Loss: 0.551\n",
      "Epoch 3294, Loss: 19.918, Final Batch Loss: 0.794\n",
      "Epoch 3295, Loss: 19.463, Final Batch Loss: 0.511\n",
      "Epoch 3296, Loss: 19.691, Final Batch Loss: 0.521\n",
      "Epoch 3297, Loss: 19.856, Final Batch Loss: 0.533\n",
      "Epoch 3298, Loss: 19.601, Final Batch Loss: 0.575\n",
      "Epoch 3299, Loss: 19.567, Final Batch Loss: 0.528\n",
      "Epoch 3300, Loss: 19.656, Final Batch Loss: 0.549\n",
      "Epoch 3301, Loss: 19.639, Final Batch Loss: 0.490\n",
      "Epoch 3302, Loss: 19.693, Final Batch Loss: 0.494\n",
      "Epoch 3303, Loss: 20.027, Final Batch Loss: 0.662\n",
      "Epoch 3304, Loss: 19.658, Final Batch Loss: 0.486\n",
      "Epoch 3305, Loss: 19.470, Final Batch Loss: 0.518\n",
      "Epoch 3306, Loss: 19.448, Final Batch Loss: 0.485\n",
      "Epoch 3307, Loss: 19.465, Final Batch Loss: 0.545\n",
      "Epoch 3308, Loss: 19.561, Final Batch Loss: 0.506\n",
      "Epoch 3309, Loss: 19.647, Final Batch Loss: 0.382\n",
      "Epoch 3310, Loss: 19.727, Final Batch Loss: 0.595\n",
      "Epoch 3311, Loss: 19.492, Final Batch Loss: 0.585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3312, Loss: 19.647, Final Batch Loss: 0.471\n",
      "Epoch 3313, Loss: 19.564, Final Batch Loss: 0.474\n",
      "Epoch 3314, Loss: 19.697, Final Batch Loss: 0.650\n",
      "Epoch 3315, Loss: 19.724, Final Batch Loss: 0.591\n",
      "Epoch 3316, Loss: 19.712, Final Batch Loss: 0.631\n",
      "Epoch 3317, Loss: 19.647, Final Batch Loss: 0.491\n",
      "Epoch 3318, Loss: 19.326, Final Batch Loss: 0.461\n",
      "Epoch 3319, Loss: 19.929, Final Batch Loss: 0.750\n",
      "Epoch 3320, Loss: 19.697, Final Batch Loss: 0.505\n",
      "Epoch 3321, Loss: 19.534, Final Batch Loss: 0.410\n",
      "Epoch 3322, Loss: 19.454, Final Batch Loss: 0.473\n",
      "Epoch 3323, Loss: 19.559, Final Batch Loss: 0.524\n",
      "Epoch 3324, Loss: 19.523, Final Batch Loss: 0.486\n",
      "Epoch 3325, Loss: 19.743, Final Batch Loss: 0.517\n",
      "Epoch 3326, Loss: 19.547, Final Batch Loss: 0.526\n",
      "Epoch 3327, Loss: 19.730, Final Batch Loss: 0.518\n",
      "Epoch 3328, Loss: 19.703, Final Batch Loss: 0.440\n",
      "Epoch 3329, Loss: 19.789, Final Batch Loss: 0.582\n",
      "Epoch 3330, Loss: 19.686, Final Batch Loss: 0.494\n",
      "Epoch 3331, Loss: 19.459, Final Batch Loss: 0.441\n",
      "Epoch 3332, Loss: 19.422, Final Batch Loss: 0.519\n",
      "Epoch 3333, Loss: 19.755, Final Batch Loss: 0.682\n",
      "Epoch 3334, Loss: 19.595, Final Batch Loss: 0.512\n",
      "Epoch 3335, Loss: 19.624, Final Batch Loss: 0.547\n",
      "Epoch 3336, Loss: 19.819, Final Batch Loss: 0.667\n",
      "Epoch 3337, Loss: 19.723, Final Batch Loss: 0.500\n",
      "Epoch 3338, Loss: 19.819, Final Batch Loss: 0.489\n",
      "Epoch 3339, Loss: 19.658, Final Batch Loss: 0.717\n",
      "Epoch 3340, Loss: 19.303, Final Batch Loss: 0.531\n",
      "Epoch 3341, Loss: 19.628, Final Batch Loss: 0.550\n",
      "Epoch 3342, Loss: 19.601, Final Batch Loss: 0.493\n",
      "Epoch 3343, Loss: 19.663, Final Batch Loss: 0.455\n",
      "Epoch 3344, Loss: 19.544, Final Batch Loss: 0.561\n",
      "Epoch 3345, Loss: 19.629, Final Batch Loss: 0.485\n",
      "Epoch 3346, Loss: 19.553, Final Batch Loss: 0.659\n",
      "Epoch 3347, Loss: 19.622, Final Batch Loss: 0.503\n",
      "Epoch 3348, Loss: 19.676, Final Batch Loss: 0.523\n",
      "Epoch 3349, Loss: 19.991, Final Batch Loss: 0.584\n",
      "Epoch 3350, Loss: 19.493, Final Batch Loss: 0.467\n",
      "Epoch 3351, Loss: 19.546, Final Batch Loss: 0.526\n",
      "Epoch 3352, Loss: 19.640, Final Batch Loss: 0.582\n",
      "Epoch 3353, Loss: 19.653, Final Batch Loss: 0.528\n",
      "Epoch 3354, Loss: 19.263, Final Batch Loss: 0.586\n",
      "Epoch 3355, Loss: 19.572, Final Batch Loss: 0.551\n",
      "Epoch 3356, Loss: 19.644, Final Batch Loss: 0.485\n",
      "Epoch 3357, Loss: 19.591, Final Batch Loss: 0.497\n",
      "Epoch 3358, Loss: 19.598, Final Batch Loss: 0.534\n",
      "Epoch 3359, Loss: 19.685, Final Batch Loss: 0.549\n",
      "Epoch 3360, Loss: 19.477, Final Batch Loss: 0.581\n",
      "Epoch 3361, Loss: 19.631, Final Batch Loss: 0.569\n",
      "Epoch 3362, Loss: 19.423, Final Batch Loss: 0.382\n",
      "Epoch 3363, Loss: 19.751, Final Batch Loss: 0.666\n",
      "Epoch 3364, Loss: 19.633, Final Batch Loss: 0.628\n",
      "Epoch 3365, Loss: 19.722, Final Batch Loss: 0.609\n",
      "Epoch 3366, Loss: 19.719, Final Batch Loss: 0.637\n",
      "Epoch 3367, Loss: 19.610, Final Batch Loss: 0.461\n",
      "Epoch 3368, Loss: 19.496, Final Batch Loss: 0.425\n",
      "Epoch 3369, Loss: 19.502, Final Batch Loss: 0.569\n",
      "Epoch 3370, Loss: 19.483, Final Batch Loss: 0.444\n",
      "Epoch 3371, Loss: 19.889, Final Batch Loss: 0.500\n",
      "Epoch 3372, Loss: 19.696, Final Batch Loss: 0.622\n",
      "Epoch 3373, Loss: 19.721, Final Batch Loss: 0.686\n",
      "Epoch 3374, Loss: 19.473, Final Batch Loss: 0.470\n",
      "Epoch 3375, Loss: 19.827, Final Batch Loss: 0.554\n",
      "Epoch 3376, Loss: 19.700, Final Batch Loss: 0.539\n",
      "Epoch 3377, Loss: 19.773, Final Batch Loss: 0.574\n",
      "Epoch 3378, Loss: 19.503, Final Batch Loss: 0.456\n",
      "Epoch 3379, Loss: 19.509, Final Batch Loss: 0.558\n",
      "Epoch 3380, Loss: 19.529, Final Batch Loss: 0.534\n",
      "Epoch 3381, Loss: 19.575, Final Batch Loss: 0.636\n",
      "Epoch 3382, Loss: 19.605, Final Batch Loss: 0.510\n",
      "Epoch 3383, Loss: 19.672, Final Batch Loss: 0.605\n",
      "Epoch 3384, Loss: 19.437, Final Batch Loss: 0.484\n",
      "Epoch 3385, Loss: 19.883, Final Batch Loss: 0.587\n",
      "Epoch 3386, Loss: 19.750, Final Batch Loss: 0.582\n",
      "Epoch 3387, Loss: 19.611, Final Batch Loss: 0.450\n",
      "Epoch 3388, Loss: 19.712, Final Batch Loss: 0.658\n",
      "Epoch 3389, Loss: 19.418, Final Batch Loss: 0.580\n",
      "Epoch 3390, Loss: 19.693, Final Batch Loss: 0.548\n",
      "Epoch 3391, Loss: 19.571, Final Batch Loss: 0.444\n",
      "Epoch 3392, Loss: 19.674, Final Batch Loss: 0.535\n",
      "Epoch 3393, Loss: 19.554, Final Batch Loss: 0.568\n",
      "Epoch 3394, Loss: 19.725, Final Batch Loss: 0.640\n",
      "Epoch 3395, Loss: 19.765, Final Batch Loss: 0.528\n",
      "Epoch 3396, Loss: 19.468, Final Batch Loss: 0.481\n",
      "Epoch 3397, Loss: 19.947, Final Batch Loss: 0.478\n",
      "Epoch 3398, Loss: 19.729, Final Batch Loss: 0.602\n",
      "Epoch 3399, Loss: 19.608, Final Batch Loss: 0.531\n",
      "Epoch 3400, Loss: 19.756, Final Batch Loss: 0.550\n",
      "Epoch 3401, Loss: 19.452, Final Batch Loss: 0.520\n",
      "Epoch 3402, Loss: 19.704, Final Batch Loss: 0.416\n",
      "Epoch 3403, Loss: 19.533, Final Batch Loss: 0.497\n",
      "Epoch 3404, Loss: 19.703, Final Batch Loss: 0.541\n",
      "Epoch 3405, Loss: 19.516, Final Batch Loss: 0.458\n",
      "Epoch 3406, Loss: 19.577, Final Batch Loss: 0.678\n",
      "Epoch 3407, Loss: 19.778, Final Batch Loss: 0.587\n",
      "Epoch 3408, Loss: 19.704, Final Batch Loss: 0.496\n",
      "Epoch 3409, Loss: 19.597, Final Batch Loss: 0.656\n",
      "Epoch 3410, Loss: 19.568, Final Batch Loss: 0.532\n",
      "Epoch 3411, Loss: 19.677, Final Batch Loss: 0.541\n",
      "Epoch 3412, Loss: 19.938, Final Batch Loss: 0.611\n",
      "Epoch 3413, Loss: 19.724, Final Batch Loss: 0.543\n",
      "Epoch 3414, Loss: 19.556, Final Batch Loss: 0.510\n",
      "Epoch 3415, Loss: 19.657, Final Batch Loss: 0.503\n",
      "Epoch 3416, Loss: 19.514, Final Batch Loss: 0.620\n",
      "Epoch 3417, Loss: 19.534, Final Batch Loss: 0.535\n",
      "Epoch 3418, Loss: 19.920, Final Batch Loss: 0.582\n",
      "Epoch 3419, Loss: 19.630, Final Batch Loss: 0.689\n",
      "Epoch 3420, Loss: 19.361, Final Batch Loss: 0.496\n",
      "Epoch 3421, Loss: 19.534, Final Batch Loss: 0.471\n",
      "Epoch 3422, Loss: 19.773, Final Batch Loss: 0.681\n",
      "Epoch 3423, Loss: 19.840, Final Batch Loss: 0.483\n",
      "Epoch 3424, Loss: 19.425, Final Batch Loss: 0.461\n",
      "Epoch 3425, Loss: 19.650, Final Batch Loss: 0.619\n",
      "Epoch 3426, Loss: 19.958, Final Batch Loss: 0.499\n",
      "Epoch 3427, Loss: 19.516, Final Batch Loss: 0.486\n",
      "Epoch 3428, Loss: 19.336, Final Batch Loss: 0.555\n",
      "Epoch 3429, Loss: 19.429, Final Batch Loss: 0.447\n",
      "Epoch 3430, Loss: 19.496, Final Batch Loss: 0.442\n",
      "Epoch 3431, Loss: 19.465, Final Batch Loss: 0.473\n",
      "Epoch 3432, Loss: 19.486, Final Batch Loss: 0.507\n",
      "Epoch 3433, Loss: 19.495, Final Batch Loss: 0.574\n",
      "Epoch 3434, Loss: 19.612, Final Batch Loss: 0.474\n",
      "Epoch 3435, Loss: 19.627, Final Batch Loss: 0.516\n",
      "Epoch 3436, Loss: 19.797, Final Batch Loss: 0.681\n",
      "Epoch 3437, Loss: 19.292, Final Batch Loss: 0.474\n",
      "Epoch 3438, Loss: 19.592, Final Batch Loss: 0.546\n",
      "Epoch 3439, Loss: 19.477, Final Batch Loss: 0.559\n",
      "Epoch 3440, Loss: 19.455, Final Batch Loss: 0.513\n",
      "Epoch 3441, Loss: 19.481, Final Batch Loss: 0.482\n",
      "Epoch 3442, Loss: 19.646, Final Batch Loss: 0.562\n",
      "Epoch 3443, Loss: 19.369, Final Batch Loss: 0.557\n",
      "Epoch 3444, Loss: 19.283, Final Batch Loss: 0.514\n",
      "Epoch 3445, Loss: 19.417, Final Batch Loss: 0.521\n",
      "Epoch 3446, Loss: 19.687, Final Batch Loss: 0.502\n",
      "Epoch 3447, Loss: 19.592, Final Batch Loss: 0.516\n",
      "Epoch 3448, Loss: 19.726, Final Batch Loss: 0.452\n",
      "Epoch 3449, Loss: 19.449, Final Batch Loss: 0.491\n",
      "Epoch 3450, Loss: 19.541, Final Batch Loss: 0.583\n",
      "Epoch 3451, Loss: 19.488, Final Batch Loss: 0.465\n",
      "Epoch 3452, Loss: 19.393, Final Batch Loss: 0.626\n",
      "Epoch 3453, Loss: 19.731, Final Batch Loss: 0.536\n",
      "Epoch 3454, Loss: 19.668, Final Batch Loss: 0.519\n",
      "Epoch 3455, Loss: 19.527, Final Batch Loss: 0.532\n",
      "Epoch 3456, Loss: 19.657, Final Batch Loss: 0.438\n",
      "Epoch 3457, Loss: 19.678, Final Batch Loss: 0.498\n",
      "Epoch 3458, Loss: 19.886, Final Batch Loss: 0.526\n",
      "Epoch 3459, Loss: 19.643, Final Batch Loss: 0.457\n",
      "Epoch 3460, Loss: 19.611, Final Batch Loss: 0.553\n",
      "Epoch 3461, Loss: 19.666, Final Batch Loss: 0.534\n",
      "Epoch 3462, Loss: 19.762, Final Batch Loss: 0.682\n",
      "Epoch 3463, Loss: 19.415, Final Batch Loss: 0.615\n",
      "Epoch 3464, Loss: 19.440, Final Batch Loss: 0.506\n",
      "Epoch 3465, Loss: 19.490, Final Batch Loss: 0.534\n",
      "Epoch 3466, Loss: 19.841, Final Batch Loss: 0.551\n",
      "Epoch 3467, Loss: 19.660, Final Batch Loss: 0.411\n",
      "Epoch 3468, Loss: 19.406, Final Batch Loss: 0.573\n",
      "Epoch 3469, Loss: 19.535, Final Batch Loss: 0.410\n",
      "Epoch 3470, Loss: 19.720, Final Batch Loss: 0.508\n",
      "Epoch 3471, Loss: 19.734, Final Batch Loss: 0.451\n",
      "Epoch 3472, Loss: 19.527, Final Batch Loss: 0.514\n",
      "Epoch 3473, Loss: 19.521, Final Batch Loss: 0.527\n",
      "Epoch 3474, Loss: 19.571, Final Batch Loss: 0.476\n",
      "Epoch 3475, Loss: 19.673, Final Batch Loss: 0.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3476, Loss: 19.625, Final Batch Loss: 0.460\n",
      "Epoch 3477, Loss: 19.613, Final Batch Loss: 0.484\n",
      "Epoch 3478, Loss: 19.649, Final Batch Loss: 0.532\n",
      "Epoch 3479, Loss: 19.496, Final Batch Loss: 0.470\n",
      "Epoch 3480, Loss: 19.647, Final Batch Loss: 0.483\n",
      "Epoch 3481, Loss: 19.584, Final Batch Loss: 0.648\n",
      "Epoch 3482, Loss: 19.744, Final Batch Loss: 0.551\n",
      "Epoch 3483, Loss: 19.489, Final Batch Loss: 0.523\n",
      "Epoch 3484, Loss: 19.437, Final Batch Loss: 0.541\n",
      "Epoch 3485, Loss: 19.696, Final Batch Loss: 0.586\n",
      "Epoch 3486, Loss: 19.310, Final Batch Loss: 0.481\n",
      "Epoch 3487, Loss: 19.805, Final Batch Loss: 0.677\n",
      "Epoch 3488, Loss: 19.471, Final Batch Loss: 0.532\n",
      "Epoch 3489, Loss: 19.491, Final Batch Loss: 0.483\n",
      "Epoch 3490, Loss: 19.632, Final Batch Loss: 0.561\n",
      "Epoch 3491, Loss: 19.806, Final Batch Loss: 0.513\n",
      "Epoch 3492, Loss: 19.433, Final Batch Loss: 0.427\n",
      "Epoch 3493, Loss: 19.686, Final Batch Loss: 0.554\n",
      "Epoch 3494, Loss: 19.560, Final Batch Loss: 0.551\n",
      "Epoch 3495, Loss: 19.636, Final Batch Loss: 0.515\n",
      "Epoch 3496, Loss: 19.490, Final Batch Loss: 0.588\n",
      "Epoch 3497, Loss: 19.703, Final Batch Loss: 0.639\n",
      "Epoch 3498, Loss: 19.524, Final Batch Loss: 0.413\n",
      "Epoch 3499, Loss: 19.788, Final Batch Loss: 0.514\n",
      "Epoch 3500, Loss: 19.472, Final Batch Loss: 0.418\n",
      "Epoch 3501, Loss: 19.421, Final Batch Loss: 0.471\n",
      "Epoch 3502, Loss: 19.496, Final Batch Loss: 0.460\n",
      "Epoch 3503, Loss: 19.382, Final Batch Loss: 0.519\n",
      "Epoch 3504, Loss: 19.475, Final Batch Loss: 0.539\n",
      "Epoch 3505, Loss: 19.595, Final Batch Loss: 0.550\n",
      "Epoch 3506, Loss: 19.227, Final Batch Loss: 0.460\n",
      "Epoch 3507, Loss: 19.752, Final Batch Loss: 0.536\n",
      "Epoch 3508, Loss: 19.661, Final Batch Loss: 0.541\n",
      "Epoch 3509, Loss: 19.235, Final Batch Loss: 0.472\n",
      "Epoch 3510, Loss: 19.475, Final Batch Loss: 0.518\n",
      "Epoch 3511, Loss: 19.510, Final Batch Loss: 0.560\n",
      "Epoch 3512, Loss: 19.385, Final Batch Loss: 0.503\n",
      "Epoch 3513, Loss: 19.480, Final Batch Loss: 0.623\n",
      "Epoch 3514, Loss: 19.438, Final Batch Loss: 0.562\n",
      "Epoch 3515, Loss: 19.458, Final Batch Loss: 0.593\n",
      "Epoch 3516, Loss: 19.323, Final Batch Loss: 0.442\n",
      "Epoch 3517, Loss: 19.320, Final Batch Loss: 0.499\n",
      "Epoch 3518, Loss: 19.500, Final Batch Loss: 0.598\n",
      "Epoch 3519, Loss: 19.416, Final Batch Loss: 0.532\n",
      "Epoch 3520, Loss: 19.454, Final Batch Loss: 0.491\n",
      "Epoch 3521, Loss: 19.361, Final Batch Loss: 0.519\n",
      "Epoch 3522, Loss: 19.331, Final Batch Loss: 0.530\n",
      "Epoch 3523, Loss: 19.588, Final Batch Loss: 0.536\n",
      "Epoch 3524, Loss: 19.470, Final Batch Loss: 0.567\n",
      "Epoch 3525, Loss: 19.570, Final Batch Loss: 0.480\n",
      "Epoch 3526, Loss: 19.564, Final Batch Loss: 0.484\n",
      "Epoch 3527, Loss: 19.395, Final Batch Loss: 0.528\n",
      "Epoch 3528, Loss: 19.574, Final Batch Loss: 0.561\n",
      "Epoch 3529, Loss: 19.766, Final Batch Loss: 0.553\n",
      "Epoch 3530, Loss: 19.429, Final Batch Loss: 0.586\n",
      "Epoch 3531, Loss: 19.434, Final Batch Loss: 0.557\n",
      "Epoch 3532, Loss: 19.560, Final Batch Loss: 0.613\n",
      "Epoch 3533, Loss: 19.169, Final Batch Loss: 0.635\n",
      "Epoch 3534, Loss: 19.454, Final Batch Loss: 0.664\n",
      "Epoch 3535, Loss: 19.571, Final Batch Loss: 0.591\n",
      "Epoch 3536, Loss: 19.662, Final Batch Loss: 0.566\n",
      "Epoch 3537, Loss: 19.910, Final Batch Loss: 0.553\n",
      "Epoch 3538, Loss: 19.709, Final Batch Loss: 0.670\n",
      "Epoch 3539, Loss: 19.448, Final Batch Loss: 0.523\n",
      "Epoch 3540, Loss: 19.533, Final Batch Loss: 0.548\n",
      "Epoch 3541, Loss: 19.347, Final Batch Loss: 0.580\n",
      "Epoch 3542, Loss: 19.632, Final Batch Loss: 0.529\n",
      "Epoch 3543, Loss: 19.623, Final Batch Loss: 0.469\n",
      "Epoch 3544, Loss: 19.433, Final Batch Loss: 0.473\n",
      "Epoch 3545, Loss: 19.657, Final Batch Loss: 0.466\n",
      "Epoch 3546, Loss: 19.665, Final Batch Loss: 0.509\n",
      "Epoch 3547, Loss: 19.400, Final Batch Loss: 0.580\n",
      "Epoch 3548, Loss: 19.488, Final Batch Loss: 0.526\n",
      "Epoch 3549, Loss: 19.345, Final Batch Loss: 0.474\n",
      "Epoch 3550, Loss: 19.578, Final Batch Loss: 0.708\n",
      "Epoch 3551, Loss: 19.397, Final Batch Loss: 0.392\n",
      "Epoch 3552, Loss: 19.441, Final Batch Loss: 0.588\n",
      "Epoch 3553, Loss: 19.638, Final Batch Loss: 0.519\n",
      "Epoch 3554, Loss: 19.258, Final Batch Loss: 0.409\n",
      "Epoch 3555, Loss: 19.445, Final Batch Loss: 0.548\n",
      "Epoch 3556, Loss: 19.702, Final Batch Loss: 0.539\n",
      "Epoch 3557, Loss: 19.567, Final Batch Loss: 0.604\n",
      "Epoch 3558, Loss: 19.693, Final Batch Loss: 0.716\n",
      "Epoch 3559, Loss: 19.468, Final Batch Loss: 0.578\n",
      "Epoch 3560, Loss: 19.613, Final Batch Loss: 0.573\n",
      "Epoch 3561, Loss: 19.636, Final Batch Loss: 0.567\n",
      "Epoch 3562, Loss: 19.504, Final Batch Loss: 0.427\n",
      "Epoch 3563, Loss: 19.568, Final Batch Loss: 0.545\n",
      "Epoch 3564, Loss: 19.558, Final Batch Loss: 0.584\n",
      "Epoch 3565, Loss: 19.524, Final Batch Loss: 0.583\n",
      "Epoch 3566, Loss: 19.337, Final Batch Loss: 0.591\n",
      "Epoch 3567, Loss: 19.496, Final Batch Loss: 0.562\n",
      "Epoch 3568, Loss: 19.552, Final Batch Loss: 0.531\n",
      "Epoch 3569, Loss: 19.496, Final Batch Loss: 0.501\n",
      "Epoch 3570, Loss: 19.317, Final Batch Loss: 0.483\n",
      "Epoch 3571, Loss: 19.589, Final Batch Loss: 0.632\n",
      "Epoch 3572, Loss: 19.449, Final Batch Loss: 0.575\n",
      "Epoch 3573, Loss: 19.614, Final Batch Loss: 0.555\n",
      "Epoch 3574, Loss: 19.310, Final Batch Loss: 0.577\n",
      "Epoch 3575, Loss: 19.447, Final Batch Loss: 0.502\n",
      "Epoch 3576, Loss: 19.545, Final Batch Loss: 0.697\n",
      "Epoch 3577, Loss: 19.531, Final Batch Loss: 0.570\n",
      "Epoch 3578, Loss: 19.587, Final Batch Loss: 0.633\n",
      "Epoch 3579, Loss: 19.678, Final Batch Loss: 0.488\n",
      "Epoch 3580, Loss: 19.294, Final Batch Loss: 0.436\n",
      "Epoch 3581, Loss: 19.582, Final Batch Loss: 0.542\n",
      "Epoch 3582, Loss: 19.582, Final Batch Loss: 0.494\n",
      "Epoch 3583, Loss: 19.507, Final Batch Loss: 0.554\n",
      "Epoch 3584, Loss: 19.687, Final Batch Loss: 0.475\n",
      "Epoch 3585, Loss: 19.693, Final Batch Loss: 0.571\n",
      "Epoch 3586, Loss: 19.490, Final Batch Loss: 0.601\n",
      "Epoch 3587, Loss: 19.561, Final Batch Loss: 0.630\n",
      "Epoch 3588, Loss: 19.598, Final Batch Loss: 0.598\n",
      "Epoch 3589, Loss: 19.379, Final Batch Loss: 0.492\n",
      "Epoch 3590, Loss: 19.452, Final Batch Loss: 0.562\n",
      "Epoch 3591, Loss: 19.420, Final Batch Loss: 0.518\n",
      "Epoch 3592, Loss: 19.668, Final Batch Loss: 0.463\n",
      "Epoch 3593, Loss: 19.566, Final Batch Loss: 0.554\n",
      "Epoch 3594, Loss: 19.657, Final Batch Loss: 0.670\n",
      "Epoch 3595, Loss: 19.658, Final Batch Loss: 0.611\n",
      "Epoch 3596, Loss: 19.642, Final Batch Loss: 0.563\n",
      "Epoch 3597, Loss: 19.707, Final Batch Loss: 0.672\n",
      "Epoch 3598, Loss: 19.323, Final Batch Loss: 0.569\n",
      "Epoch 3599, Loss: 19.514, Final Batch Loss: 0.600\n",
      "Epoch 3600, Loss: 19.622, Final Batch Loss: 0.516\n",
      "Epoch 3601, Loss: 19.559, Final Batch Loss: 0.511\n",
      "Epoch 3602, Loss: 19.569, Final Batch Loss: 0.507\n",
      "Epoch 3603, Loss: 19.564, Final Batch Loss: 0.562\n",
      "Epoch 3604, Loss: 19.460, Final Batch Loss: 0.534\n",
      "Epoch 3605, Loss: 19.574, Final Batch Loss: 0.551\n",
      "Epoch 3606, Loss: 19.328, Final Batch Loss: 0.516\n",
      "Epoch 3607, Loss: 19.490, Final Batch Loss: 0.573\n",
      "Epoch 3608, Loss: 19.454, Final Batch Loss: 0.543\n",
      "Epoch 3609, Loss: 19.566, Final Batch Loss: 0.553\n",
      "Epoch 3610, Loss: 19.364, Final Batch Loss: 0.476\n",
      "Epoch 3611, Loss: 19.379, Final Batch Loss: 0.683\n",
      "Epoch 3612, Loss: 19.472, Final Batch Loss: 0.550\n",
      "Epoch 3613, Loss: 19.400, Final Batch Loss: 0.482\n",
      "Epoch 3614, Loss: 19.505, Final Batch Loss: 0.578\n",
      "Epoch 3615, Loss: 19.613, Final Batch Loss: 0.668\n",
      "Epoch 3616, Loss: 19.357, Final Batch Loss: 0.562\n",
      "Epoch 3617, Loss: 19.479, Final Batch Loss: 0.447\n",
      "Epoch 3618, Loss: 19.568, Final Batch Loss: 0.565\n",
      "Epoch 3619, Loss: 19.668, Final Batch Loss: 0.496\n",
      "Epoch 3620, Loss: 19.251, Final Batch Loss: 0.492\n",
      "Epoch 3621, Loss: 19.478, Final Batch Loss: 0.586\n",
      "Epoch 3622, Loss: 19.596, Final Batch Loss: 0.559\n",
      "Epoch 3623, Loss: 19.381, Final Batch Loss: 0.492\n",
      "Epoch 3624, Loss: 19.493, Final Batch Loss: 0.536\n",
      "Epoch 3625, Loss: 19.599, Final Batch Loss: 0.538\n",
      "Epoch 3626, Loss: 19.493, Final Batch Loss: 0.515\n",
      "Epoch 3627, Loss: 19.491, Final Batch Loss: 0.488\n",
      "Epoch 3628, Loss: 19.565, Final Batch Loss: 0.537\n",
      "Epoch 3629, Loss: 19.423, Final Batch Loss: 0.627\n",
      "Epoch 3630, Loss: 19.367, Final Batch Loss: 0.508\n",
      "Epoch 3631, Loss: 19.456, Final Batch Loss: 0.528\n",
      "Epoch 3632, Loss: 19.460, Final Batch Loss: 0.488\n",
      "Epoch 3633, Loss: 19.611, Final Batch Loss: 0.452\n",
      "Epoch 3634, Loss: 19.424, Final Batch Loss: 0.619\n",
      "Epoch 3635, Loss: 19.178, Final Batch Loss: 0.556\n",
      "Epoch 3636, Loss: 19.441, Final Batch Loss: 0.499\n",
      "Epoch 3637, Loss: 19.346, Final Batch Loss: 0.483\n",
      "Epoch 3638, Loss: 19.436, Final Batch Loss: 0.431\n",
      "Epoch 3639, Loss: 19.574, Final Batch Loss: 0.523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3640, Loss: 19.766, Final Batch Loss: 0.521\n",
      "Epoch 3641, Loss: 19.532, Final Batch Loss: 0.564\n",
      "Epoch 3642, Loss: 19.526, Final Batch Loss: 0.456\n",
      "Epoch 3643, Loss: 19.426, Final Batch Loss: 0.570\n",
      "Epoch 3644, Loss: 19.507, Final Batch Loss: 0.564\n",
      "Epoch 3645, Loss: 19.357, Final Batch Loss: 0.600\n",
      "Epoch 3646, Loss: 19.484, Final Batch Loss: 0.607\n",
      "Epoch 3647, Loss: 19.496, Final Batch Loss: 0.526\n",
      "Epoch 3648, Loss: 19.359, Final Batch Loss: 0.532\n",
      "Epoch 3649, Loss: 19.682, Final Batch Loss: 0.489\n",
      "Epoch 3650, Loss: 19.661, Final Batch Loss: 0.446\n",
      "Epoch 3651, Loss: 19.305, Final Batch Loss: 0.581\n",
      "Epoch 3652, Loss: 19.354, Final Batch Loss: 0.543\n",
      "Epoch 3653, Loss: 19.468, Final Batch Loss: 0.492\n",
      "Epoch 3654, Loss: 19.416, Final Batch Loss: 0.551\n",
      "Epoch 3655, Loss: 19.483, Final Batch Loss: 0.477\n",
      "Epoch 3656, Loss: 19.513, Final Batch Loss: 0.612\n",
      "Epoch 3657, Loss: 19.549, Final Batch Loss: 0.588\n",
      "Epoch 3658, Loss: 19.634, Final Batch Loss: 0.552\n",
      "Epoch 3659, Loss: 19.562, Final Batch Loss: 0.530\n",
      "Epoch 3660, Loss: 19.543, Final Batch Loss: 0.441\n",
      "Epoch 3661, Loss: 19.567, Final Batch Loss: 0.513\n",
      "Epoch 3662, Loss: 19.317, Final Batch Loss: 0.530\n",
      "Epoch 3663, Loss: 19.264, Final Batch Loss: 0.528\n",
      "Epoch 3664, Loss: 19.610, Final Batch Loss: 0.368\n",
      "Epoch 3665, Loss: 19.424, Final Batch Loss: 0.650\n",
      "Epoch 3666, Loss: 19.882, Final Batch Loss: 0.584\n",
      "Epoch 3667, Loss: 19.515, Final Batch Loss: 0.531\n",
      "Epoch 3668, Loss: 19.481, Final Batch Loss: 0.490\n",
      "Epoch 3669, Loss: 19.462, Final Batch Loss: 0.543\n",
      "Epoch 3670, Loss: 19.539, Final Batch Loss: 0.640\n",
      "Epoch 3671, Loss: 19.670, Final Batch Loss: 0.603\n",
      "Epoch 3672, Loss: 19.267, Final Batch Loss: 0.491\n",
      "Epoch 3673, Loss: 19.518, Final Batch Loss: 0.735\n",
      "Epoch 3674, Loss: 19.545, Final Batch Loss: 0.582\n",
      "Epoch 3675, Loss: 19.704, Final Batch Loss: 0.491\n",
      "Epoch 3676, Loss: 19.609, Final Batch Loss: 0.535\n",
      "Epoch 3677, Loss: 19.553, Final Batch Loss: 0.510\n",
      "Epoch 3678, Loss: 19.379, Final Batch Loss: 0.552\n",
      "Epoch 3679, Loss: 19.357, Final Batch Loss: 0.536\n",
      "Epoch 3680, Loss: 19.583, Final Batch Loss: 0.561\n",
      "Epoch 3681, Loss: 19.445, Final Batch Loss: 0.502\n",
      "Epoch 3682, Loss: 19.400, Final Batch Loss: 0.468\n",
      "Epoch 3683, Loss: 19.762, Final Batch Loss: 0.620\n",
      "Epoch 3684, Loss: 19.410, Final Batch Loss: 0.585\n",
      "Epoch 3685, Loss: 19.580, Final Batch Loss: 0.659\n",
      "Epoch 3686, Loss: 19.607, Final Batch Loss: 0.446\n",
      "Epoch 3687, Loss: 19.635, Final Batch Loss: 0.597\n",
      "Epoch 3688, Loss: 19.383, Final Batch Loss: 0.543\n",
      "Epoch 3689, Loss: 19.485, Final Batch Loss: 0.566\n",
      "Epoch 3690, Loss: 19.344, Final Batch Loss: 0.547\n",
      "Epoch 3691, Loss: 19.276, Final Batch Loss: 0.438\n",
      "Epoch 3692, Loss: 19.351, Final Batch Loss: 0.618\n",
      "Epoch 3693, Loss: 19.202, Final Batch Loss: 0.534\n",
      "Epoch 3694, Loss: 19.775, Final Batch Loss: 0.546\n",
      "Epoch 3695, Loss: 19.565, Final Batch Loss: 0.515\n",
      "Epoch 3696, Loss: 19.461, Final Batch Loss: 0.459\n",
      "Epoch 3697, Loss: 19.305, Final Batch Loss: 0.519\n",
      "Epoch 3698, Loss: 19.468, Final Batch Loss: 0.540\n",
      "Epoch 3699, Loss: 19.476, Final Batch Loss: 0.529\n",
      "Epoch 3700, Loss: 19.569, Final Batch Loss: 0.610\n",
      "Epoch 3701, Loss: 19.486, Final Batch Loss: 0.515\n",
      "Epoch 3702, Loss: 19.536, Final Batch Loss: 0.493\n",
      "Epoch 3703, Loss: 19.362, Final Batch Loss: 0.558\n",
      "Epoch 3704, Loss: 19.532, Final Batch Loss: 0.469\n",
      "Epoch 3705, Loss: 19.583, Final Batch Loss: 0.514\n",
      "Epoch 3706, Loss: 19.299, Final Batch Loss: 0.575\n",
      "Epoch 3707, Loss: 19.144, Final Batch Loss: 0.485\n",
      "Epoch 3708, Loss: 19.411, Final Batch Loss: 0.545\n",
      "Epoch 3709, Loss: 19.412, Final Batch Loss: 0.509\n",
      "Epoch 3710, Loss: 19.216, Final Batch Loss: 0.594\n",
      "Epoch 3711, Loss: 19.349, Final Batch Loss: 0.507\n",
      "Epoch 3712, Loss: 19.717, Final Batch Loss: 0.664\n",
      "Epoch 3713, Loss: 19.264, Final Batch Loss: 0.507\n",
      "Epoch 3714, Loss: 19.412, Final Batch Loss: 0.500\n",
      "Epoch 3715, Loss: 19.605, Final Batch Loss: 0.485\n",
      "Epoch 3716, Loss: 19.466, Final Batch Loss: 0.474\n",
      "Epoch 3717, Loss: 19.730, Final Batch Loss: 0.608\n",
      "Epoch 3718, Loss: 19.297, Final Batch Loss: 0.517\n",
      "Epoch 3719, Loss: 19.362, Final Batch Loss: 0.572\n",
      "Epoch 3720, Loss: 19.547, Final Batch Loss: 0.594\n",
      "Epoch 3721, Loss: 19.350, Final Batch Loss: 0.455\n",
      "Epoch 3722, Loss: 19.426, Final Batch Loss: 0.601\n",
      "Epoch 3723, Loss: 19.340, Final Batch Loss: 0.687\n",
      "Epoch 3724, Loss: 19.389, Final Batch Loss: 0.489\n",
      "Epoch 3725, Loss: 19.732, Final Batch Loss: 0.514\n",
      "Epoch 3726, Loss: 19.510, Final Batch Loss: 0.592\n",
      "Epoch 3727, Loss: 19.540, Final Batch Loss: 0.520\n",
      "Epoch 3728, Loss: 19.270, Final Batch Loss: 0.536\n",
      "Epoch 3729, Loss: 19.651, Final Batch Loss: 0.586\n",
      "Epoch 3730, Loss: 19.463, Final Batch Loss: 0.494\n",
      "Epoch 3731, Loss: 19.504, Final Batch Loss: 0.520\n",
      "Epoch 3732, Loss: 19.708, Final Batch Loss: 0.581\n",
      "Epoch 3733, Loss: 19.347, Final Batch Loss: 0.510\n",
      "Epoch 3734, Loss: 19.541, Final Batch Loss: 0.483\n",
      "Epoch 3735, Loss: 19.438, Final Batch Loss: 0.487\n",
      "Epoch 3736, Loss: 19.523, Final Batch Loss: 0.529\n",
      "Epoch 3737, Loss: 19.485, Final Batch Loss: 0.518\n",
      "Epoch 3738, Loss: 19.582, Final Batch Loss: 0.530\n",
      "Epoch 3739, Loss: 19.348, Final Batch Loss: 0.542\n",
      "Epoch 3740, Loss: 19.352, Final Batch Loss: 0.563\n",
      "Epoch 3741, Loss: 19.719, Final Batch Loss: 0.593\n",
      "Epoch 3742, Loss: 19.325, Final Batch Loss: 0.706\n",
      "Epoch 3743, Loss: 19.332, Final Batch Loss: 0.475\n",
      "Epoch 3744, Loss: 19.319, Final Batch Loss: 0.573\n",
      "Epoch 3745, Loss: 19.620, Final Batch Loss: 0.526\n",
      "Epoch 3746, Loss: 19.608, Final Batch Loss: 0.491\n",
      "Epoch 3747, Loss: 19.268, Final Batch Loss: 0.515\n",
      "Epoch 3748, Loss: 19.641, Final Batch Loss: 0.547\n",
      "Epoch 3749, Loss: 19.272, Final Batch Loss: 0.602\n",
      "Epoch 3750, Loss: 19.488, Final Batch Loss: 0.617\n",
      "Epoch 3751, Loss: 19.337, Final Batch Loss: 0.557\n",
      "Epoch 3752, Loss: 19.458, Final Batch Loss: 0.501\n",
      "Epoch 3753, Loss: 19.403, Final Batch Loss: 0.565\n",
      "Epoch 3754, Loss: 19.424, Final Batch Loss: 0.526\n",
      "Epoch 3755, Loss: 19.440, Final Batch Loss: 0.482\n",
      "Epoch 3756, Loss: 19.426, Final Batch Loss: 0.489\n",
      "Epoch 3757, Loss: 19.594, Final Batch Loss: 0.623\n",
      "Epoch 3758, Loss: 19.410, Final Batch Loss: 0.479\n",
      "Epoch 3759, Loss: 19.319, Final Batch Loss: 0.517\n",
      "Epoch 3760, Loss: 19.509, Final Batch Loss: 0.561\n",
      "Epoch 3761, Loss: 19.447, Final Batch Loss: 0.469\n",
      "Epoch 3762, Loss: 19.529, Final Batch Loss: 0.496\n",
      "Epoch 3763, Loss: 19.435, Final Batch Loss: 0.620\n",
      "Epoch 3764, Loss: 19.229, Final Batch Loss: 0.497\n",
      "Epoch 3765, Loss: 19.347, Final Batch Loss: 0.408\n",
      "Epoch 3766, Loss: 19.361, Final Batch Loss: 0.538\n",
      "Epoch 3767, Loss: 19.461, Final Batch Loss: 0.539\n",
      "Epoch 3768, Loss: 19.616, Final Batch Loss: 0.630\n",
      "Epoch 3769, Loss: 19.413, Final Batch Loss: 0.494\n",
      "Epoch 3770, Loss: 19.449, Final Batch Loss: 0.592\n",
      "Epoch 3771, Loss: 19.409, Final Batch Loss: 0.553\n",
      "Epoch 3772, Loss: 19.597, Final Batch Loss: 0.664\n",
      "Epoch 3773, Loss: 19.146, Final Batch Loss: 0.550\n",
      "Epoch 3774, Loss: 19.296, Final Batch Loss: 0.525\n",
      "Epoch 3775, Loss: 19.379, Final Batch Loss: 0.559\n",
      "Epoch 3776, Loss: 19.611, Final Batch Loss: 0.719\n",
      "Epoch 3777, Loss: 19.346, Final Batch Loss: 0.468\n",
      "Epoch 3778, Loss: 19.683, Final Batch Loss: 0.580\n",
      "Epoch 3779, Loss: 19.296, Final Batch Loss: 0.420\n",
      "Epoch 3780, Loss: 19.471, Final Batch Loss: 0.527\n",
      "Epoch 3781, Loss: 19.389, Final Batch Loss: 0.584\n",
      "Epoch 3782, Loss: 19.348, Final Batch Loss: 0.561\n",
      "Epoch 3783, Loss: 19.200, Final Batch Loss: 0.465\n",
      "Epoch 3784, Loss: 19.562, Final Batch Loss: 0.526\n",
      "Epoch 3785, Loss: 19.410, Final Batch Loss: 0.621\n",
      "Epoch 3786, Loss: 19.483, Final Batch Loss: 0.539\n",
      "Epoch 3787, Loss: 19.454, Final Batch Loss: 0.583\n",
      "Epoch 3788, Loss: 19.448, Final Batch Loss: 0.561\n",
      "Epoch 3789, Loss: 19.572, Final Batch Loss: 0.587\n",
      "Epoch 3790, Loss: 19.598, Final Batch Loss: 0.506\n",
      "Epoch 3791, Loss: 19.336, Final Batch Loss: 0.559\n",
      "Epoch 3792, Loss: 19.141, Final Batch Loss: 0.480\n",
      "Epoch 3793, Loss: 19.357, Final Batch Loss: 0.635\n",
      "Epoch 3794, Loss: 19.335, Final Batch Loss: 0.541\n",
      "Epoch 3795, Loss: 19.313, Final Batch Loss: 0.561\n",
      "Epoch 3796, Loss: 19.497, Final Batch Loss: 0.617\n",
      "Epoch 3797, Loss: 19.236, Final Batch Loss: 0.543\n",
      "Epoch 3798, Loss: 19.229, Final Batch Loss: 0.566\n",
      "Epoch 3799, Loss: 19.435, Final Batch Loss: 0.550\n",
      "Epoch 3800, Loss: 18.975, Final Batch Loss: 0.524\n",
      "Epoch 3801, Loss: 19.270, Final Batch Loss: 0.598\n",
      "Epoch 3802, Loss: 19.511, Final Batch Loss: 0.451\n",
      "Epoch 3803, Loss: 19.561, Final Batch Loss: 0.496\n",
      "Epoch 3804, Loss: 19.267, Final Batch Loss: 0.564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3805, Loss: 19.371, Final Batch Loss: 0.515\n",
      "Epoch 3806, Loss: 19.158, Final Batch Loss: 0.494\n",
      "Epoch 3807, Loss: 19.366, Final Batch Loss: 0.556\n",
      "Epoch 3808, Loss: 19.350, Final Batch Loss: 0.552\n",
      "Epoch 3809, Loss: 19.157, Final Batch Loss: 0.710\n",
      "Epoch 3810, Loss: 19.353, Final Batch Loss: 0.515\n",
      "Epoch 3811, Loss: 19.443, Final Batch Loss: 0.482\n",
      "Epoch 3812, Loss: 19.514, Final Batch Loss: 0.562\n",
      "Epoch 3813, Loss: 19.618, Final Batch Loss: 0.459\n",
      "Epoch 3814, Loss: 19.770, Final Batch Loss: 0.579\n",
      "Epoch 3815, Loss: 19.462, Final Batch Loss: 0.457\n",
      "Epoch 3816, Loss: 19.385, Final Batch Loss: 0.628\n",
      "Epoch 3817, Loss: 19.476, Final Batch Loss: 0.621\n",
      "Epoch 3818, Loss: 19.495, Final Batch Loss: 0.611\n",
      "Epoch 3819, Loss: 19.727, Final Batch Loss: 0.521\n",
      "Epoch 3820, Loss: 19.626, Final Batch Loss: 0.666\n",
      "Epoch 3821, Loss: 19.279, Final Batch Loss: 0.478\n",
      "Epoch 3822, Loss: 19.234, Final Batch Loss: 0.475\n",
      "Epoch 3823, Loss: 19.394, Final Batch Loss: 0.548\n",
      "Epoch 3824, Loss: 19.396, Final Batch Loss: 0.443\n",
      "Epoch 3825, Loss: 19.351, Final Batch Loss: 0.630\n",
      "Epoch 3826, Loss: 19.184, Final Batch Loss: 0.574\n",
      "Epoch 3827, Loss: 19.405, Final Batch Loss: 0.482\n",
      "Epoch 3828, Loss: 19.289, Final Batch Loss: 0.512\n",
      "Epoch 3829, Loss: 19.491, Final Batch Loss: 0.628\n",
      "Epoch 3830, Loss: 19.360, Final Batch Loss: 0.415\n",
      "Epoch 3831, Loss: 19.454, Final Batch Loss: 0.601\n",
      "Epoch 3832, Loss: 19.123, Final Batch Loss: 0.532\n",
      "Epoch 3833, Loss: 19.543, Final Batch Loss: 0.606\n",
      "Epoch 3834, Loss: 19.525, Final Batch Loss: 0.609\n",
      "Epoch 3835, Loss: 19.356, Final Batch Loss: 0.599\n",
      "Epoch 3836, Loss: 19.425, Final Batch Loss: 0.644\n",
      "Epoch 3837, Loss: 19.333, Final Batch Loss: 0.558\n",
      "Epoch 3838, Loss: 19.422, Final Batch Loss: 0.521\n",
      "Epoch 3839, Loss: 19.529, Final Batch Loss: 0.648\n",
      "Epoch 3840, Loss: 19.330, Final Batch Loss: 0.507\n",
      "Epoch 3841, Loss: 19.337, Final Batch Loss: 0.704\n",
      "Epoch 3842, Loss: 19.357, Final Batch Loss: 0.481\n",
      "Epoch 3843, Loss: 19.564, Final Batch Loss: 0.570\n",
      "Epoch 3844, Loss: 19.218, Final Batch Loss: 0.522\n",
      "Epoch 3845, Loss: 19.672, Final Batch Loss: 0.539\n",
      "Epoch 3846, Loss: 19.357, Final Batch Loss: 0.609\n",
      "Epoch 3847, Loss: 19.299, Final Batch Loss: 0.522\n",
      "Epoch 3848, Loss: 19.332, Final Batch Loss: 0.553\n",
      "Epoch 3849, Loss: 19.645, Final Batch Loss: 0.538\n",
      "Epoch 3850, Loss: 19.512, Final Batch Loss: 0.573\n",
      "Epoch 3851, Loss: 19.302, Final Batch Loss: 0.578\n",
      "Epoch 3852, Loss: 19.390, Final Batch Loss: 0.429\n",
      "Epoch 3853, Loss: 19.564, Final Batch Loss: 0.574\n",
      "Epoch 3854, Loss: 19.558, Final Batch Loss: 0.596\n",
      "Epoch 3855, Loss: 19.532, Final Batch Loss: 0.570\n",
      "Epoch 3856, Loss: 19.059, Final Batch Loss: 0.442\n",
      "Epoch 3857, Loss: 19.312, Final Batch Loss: 0.470\n",
      "Epoch 3858, Loss: 19.574, Final Batch Loss: 0.620\n",
      "Epoch 3859, Loss: 19.395, Final Batch Loss: 0.542\n",
      "Epoch 3860, Loss: 19.406, Final Batch Loss: 0.579\n",
      "Epoch 3861, Loss: 19.257, Final Batch Loss: 0.534\n",
      "Epoch 3862, Loss: 19.566, Final Batch Loss: 0.509\n",
      "Epoch 3863, Loss: 19.119, Final Batch Loss: 0.530\n",
      "Epoch 3864, Loss: 19.340, Final Batch Loss: 0.565\n",
      "Epoch 3865, Loss: 19.515, Final Batch Loss: 0.541\n",
      "Epoch 3866, Loss: 19.496, Final Batch Loss: 0.594\n",
      "Epoch 3867, Loss: 19.422, Final Batch Loss: 0.494\n",
      "Epoch 3868, Loss: 19.584, Final Batch Loss: 0.641\n",
      "Epoch 3869, Loss: 19.387, Final Batch Loss: 0.587\n",
      "Epoch 3870, Loss: 19.345, Final Batch Loss: 0.568\n",
      "Epoch 3871, Loss: 19.405, Final Batch Loss: 0.594\n",
      "Epoch 3872, Loss: 19.466, Final Batch Loss: 0.571\n",
      "Epoch 3873, Loss: 19.256, Final Batch Loss: 0.539\n",
      "Epoch 3874, Loss: 19.346, Final Batch Loss: 0.615\n",
      "Epoch 3875, Loss: 19.335, Final Batch Loss: 0.500\n",
      "Epoch 3876, Loss: 19.315, Final Batch Loss: 0.450\n",
      "Epoch 3877, Loss: 19.192, Final Batch Loss: 0.533\n",
      "Epoch 3878, Loss: 19.288, Final Batch Loss: 0.531\n",
      "Epoch 3879, Loss: 19.390, Final Batch Loss: 0.616\n",
      "Epoch 3880, Loss: 19.384, Final Batch Loss: 0.528\n",
      "Epoch 3881, Loss: 19.461, Final Batch Loss: 0.501\n",
      "Epoch 3882, Loss: 19.313, Final Batch Loss: 0.590\n",
      "Epoch 3883, Loss: 19.211, Final Batch Loss: 0.452\n",
      "Epoch 3884, Loss: 19.392, Final Batch Loss: 0.514\n",
      "Epoch 3885, Loss: 19.356, Final Batch Loss: 0.472\n",
      "Epoch 3886, Loss: 19.387, Final Batch Loss: 0.564\n",
      "Epoch 3887, Loss: 19.319, Final Batch Loss: 0.534\n",
      "Epoch 3888, Loss: 19.393, Final Batch Loss: 0.613\n",
      "Epoch 3889, Loss: 19.473, Final Batch Loss: 0.437\n",
      "Epoch 3890, Loss: 19.345, Final Batch Loss: 0.520\n",
      "Epoch 3891, Loss: 19.365, Final Batch Loss: 0.471\n",
      "Epoch 3892, Loss: 19.484, Final Batch Loss: 0.499\n",
      "Epoch 3893, Loss: 19.537, Final Batch Loss: 0.437\n",
      "Epoch 3894, Loss: 19.254, Final Batch Loss: 0.470\n",
      "Epoch 3895, Loss: 19.223, Final Batch Loss: 0.598\n",
      "Epoch 3896, Loss: 19.473, Final Batch Loss: 0.627\n",
      "Epoch 3897, Loss: 19.149, Final Batch Loss: 0.559\n",
      "Epoch 3898, Loss: 19.348, Final Batch Loss: 0.514\n",
      "Epoch 3899, Loss: 19.451, Final Batch Loss: 0.528\n",
      "Epoch 3900, Loss: 19.386, Final Batch Loss: 0.517\n",
      "Epoch 3901, Loss: 19.176, Final Batch Loss: 0.498\n",
      "Epoch 3902, Loss: 19.737, Final Batch Loss: 0.678\n",
      "Epoch 3903, Loss: 19.187, Final Batch Loss: 0.631\n",
      "Epoch 3904, Loss: 19.568, Final Batch Loss: 0.532\n",
      "Epoch 3905, Loss: 19.579, Final Batch Loss: 0.562\n",
      "Epoch 3906, Loss: 19.310, Final Batch Loss: 0.704\n",
      "Epoch 3907, Loss: 19.466, Final Batch Loss: 0.454\n",
      "Epoch 3908, Loss: 19.325, Final Batch Loss: 0.498\n",
      "Epoch 3909, Loss: 19.258, Final Batch Loss: 0.395\n",
      "Epoch 3910, Loss: 19.250, Final Batch Loss: 0.489\n",
      "Epoch 3911, Loss: 19.232, Final Batch Loss: 0.566\n",
      "Epoch 3912, Loss: 19.252, Final Batch Loss: 0.577\n",
      "Epoch 3913, Loss: 19.208, Final Batch Loss: 0.508\n",
      "Epoch 3914, Loss: 19.400, Final Batch Loss: 0.630\n",
      "Epoch 3915, Loss: 19.510, Final Batch Loss: 0.505\n",
      "Epoch 3916, Loss: 19.479, Final Batch Loss: 0.658\n",
      "Epoch 3917, Loss: 19.468, Final Batch Loss: 0.564\n",
      "Epoch 3918, Loss: 19.434, Final Batch Loss: 0.527\n",
      "Epoch 3919, Loss: 19.276, Final Batch Loss: 0.562\n",
      "Epoch 3920, Loss: 19.675, Final Batch Loss: 0.507\n",
      "Epoch 3921, Loss: 19.414, Final Batch Loss: 0.586\n",
      "Epoch 3922, Loss: 19.387, Final Batch Loss: 0.438\n",
      "Epoch 3923, Loss: 19.298, Final Batch Loss: 0.485\n",
      "Epoch 3924, Loss: 19.377, Final Batch Loss: 0.484\n",
      "Epoch 3925, Loss: 19.427, Final Batch Loss: 0.575\n",
      "Epoch 3926, Loss: 19.215, Final Batch Loss: 0.431\n",
      "Epoch 3927, Loss: 19.138, Final Batch Loss: 0.421\n",
      "Epoch 3928, Loss: 19.142, Final Batch Loss: 0.470\n",
      "Epoch 3929, Loss: 19.273, Final Batch Loss: 0.500\n",
      "Epoch 3930, Loss: 19.269, Final Batch Loss: 0.554\n",
      "Epoch 3931, Loss: 19.314, Final Batch Loss: 0.442\n",
      "Epoch 3932, Loss: 19.364, Final Batch Loss: 0.419\n",
      "Epoch 3933, Loss: 19.065, Final Batch Loss: 0.466\n",
      "Epoch 3934, Loss: 19.221, Final Batch Loss: 0.517\n",
      "Epoch 3935, Loss: 19.561, Final Batch Loss: 0.590\n",
      "Epoch 3936, Loss: 19.088, Final Batch Loss: 0.540\n",
      "Epoch 3937, Loss: 19.211, Final Batch Loss: 0.523\n",
      "Epoch 3938, Loss: 19.420, Final Batch Loss: 0.462\n",
      "Epoch 3939, Loss: 19.594, Final Batch Loss: 0.590\n",
      "Epoch 3940, Loss: 19.148, Final Batch Loss: 0.523\n",
      "Epoch 3941, Loss: 19.359, Final Batch Loss: 0.540\n",
      "Epoch 3942, Loss: 19.351, Final Batch Loss: 0.565\n",
      "Epoch 3943, Loss: 19.317, Final Batch Loss: 0.622\n",
      "Epoch 3944, Loss: 19.130, Final Batch Loss: 0.528\n",
      "Epoch 3945, Loss: 19.386, Final Batch Loss: 0.510\n",
      "Epoch 3946, Loss: 19.461, Final Batch Loss: 0.505\n",
      "Epoch 3947, Loss: 19.458, Final Batch Loss: 0.465\n",
      "Epoch 3948, Loss: 19.466, Final Batch Loss: 0.615\n",
      "Epoch 3949, Loss: 19.446, Final Batch Loss: 0.654\n",
      "Epoch 3950, Loss: 19.383, Final Batch Loss: 0.594\n",
      "Epoch 3951, Loss: 19.518, Final Batch Loss: 0.540\n",
      "Epoch 3952, Loss: 19.304, Final Batch Loss: 0.585\n",
      "Epoch 3953, Loss: 19.427, Final Batch Loss: 0.465\n",
      "Epoch 3954, Loss: 19.496, Final Batch Loss: 0.636\n",
      "Epoch 3955, Loss: 19.179, Final Batch Loss: 0.495\n",
      "Epoch 3956, Loss: 19.481, Final Batch Loss: 0.609\n",
      "Epoch 3957, Loss: 19.356, Final Batch Loss: 0.487\n",
      "Epoch 3958, Loss: 19.272, Final Batch Loss: 0.559\n",
      "Epoch 3959, Loss: 19.366, Final Batch Loss: 0.546\n",
      "Epoch 3960, Loss: 19.159, Final Batch Loss: 0.557\n",
      "Epoch 3961, Loss: 19.419, Final Batch Loss: 0.558\n",
      "Epoch 3962, Loss: 19.380, Final Batch Loss: 0.636\n",
      "Epoch 3963, Loss: 19.383, Final Batch Loss: 0.579\n",
      "Epoch 3964, Loss: 19.445, Final Batch Loss: 0.698\n",
      "Epoch 3965, Loss: 19.407, Final Batch Loss: 0.463\n",
      "Epoch 3966, Loss: 19.424, Final Batch Loss: 0.619\n",
      "Epoch 3967, Loss: 19.181, Final Batch Loss: 0.574\n",
      "Epoch 3968, Loss: 19.243, Final Batch Loss: 0.560\n",
      "Epoch 3969, Loss: 19.321, Final Batch Loss: 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3970, Loss: 19.393, Final Batch Loss: 0.505\n",
      "Epoch 3971, Loss: 19.313, Final Batch Loss: 0.601\n",
      "Epoch 3972, Loss: 19.366, Final Batch Loss: 0.623\n",
      "Epoch 3973, Loss: 19.173, Final Batch Loss: 0.569\n",
      "Epoch 3974, Loss: 19.358, Final Batch Loss: 0.467\n",
      "Epoch 3975, Loss: 19.545, Final Batch Loss: 0.627\n",
      "Epoch 3976, Loss: 19.605, Final Batch Loss: 0.538\n",
      "Epoch 3977, Loss: 19.576, Final Batch Loss: 0.610\n",
      "Epoch 3978, Loss: 19.327, Final Batch Loss: 0.565\n",
      "Epoch 3979, Loss: 19.499, Final Batch Loss: 0.471\n",
      "Epoch 3980, Loss: 19.173, Final Batch Loss: 0.554\n",
      "Epoch 3981, Loss: 19.345, Final Batch Loss: 0.498\n",
      "Epoch 3982, Loss: 19.326, Final Batch Loss: 0.501\n",
      "Epoch 3983, Loss: 19.303, Final Batch Loss: 0.437\n",
      "Epoch 3984, Loss: 19.144, Final Batch Loss: 0.493\n",
      "Epoch 3985, Loss: 19.365, Final Batch Loss: 0.440\n",
      "Epoch 3986, Loss: 19.563, Final Batch Loss: 0.616\n",
      "Epoch 3987, Loss: 19.258, Final Batch Loss: 0.435\n",
      "Epoch 3988, Loss: 19.323, Final Batch Loss: 0.520\n",
      "Epoch 3989, Loss: 19.611, Final Batch Loss: 0.565\n",
      "Epoch 3990, Loss: 19.298, Final Batch Loss: 0.538\n",
      "Epoch 3991, Loss: 19.443, Final Batch Loss: 0.512\n",
      "Epoch 3992, Loss: 19.666, Final Batch Loss: 0.564\n",
      "Epoch 3993, Loss: 19.476, Final Batch Loss: 0.488\n",
      "Epoch 3994, Loss: 19.185, Final Batch Loss: 0.475\n",
      "Epoch 3995, Loss: 19.345, Final Batch Loss: 0.472\n",
      "Epoch 3996, Loss: 19.249, Final Batch Loss: 0.564\n",
      "Epoch 3997, Loss: 19.183, Final Batch Loss: 0.533\n",
      "Epoch 3998, Loss: 19.602, Final Batch Loss: 0.473\n",
      "Epoch 3999, Loss: 19.366, Final Batch Loss: 0.552\n",
      "Epoch 4000, Loss: 19.062, Final Batch Loss: 0.503\n",
      "Epoch 4001, Loss: 19.225, Final Batch Loss: 0.554\n",
      "Epoch 4002, Loss: 19.321, Final Batch Loss: 0.519\n",
      "Epoch 4003, Loss: 19.502, Final Batch Loss: 0.564\n",
      "Epoch 4004, Loss: 19.275, Final Batch Loss: 0.535\n",
      "Epoch 4005, Loss: 19.324, Final Batch Loss: 0.489\n",
      "Epoch 4006, Loss: 19.375, Final Batch Loss: 0.466\n",
      "Epoch 4007, Loss: 19.327, Final Batch Loss: 0.533\n",
      "Epoch 4008, Loss: 19.198, Final Batch Loss: 0.505\n",
      "Epoch 4009, Loss: 19.378, Final Batch Loss: 0.642\n",
      "Epoch 4010, Loss: 19.231, Final Batch Loss: 0.588\n",
      "Epoch 4011, Loss: 19.428, Final Batch Loss: 0.577\n",
      "Epoch 4012, Loss: 18.951, Final Batch Loss: 0.499\n",
      "Epoch 4013, Loss: 19.572, Final Batch Loss: 0.733\n",
      "Epoch 4014, Loss: 19.276, Final Batch Loss: 0.434\n",
      "Epoch 4015, Loss: 19.367, Final Batch Loss: 0.546\n",
      "Epoch 4016, Loss: 19.098, Final Batch Loss: 0.440\n",
      "Epoch 4017, Loss: 19.252, Final Batch Loss: 0.457\n",
      "Epoch 4018, Loss: 19.389, Final Batch Loss: 0.578\n",
      "Epoch 4019, Loss: 19.328, Final Batch Loss: 0.569\n",
      "Epoch 4020, Loss: 19.420, Final Batch Loss: 0.516\n",
      "Epoch 4021, Loss: 19.377, Final Batch Loss: 0.653\n",
      "Epoch 4022, Loss: 19.694, Final Batch Loss: 0.442\n",
      "Epoch 4023, Loss: 19.344, Final Batch Loss: 0.477\n",
      "Epoch 4024, Loss: 19.322, Final Batch Loss: 0.476\n",
      "Epoch 4025, Loss: 19.210, Final Batch Loss: 0.470\n",
      "Epoch 4026, Loss: 19.276, Final Batch Loss: 0.616\n",
      "Epoch 4027, Loss: 19.544, Final Batch Loss: 0.484\n",
      "Epoch 4028, Loss: 19.453, Final Batch Loss: 0.646\n",
      "Epoch 4029, Loss: 19.379, Final Batch Loss: 0.466\n",
      "Epoch 4030, Loss: 19.168, Final Batch Loss: 0.536\n",
      "Epoch 4031, Loss: 19.326, Final Batch Loss: 0.417\n",
      "Epoch 4032, Loss: 19.564, Final Batch Loss: 0.442\n",
      "Epoch 4033, Loss: 19.278, Final Batch Loss: 0.488\n",
      "Epoch 4034, Loss: 19.347, Final Batch Loss: 0.509\n",
      "Epoch 4035, Loss: 19.343, Final Batch Loss: 0.433\n",
      "Epoch 4036, Loss: 19.342, Final Batch Loss: 0.537\n",
      "Epoch 4037, Loss: 19.179, Final Batch Loss: 0.591\n",
      "Epoch 4038, Loss: 19.329, Final Batch Loss: 0.610\n",
      "Epoch 4039, Loss: 19.342, Final Batch Loss: 0.557\n",
      "Epoch 4040, Loss: 19.383, Final Batch Loss: 0.630\n",
      "Epoch 4041, Loss: 19.316, Final Batch Loss: 0.517\n",
      "Epoch 4042, Loss: 19.104, Final Batch Loss: 0.547\n",
      "Epoch 4043, Loss: 19.185, Final Batch Loss: 0.574\n",
      "Epoch 4044, Loss: 19.235, Final Batch Loss: 0.559\n",
      "Epoch 4045, Loss: 19.392, Final Batch Loss: 0.485\n",
      "Epoch 4046, Loss: 19.171, Final Batch Loss: 0.494\n",
      "Epoch 4047, Loss: 19.383, Final Batch Loss: 0.618\n",
      "Epoch 4048, Loss: 19.345, Final Batch Loss: 0.556\n",
      "Epoch 4049, Loss: 19.300, Final Batch Loss: 0.600\n",
      "Epoch 4050, Loss: 19.338, Final Batch Loss: 0.553\n",
      "Epoch 4051, Loss: 19.187, Final Batch Loss: 0.517\n",
      "Epoch 4052, Loss: 19.312, Final Batch Loss: 0.570\n",
      "Epoch 4053, Loss: 19.087, Final Batch Loss: 0.455\n",
      "Epoch 4054, Loss: 19.214, Final Batch Loss: 0.506\n",
      "Epoch 4055, Loss: 19.347, Final Batch Loss: 0.475\n",
      "Epoch 4056, Loss: 19.280, Final Batch Loss: 0.655\n",
      "Epoch 4057, Loss: 19.309, Final Batch Loss: 0.642\n",
      "Epoch 4058, Loss: 19.147, Final Batch Loss: 0.467\n",
      "Epoch 4059, Loss: 19.228, Final Batch Loss: 0.627\n",
      "Epoch 4060, Loss: 19.354, Final Batch Loss: 0.540\n",
      "Epoch 4061, Loss: 19.614, Final Batch Loss: 0.759\n",
      "Epoch 4062, Loss: 19.566, Final Batch Loss: 0.671\n",
      "Epoch 4063, Loss: 19.139, Final Batch Loss: 0.522\n",
      "Epoch 4064, Loss: 19.380, Final Batch Loss: 0.562\n",
      "Epoch 4065, Loss: 19.356, Final Batch Loss: 0.523\n",
      "Epoch 4066, Loss: 19.268, Final Batch Loss: 0.645\n",
      "Epoch 4067, Loss: 19.289, Final Batch Loss: 0.497\n",
      "Epoch 4068, Loss: 19.300, Final Batch Loss: 0.530\n",
      "Epoch 4069, Loss: 19.190, Final Batch Loss: 0.522\n",
      "Epoch 4070, Loss: 19.290, Final Batch Loss: 0.520\n",
      "Epoch 4071, Loss: 19.260, Final Batch Loss: 0.492\n",
      "Epoch 4072, Loss: 19.236, Final Batch Loss: 0.509\n",
      "Epoch 4073, Loss: 19.075, Final Batch Loss: 0.661\n",
      "Epoch 4074, Loss: 19.397, Final Batch Loss: 0.374\n",
      "Epoch 4075, Loss: 19.236, Final Batch Loss: 0.529\n",
      "Epoch 4076, Loss: 19.533, Final Batch Loss: 0.571\n",
      "Epoch 4077, Loss: 19.115, Final Batch Loss: 0.464\n",
      "Epoch 4078, Loss: 19.161, Final Batch Loss: 0.436\n",
      "Epoch 4079, Loss: 19.393, Final Batch Loss: 0.557\n",
      "Epoch 4080, Loss: 19.352, Final Batch Loss: 0.528\n",
      "Epoch 4081, Loss: 19.257, Final Batch Loss: 0.411\n",
      "Epoch 4082, Loss: 19.334, Final Batch Loss: 0.546\n",
      "Epoch 4083, Loss: 19.330, Final Batch Loss: 0.449\n",
      "Epoch 4084, Loss: 19.292, Final Batch Loss: 0.637\n",
      "Epoch 4085, Loss: 19.303, Final Batch Loss: 0.517\n",
      "Epoch 4086, Loss: 19.522, Final Batch Loss: 0.721\n",
      "Epoch 4087, Loss: 19.372, Final Batch Loss: 0.599\n",
      "Epoch 4088, Loss: 19.083, Final Batch Loss: 0.366\n",
      "Epoch 4089, Loss: 19.307, Final Batch Loss: 0.581\n",
      "Epoch 4090, Loss: 19.147, Final Batch Loss: 0.497\n",
      "Epoch 4091, Loss: 19.372, Final Batch Loss: 0.583\n",
      "Epoch 4092, Loss: 19.281, Final Batch Loss: 0.628\n",
      "Epoch 4093, Loss: 19.281, Final Batch Loss: 0.558\n",
      "Epoch 4094, Loss: 19.520, Final Batch Loss: 0.565\n",
      "Epoch 4095, Loss: 19.269, Final Batch Loss: 0.537\n",
      "Epoch 4096, Loss: 19.482, Final Batch Loss: 0.665\n",
      "Epoch 4097, Loss: 19.085, Final Batch Loss: 0.577\n",
      "Epoch 4098, Loss: 19.349, Final Batch Loss: 0.570\n",
      "Epoch 4099, Loss: 19.021, Final Batch Loss: 0.536\n",
      "Epoch 4100, Loss: 19.654, Final Batch Loss: 0.556\n",
      "Epoch 4101, Loss: 19.282, Final Batch Loss: 0.543\n",
      "Epoch 4102, Loss: 19.243, Final Batch Loss: 0.568\n",
      "Epoch 4103, Loss: 19.339, Final Batch Loss: 0.507\n",
      "Epoch 4104, Loss: 19.416, Final Batch Loss: 0.604\n",
      "Epoch 4105, Loss: 19.270, Final Batch Loss: 0.541\n",
      "Epoch 4106, Loss: 19.017, Final Batch Loss: 0.430\n",
      "Epoch 4107, Loss: 19.513, Final Batch Loss: 0.540\n",
      "Epoch 4108, Loss: 19.320, Final Batch Loss: 0.600\n",
      "Epoch 4109, Loss: 19.173, Final Batch Loss: 0.551\n",
      "Epoch 4110, Loss: 19.290, Final Batch Loss: 0.599\n",
      "Epoch 4111, Loss: 19.183, Final Batch Loss: 0.572\n",
      "Epoch 4112, Loss: 19.271, Final Batch Loss: 0.623\n",
      "Epoch 4113, Loss: 19.383, Final Batch Loss: 0.589\n",
      "Epoch 4114, Loss: 19.083, Final Batch Loss: 0.517\n",
      "Epoch 4115, Loss: 19.355, Final Batch Loss: 0.510\n",
      "Epoch 4116, Loss: 19.428, Final Batch Loss: 0.577\n",
      "Epoch 4117, Loss: 19.270, Final Batch Loss: 0.481\n",
      "Epoch 4118, Loss: 19.214, Final Batch Loss: 0.521\n",
      "Epoch 4119, Loss: 19.203, Final Batch Loss: 0.507\n",
      "Epoch 4120, Loss: 19.379, Final Batch Loss: 0.555\n",
      "Epoch 4121, Loss: 19.312, Final Batch Loss: 0.515\n",
      "Epoch 4122, Loss: 19.038, Final Batch Loss: 0.484\n",
      "Epoch 4123, Loss: 19.424, Final Batch Loss: 0.527\n",
      "Epoch 4124, Loss: 19.230, Final Batch Loss: 0.450\n",
      "Epoch 4125, Loss: 19.243, Final Batch Loss: 0.541\n",
      "Epoch 4126, Loss: 19.662, Final Batch Loss: 0.490\n",
      "Epoch 4127, Loss: 19.451, Final Batch Loss: 0.589\n",
      "Epoch 4128, Loss: 19.103, Final Batch Loss: 0.518\n",
      "Epoch 4129, Loss: 19.204, Final Batch Loss: 0.543\n",
      "Epoch 4130, Loss: 19.232, Final Batch Loss: 0.471\n",
      "Epoch 4131, Loss: 19.348, Final Batch Loss: 0.523\n",
      "Epoch 4132, Loss: 19.419, Final Batch Loss: 0.402\n",
      "Epoch 4133, Loss: 19.137, Final Batch Loss: 0.450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4134, Loss: 19.344, Final Batch Loss: 0.490\n",
      "Epoch 4135, Loss: 19.238, Final Batch Loss: 0.549\n",
      "Epoch 4136, Loss: 18.973, Final Batch Loss: 0.561\n",
      "Epoch 4137, Loss: 19.307, Final Batch Loss: 0.501\n",
      "Epoch 4138, Loss: 19.062, Final Batch Loss: 0.559\n",
      "Epoch 4139, Loss: 18.922, Final Batch Loss: 0.404\n",
      "Epoch 4140, Loss: 19.200, Final Batch Loss: 0.503\n",
      "Epoch 4141, Loss: 19.427, Final Batch Loss: 0.440\n",
      "Epoch 4142, Loss: 19.415, Final Batch Loss: 0.517\n",
      "Epoch 4143, Loss: 19.313, Final Batch Loss: 0.548\n",
      "Epoch 4144, Loss: 19.157, Final Batch Loss: 0.469\n",
      "Epoch 4145, Loss: 19.317, Final Batch Loss: 0.512\n",
      "Epoch 4146, Loss: 19.349, Final Batch Loss: 0.559\n",
      "Epoch 4147, Loss: 18.957, Final Batch Loss: 0.633\n",
      "Epoch 4148, Loss: 19.098, Final Batch Loss: 0.599\n",
      "Epoch 4149, Loss: 19.346, Final Batch Loss: 0.581\n",
      "Epoch 4150, Loss: 19.184, Final Batch Loss: 0.450\n",
      "Epoch 4151, Loss: 19.052, Final Batch Loss: 0.601\n",
      "Epoch 4152, Loss: 19.324, Final Batch Loss: 0.490\n",
      "Epoch 4153, Loss: 19.462, Final Batch Loss: 0.588\n",
      "Epoch 4154, Loss: 19.178, Final Batch Loss: 0.588\n",
      "Epoch 4155, Loss: 19.232, Final Batch Loss: 0.631\n",
      "Epoch 4156, Loss: 19.318, Final Batch Loss: 0.602\n",
      "Epoch 4157, Loss: 19.063, Final Batch Loss: 0.522\n",
      "Epoch 4158, Loss: 19.230, Final Batch Loss: 0.561\n",
      "Epoch 4159, Loss: 19.320, Final Batch Loss: 0.478\n",
      "Epoch 4160, Loss: 19.194, Final Batch Loss: 0.611\n",
      "Epoch 4161, Loss: 19.607, Final Batch Loss: 0.601\n",
      "Epoch 4162, Loss: 19.128, Final Batch Loss: 0.536\n",
      "Epoch 4163, Loss: 19.240, Final Batch Loss: 0.505\n",
      "Epoch 4164, Loss: 19.357, Final Batch Loss: 0.572\n",
      "Epoch 4165, Loss: 19.206, Final Batch Loss: 0.545\n",
      "Epoch 4166, Loss: 19.304, Final Batch Loss: 0.533\n",
      "Epoch 4167, Loss: 19.126, Final Batch Loss: 0.594\n",
      "Epoch 4168, Loss: 19.489, Final Batch Loss: 0.609\n",
      "Epoch 4169, Loss: 19.476, Final Batch Loss: 0.516\n",
      "Epoch 4170, Loss: 19.456, Final Batch Loss: 0.631\n",
      "Epoch 4171, Loss: 19.096, Final Batch Loss: 0.559\n",
      "Epoch 4172, Loss: 19.000, Final Batch Loss: 0.520\n",
      "Epoch 4173, Loss: 19.161, Final Batch Loss: 0.661\n",
      "Epoch 4174, Loss: 19.389, Final Batch Loss: 0.558\n",
      "Epoch 4175, Loss: 19.292, Final Batch Loss: 0.469\n",
      "Epoch 4176, Loss: 19.104, Final Batch Loss: 0.591\n",
      "Epoch 4177, Loss: 19.246, Final Batch Loss: 0.574\n",
      "Epoch 4178, Loss: 19.147, Final Batch Loss: 0.600\n",
      "Epoch 4179, Loss: 19.310, Final Batch Loss: 0.495\n",
      "Epoch 4180, Loss: 19.323, Final Batch Loss: 0.527\n",
      "Epoch 4181, Loss: 19.449, Final Batch Loss: 0.473\n",
      "Epoch 4182, Loss: 19.105, Final Batch Loss: 0.458\n",
      "Epoch 4183, Loss: 19.237, Final Batch Loss: 0.498\n",
      "Epoch 4184, Loss: 19.027, Final Batch Loss: 0.499\n",
      "Epoch 4185, Loss: 19.301, Final Batch Loss: 0.585\n",
      "Epoch 4186, Loss: 19.318, Final Batch Loss: 0.549\n",
      "Epoch 4187, Loss: 19.199, Final Batch Loss: 0.553\n",
      "Epoch 4188, Loss: 19.180, Final Batch Loss: 0.452\n",
      "Epoch 4189, Loss: 19.242, Final Batch Loss: 0.602\n",
      "Epoch 4190, Loss: 19.334, Final Batch Loss: 0.543\n",
      "Epoch 4191, Loss: 19.204, Final Batch Loss: 0.443\n",
      "Epoch 4192, Loss: 19.279, Final Batch Loss: 0.536\n",
      "Epoch 4193, Loss: 19.514, Final Batch Loss: 0.548\n",
      "Epoch 4194, Loss: 19.112, Final Batch Loss: 0.527\n",
      "Epoch 4195, Loss: 19.049, Final Batch Loss: 0.488\n",
      "Epoch 4196, Loss: 19.260, Final Batch Loss: 0.531\n",
      "Epoch 4197, Loss: 19.345, Final Batch Loss: 0.542\n",
      "Epoch 4198, Loss: 19.373, Final Batch Loss: 0.577\n",
      "Epoch 4199, Loss: 19.316, Final Batch Loss: 0.530\n",
      "Epoch 4200, Loss: 19.287, Final Batch Loss: 0.440\n",
      "Epoch 4201, Loss: 19.184, Final Batch Loss: 0.497\n",
      "Epoch 4202, Loss: 18.956, Final Batch Loss: 0.430\n",
      "Epoch 4203, Loss: 19.396, Final Batch Loss: 0.554\n",
      "Epoch 4204, Loss: 19.369, Final Batch Loss: 0.591\n",
      "Epoch 4205, Loss: 19.231, Final Batch Loss: 0.658\n",
      "Epoch 4206, Loss: 19.470, Final Batch Loss: 0.534\n",
      "Epoch 4207, Loss: 19.237, Final Batch Loss: 0.509\n",
      "Epoch 4208, Loss: 19.207, Final Batch Loss: 0.541\n",
      "Epoch 4209, Loss: 19.041, Final Batch Loss: 0.567\n",
      "Epoch 4210, Loss: 19.108, Final Batch Loss: 0.395\n",
      "Epoch 4211, Loss: 19.141, Final Batch Loss: 0.525\n",
      "Epoch 4212, Loss: 19.366, Final Batch Loss: 0.649\n",
      "Epoch 4213, Loss: 19.253, Final Batch Loss: 0.540\n",
      "Epoch 4214, Loss: 19.360, Final Batch Loss: 0.499\n",
      "Epoch 4215, Loss: 19.208, Final Batch Loss: 0.616\n",
      "Epoch 4216, Loss: 19.362, Final Batch Loss: 0.436\n",
      "Epoch 4217, Loss: 19.174, Final Batch Loss: 0.591\n",
      "Epoch 4218, Loss: 19.096, Final Batch Loss: 0.518\n",
      "Epoch 4219, Loss: 19.124, Final Batch Loss: 0.487\n",
      "Epoch 4220, Loss: 19.257, Final Batch Loss: 0.528\n",
      "Epoch 4221, Loss: 19.338, Final Batch Loss: 0.448\n",
      "Epoch 4222, Loss: 19.197, Final Batch Loss: 0.582\n",
      "Epoch 4223, Loss: 18.983, Final Batch Loss: 0.484\n",
      "Epoch 4224, Loss: 19.231, Final Batch Loss: 0.619\n",
      "Epoch 4225, Loss: 19.409, Final Batch Loss: 0.628\n",
      "Epoch 4226, Loss: 19.145, Final Batch Loss: 0.654\n",
      "Epoch 4227, Loss: 19.061, Final Batch Loss: 0.528\n",
      "Epoch 4228, Loss: 19.311, Final Batch Loss: 0.504\n",
      "Epoch 4229, Loss: 19.596, Final Batch Loss: 0.634\n",
      "Epoch 4230, Loss: 19.142, Final Batch Loss: 0.529\n",
      "Epoch 4231, Loss: 18.954, Final Batch Loss: 0.478\n",
      "Epoch 4232, Loss: 19.223, Final Batch Loss: 0.689\n",
      "Epoch 4233, Loss: 19.117, Final Batch Loss: 0.587\n",
      "Epoch 4234, Loss: 19.107, Final Batch Loss: 0.486\n",
      "Epoch 4235, Loss: 19.292, Final Batch Loss: 0.589\n",
      "Epoch 4236, Loss: 19.222, Final Batch Loss: 0.506\n",
      "Epoch 4237, Loss: 19.039, Final Batch Loss: 0.502\n",
      "Epoch 4238, Loss: 19.206, Final Batch Loss: 0.462\n",
      "Epoch 4239, Loss: 19.273, Final Batch Loss: 0.487\n",
      "Epoch 4240, Loss: 19.269, Final Batch Loss: 0.547\n",
      "Epoch 4241, Loss: 19.306, Final Batch Loss: 0.638\n",
      "Epoch 4242, Loss: 19.594, Final Batch Loss: 0.543\n",
      "Epoch 4243, Loss: 19.021, Final Batch Loss: 0.570\n",
      "Epoch 4244, Loss: 19.239, Final Batch Loss: 0.551\n",
      "Epoch 4245, Loss: 19.122, Final Batch Loss: 0.450\n",
      "Epoch 4246, Loss: 19.359, Final Batch Loss: 0.509\n",
      "Epoch 4247, Loss: 19.039, Final Batch Loss: 0.558\n",
      "Epoch 4248, Loss: 19.171, Final Batch Loss: 0.573\n",
      "Epoch 4249, Loss: 19.089, Final Batch Loss: 0.516\n",
      "Epoch 4250, Loss: 19.233, Final Batch Loss: 0.569\n",
      "Epoch 4251, Loss: 19.157, Final Batch Loss: 0.609\n",
      "Epoch 4252, Loss: 19.293, Final Batch Loss: 0.691\n",
      "Epoch 4253, Loss: 19.349, Final Batch Loss: 0.547\n",
      "Epoch 4254, Loss: 19.371, Final Batch Loss: 0.625\n",
      "Epoch 4255, Loss: 19.168, Final Batch Loss: 0.488\n",
      "Epoch 4256, Loss: 19.249, Final Batch Loss: 0.353\n",
      "Epoch 4257, Loss: 19.114, Final Batch Loss: 0.500\n",
      "Epoch 4258, Loss: 19.067, Final Batch Loss: 0.482\n",
      "Epoch 4259, Loss: 18.793, Final Batch Loss: 0.525\n",
      "Epoch 4260, Loss: 19.186, Final Batch Loss: 0.486\n",
      "Epoch 4261, Loss: 19.210, Final Batch Loss: 0.473\n",
      "Epoch 4262, Loss: 19.113, Final Batch Loss: 0.559\n",
      "Epoch 4263, Loss: 19.133, Final Batch Loss: 0.584\n",
      "Epoch 4264, Loss: 18.951, Final Batch Loss: 0.475\n",
      "Epoch 4265, Loss: 19.195, Final Batch Loss: 0.659\n",
      "Epoch 4266, Loss: 19.589, Final Batch Loss: 0.584\n",
      "Epoch 4267, Loss: 19.481, Final Batch Loss: 0.614\n",
      "Epoch 4268, Loss: 19.109, Final Batch Loss: 0.423\n",
      "Epoch 4269, Loss: 19.501, Final Batch Loss: 0.474\n",
      "Epoch 4270, Loss: 19.176, Final Batch Loss: 0.623\n",
      "Epoch 4271, Loss: 19.095, Final Batch Loss: 0.425\n",
      "Epoch 4272, Loss: 18.920, Final Batch Loss: 0.507\n",
      "Epoch 4273, Loss: 18.887, Final Batch Loss: 0.577\n",
      "Epoch 4274, Loss: 19.127, Final Batch Loss: 0.625\n",
      "Epoch 4275, Loss: 19.271, Final Batch Loss: 0.535\n",
      "Epoch 4276, Loss: 19.094, Final Batch Loss: 0.577\n",
      "Epoch 4277, Loss: 19.230, Final Batch Loss: 0.480\n",
      "Epoch 4278, Loss: 18.851, Final Batch Loss: 0.415\n",
      "Epoch 4279, Loss: 19.123, Final Batch Loss: 0.496\n",
      "Epoch 4280, Loss: 19.279, Final Batch Loss: 0.600\n",
      "Epoch 4281, Loss: 19.096, Final Batch Loss: 0.528\n",
      "Epoch 4282, Loss: 19.150, Final Batch Loss: 0.538\n",
      "Epoch 4283, Loss: 19.126, Final Batch Loss: 0.518\n",
      "Epoch 4284, Loss: 19.262, Final Batch Loss: 0.530\n",
      "Epoch 4285, Loss: 18.982, Final Batch Loss: 0.497\n",
      "Epoch 4286, Loss: 19.051, Final Batch Loss: 0.473\n",
      "Epoch 4287, Loss: 19.152, Final Batch Loss: 0.567\n",
      "Epoch 4288, Loss: 19.218, Final Batch Loss: 0.451\n",
      "Epoch 4289, Loss: 19.251, Final Batch Loss: 0.528\n",
      "Epoch 4290, Loss: 19.011, Final Batch Loss: 0.580\n",
      "Epoch 4291, Loss: 19.067, Final Batch Loss: 0.583\n",
      "Epoch 4292, Loss: 19.112, Final Batch Loss: 0.390\n",
      "Epoch 4293, Loss: 19.227, Final Batch Loss: 0.608\n",
      "Epoch 4294, Loss: 19.276, Final Batch Loss: 0.685\n",
      "Epoch 4295, Loss: 18.996, Final Batch Loss: 0.536\n",
      "Epoch 4296, Loss: 18.969, Final Batch Loss: 0.493\n",
      "Epoch 4297, Loss: 19.100, Final Batch Loss: 0.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4298, Loss: 19.230, Final Batch Loss: 0.461\n",
      "Epoch 4299, Loss: 19.076, Final Batch Loss: 0.433\n",
      "Epoch 4300, Loss: 19.342, Final Batch Loss: 0.452\n",
      "Epoch 4301, Loss: 19.220, Final Batch Loss: 0.469\n",
      "Epoch 4302, Loss: 19.194, Final Batch Loss: 0.594\n",
      "Epoch 4303, Loss: 19.300, Final Batch Loss: 0.524\n",
      "Epoch 4304, Loss: 19.434, Final Batch Loss: 0.587\n",
      "Epoch 4305, Loss: 19.242, Final Batch Loss: 0.615\n",
      "Epoch 4306, Loss: 19.187, Final Batch Loss: 0.531\n",
      "Epoch 4307, Loss: 19.115, Final Batch Loss: 0.479\n",
      "Epoch 4308, Loss: 19.083, Final Batch Loss: 0.542\n",
      "Epoch 4309, Loss: 19.336, Final Batch Loss: 0.561\n",
      "Epoch 4310, Loss: 19.361, Final Batch Loss: 0.630\n",
      "Epoch 4311, Loss: 18.911, Final Batch Loss: 0.557\n",
      "Epoch 4312, Loss: 19.401, Final Batch Loss: 0.547\n",
      "Epoch 4313, Loss: 19.205, Final Batch Loss: 0.572\n",
      "Epoch 4314, Loss: 19.153, Final Batch Loss: 0.598\n",
      "Epoch 4315, Loss: 18.980, Final Batch Loss: 0.615\n",
      "Epoch 4316, Loss: 19.257, Final Batch Loss: 0.538\n",
      "Epoch 4317, Loss: 18.997, Final Batch Loss: 0.481\n",
      "Epoch 4318, Loss: 19.165, Final Batch Loss: 0.494\n",
      "Epoch 4319, Loss: 19.141, Final Batch Loss: 0.468\n",
      "Epoch 4320, Loss: 18.845, Final Batch Loss: 0.424\n",
      "Epoch 4321, Loss: 19.277, Final Batch Loss: 0.470\n",
      "Epoch 4322, Loss: 18.914, Final Batch Loss: 0.504\n",
      "Epoch 4323, Loss: 19.158, Final Batch Loss: 0.524\n",
      "Epoch 4324, Loss: 18.853, Final Batch Loss: 0.392\n",
      "Epoch 4325, Loss: 19.116, Final Batch Loss: 0.441\n",
      "Epoch 4326, Loss: 19.223, Final Batch Loss: 0.563\n",
      "Epoch 4327, Loss: 19.397, Final Batch Loss: 0.582\n",
      "Epoch 4328, Loss: 19.410, Final Batch Loss: 0.626\n",
      "Epoch 4329, Loss: 19.561, Final Batch Loss: 0.483\n",
      "Epoch 4330, Loss: 19.092, Final Batch Loss: 0.483\n",
      "Epoch 4331, Loss: 19.218, Final Batch Loss: 0.534\n",
      "Epoch 4332, Loss: 19.316, Final Batch Loss: 0.584\n",
      "Epoch 4333, Loss: 18.959, Final Batch Loss: 0.490\n",
      "Epoch 4334, Loss: 19.233, Final Batch Loss: 0.594\n",
      "Epoch 4335, Loss: 19.293, Final Batch Loss: 0.489\n",
      "Epoch 4336, Loss: 19.126, Final Batch Loss: 0.400\n",
      "Epoch 4337, Loss: 19.299, Final Batch Loss: 0.584\n",
      "Epoch 4338, Loss: 18.970, Final Batch Loss: 0.575\n",
      "Epoch 4339, Loss: 19.187, Final Batch Loss: 0.502\n",
      "Epoch 4340, Loss: 19.014, Final Batch Loss: 0.535\n",
      "Epoch 4341, Loss: 19.146, Final Batch Loss: 0.545\n",
      "Epoch 4342, Loss: 19.131, Final Batch Loss: 0.438\n",
      "Epoch 4343, Loss: 19.295, Final Batch Loss: 0.489\n",
      "Epoch 4344, Loss: 19.231, Final Batch Loss: 0.591\n",
      "Epoch 4345, Loss: 19.318, Final Batch Loss: 0.473\n",
      "Epoch 4346, Loss: 19.066, Final Batch Loss: 0.463\n",
      "Epoch 4347, Loss: 19.111, Final Batch Loss: 0.386\n",
      "Epoch 4348, Loss: 19.150, Final Batch Loss: 0.413\n",
      "Epoch 4349, Loss: 19.094, Final Batch Loss: 0.508\n",
      "Epoch 4350, Loss: 19.029, Final Batch Loss: 0.514\n",
      "Epoch 4351, Loss: 19.217, Final Batch Loss: 0.603\n",
      "Epoch 4352, Loss: 19.256, Final Batch Loss: 0.594\n",
      "Epoch 4353, Loss: 19.166, Final Batch Loss: 0.630\n",
      "Epoch 4354, Loss: 19.121, Final Batch Loss: 0.565\n",
      "Epoch 4355, Loss: 19.023, Final Batch Loss: 0.482\n",
      "Epoch 4356, Loss: 19.088, Final Batch Loss: 0.562\n",
      "Epoch 4357, Loss: 19.149, Final Batch Loss: 0.499\n",
      "Epoch 4358, Loss: 19.163, Final Batch Loss: 0.578\n",
      "Epoch 4359, Loss: 19.244, Final Batch Loss: 0.455\n",
      "Epoch 4360, Loss: 19.094, Final Batch Loss: 0.436\n",
      "Epoch 4361, Loss: 19.157, Final Batch Loss: 0.543\n",
      "Epoch 4362, Loss: 19.232, Final Batch Loss: 0.460\n",
      "Epoch 4363, Loss: 19.345, Final Batch Loss: 0.537\n",
      "Epoch 4364, Loss: 19.361, Final Batch Loss: 0.573\n",
      "Epoch 4365, Loss: 19.185, Final Batch Loss: 0.577\n",
      "Epoch 4366, Loss: 19.226, Final Batch Loss: 0.554\n",
      "Epoch 4367, Loss: 19.205, Final Batch Loss: 0.506\n",
      "Epoch 4368, Loss: 19.114, Final Batch Loss: 0.532\n",
      "Epoch 4369, Loss: 19.047, Final Batch Loss: 0.500\n",
      "Epoch 4370, Loss: 19.057, Final Batch Loss: 0.508\n",
      "Epoch 4371, Loss: 19.242, Final Batch Loss: 0.523\n",
      "Epoch 4372, Loss: 18.974, Final Batch Loss: 0.499\n",
      "Epoch 4373, Loss: 19.295, Final Batch Loss: 0.551\n",
      "Epoch 4374, Loss: 19.162, Final Batch Loss: 0.547\n",
      "Epoch 4375, Loss: 19.060, Final Batch Loss: 0.508\n",
      "Epoch 4376, Loss: 19.113, Final Batch Loss: 0.553\n",
      "Epoch 4377, Loss: 19.080, Final Batch Loss: 0.505\n",
      "Epoch 4378, Loss: 18.902, Final Batch Loss: 0.445\n",
      "Epoch 4379, Loss: 19.240, Final Batch Loss: 0.485\n",
      "Epoch 4380, Loss: 19.051, Final Batch Loss: 0.681\n",
      "Epoch 4381, Loss: 19.016, Final Batch Loss: 0.507\n",
      "Epoch 4382, Loss: 19.277, Final Batch Loss: 0.427\n",
      "Epoch 4383, Loss: 19.122, Final Batch Loss: 0.513\n",
      "Epoch 4384, Loss: 19.170, Final Batch Loss: 0.593\n",
      "Epoch 4385, Loss: 19.080, Final Batch Loss: 0.490\n",
      "Epoch 4386, Loss: 19.239, Final Batch Loss: 0.585\n",
      "Epoch 4387, Loss: 19.418, Final Batch Loss: 0.542\n",
      "Epoch 4388, Loss: 19.091, Final Batch Loss: 0.459\n",
      "Epoch 4389, Loss: 19.407, Final Batch Loss: 0.556\n",
      "Epoch 4390, Loss: 19.225, Final Batch Loss: 0.582\n",
      "Epoch 4391, Loss: 19.098, Final Batch Loss: 0.481\n",
      "Epoch 4392, Loss: 18.998, Final Batch Loss: 0.472\n",
      "Epoch 4393, Loss: 19.324, Final Batch Loss: 0.561\n",
      "Epoch 4394, Loss: 18.955, Final Batch Loss: 0.526\n",
      "Epoch 4395, Loss: 18.697, Final Batch Loss: 0.384\n",
      "Epoch 4396, Loss: 18.972, Final Batch Loss: 0.559\n",
      "Epoch 4397, Loss: 19.013, Final Batch Loss: 0.404\n",
      "Epoch 4398, Loss: 18.890, Final Batch Loss: 0.706\n",
      "Epoch 4399, Loss: 19.092, Final Batch Loss: 0.493\n",
      "Epoch 4400, Loss: 18.971, Final Batch Loss: 0.533\n",
      "Epoch 4401, Loss: 18.831, Final Batch Loss: 0.467\n",
      "Epoch 4402, Loss: 19.005, Final Batch Loss: 0.471\n",
      "Epoch 4403, Loss: 19.135, Final Batch Loss: 0.480\n",
      "Epoch 4404, Loss: 19.361, Final Batch Loss: 0.555\n",
      "Epoch 4405, Loss: 19.166, Final Batch Loss: 0.475\n",
      "Epoch 4406, Loss: 19.081, Final Batch Loss: 0.488\n",
      "Epoch 4407, Loss: 19.243, Final Batch Loss: 0.464\n",
      "Epoch 4408, Loss: 19.025, Final Batch Loss: 0.541\n",
      "Epoch 4409, Loss: 19.172, Final Batch Loss: 0.452\n",
      "Epoch 4410, Loss: 19.459, Final Batch Loss: 0.490\n",
      "Epoch 4411, Loss: 18.931, Final Batch Loss: 0.504\n",
      "Epoch 4412, Loss: 18.929, Final Batch Loss: 0.464\n",
      "Epoch 4413, Loss: 19.282, Final Batch Loss: 0.509\n",
      "Epoch 4414, Loss: 19.054, Final Batch Loss: 0.610\n",
      "Epoch 4415, Loss: 19.396, Final Batch Loss: 0.596\n",
      "Epoch 4416, Loss: 19.168, Final Batch Loss: 0.439\n",
      "Epoch 4417, Loss: 19.089, Final Batch Loss: 0.568\n",
      "Epoch 4418, Loss: 19.375, Final Batch Loss: 0.570\n",
      "Epoch 4419, Loss: 19.216, Final Batch Loss: 0.587\n",
      "Epoch 4420, Loss: 19.127, Final Batch Loss: 0.615\n",
      "Epoch 4421, Loss: 18.865, Final Batch Loss: 0.497\n",
      "Epoch 4422, Loss: 19.256, Final Batch Loss: 0.683\n",
      "Epoch 4423, Loss: 19.204, Final Batch Loss: 0.576\n",
      "Epoch 4424, Loss: 19.201, Final Batch Loss: 0.470\n",
      "Epoch 4425, Loss: 18.875, Final Batch Loss: 0.535\n",
      "Epoch 4426, Loss: 19.119, Final Batch Loss: 0.520\n",
      "Epoch 4427, Loss: 19.129, Final Batch Loss: 0.572\n",
      "Epoch 4428, Loss: 18.948, Final Batch Loss: 0.564\n",
      "Epoch 4429, Loss: 19.240, Final Batch Loss: 0.567\n",
      "Epoch 4430, Loss: 19.268, Final Batch Loss: 0.549\n",
      "Epoch 4431, Loss: 18.925, Final Batch Loss: 0.519\n",
      "Epoch 4432, Loss: 19.000, Final Batch Loss: 0.557\n",
      "Epoch 4433, Loss: 19.281, Final Batch Loss: 0.613\n",
      "Epoch 4434, Loss: 19.089, Final Batch Loss: 0.613\n",
      "Epoch 4435, Loss: 19.102, Final Batch Loss: 0.534\n",
      "Epoch 4436, Loss: 19.037, Final Batch Loss: 0.521\n",
      "Epoch 4437, Loss: 19.154, Final Batch Loss: 0.555\n",
      "Epoch 4438, Loss: 19.211, Final Batch Loss: 0.637\n",
      "Epoch 4439, Loss: 19.089, Final Batch Loss: 0.545\n",
      "Epoch 4440, Loss: 19.021, Final Batch Loss: 0.560\n",
      "Epoch 4441, Loss: 19.453, Final Batch Loss: 0.480\n",
      "Epoch 4442, Loss: 19.107, Final Batch Loss: 0.544\n",
      "Epoch 4443, Loss: 19.496, Final Batch Loss: 0.608\n",
      "Epoch 4444, Loss: 18.935, Final Batch Loss: 0.491\n",
      "Epoch 4445, Loss: 19.224, Final Batch Loss: 0.585\n",
      "Epoch 4446, Loss: 18.949, Final Batch Loss: 0.455\n",
      "Epoch 4447, Loss: 19.235, Final Batch Loss: 0.594\n",
      "Epoch 4448, Loss: 19.140, Final Batch Loss: 0.427\n",
      "Epoch 4449, Loss: 18.958, Final Batch Loss: 0.501\n",
      "Epoch 4450, Loss: 19.054, Final Batch Loss: 0.455\n",
      "Epoch 4451, Loss: 19.078, Final Batch Loss: 0.576\n",
      "Epoch 4452, Loss: 19.147, Final Batch Loss: 0.529\n",
      "Epoch 4453, Loss: 19.194, Final Batch Loss: 0.462\n",
      "Epoch 4454, Loss: 19.150, Final Batch Loss: 0.458\n",
      "Epoch 4455, Loss: 18.874, Final Batch Loss: 0.504\n",
      "Epoch 4456, Loss: 19.112, Final Batch Loss: 0.490\n",
      "Epoch 4457, Loss: 19.325, Final Batch Loss: 0.692\n",
      "Epoch 4458, Loss: 19.005, Final Batch Loss: 0.459\n",
      "Epoch 4459, Loss: 18.985, Final Batch Loss: 0.621\n",
      "Epoch 4460, Loss: 19.034, Final Batch Loss: 0.517\n",
      "Epoch 4461, Loss: 19.174, Final Batch Loss: 0.529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4462, Loss: 19.062, Final Batch Loss: 0.495\n",
      "Epoch 4463, Loss: 19.292, Final Batch Loss: 0.516\n",
      "Epoch 4464, Loss: 19.276, Final Batch Loss: 0.565\n",
      "Epoch 4465, Loss: 19.137, Final Batch Loss: 0.553\n",
      "Epoch 4466, Loss: 19.120, Final Batch Loss: 0.580\n",
      "Epoch 4467, Loss: 19.386, Final Batch Loss: 0.611\n",
      "Epoch 4468, Loss: 19.011, Final Batch Loss: 0.539\n",
      "Epoch 4469, Loss: 18.925, Final Batch Loss: 0.475\n",
      "Epoch 4470, Loss: 19.195, Final Batch Loss: 0.528\n",
      "Epoch 4471, Loss: 18.957, Final Batch Loss: 0.536\n",
      "Epoch 4472, Loss: 19.164, Final Batch Loss: 0.521\n",
      "Epoch 4473, Loss: 19.163, Final Batch Loss: 0.548\n",
      "Epoch 4474, Loss: 19.040, Final Batch Loss: 0.547\n",
      "Epoch 4475, Loss: 19.258, Final Batch Loss: 0.645\n",
      "Epoch 4476, Loss: 18.998, Final Batch Loss: 0.517\n",
      "Epoch 4477, Loss: 19.045, Final Batch Loss: 0.416\n",
      "Epoch 4478, Loss: 19.030, Final Batch Loss: 0.557\n",
      "Epoch 4479, Loss: 19.264, Final Batch Loss: 0.557\n",
      "Epoch 4480, Loss: 19.004, Final Batch Loss: 0.548\n",
      "Epoch 4481, Loss: 19.084, Final Batch Loss: 0.583\n",
      "Epoch 4482, Loss: 19.147, Final Batch Loss: 0.631\n",
      "Epoch 4483, Loss: 18.817, Final Batch Loss: 0.488\n",
      "Epoch 4484, Loss: 19.405, Final Batch Loss: 0.551\n",
      "Epoch 4485, Loss: 18.930, Final Batch Loss: 0.551\n",
      "Epoch 4486, Loss: 19.054, Final Batch Loss: 0.574\n",
      "Epoch 4487, Loss: 19.002, Final Batch Loss: 0.472\n",
      "Epoch 4488, Loss: 19.043, Final Batch Loss: 0.535\n",
      "Epoch 4489, Loss: 18.903, Final Batch Loss: 0.543\n",
      "Epoch 4490, Loss: 19.164, Final Batch Loss: 0.634\n",
      "Epoch 4491, Loss: 18.914, Final Batch Loss: 0.525\n",
      "Epoch 4492, Loss: 18.934, Final Batch Loss: 0.593\n",
      "Epoch 4493, Loss: 18.880, Final Batch Loss: 0.494\n",
      "Epoch 4494, Loss: 19.089, Final Batch Loss: 0.539\n",
      "Epoch 4495, Loss: 19.025, Final Batch Loss: 0.508\n",
      "Epoch 4496, Loss: 19.166, Final Batch Loss: 0.528\n",
      "Epoch 4497, Loss: 18.902, Final Batch Loss: 0.441\n",
      "Epoch 4498, Loss: 19.091, Final Batch Loss: 0.557\n",
      "Epoch 4499, Loss: 19.157, Final Batch Loss: 0.733\n",
      "Epoch 4500, Loss: 19.260, Final Batch Loss: 0.597\n",
      "Epoch 4501, Loss: 19.073, Final Batch Loss: 0.489\n",
      "Epoch 4502, Loss: 19.168, Final Batch Loss: 0.466\n",
      "Epoch 4503, Loss: 18.992, Final Batch Loss: 0.522\n",
      "Epoch 4504, Loss: 19.079, Final Batch Loss: 0.483\n",
      "Epoch 4505, Loss: 19.167, Final Batch Loss: 0.519\n",
      "Epoch 4506, Loss: 19.052, Final Batch Loss: 0.609\n",
      "Epoch 4507, Loss: 19.324, Final Batch Loss: 0.635\n",
      "Epoch 4508, Loss: 19.255, Final Batch Loss: 0.463\n",
      "Epoch 4509, Loss: 19.181, Final Batch Loss: 0.525\n",
      "Epoch 4510, Loss: 19.033, Final Batch Loss: 0.599\n",
      "Epoch 4511, Loss: 19.107, Final Batch Loss: 0.449\n",
      "Epoch 4512, Loss: 19.129, Final Batch Loss: 0.595\n",
      "Epoch 4513, Loss: 19.273, Final Batch Loss: 0.500\n",
      "Epoch 4514, Loss: 18.880, Final Batch Loss: 0.556\n",
      "Epoch 4515, Loss: 18.833, Final Batch Loss: 0.502\n",
      "Epoch 4516, Loss: 19.036, Final Batch Loss: 0.488\n",
      "Epoch 4517, Loss: 18.785, Final Batch Loss: 0.478\n",
      "Epoch 4518, Loss: 18.891, Final Batch Loss: 0.536\n",
      "Epoch 4519, Loss: 18.942, Final Batch Loss: 0.582\n",
      "Epoch 4520, Loss: 18.999, Final Batch Loss: 0.483\n",
      "Epoch 4521, Loss: 18.879, Final Batch Loss: 0.465\n",
      "Epoch 4522, Loss: 19.083, Final Batch Loss: 0.532\n",
      "Epoch 4523, Loss: 18.912, Final Batch Loss: 0.505\n",
      "Epoch 4524, Loss: 19.198, Final Batch Loss: 0.553\n",
      "Epoch 4525, Loss: 18.907, Final Batch Loss: 0.462\n",
      "Epoch 4526, Loss: 18.856, Final Batch Loss: 0.506\n",
      "Epoch 4527, Loss: 18.855, Final Batch Loss: 0.488\n",
      "Epoch 4528, Loss: 19.014, Final Batch Loss: 0.474\n",
      "Epoch 4529, Loss: 18.963, Final Batch Loss: 0.507\n",
      "Epoch 4530, Loss: 18.892, Final Batch Loss: 0.475\n",
      "Epoch 4531, Loss: 18.847, Final Batch Loss: 0.527\n",
      "Epoch 4532, Loss: 18.891, Final Batch Loss: 0.527\n",
      "Epoch 4533, Loss: 19.136, Final Batch Loss: 0.583\n",
      "Epoch 4534, Loss: 18.779, Final Batch Loss: 0.595\n",
      "Epoch 4535, Loss: 19.118, Final Batch Loss: 0.486\n",
      "Epoch 4536, Loss: 19.108, Final Batch Loss: 0.506\n",
      "Epoch 4537, Loss: 18.999, Final Batch Loss: 0.577\n",
      "Epoch 4538, Loss: 19.242, Final Batch Loss: 0.502\n",
      "Epoch 4539, Loss: 19.060, Final Batch Loss: 0.495\n",
      "Epoch 4540, Loss: 19.073, Final Batch Loss: 0.483\n",
      "Epoch 4541, Loss: 18.946, Final Batch Loss: 0.529\n",
      "Epoch 4542, Loss: 18.987, Final Batch Loss: 0.624\n",
      "Epoch 4543, Loss: 18.890, Final Batch Loss: 0.620\n",
      "Epoch 4544, Loss: 18.973, Final Batch Loss: 0.571\n",
      "Epoch 4545, Loss: 19.002, Final Batch Loss: 0.509\n",
      "Epoch 4546, Loss: 19.108, Final Batch Loss: 0.469\n",
      "Epoch 4547, Loss: 19.162, Final Batch Loss: 0.479\n",
      "Epoch 4548, Loss: 19.135, Final Batch Loss: 0.471\n",
      "Epoch 4549, Loss: 18.986, Final Batch Loss: 0.520\n",
      "Epoch 4550, Loss: 18.874, Final Batch Loss: 0.508\n",
      "Epoch 4551, Loss: 19.071, Final Batch Loss: 0.483\n",
      "Epoch 4552, Loss: 18.909, Final Batch Loss: 0.538\n",
      "Epoch 4553, Loss: 19.267, Final Batch Loss: 0.492\n",
      "Epoch 4554, Loss: 18.845, Final Batch Loss: 0.558\n",
      "Epoch 4555, Loss: 18.929, Final Batch Loss: 0.516\n",
      "Epoch 4556, Loss: 19.039, Final Batch Loss: 0.501\n",
      "Epoch 4557, Loss: 19.199, Final Batch Loss: 0.463\n",
      "Epoch 4558, Loss: 19.119, Final Batch Loss: 0.542\n",
      "Epoch 4559, Loss: 19.131, Final Batch Loss: 0.488\n",
      "Epoch 4560, Loss: 19.260, Final Batch Loss: 0.518\n",
      "Epoch 4561, Loss: 18.761, Final Batch Loss: 0.510\n",
      "Epoch 4562, Loss: 18.927, Final Batch Loss: 0.434\n",
      "Epoch 4563, Loss: 19.026, Final Batch Loss: 0.662\n",
      "Epoch 4564, Loss: 19.056, Final Batch Loss: 0.559\n",
      "Epoch 4565, Loss: 19.142, Final Batch Loss: 0.524\n",
      "Epoch 4566, Loss: 19.074, Final Batch Loss: 0.574\n",
      "Epoch 4567, Loss: 19.130, Final Batch Loss: 0.436\n",
      "Epoch 4568, Loss: 19.097, Final Batch Loss: 0.596\n",
      "Epoch 4569, Loss: 19.110, Final Batch Loss: 0.489\n",
      "Epoch 4570, Loss: 19.317, Final Batch Loss: 0.580\n",
      "Epoch 4571, Loss: 18.832, Final Batch Loss: 0.564\n",
      "Epoch 4572, Loss: 19.037, Final Batch Loss: 0.481\n",
      "Epoch 4573, Loss: 18.964, Final Batch Loss: 0.518\n",
      "Epoch 4574, Loss: 19.194, Final Batch Loss: 0.571\n",
      "Epoch 4575, Loss: 19.235, Final Batch Loss: 0.478\n",
      "Epoch 4576, Loss: 19.091, Final Batch Loss: 0.550\n",
      "Epoch 4577, Loss: 18.888, Final Batch Loss: 0.581\n",
      "Epoch 4578, Loss: 18.702, Final Batch Loss: 0.483\n",
      "Epoch 4579, Loss: 19.323, Final Batch Loss: 0.516\n",
      "Epoch 4580, Loss: 19.083, Final Batch Loss: 0.448\n",
      "Epoch 4581, Loss: 19.153, Final Batch Loss: 0.586\n",
      "Epoch 4582, Loss: 18.952, Final Batch Loss: 0.533\n",
      "Epoch 4583, Loss: 18.934, Final Batch Loss: 0.567\n",
      "Epoch 4584, Loss: 19.190, Final Batch Loss: 0.503\n",
      "Epoch 4585, Loss: 18.990, Final Batch Loss: 0.544\n",
      "Epoch 4586, Loss: 19.079, Final Batch Loss: 0.452\n",
      "Epoch 4587, Loss: 18.947, Final Batch Loss: 0.446\n",
      "Epoch 4588, Loss: 19.128, Final Batch Loss: 0.611\n",
      "Epoch 4589, Loss: 18.934, Final Batch Loss: 0.434\n",
      "Epoch 4590, Loss: 18.829, Final Batch Loss: 0.453\n",
      "Epoch 4591, Loss: 18.823, Final Batch Loss: 0.506\n",
      "Epoch 4592, Loss: 18.881, Final Batch Loss: 0.576\n",
      "Epoch 4593, Loss: 19.066, Final Batch Loss: 0.542\n",
      "Epoch 4594, Loss: 18.941, Final Batch Loss: 0.487\n",
      "Epoch 4595, Loss: 18.921, Final Batch Loss: 0.558\n",
      "Epoch 4596, Loss: 18.987, Final Batch Loss: 0.518\n",
      "Epoch 4597, Loss: 18.797, Final Batch Loss: 0.574\n",
      "Epoch 4598, Loss: 19.168, Final Batch Loss: 0.582\n",
      "Epoch 4599, Loss: 19.093, Final Batch Loss: 0.511\n",
      "Epoch 4600, Loss: 19.249, Final Batch Loss: 0.475\n",
      "Epoch 4601, Loss: 19.343, Final Batch Loss: 0.604\n",
      "Epoch 4602, Loss: 19.144, Final Batch Loss: 0.703\n",
      "Epoch 4603, Loss: 18.903, Final Batch Loss: 0.601\n",
      "Epoch 4604, Loss: 18.826, Final Batch Loss: 0.606\n",
      "Epoch 4605, Loss: 19.086, Final Batch Loss: 0.584\n",
      "Epoch 4606, Loss: 19.222, Final Batch Loss: 0.562\n",
      "Epoch 4607, Loss: 18.947, Final Batch Loss: 0.520\n",
      "Epoch 4608, Loss: 18.988, Final Batch Loss: 0.540\n",
      "Epoch 4609, Loss: 19.186, Final Batch Loss: 0.449\n",
      "Epoch 4610, Loss: 18.977, Final Batch Loss: 0.481\n",
      "Epoch 4611, Loss: 18.813, Final Batch Loss: 0.563\n",
      "Epoch 4612, Loss: 19.066, Final Batch Loss: 0.512\n",
      "Epoch 4613, Loss: 18.923, Final Batch Loss: 0.542\n",
      "Epoch 4614, Loss: 18.955, Final Batch Loss: 0.581\n",
      "Epoch 4615, Loss: 19.011, Final Batch Loss: 0.528\n",
      "Epoch 4616, Loss: 18.721, Final Batch Loss: 0.483\n",
      "Epoch 4617, Loss: 18.976, Final Batch Loss: 0.599\n",
      "Epoch 4618, Loss: 19.091, Final Batch Loss: 0.533\n",
      "Epoch 4619, Loss: 19.080, Final Batch Loss: 0.465\n",
      "Epoch 4620, Loss: 19.142, Final Batch Loss: 0.455\n",
      "Epoch 4621, Loss: 18.870, Final Batch Loss: 0.419\n",
      "Epoch 4622, Loss: 19.033, Final Batch Loss: 0.627\n",
      "Epoch 4623, Loss: 18.775, Final Batch Loss: 0.454\n",
      "Epoch 4624, Loss: 19.075, Final Batch Loss: 0.482\n",
      "Epoch 4625, Loss: 18.873, Final Batch Loss: 0.587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4626, Loss: 18.759, Final Batch Loss: 0.573\n",
      "Epoch 4627, Loss: 19.120, Final Batch Loss: 0.698\n",
      "Epoch 4628, Loss: 18.890, Final Batch Loss: 0.629\n",
      "Epoch 4629, Loss: 19.077, Final Batch Loss: 0.630\n",
      "Epoch 4630, Loss: 18.930, Final Batch Loss: 0.483\n",
      "Epoch 4631, Loss: 18.944, Final Batch Loss: 0.395\n",
      "Epoch 4632, Loss: 19.074, Final Batch Loss: 0.618\n",
      "Epoch 4633, Loss: 19.064, Final Batch Loss: 0.510\n",
      "Epoch 4634, Loss: 19.108, Final Batch Loss: 0.468\n",
      "Epoch 4635, Loss: 19.143, Final Batch Loss: 0.466\n",
      "Epoch 4636, Loss: 18.775, Final Batch Loss: 0.505\n",
      "Epoch 4637, Loss: 19.063, Final Batch Loss: 0.568\n",
      "Epoch 4638, Loss: 18.909, Final Batch Loss: 0.567\n",
      "Epoch 4639, Loss: 19.119, Final Batch Loss: 0.570\n",
      "Epoch 4640, Loss: 19.166, Final Batch Loss: 0.781\n",
      "Epoch 4641, Loss: 18.804, Final Batch Loss: 0.651\n",
      "Epoch 4642, Loss: 18.885, Final Batch Loss: 0.554\n",
      "Epoch 4643, Loss: 19.009, Final Batch Loss: 0.503\n",
      "Epoch 4644, Loss: 18.902, Final Batch Loss: 0.472\n",
      "Epoch 4645, Loss: 19.045, Final Batch Loss: 0.538\n",
      "Epoch 4646, Loss: 18.736, Final Batch Loss: 0.509\n",
      "Epoch 4647, Loss: 18.881, Final Batch Loss: 0.482\n",
      "Epoch 4648, Loss: 18.964, Final Batch Loss: 0.655\n",
      "Epoch 4649, Loss: 18.749, Final Batch Loss: 0.459\n",
      "Epoch 4650, Loss: 18.979, Final Batch Loss: 0.515\n",
      "Epoch 4651, Loss: 18.749, Final Batch Loss: 0.447\n",
      "Epoch 4652, Loss: 19.166, Final Batch Loss: 0.588\n",
      "Epoch 4653, Loss: 18.693, Final Batch Loss: 0.469\n",
      "Epoch 4654, Loss: 18.686, Final Batch Loss: 0.446\n",
      "Epoch 4655, Loss: 18.942, Final Batch Loss: 0.691\n",
      "Epoch 4656, Loss: 18.929, Final Batch Loss: 0.594\n",
      "Epoch 4657, Loss: 19.028, Final Batch Loss: 0.669\n",
      "Epoch 4658, Loss: 18.851, Final Batch Loss: 0.480\n",
      "Epoch 4659, Loss: 18.727, Final Batch Loss: 0.555\n",
      "Epoch 4660, Loss: 18.912, Final Batch Loss: 0.556\n",
      "Epoch 4661, Loss: 19.147, Final Batch Loss: 0.618\n",
      "Epoch 4662, Loss: 19.231, Final Batch Loss: 0.526\n",
      "Epoch 4663, Loss: 18.820, Final Batch Loss: 0.502\n",
      "Epoch 4664, Loss: 18.994, Final Batch Loss: 0.472\n",
      "Epoch 4665, Loss: 18.872, Final Batch Loss: 0.494\n",
      "Epoch 4666, Loss: 19.034, Final Batch Loss: 0.451\n",
      "Epoch 4667, Loss: 18.798, Final Batch Loss: 0.531\n",
      "Epoch 4668, Loss: 18.875, Final Batch Loss: 0.631\n",
      "Epoch 4669, Loss: 18.799, Final Batch Loss: 0.575\n",
      "Epoch 4670, Loss: 18.977, Final Batch Loss: 0.529\n",
      "Epoch 4671, Loss: 18.749, Final Batch Loss: 0.474\n",
      "Epoch 4672, Loss: 18.668, Final Batch Loss: 0.440\n",
      "Epoch 4673, Loss: 18.921, Final Batch Loss: 0.588\n",
      "Epoch 4674, Loss: 19.061, Final Batch Loss: 0.537\n",
      "Epoch 4675, Loss: 19.107, Final Batch Loss: 0.577\n",
      "Epoch 4676, Loss: 19.053, Final Batch Loss: 0.516\n",
      "Epoch 4677, Loss: 18.880, Final Batch Loss: 0.573\n",
      "Epoch 4678, Loss: 18.971, Final Batch Loss: 0.542\n",
      "Epoch 4679, Loss: 19.253, Final Batch Loss: 0.600\n",
      "Epoch 4680, Loss: 19.083, Final Batch Loss: 0.481\n",
      "Epoch 4681, Loss: 18.958, Final Batch Loss: 0.573\n",
      "Epoch 4682, Loss: 18.715, Final Batch Loss: 0.420\n",
      "Epoch 4683, Loss: 19.142, Final Batch Loss: 0.542\n",
      "Epoch 4684, Loss: 18.940, Final Batch Loss: 0.484\n",
      "Epoch 4685, Loss: 18.888, Final Batch Loss: 0.603\n",
      "Epoch 4686, Loss: 18.756, Final Batch Loss: 0.453\n",
      "Epoch 4687, Loss: 18.972, Final Batch Loss: 0.522\n",
      "Epoch 4688, Loss: 18.684, Final Batch Loss: 0.489\n",
      "Epoch 4689, Loss: 18.783, Final Batch Loss: 0.487\n",
      "Epoch 4690, Loss: 19.037, Final Batch Loss: 0.530\n",
      "Epoch 4691, Loss: 18.683, Final Batch Loss: 0.519\n",
      "Epoch 4692, Loss: 19.160, Final Batch Loss: 0.596\n",
      "Epoch 4693, Loss: 18.966, Final Batch Loss: 0.540\n",
      "Epoch 4694, Loss: 18.856, Final Batch Loss: 0.496\n",
      "Epoch 4695, Loss: 18.676, Final Batch Loss: 0.499\n",
      "Epoch 4696, Loss: 18.860, Final Batch Loss: 0.444\n",
      "Epoch 4697, Loss: 18.824, Final Batch Loss: 0.537\n",
      "Epoch 4698, Loss: 18.871, Final Batch Loss: 0.506\n",
      "Epoch 4699, Loss: 18.792, Final Batch Loss: 0.394\n",
      "Epoch 4700, Loss: 18.954, Final Batch Loss: 0.529\n",
      "Epoch 4701, Loss: 18.675, Final Batch Loss: 0.471\n",
      "Epoch 4702, Loss: 19.001, Final Batch Loss: 0.550\n",
      "Epoch 4703, Loss: 18.662, Final Batch Loss: 0.453\n",
      "Epoch 4704, Loss: 18.846, Final Batch Loss: 0.495\n",
      "Epoch 4705, Loss: 18.958, Final Batch Loss: 0.668\n",
      "Epoch 4706, Loss: 18.767, Final Batch Loss: 0.455\n",
      "Epoch 4707, Loss: 18.764, Final Batch Loss: 0.502\n",
      "Epoch 4708, Loss: 18.777, Final Batch Loss: 0.483\n",
      "Epoch 4709, Loss: 19.110, Final Batch Loss: 0.629\n",
      "Epoch 4710, Loss: 18.906, Final Batch Loss: 0.544\n",
      "Epoch 4711, Loss: 18.902, Final Batch Loss: 0.485\n",
      "Epoch 4712, Loss: 19.068, Final Batch Loss: 0.434\n",
      "Epoch 4713, Loss: 19.024, Final Batch Loss: 0.511\n",
      "Epoch 4714, Loss: 18.923, Final Batch Loss: 0.510\n",
      "Epoch 4715, Loss: 18.925, Final Batch Loss: 0.543\n",
      "Epoch 4716, Loss: 18.974, Final Batch Loss: 0.594\n",
      "Epoch 4717, Loss: 18.488, Final Batch Loss: 0.330\n",
      "Epoch 4718, Loss: 18.872, Final Batch Loss: 0.515\n",
      "Epoch 4719, Loss: 18.763, Final Batch Loss: 0.535\n",
      "Epoch 4720, Loss: 19.163, Final Batch Loss: 0.695\n",
      "Epoch 4721, Loss: 18.810, Final Batch Loss: 0.536\n",
      "Epoch 4722, Loss: 18.887, Final Batch Loss: 0.550\n",
      "Epoch 4723, Loss: 19.054, Final Batch Loss: 0.597\n",
      "Epoch 4724, Loss: 19.134, Final Batch Loss: 0.522\n",
      "Epoch 4725, Loss: 18.958, Final Batch Loss: 0.569\n",
      "Epoch 4726, Loss: 18.860, Final Batch Loss: 0.619\n",
      "Epoch 4727, Loss: 18.818, Final Batch Loss: 0.422\n",
      "Epoch 4728, Loss: 18.947, Final Batch Loss: 0.652\n",
      "Epoch 4729, Loss: 18.987, Final Batch Loss: 0.530\n",
      "Epoch 4730, Loss: 18.960, Final Batch Loss: 0.491\n",
      "Epoch 4731, Loss: 18.985, Final Batch Loss: 0.698\n",
      "Epoch 4732, Loss: 18.938, Final Batch Loss: 0.524\n",
      "Epoch 4733, Loss: 18.740, Final Batch Loss: 0.440\n",
      "Epoch 4734, Loss: 19.109, Final Batch Loss: 0.531\n",
      "Epoch 4735, Loss: 18.720, Final Batch Loss: 0.412\n",
      "Epoch 4736, Loss: 18.652, Final Batch Loss: 0.494\n",
      "Epoch 4737, Loss: 18.721, Final Batch Loss: 0.476\n",
      "Epoch 4738, Loss: 18.811, Final Batch Loss: 0.538\n",
      "Epoch 4739, Loss: 18.822, Final Batch Loss: 0.538\n",
      "Epoch 4740, Loss: 18.783, Final Batch Loss: 0.449\n",
      "Epoch 4741, Loss: 18.821, Final Batch Loss: 0.569\n",
      "Epoch 4742, Loss: 18.865, Final Batch Loss: 0.559\n",
      "Epoch 4743, Loss: 18.673, Final Batch Loss: 0.483\n",
      "Epoch 4744, Loss: 18.957, Final Batch Loss: 0.503\n",
      "Epoch 4745, Loss: 18.889, Final Batch Loss: 0.517\n",
      "Epoch 4746, Loss: 18.823, Final Batch Loss: 0.470\n",
      "Epoch 4747, Loss: 18.625, Final Batch Loss: 0.535\n",
      "Epoch 4748, Loss: 18.846, Final Batch Loss: 0.522\n",
      "Epoch 4749, Loss: 18.654, Final Batch Loss: 0.486\n",
      "Epoch 4750, Loss: 19.021, Final Batch Loss: 0.517\n",
      "Epoch 4751, Loss: 18.562, Final Batch Loss: 0.488\n",
      "Epoch 4752, Loss: 18.834, Final Batch Loss: 0.513\n",
      "Epoch 4753, Loss: 18.672, Final Batch Loss: 0.461\n",
      "Epoch 4754, Loss: 18.817, Final Batch Loss: 0.507\n",
      "Epoch 4755, Loss: 18.785, Final Batch Loss: 0.447\n",
      "Epoch 4756, Loss: 18.842, Final Batch Loss: 0.550\n",
      "Epoch 4757, Loss: 18.765, Final Batch Loss: 0.489\n",
      "Epoch 4758, Loss: 18.949, Final Batch Loss: 0.722\n",
      "Epoch 4759, Loss: 18.864, Final Batch Loss: 0.382\n",
      "Epoch 4760, Loss: 18.876, Final Batch Loss: 0.498\n",
      "Epoch 4761, Loss: 18.671, Final Batch Loss: 0.465\n",
      "Epoch 4762, Loss: 18.789, Final Batch Loss: 0.562\n",
      "Epoch 4763, Loss: 18.800, Final Batch Loss: 0.564\n",
      "Epoch 4764, Loss: 18.778, Final Batch Loss: 0.532\n",
      "Epoch 4765, Loss: 18.872, Final Batch Loss: 0.644\n",
      "Epoch 4766, Loss: 18.666, Final Batch Loss: 0.527\n",
      "Epoch 4767, Loss: 18.658, Final Batch Loss: 0.537\n",
      "Epoch 4768, Loss: 18.836, Final Batch Loss: 0.514\n",
      "Epoch 4769, Loss: 18.733, Final Batch Loss: 0.532\n",
      "Epoch 4770, Loss: 18.624, Final Batch Loss: 0.509\n",
      "Epoch 4771, Loss: 18.800, Final Batch Loss: 0.513\n",
      "Epoch 4772, Loss: 18.621, Final Batch Loss: 0.482\n",
      "Epoch 4773, Loss: 19.017, Final Batch Loss: 0.624\n",
      "Epoch 4774, Loss: 18.838, Final Batch Loss: 0.467\n",
      "Epoch 4775, Loss: 18.841, Final Batch Loss: 0.477\n",
      "Epoch 4776, Loss: 18.877, Final Batch Loss: 0.510\n",
      "Epoch 4777, Loss: 18.843, Final Batch Loss: 0.510\n",
      "Epoch 4778, Loss: 18.917, Final Batch Loss: 0.453\n",
      "Epoch 4779, Loss: 18.885, Final Batch Loss: 0.552\n",
      "Epoch 4780, Loss: 18.591, Final Batch Loss: 0.601\n",
      "Epoch 4781, Loss: 18.721, Final Batch Loss: 0.605\n",
      "Epoch 4782, Loss: 19.098, Final Batch Loss: 0.616\n",
      "Epoch 4783, Loss: 18.746, Final Batch Loss: 0.446\n",
      "Epoch 4784, Loss: 18.902, Final Batch Loss: 0.572\n",
      "Epoch 4785, Loss: 18.655, Final Batch Loss: 0.517\n",
      "Epoch 4786, Loss: 18.613, Final Batch Loss: 0.528\n",
      "Epoch 4787, Loss: 18.974, Final Batch Loss: 0.561\n",
      "Epoch 4788, Loss: 18.844, Final Batch Loss: 0.436\n",
      "Epoch 4789, Loss: 18.630, Final Batch Loss: 0.540\n",
      "Epoch 4790, Loss: 18.652, Final Batch Loss: 0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4791, Loss: 18.692, Final Batch Loss: 0.565\n",
      "Epoch 4792, Loss: 18.811, Final Batch Loss: 0.504\n",
      "Epoch 4793, Loss: 18.727, Final Batch Loss: 0.436\n",
      "Epoch 4794, Loss: 18.772, Final Batch Loss: 0.563\n",
      "Epoch 4795, Loss: 18.689, Final Batch Loss: 0.577\n",
      "Epoch 4796, Loss: 18.578, Final Batch Loss: 0.602\n",
      "Epoch 4797, Loss: 18.738, Final Batch Loss: 0.506\n",
      "Epoch 4798, Loss: 18.920, Final Batch Loss: 0.526\n",
      "Epoch 4799, Loss: 18.666, Final Batch Loss: 0.400\n",
      "Epoch 4800, Loss: 18.550, Final Batch Loss: 0.481\n",
      "Epoch 4801, Loss: 18.899, Final Batch Loss: 0.552\n",
      "Epoch 4802, Loss: 18.661, Final Batch Loss: 0.637\n",
      "Epoch 4803, Loss: 18.613, Final Batch Loss: 0.570\n",
      "Epoch 4804, Loss: 18.828, Final Batch Loss: 0.537\n",
      "Epoch 4805, Loss: 18.730, Final Batch Loss: 0.559\n",
      "Epoch 4806, Loss: 18.875, Final Batch Loss: 0.534\n",
      "Epoch 4807, Loss: 18.793, Final Batch Loss: 0.551\n",
      "Epoch 4808, Loss: 18.814, Final Batch Loss: 0.506\n",
      "Epoch 4809, Loss: 18.790, Final Batch Loss: 0.477\n",
      "Epoch 4810, Loss: 18.615, Final Batch Loss: 0.537\n",
      "Epoch 4811, Loss: 18.670, Final Batch Loss: 0.515\n",
      "Epoch 4812, Loss: 18.653, Final Batch Loss: 0.463\n",
      "Epoch 4813, Loss: 18.582, Final Batch Loss: 0.528\n",
      "Epoch 4814, Loss: 18.744, Final Batch Loss: 0.576\n",
      "Epoch 4815, Loss: 18.833, Final Batch Loss: 0.595\n",
      "Epoch 4816, Loss: 18.773, Final Batch Loss: 0.514\n",
      "Epoch 4817, Loss: 18.763, Final Batch Loss: 0.657\n",
      "Epoch 4818, Loss: 18.876, Final Batch Loss: 0.498\n",
      "Epoch 4819, Loss: 18.760, Final Batch Loss: 0.449\n",
      "Epoch 4820, Loss: 18.874, Final Batch Loss: 0.627\n",
      "Epoch 4821, Loss: 18.887, Final Batch Loss: 0.514\n",
      "Epoch 4822, Loss: 18.665, Final Batch Loss: 0.458\n",
      "Epoch 4823, Loss: 18.887, Final Batch Loss: 0.538\n",
      "Epoch 4824, Loss: 18.533, Final Batch Loss: 0.475\n",
      "Epoch 4825, Loss: 18.717, Final Batch Loss: 0.530\n",
      "Epoch 4826, Loss: 18.849, Final Batch Loss: 0.599\n",
      "Epoch 4827, Loss: 18.796, Final Batch Loss: 0.438\n",
      "Epoch 4828, Loss: 18.784, Final Batch Loss: 0.547\n",
      "Epoch 4829, Loss: 18.746, Final Batch Loss: 0.547\n",
      "Epoch 4830, Loss: 18.584, Final Batch Loss: 0.527\n",
      "Epoch 4831, Loss: 18.428, Final Batch Loss: 0.387\n",
      "Epoch 4832, Loss: 18.769, Final Batch Loss: 0.582\n",
      "Epoch 4833, Loss: 18.898, Final Batch Loss: 0.473\n",
      "Epoch 4834, Loss: 18.284, Final Batch Loss: 0.564\n",
      "Epoch 4835, Loss: 18.499, Final Batch Loss: 0.557\n",
      "Epoch 4836, Loss: 18.599, Final Batch Loss: 0.494\n",
      "Epoch 4837, Loss: 18.659, Final Batch Loss: 0.477\n",
      "Epoch 4838, Loss: 18.638, Final Batch Loss: 0.416\n",
      "Epoch 4839, Loss: 18.824, Final Batch Loss: 0.494\n",
      "Epoch 4840, Loss: 18.627, Final Batch Loss: 0.438\n",
      "Epoch 4841, Loss: 18.444, Final Batch Loss: 0.515\n",
      "Epoch 4842, Loss: 18.776, Final Batch Loss: 0.465\n",
      "Epoch 4843, Loss: 18.652, Final Batch Loss: 0.534\n",
      "Epoch 4844, Loss: 18.536, Final Batch Loss: 0.499\n",
      "Epoch 4845, Loss: 18.841, Final Batch Loss: 0.491\n",
      "Epoch 4846, Loss: 18.523, Final Batch Loss: 0.403\n",
      "Epoch 4847, Loss: 18.716, Final Batch Loss: 0.465\n",
      "Epoch 4848, Loss: 18.789, Final Batch Loss: 0.516\n",
      "Epoch 4849, Loss: 18.715, Final Batch Loss: 0.479\n",
      "Epoch 4850, Loss: 18.853, Final Batch Loss: 0.627\n",
      "Epoch 4851, Loss: 18.709, Final Batch Loss: 0.658\n",
      "Epoch 4852, Loss: 18.447, Final Batch Loss: 0.554\n",
      "Epoch 4853, Loss: 18.959, Final Batch Loss: 0.560\n",
      "Epoch 4854, Loss: 18.498, Final Batch Loss: 0.554\n",
      "Epoch 4855, Loss: 18.737, Final Batch Loss: 0.470\n",
      "Epoch 4856, Loss: 18.801, Final Batch Loss: 0.458\n",
      "Epoch 4857, Loss: 18.635, Final Batch Loss: 0.492\n",
      "Epoch 4858, Loss: 18.579, Final Batch Loss: 0.486\n",
      "Epoch 4859, Loss: 18.535, Final Batch Loss: 0.415\n",
      "Epoch 4860, Loss: 18.601, Final Batch Loss: 0.476\n",
      "Epoch 4861, Loss: 18.503, Final Batch Loss: 0.391\n",
      "Epoch 4862, Loss: 18.446, Final Batch Loss: 0.602\n",
      "Epoch 4863, Loss: 18.500, Final Batch Loss: 0.382\n",
      "Epoch 4864, Loss: 19.084, Final Batch Loss: 0.596\n",
      "Epoch 4865, Loss: 18.532, Final Batch Loss: 0.482\n",
      "Epoch 4866, Loss: 18.789, Final Batch Loss: 0.494\n",
      "Epoch 4867, Loss: 18.378, Final Batch Loss: 0.468\n",
      "Epoch 4868, Loss: 18.655, Final Batch Loss: 0.459\n",
      "Epoch 4869, Loss: 18.532, Final Batch Loss: 0.464\n",
      "Epoch 4870, Loss: 18.506, Final Batch Loss: 0.461\n",
      "Epoch 4871, Loss: 18.684, Final Batch Loss: 0.559\n",
      "Epoch 4872, Loss: 18.616, Final Batch Loss: 0.607\n",
      "Epoch 4873, Loss: 18.703, Final Batch Loss: 0.457\n",
      "Epoch 4874, Loss: 18.875, Final Batch Loss: 0.517\n",
      "Epoch 4875, Loss: 18.649, Final Batch Loss: 0.592\n",
      "Epoch 4876, Loss: 18.549, Final Batch Loss: 0.524\n",
      "Epoch 4877, Loss: 18.819, Final Batch Loss: 0.604\n",
      "Epoch 4878, Loss: 18.350, Final Batch Loss: 0.438\n",
      "Epoch 4879, Loss: 18.653, Final Batch Loss: 0.515\n",
      "Epoch 4880, Loss: 18.223, Final Batch Loss: 0.607\n",
      "Epoch 4881, Loss: 18.378, Final Batch Loss: 0.571\n",
      "Epoch 4882, Loss: 18.737, Final Batch Loss: 0.498\n",
      "Epoch 4883, Loss: 18.689, Final Batch Loss: 0.564\n",
      "Epoch 4884, Loss: 18.331, Final Batch Loss: 0.451\n",
      "Epoch 4885, Loss: 18.587, Final Batch Loss: 0.624\n",
      "Epoch 4886, Loss: 18.414, Final Batch Loss: 0.510\n",
      "Epoch 4887, Loss: 18.539, Final Batch Loss: 0.407\n",
      "Epoch 4888, Loss: 18.423, Final Batch Loss: 0.570\n",
      "Epoch 4889, Loss: 18.838, Final Batch Loss: 0.547\n",
      "Epoch 4890, Loss: 18.654, Final Batch Loss: 0.607\n",
      "Epoch 4891, Loss: 18.809, Final Batch Loss: 0.664\n",
      "Epoch 4892, Loss: 18.446, Final Batch Loss: 0.461\n",
      "Epoch 4893, Loss: 18.548, Final Batch Loss: 0.451\n",
      "Epoch 4894, Loss: 18.271, Final Batch Loss: 0.472\n",
      "Epoch 4895, Loss: 18.525, Final Batch Loss: 0.510\n",
      "Epoch 4896, Loss: 18.650, Final Batch Loss: 0.623\n",
      "Epoch 4897, Loss: 18.495, Final Batch Loss: 0.536\n",
      "Epoch 4898, Loss: 18.465, Final Batch Loss: 0.541\n",
      "Epoch 4899, Loss: 18.230, Final Batch Loss: 0.476\n",
      "Epoch 4900, Loss: 18.625, Final Batch Loss: 0.555\n",
      "Epoch 4901, Loss: 18.813, Final Batch Loss: 0.478\n",
      "Epoch 4902, Loss: 18.553, Final Batch Loss: 0.540\n",
      "Epoch 4903, Loss: 18.526, Final Batch Loss: 0.536\n",
      "Epoch 4904, Loss: 18.560, Final Batch Loss: 0.525\n",
      "Epoch 4905, Loss: 18.601, Final Batch Loss: 0.558\n",
      "Epoch 4906, Loss: 18.329, Final Batch Loss: 0.516\n",
      "Epoch 4907, Loss: 18.470, Final Batch Loss: 0.477\n",
      "Epoch 4908, Loss: 18.710, Final Batch Loss: 0.586\n",
      "Epoch 4909, Loss: 18.598, Final Batch Loss: 0.603\n",
      "Epoch 4910, Loss: 18.518, Final Batch Loss: 0.553\n",
      "Epoch 4911, Loss: 18.416, Final Batch Loss: 0.526\n",
      "Epoch 4912, Loss: 18.270, Final Batch Loss: 0.482\n",
      "Epoch 4913, Loss: 18.512, Final Batch Loss: 0.444\n",
      "Epoch 4914, Loss: 18.680, Final Batch Loss: 0.509\n",
      "Epoch 4915, Loss: 18.356, Final Batch Loss: 0.501\n",
      "Epoch 4916, Loss: 18.169, Final Batch Loss: 0.526\n",
      "Epoch 4917, Loss: 18.477, Final Batch Loss: 0.552\n",
      "Epoch 4918, Loss: 18.207, Final Batch Loss: 0.505\n",
      "Epoch 4919, Loss: 18.570, Final Batch Loss: 0.520\n",
      "Epoch 4920, Loss: 18.455, Final Batch Loss: 0.455\n",
      "Epoch 4921, Loss: 18.468, Final Batch Loss: 0.539\n",
      "Epoch 4922, Loss: 18.675, Final Batch Loss: 0.520\n",
      "Epoch 4923, Loss: 18.326, Final Batch Loss: 0.501\n",
      "Epoch 4924, Loss: 18.506, Final Batch Loss: 0.587\n",
      "Epoch 4925, Loss: 18.498, Final Batch Loss: 0.483\n",
      "Epoch 4926, Loss: 18.296, Final Batch Loss: 0.413\n",
      "Epoch 4927, Loss: 18.494, Final Batch Loss: 0.514\n",
      "Epoch 4928, Loss: 18.386, Final Batch Loss: 0.500\n",
      "Epoch 4929, Loss: 18.547, Final Batch Loss: 0.552\n",
      "Epoch 4930, Loss: 18.302, Final Batch Loss: 0.603\n",
      "Epoch 4931, Loss: 18.788, Final Batch Loss: 0.524\n",
      "Epoch 4932, Loss: 18.359, Final Batch Loss: 0.455\n",
      "Epoch 4933, Loss: 18.630, Final Batch Loss: 0.469\n",
      "Epoch 4934, Loss: 18.500, Final Batch Loss: 0.584\n",
      "Epoch 4935, Loss: 18.393, Final Batch Loss: 0.437\n",
      "Epoch 4936, Loss: 18.244, Final Batch Loss: 0.505\n",
      "Epoch 4937, Loss: 18.104, Final Batch Loss: 0.538\n",
      "Epoch 4938, Loss: 18.572, Final Batch Loss: 0.549\n",
      "Epoch 4939, Loss: 18.182, Final Batch Loss: 0.480\n",
      "Epoch 4940, Loss: 18.260, Final Batch Loss: 0.403\n",
      "Epoch 4941, Loss: 18.633, Final Batch Loss: 0.663\n",
      "Epoch 4942, Loss: 18.205, Final Batch Loss: 0.432\n",
      "Epoch 4943, Loss: 18.597, Final Batch Loss: 0.543\n",
      "Epoch 4944, Loss: 18.234, Final Batch Loss: 0.438\n",
      "Epoch 4945, Loss: 18.521, Final Batch Loss: 0.500\n",
      "Epoch 4946, Loss: 18.266, Final Batch Loss: 0.575\n",
      "Epoch 4947, Loss: 18.608, Final Batch Loss: 0.545\n",
      "Epoch 4948, Loss: 18.431, Final Batch Loss: 0.440\n",
      "Epoch 4949, Loss: 18.352, Final Batch Loss: 0.444\n",
      "Epoch 4950, Loss: 18.295, Final Batch Loss: 0.558\n",
      "Epoch 4951, Loss: 18.688, Final Batch Loss: 0.475\n",
      "Epoch 4952, Loss: 18.164, Final Batch Loss: 0.446\n",
      "Epoch 4953, Loss: 18.602, Final Batch Loss: 0.462\n",
      "Epoch 4954, Loss: 18.280, Final Batch Loss: 0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4955, Loss: 18.331, Final Batch Loss: 0.487\n",
      "Epoch 4956, Loss: 18.541, Final Batch Loss: 0.529\n",
      "Epoch 4957, Loss: 18.314, Final Batch Loss: 0.437\n",
      "Epoch 4958, Loss: 18.241, Final Batch Loss: 0.469\n",
      "Epoch 4959, Loss: 18.340, Final Batch Loss: 0.541\n",
      "Epoch 4960, Loss: 18.403, Final Batch Loss: 0.529\n",
      "Epoch 4961, Loss: 18.274, Final Batch Loss: 0.469\n",
      "Epoch 4962, Loss: 18.573, Final Batch Loss: 0.390\n",
      "Epoch 4963, Loss: 18.186, Final Batch Loss: 0.476\n",
      "Epoch 4964, Loss: 18.366, Final Batch Loss: 0.516\n",
      "Epoch 4965, Loss: 18.347, Final Batch Loss: 0.536\n",
      "Epoch 4966, Loss: 18.380, Final Batch Loss: 0.530\n",
      "Epoch 4967, Loss: 18.440, Final Batch Loss: 0.530\n",
      "Epoch 4968, Loss: 18.683, Final Batch Loss: 0.578\n",
      "Epoch 4969, Loss: 18.381, Final Batch Loss: 0.459\n",
      "Epoch 4970, Loss: 18.361, Final Batch Loss: 0.494\n",
      "Epoch 4971, Loss: 18.363, Final Batch Loss: 0.433\n",
      "Epoch 4972, Loss: 18.231, Final Batch Loss: 0.515\n",
      "Epoch 4973, Loss: 18.434, Final Batch Loss: 0.529\n",
      "Epoch 4974, Loss: 18.137, Final Batch Loss: 0.453\n",
      "Epoch 4975, Loss: 18.208, Final Batch Loss: 0.479\n",
      "Epoch 4976, Loss: 18.534, Final Batch Loss: 0.568\n",
      "Epoch 4977, Loss: 18.393, Final Batch Loss: 0.580\n",
      "Epoch 4978, Loss: 18.481, Final Batch Loss: 0.502\n",
      "Epoch 4979, Loss: 18.487, Final Batch Loss: 0.486\n",
      "Epoch 4980, Loss: 18.337, Final Batch Loss: 0.556\n",
      "Epoch 4981, Loss: 18.552, Final Batch Loss: 0.502\n",
      "Epoch 4982, Loss: 18.350, Final Batch Loss: 0.424\n",
      "Epoch 4983, Loss: 18.414, Final Batch Loss: 0.475\n",
      "Epoch 4984, Loss: 18.143, Final Batch Loss: 0.495\n",
      "Epoch 4985, Loss: 18.118, Final Batch Loss: 0.461\n",
      "Epoch 4986, Loss: 17.988, Final Batch Loss: 0.427\n",
      "Epoch 4987, Loss: 18.256, Final Batch Loss: 0.553\n",
      "Epoch 4988, Loss: 18.179, Final Batch Loss: 0.473\n",
      "Epoch 4989, Loss: 18.368, Final Batch Loss: 0.473\n",
      "Epoch 4990, Loss: 18.665, Final Batch Loss: 0.577\n",
      "Epoch 4991, Loss: 18.579, Final Batch Loss: 0.549\n",
      "Epoch 4992, Loss: 18.308, Final Batch Loss: 0.555\n",
      "Epoch 4993, Loss: 18.235, Final Batch Loss: 0.502\n",
      "Epoch 4994, Loss: 18.234, Final Batch Loss: 0.583\n",
      "Epoch 4995, Loss: 18.071, Final Batch Loss: 0.563\n",
      "Epoch 4996, Loss: 18.193, Final Batch Loss: 0.364\n",
      "Epoch 4997, Loss: 18.418, Final Batch Loss: 0.557\n",
      "Epoch 4998, Loss: 18.297, Final Batch Loss: 0.432\n",
      "Epoch 4999, Loss: 18.078, Final Batch Loss: 0.557\n",
      "Epoch 5000, Loss: 18.260, Final Batch Loss: 0.529\n",
      "Epoch 5001, Loss: 18.290, Final Batch Loss: 0.528\n",
      "Epoch 5002, Loss: 18.303, Final Batch Loss: 0.515\n",
      "Epoch 5003, Loss: 17.989, Final Batch Loss: 0.577\n",
      "Epoch 5004, Loss: 18.315, Final Batch Loss: 0.556\n",
      "Epoch 5005, Loss: 18.268, Final Batch Loss: 0.552\n",
      "Epoch 5006, Loss: 17.927, Final Batch Loss: 0.467\n",
      "Epoch 5007, Loss: 18.387, Final Batch Loss: 0.497\n",
      "Epoch 5008, Loss: 18.180, Final Batch Loss: 0.486\n",
      "Epoch 5009, Loss: 18.173, Final Batch Loss: 0.448\n",
      "Epoch 5010, Loss: 18.029, Final Batch Loss: 0.471\n",
      "Epoch 5011, Loss: 18.153, Final Batch Loss: 0.504\n",
      "Epoch 5012, Loss: 18.072, Final Batch Loss: 0.531\n",
      "Epoch 5013, Loss: 18.180, Final Batch Loss: 0.544\n",
      "Epoch 5014, Loss: 17.939, Final Batch Loss: 0.460\n",
      "Epoch 5015, Loss: 18.130, Final Batch Loss: 0.494\n",
      "Epoch 5016, Loss: 18.051, Final Batch Loss: 0.638\n",
      "Epoch 5017, Loss: 17.898, Final Batch Loss: 0.396\n",
      "Epoch 5018, Loss: 18.140, Final Batch Loss: 0.461\n",
      "Epoch 5019, Loss: 18.254, Final Batch Loss: 0.538\n",
      "Epoch 5020, Loss: 18.127, Final Batch Loss: 0.463\n",
      "Epoch 5021, Loss: 18.015, Final Batch Loss: 0.491\n",
      "Epoch 5022, Loss: 18.302, Final Batch Loss: 0.597\n",
      "Epoch 5023, Loss: 18.129, Final Batch Loss: 0.474\n",
      "Epoch 5024, Loss: 17.947, Final Batch Loss: 0.409\n",
      "Epoch 5025, Loss: 18.227, Final Batch Loss: 0.469\n",
      "Epoch 5026, Loss: 18.250, Final Batch Loss: 0.500\n",
      "Epoch 5027, Loss: 18.073, Final Batch Loss: 0.497\n",
      "Epoch 5028, Loss: 18.168, Final Batch Loss: 0.437\n",
      "Epoch 5029, Loss: 17.987, Final Batch Loss: 0.501\n",
      "Epoch 5030, Loss: 18.205, Final Batch Loss: 0.539\n",
      "Epoch 5031, Loss: 17.932, Final Batch Loss: 0.454\n",
      "Epoch 5032, Loss: 18.180, Final Batch Loss: 0.476\n",
      "Epoch 5033, Loss: 18.163, Final Batch Loss: 0.619\n",
      "Epoch 5034, Loss: 18.336, Final Batch Loss: 0.513\n",
      "Epoch 5035, Loss: 18.284, Final Batch Loss: 0.389\n",
      "Epoch 5036, Loss: 17.992, Final Batch Loss: 0.615\n",
      "Epoch 5037, Loss: 17.928, Final Batch Loss: 0.495\n",
      "Epoch 5038, Loss: 17.925, Final Batch Loss: 0.521\n",
      "Epoch 5039, Loss: 17.894, Final Batch Loss: 0.472\n",
      "Epoch 5040, Loss: 18.115, Final Batch Loss: 0.578\n",
      "Epoch 5041, Loss: 18.025, Final Batch Loss: 0.466\n",
      "Epoch 5042, Loss: 17.948, Final Batch Loss: 0.459\n",
      "Epoch 5043, Loss: 17.929, Final Batch Loss: 0.409\n",
      "Epoch 5044, Loss: 18.255, Final Batch Loss: 0.548\n",
      "Epoch 5045, Loss: 18.149, Final Batch Loss: 0.500\n",
      "Epoch 5046, Loss: 18.285, Final Batch Loss: 0.472\n",
      "Epoch 5047, Loss: 17.989, Final Batch Loss: 0.367\n",
      "Epoch 5048, Loss: 17.999, Final Batch Loss: 0.388\n",
      "Epoch 5049, Loss: 18.109, Final Batch Loss: 0.511\n",
      "Epoch 5050, Loss: 18.354, Final Batch Loss: 0.453\n",
      "Epoch 5051, Loss: 18.060, Final Batch Loss: 0.466\n",
      "Epoch 5052, Loss: 18.062, Final Batch Loss: 0.525\n",
      "Epoch 5053, Loss: 18.254, Final Batch Loss: 0.490\n",
      "Epoch 5054, Loss: 18.301, Final Batch Loss: 0.598\n",
      "Epoch 5055, Loss: 18.144, Final Batch Loss: 0.535\n",
      "Epoch 5056, Loss: 18.366, Final Batch Loss: 0.471\n",
      "Epoch 5057, Loss: 18.119, Final Batch Loss: 0.538\n",
      "Epoch 5058, Loss: 17.897, Final Batch Loss: 0.530\n",
      "Epoch 5059, Loss: 17.971, Final Batch Loss: 0.499\n",
      "Epoch 5060, Loss: 17.837, Final Batch Loss: 0.496\n",
      "Epoch 5061, Loss: 18.107, Final Batch Loss: 0.605\n",
      "Epoch 5062, Loss: 17.959, Final Batch Loss: 0.366\n",
      "Epoch 5063, Loss: 18.048, Final Batch Loss: 0.445\n",
      "Epoch 5064, Loss: 18.115, Final Batch Loss: 0.416\n",
      "Epoch 5065, Loss: 18.118, Final Batch Loss: 0.445\n",
      "Epoch 5066, Loss: 18.048, Final Batch Loss: 0.430\n",
      "Epoch 5067, Loss: 17.928, Final Batch Loss: 0.481\n",
      "Epoch 5068, Loss: 17.860, Final Batch Loss: 0.570\n",
      "Epoch 5069, Loss: 17.958, Final Batch Loss: 0.538\n",
      "Epoch 5070, Loss: 17.980, Final Batch Loss: 0.513\n",
      "Epoch 5071, Loss: 18.062, Final Batch Loss: 0.498\n",
      "Epoch 5072, Loss: 18.130, Final Batch Loss: 0.532\n",
      "Epoch 5073, Loss: 18.027, Final Batch Loss: 0.522\n",
      "Epoch 5074, Loss: 17.803, Final Batch Loss: 0.533\n",
      "Epoch 5075, Loss: 18.019, Final Batch Loss: 0.603\n",
      "Epoch 5076, Loss: 18.225, Final Batch Loss: 0.575\n",
      "Epoch 5077, Loss: 17.918, Final Batch Loss: 0.408\n",
      "Epoch 5078, Loss: 17.771, Final Batch Loss: 0.503\n",
      "Epoch 5079, Loss: 17.812, Final Batch Loss: 0.417\n",
      "Epoch 5080, Loss: 18.027, Final Batch Loss: 0.475\n",
      "Epoch 5081, Loss: 17.854, Final Batch Loss: 0.458\n",
      "Epoch 5082, Loss: 18.101, Final Batch Loss: 0.454\n",
      "Epoch 5083, Loss: 18.032, Final Batch Loss: 0.545\n",
      "Epoch 5084, Loss: 18.094, Final Batch Loss: 0.513\n",
      "Epoch 5085, Loss: 18.135, Final Batch Loss: 0.502\n",
      "Epoch 5086, Loss: 18.079, Final Batch Loss: 0.542\n",
      "Epoch 5087, Loss: 18.009, Final Batch Loss: 0.520\n",
      "Epoch 5088, Loss: 17.932, Final Batch Loss: 0.507\n",
      "Epoch 5089, Loss: 18.122, Final Batch Loss: 0.520\n",
      "Epoch 5090, Loss: 17.987, Final Batch Loss: 0.530\n",
      "Epoch 5091, Loss: 18.075, Final Batch Loss: 0.672\n",
      "Epoch 5092, Loss: 18.004, Final Batch Loss: 0.407\n",
      "Epoch 5093, Loss: 18.213, Final Batch Loss: 0.521\n",
      "Epoch 5094, Loss: 17.976, Final Batch Loss: 0.452\n",
      "Epoch 5095, Loss: 17.981, Final Batch Loss: 0.526\n",
      "Epoch 5096, Loss: 17.984, Final Batch Loss: 0.446\n",
      "Epoch 5097, Loss: 18.066, Final Batch Loss: 0.450\n",
      "Epoch 5098, Loss: 18.096, Final Batch Loss: 0.430\n",
      "Epoch 5099, Loss: 18.084, Final Batch Loss: 0.546\n",
      "Epoch 5100, Loss: 18.089, Final Batch Loss: 0.640\n",
      "Epoch 5101, Loss: 17.957, Final Batch Loss: 0.472\n",
      "Epoch 5102, Loss: 18.001, Final Batch Loss: 0.508\n",
      "Epoch 5103, Loss: 18.107, Final Batch Loss: 0.513\n",
      "Epoch 5104, Loss: 17.990, Final Batch Loss: 0.484\n",
      "Epoch 5105, Loss: 18.140, Final Batch Loss: 0.504\n",
      "Epoch 5106, Loss: 18.072, Final Batch Loss: 0.503\n",
      "Epoch 5107, Loss: 18.067, Final Batch Loss: 0.497\n",
      "Epoch 5108, Loss: 17.849, Final Batch Loss: 0.556\n",
      "Epoch 5109, Loss: 18.097, Final Batch Loss: 0.484\n",
      "Epoch 5110, Loss: 18.084, Final Batch Loss: 0.527\n",
      "Epoch 5111, Loss: 18.012, Final Batch Loss: 0.507\n",
      "Epoch 5112, Loss: 18.191, Final Batch Loss: 0.485\n",
      "Epoch 5113, Loss: 18.144, Final Batch Loss: 0.590\n",
      "Epoch 5114, Loss: 18.031, Final Batch Loss: 0.478\n",
      "Epoch 5115, Loss: 18.076, Final Batch Loss: 0.461\n",
      "Epoch 5116, Loss: 18.280, Final Batch Loss: 0.523\n",
      "Epoch 5117, Loss: 18.005, Final Batch Loss: 0.550\n",
      "Epoch 5118, Loss: 18.177, Final Batch Loss: 0.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5119, Loss: 18.111, Final Batch Loss: 0.500\n",
      "Epoch 5120, Loss: 17.841, Final Batch Loss: 0.465\n",
      "Epoch 5121, Loss: 18.001, Final Batch Loss: 0.470\n",
      "Epoch 5122, Loss: 18.222, Final Batch Loss: 0.517\n",
      "Epoch 5123, Loss: 18.105, Final Batch Loss: 0.486\n",
      "Epoch 5124, Loss: 17.946, Final Batch Loss: 0.462\n",
      "Epoch 5125, Loss: 17.824, Final Batch Loss: 0.587\n",
      "Epoch 5126, Loss: 18.188, Final Batch Loss: 0.469\n",
      "Epoch 5127, Loss: 17.880, Final Batch Loss: 0.544\n",
      "Epoch 5128, Loss: 18.272, Final Batch Loss: 0.416\n",
      "Epoch 5129, Loss: 18.074, Final Batch Loss: 0.422\n",
      "Epoch 5130, Loss: 17.909, Final Batch Loss: 0.506\n",
      "Epoch 5131, Loss: 17.806, Final Batch Loss: 0.488\n",
      "Epoch 5132, Loss: 17.914, Final Batch Loss: 0.391\n",
      "Epoch 5133, Loss: 17.788, Final Batch Loss: 0.498\n",
      "Epoch 5134, Loss: 17.960, Final Batch Loss: 0.501\n",
      "Epoch 5135, Loss: 18.048, Final Batch Loss: 0.420\n",
      "Epoch 5136, Loss: 17.724, Final Batch Loss: 0.384\n",
      "Epoch 5137, Loss: 17.703, Final Batch Loss: 0.453\n",
      "Epoch 5138, Loss: 18.020, Final Batch Loss: 0.551\n",
      "Epoch 5139, Loss: 17.993, Final Batch Loss: 0.487\n",
      "Epoch 5140, Loss: 17.986, Final Batch Loss: 0.509\n",
      "Epoch 5141, Loss: 17.913, Final Batch Loss: 0.520\n",
      "Epoch 5142, Loss: 17.738, Final Batch Loss: 0.493\n",
      "Epoch 5143, Loss: 17.946, Final Batch Loss: 0.443\n",
      "Epoch 5144, Loss: 17.793, Final Batch Loss: 0.409\n",
      "Epoch 5145, Loss: 17.841, Final Batch Loss: 0.425\n",
      "Epoch 5146, Loss: 17.870, Final Batch Loss: 0.515\n",
      "Epoch 5147, Loss: 17.950, Final Batch Loss: 0.479\n",
      "Epoch 5148, Loss: 17.870, Final Batch Loss: 0.415\n",
      "Epoch 5149, Loss: 17.950, Final Batch Loss: 0.624\n",
      "Epoch 5150, Loss: 17.797, Final Batch Loss: 0.656\n",
      "Epoch 5151, Loss: 18.116, Final Batch Loss: 0.504\n",
      "Epoch 5152, Loss: 17.802, Final Batch Loss: 0.508\n",
      "Epoch 5153, Loss: 17.856, Final Batch Loss: 0.502\n",
      "Epoch 5154, Loss: 17.832, Final Batch Loss: 0.400\n",
      "Epoch 5155, Loss: 17.911, Final Batch Loss: 0.455\n",
      "Epoch 5156, Loss: 17.845, Final Batch Loss: 0.477\n",
      "Epoch 5157, Loss: 17.825, Final Batch Loss: 0.543\n",
      "Epoch 5158, Loss: 17.760, Final Batch Loss: 0.440\n",
      "Epoch 5159, Loss: 17.843, Final Batch Loss: 0.568\n",
      "Epoch 5160, Loss: 18.174, Final Batch Loss: 0.502\n",
      "Epoch 5161, Loss: 18.030, Final Batch Loss: 0.582\n",
      "Epoch 5162, Loss: 17.867, Final Batch Loss: 0.540\n",
      "Epoch 5163, Loss: 17.910, Final Batch Loss: 0.577\n",
      "Epoch 5164, Loss: 18.162, Final Batch Loss: 0.498\n",
      "Epoch 5165, Loss: 17.751, Final Batch Loss: 0.448\n",
      "Epoch 5166, Loss: 18.173, Final Batch Loss: 0.565\n",
      "Epoch 5167, Loss: 17.957, Final Batch Loss: 0.607\n",
      "Epoch 5168, Loss: 17.967, Final Batch Loss: 0.583\n",
      "Epoch 5169, Loss: 17.862, Final Batch Loss: 0.413\n",
      "Epoch 5170, Loss: 17.808, Final Batch Loss: 0.451\n",
      "Epoch 5171, Loss: 18.087, Final Batch Loss: 0.500\n",
      "Epoch 5172, Loss: 17.975, Final Batch Loss: 0.544\n",
      "Epoch 5173, Loss: 17.805, Final Batch Loss: 0.394\n",
      "Epoch 5174, Loss: 17.786, Final Batch Loss: 0.431\n",
      "Epoch 5175, Loss: 18.157, Final Batch Loss: 0.411\n",
      "Epoch 5176, Loss: 17.864, Final Batch Loss: 0.518\n",
      "Epoch 5177, Loss: 17.754, Final Batch Loss: 0.579\n",
      "Epoch 5178, Loss: 18.140, Final Batch Loss: 0.488\n",
      "Epoch 5179, Loss: 17.882, Final Batch Loss: 0.454\n",
      "Epoch 5180, Loss: 17.861, Final Batch Loss: 0.387\n",
      "Epoch 5181, Loss: 17.848, Final Batch Loss: 0.518\n",
      "Epoch 5182, Loss: 17.926, Final Batch Loss: 0.435\n",
      "Epoch 5183, Loss: 17.665, Final Batch Loss: 0.470\n",
      "Epoch 5184, Loss: 17.929, Final Batch Loss: 0.626\n",
      "Epoch 5185, Loss: 17.526, Final Batch Loss: 0.398\n",
      "Epoch 5186, Loss: 18.011, Final Batch Loss: 0.599\n",
      "Epoch 5187, Loss: 17.737, Final Batch Loss: 0.407\n",
      "Epoch 5188, Loss: 18.089, Final Batch Loss: 0.357\n",
      "Epoch 5189, Loss: 17.707, Final Batch Loss: 0.488\n",
      "Epoch 5190, Loss: 17.966, Final Batch Loss: 0.521\n",
      "Epoch 5191, Loss: 17.932, Final Batch Loss: 0.567\n",
      "Epoch 5192, Loss: 17.913, Final Batch Loss: 0.558\n",
      "Epoch 5193, Loss: 17.663, Final Batch Loss: 0.452\n",
      "Epoch 5194, Loss: 17.784, Final Batch Loss: 0.458\n",
      "Epoch 5195, Loss: 17.912, Final Batch Loss: 0.433\n",
      "Epoch 5196, Loss: 17.767, Final Batch Loss: 0.502\n",
      "Epoch 5197, Loss: 18.196, Final Batch Loss: 0.481\n",
      "Epoch 5198, Loss: 18.125, Final Batch Loss: 0.559\n",
      "Epoch 5199, Loss: 18.019, Final Batch Loss: 0.599\n",
      "Epoch 5200, Loss: 18.006, Final Batch Loss: 0.459\n",
      "Epoch 5201, Loss: 17.759, Final Batch Loss: 0.461\n",
      "Epoch 5202, Loss: 18.034, Final Batch Loss: 0.501\n",
      "Epoch 5203, Loss: 17.925, Final Batch Loss: 0.570\n",
      "Epoch 5204, Loss: 17.956, Final Batch Loss: 0.571\n",
      "Epoch 5205, Loss: 17.682, Final Batch Loss: 0.560\n",
      "Epoch 5206, Loss: 17.869, Final Batch Loss: 0.436\n",
      "Epoch 5207, Loss: 18.058, Final Batch Loss: 0.642\n",
      "Epoch 5208, Loss: 18.021, Final Batch Loss: 0.529\n",
      "Epoch 5209, Loss: 17.973, Final Batch Loss: 0.459\n",
      "Epoch 5210, Loss: 17.952, Final Batch Loss: 0.456\n",
      "Epoch 5211, Loss: 17.887, Final Batch Loss: 0.546\n",
      "Epoch 5212, Loss: 17.935, Final Batch Loss: 0.606\n",
      "Epoch 5213, Loss: 17.694, Final Batch Loss: 0.464\n",
      "Epoch 5214, Loss: 17.790, Final Batch Loss: 0.429\n",
      "Epoch 5215, Loss: 17.570, Final Batch Loss: 0.544\n",
      "Epoch 5216, Loss: 17.740, Final Batch Loss: 0.385\n",
      "Epoch 5217, Loss: 17.960, Final Batch Loss: 0.512\n",
      "Epoch 5218, Loss: 17.967, Final Batch Loss: 0.538\n",
      "Epoch 5219, Loss: 17.844, Final Batch Loss: 0.447\n",
      "Epoch 5220, Loss: 17.755, Final Batch Loss: 0.470\n",
      "Epoch 5221, Loss: 17.983, Final Batch Loss: 0.566\n",
      "Epoch 5222, Loss: 17.898, Final Batch Loss: 0.468\n",
      "Epoch 5223, Loss: 18.030, Final Batch Loss: 0.533\n",
      "Epoch 5224, Loss: 17.808, Final Batch Loss: 0.425\n",
      "Epoch 5225, Loss: 17.826, Final Batch Loss: 0.420\n",
      "Epoch 5226, Loss: 17.897, Final Batch Loss: 0.391\n",
      "Epoch 5227, Loss: 17.749, Final Batch Loss: 0.496\n",
      "Epoch 5228, Loss: 17.834, Final Batch Loss: 0.501\n",
      "Epoch 5229, Loss: 17.714, Final Batch Loss: 0.529\n",
      "Epoch 5230, Loss: 17.867, Final Batch Loss: 0.586\n",
      "Epoch 5231, Loss: 17.870, Final Batch Loss: 0.426\n",
      "Epoch 5232, Loss: 17.710, Final Batch Loss: 0.390\n",
      "Epoch 5233, Loss: 17.670, Final Batch Loss: 0.477\n",
      "Epoch 5234, Loss: 17.920, Final Batch Loss: 0.492\n",
      "Epoch 5235, Loss: 17.908, Final Batch Loss: 0.591\n",
      "Epoch 5236, Loss: 17.715, Final Batch Loss: 0.683\n",
      "Epoch 5237, Loss: 17.921, Final Batch Loss: 0.493\n",
      "Epoch 5238, Loss: 17.687, Final Batch Loss: 0.454\n",
      "Epoch 5239, Loss: 17.879, Final Batch Loss: 0.453\n",
      "Epoch 5240, Loss: 17.950, Final Batch Loss: 0.427\n",
      "Epoch 5241, Loss: 18.209, Final Batch Loss: 0.543\n",
      "Epoch 5242, Loss: 17.754, Final Batch Loss: 0.430\n",
      "Epoch 5243, Loss: 17.833, Final Batch Loss: 0.461\n",
      "Epoch 5244, Loss: 17.689, Final Batch Loss: 0.496\n",
      "Epoch 5245, Loss: 17.899, Final Batch Loss: 0.500\n",
      "Epoch 5246, Loss: 17.699, Final Batch Loss: 0.424\n",
      "Epoch 5247, Loss: 17.662, Final Batch Loss: 0.466\n",
      "Epoch 5248, Loss: 17.898, Final Batch Loss: 0.452\n",
      "Epoch 5249, Loss: 17.696, Final Batch Loss: 0.459\n",
      "Epoch 5250, Loss: 17.840, Final Batch Loss: 0.516\n",
      "Epoch 5251, Loss: 17.717, Final Batch Loss: 0.478\n",
      "Epoch 5252, Loss: 17.672, Final Batch Loss: 0.428\n",
      "Epoch 5253, Loss: 17.492, Final Batch Loss: 0.552\n",
      "Epoch 5254, Loss: 17.712, Final Batch Loss: 0.476\n",
      "Epoch 5255, Loss: 17.719, Final Batch Loss: 0.435\n",
      "Epoch 5256, Loss: 17.746, Final Batch Loss: 0.506\n",
      "Epoch 5257, Loss: 17.720, Final Batch Loss: 0.489\n",
      "Epoch 5258, Loss: 17.775, Final Batch Loss: 0.532\n",
      "Epoch 5259, Loss: 17.953, Final Batch Loss: 0.510\n",
      "Epoch 5260, Loss: 17.879, Final Batch Loss: 0.508\n",
      "Epoch 5261, Loss: 17.900, Final Batch Loss: 0.455\n",
      "Epoch 5262, Loss: 17.947, Final Batch Loss: 0.474\n",
      "Epoch 5263, Loss: 17.644, Final Batch Loss: 0.441\n",
      "Epoch 5264, Loss: 18.228, Final Batch Loss: 0.544\n",
      "Epoch 5265, Loss: 17.742, Final Batch Loss: 0.471\n",
      "Epoch 5266, Loss: 17.783, Final Batch Loss: 0.503\n",
      "Epoch 5267, Loss: 17.802, Final Batch Loss: 0.424\n",
      "Epoch 5268, Loss: 17.850, Final Batch Loss: 0.599\n",
      "Epoch 5269, Loss: 18.200, Final Batch Loss: 0.533\n",
      "Epoch 5270, Loss: 17.761, Final Batch Loss: 0.482\n",
      "Epoch 5271, Loss: 17.436, Final Batch Loss: 0.445\n",
      "Epoch 5272, Loss: 17.946, Final Batch Loss: 0.474\n",
      "Epoch 5273, Loss: 17.752, Final Batch Loss: 0.347\n",
      "Epoch 5274, Loss: 17.762, Final Batch Loss: 0.448\n",
      "Epoch 5275, Loss: 17.779, Final Batch Loss: 0.514\n",
      "Epoch 5276, Loss: 17.839, Final Batch Loss: 0.514\n",
      "Epoch 5277, Loss: 17.875, Final Batch Loss: 0.520\n",
      "Epoch 5278, Loss: 17.814, Final Batch Loss: 0.519\n",
      "Epoch 5279, Loss: 17.928, Final Batch Loss: 0.373\n",
      "Epoch 5280, Loss: 17.747, Final Batch Loss: 0.471\n",
      "Epoch 5281, Loss: 17.838, Final Batch Loss: 0.580\n",
      "Epoch 5282, Loss: 17.709, Final Batch Loss: 0.469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5283, Loss: 17.708, Final Batch Loss: 0.399\n",
      "Epoch 5284, Loss: 17.550, Final Batch Loss: 0.449\n",
      "Epoch 5285, Loss: 17.668, Final Batch Loss: 0.566\n",
      "Epoch 5286, Loss: 18.033, Final Batch Loss: 0.514\n",
      "Epoch 5287, Loss: 17.656, Final Batch Loss: 0.490\n",
      "Epoch 5288, Loss: 17.704, Final Batch Loss: 0.532\n",
      "Epoch 5289, Loss: 17.748, Final Batch Loss: 0.462\n",
      "Epoch 5290, Loss: 17.762, Final Batch Loss: 0.512\n",
      "Epoch 5291, Loss: 18.003, Final Batch Loss: 0.480\n",
      "Epoch 5292, Loss: 17.876, Final Batch Loss: 0.498\n",
      "Epoch 5293, Loss: 17.792, Final Batch Loss: 0.488\n",
      "Epoch 5294, Loss: 17.508, Final Batch Loss: 0.489\n",
      "Epoch 5295, Loss: 17.544, Final Batch Loss: 0.537\n",
      "Epoch 5296, Loss: 17.725, Final Batch Loss: 0.482\n",
      "Epoch 5297, Loss: 17.831, Final Batch Loss: 0.581\n",
      "Epoch 5298, Loss: 18.007, Final Batch Loss: 0.485\n",
      "Epoch 5299, Loss: 17.911, Final Batch Loss: 0.418\n",
      "Epoch 5300, Loss: 17.704, Final Batch Loss: 0.457\n",
      "Epoch 5301, Loss: 17.520, Final Batch Loss: 0.455\n",
      "Epoch 5302, Loss: 18.043, Final Batch Loss: 0.413\n",
      "Epoch 5303, Loss: 17.657, Final Batch Loss: 0.470\n",
      "Epoch 5304, Loss: 17.843, Final Batch Loss: 0.471\n",
      "Epoch 5305, Loss: 17.706, Final Batch Loss: 0.500\n",
      "Epoch 5306, Loss: 17.740, Final Batch Loss: 0.477\n",
      "Epoch 5307, Loss: 17.568, Final Batch Loss: 0.579\n",
      "Epoch 5308, Loss: 17.653, Final Batch Loss: 0.446\n",
      "Epoch 5309, Loss: 17.852, Final Batch Loss: 0.439\n",
      "Epoch 5310, Loss: 17.443, Final Batch Loss: 0.455\n",
      "Epoch 5311, Loss: 17.625, Final Batch Loss: 0.418\n",
      "Epoch 5312, Loss: 17.540, Final Batch Loss: 0.512\n",
      "Epoch 5313, Loss: 17.742, Final Batch Loss: 0.502\n",
      "Epoch 5314, Loss: 17.816, Final Batch Loss: 0.497\n",
      "Epoch 5315, Loss: 17.691, Final Batch Loss: 0.561\n",
      "Epoch 5316, Loss: 17.563, Final Batch Loss: 0.453\n",
      "Epoch 5317, Loss: 17.708, Final Batch Loss: 0.496\n",
      "Epoch 5318, Loss: 18.203, Final Batch Loss: 0.429\n",
      "Epoch 5319, Loss: 17.920, Final Batch Loss: 0.468\n",
      "Epoch 5320, Loss: 17.742, Final Batch Loss: 0.478\n",
      "Epoch 5321, Loss: 17.899, Final Batch Loss: 0.521\n",
      "Epoch 5322, Loss: 17.539, Final Batch Loss: 0.470\n",
      "Epoch 5323, Loss: 17.618, Final Batch Loss: 0.409\n",
      "Epoch 5324, Loss: 17.767, Final Batch Loss: 0.412\n",
      "Epoch 5325, Loss: 17.805, Final Batch Loss: 0.542\n",
      "Epoch 5326, Loss: 17.790, Final Batch Loss: 0.465\n",
      "Epoch 5327, Loss: 17.797, Final Batch Loss: 0.559\n",
      "Epoch 5328, Loss: 17.830, Final Batch Loss: 0.490\n",
      "Epoch 5329, Loss: 17.885, Final Batch Loss: 0.458\n",
      "Epoch 5330, Loss: 17.812, Final Batch Loss: 0.438\n",
      "Epoch 5331, Loss: 17.754, Final Batch Loss: 0.386\n",
      "Epoch 5332, Loss: 17.791, Final Batch Loss: 0.589\n",
      "Epoch 5333, Loss: 17.656, Final Batch Loss: 0.441\n",
      "Epoch 5334, Loss: 17.767, Final Batch Loss: 0.434\n",
      "Epoch 5335, Loss: 17.784, Final Batch Loss: 0.529\n",
      "Epoch 5336, Loss: 17.685, Final Batch Loss: 0.502\n",
      "Epoch 5337, Loss: 17.915, Final Batch Loss: 0.490\n",
      "Epoch 5338, Loss: 17.671, Final Batch Loss: 0.553\n",
      "Epoch 5339, Loss: 17.504, Final Batch Loss: 0.536\n",
      "Epoch 5340, Loss: 17.668, Final Batch Loss: 0.518\n",
      "Epoch 5341, Loss: 17.745, Final Batch Loss: 0.472\n",
      "Epoch 5342, Loss: 17.684, Final Batch Loss: 0.510\n",
      "Epoch 5343, Loss: 17.608, Final Batch Loss: 0.512\n",
      "Epoch 5344, Loss: 17.704, Final Batch Loss: 0.567\n",
      "Epoch 5345, Loss: 17.723, Final Batch Loss: 0.537\n",
      "Epoch 5346, Loss: 17.666, Final Batch Loss: 0.560\n",
      "Epoch 5347, Loss: 17.546, Final Batch Loss: 0.517\n",
      "Epoch 5348, Loss: 17.770, Final Batch Loss: 0.473\n",
      "Epoch 5349, Loss: 17.959, Final Batch Loss: 0.461\n",
      "Epoch 5350, Loss: 17.994, Final Batch Loss: 0.505\n",
      "Epoch 5351, Loss: 17.787, Final Batch Loss: 0.412\n",
      "Epoch 5352, Loss: 17.898, Final Batch Loss: 0.457\n",
      "Epoch 5353, Loss: 17.589, Final Batch Loss: 0.436\n",
      "Epoch 5354, Loss: 17.626, Final Batch Loss: 0.449\n",
      "Epoch 5355, Loss: 17.736, Final Batch Loss: 0.537\n",
      "Epoch 5356, Loss: 17.565, Final Batch Loss: 0.358\n",
      "Epoch 5357, Loss: 17.797, Final Batch Loss: 0.553\n",
      "Epoch 5358, Loss: 17.521, Final Batch Loss: 0.461\n",
      "Epoch 5359, Loss: 17.628, Final Batch Loss: 0.456\n",
      "Epoch 5360, Loss: 18.037, Final Batch Loss: 0.488\n",
      "Epoch 5361, Loss: 17.852, Final Batch Loss: 0.501\n",
      "Epoch 5362, Loss: 17.853, Final Batch Loss: 0.409\n",
      "Epoch 5363, Loss: 17.776, Final Batch Loss: 0.487\n",
      "Epoch 5364, Loss: 17.640, Final Batch Loss: 0.454\n",
      "Epoch 5365, Loss: 18.009, Final Batch Loss: 0.495\n",
      "Epoch 5366, Loss: 17.811, Final Batch Loss: 0.512\n",
      "Epoch 5367, Loss: 17.844, Final Batch Loss: 0.448\n",
      "Epoch 5368, Loss: 17.882, Final Batch Loss: 0.490\n",
      "Epoch 5369, Loss: 17.883, Final Batch Loss: 0.468\n",
      "Epoch 5370, Loss: 17.621, Final Batch Loss: 0.460\n",
      "Epoch 5371, Loss: 17.790, Final Batch Loss: 0.451\n",
      "Epoch 5372, Loss: 17.809, Final Batch Loss: 0.454\n",
      "Epoch 5373, Loss: 17.576, Final Batch Loss: 0.406\n",
      "Epoch 5374, Loss: 17.886, Final Batch Loss: 0.568\n",
      "Epoch 5375, Loss: 17.829, Final Batch Loss: 0.495\n",
      "Epoch 5376, Loss: 17.633, Final Batch Loss: 0.603\n",
      "Epoch 5377, Loss: 17.445, Final Batch Loss: 0.511\n",
      "Epoch 5378, Loss: 17.623, Final Batch Loss: 0.449\n",
      "Epoch 5379, Loss: 17.434, Final Batch Loss: 0.406\n",
      "Epoch 5380, Loss: 17.799, Final Batch Loss: 0.405\n",
      "Epoch 5381, Loss: 17.685, Final Batch Loss: 0.493\n",
      "Epoch 5382, Loss: 17.598, Final Batch Loss: 0.414\n",
      "Epoch 5383, Loss: 17.956, Final Batch Loss: 0.496\n",
      "Epoch 5384, Loss: 17.674, Final Batch Loss: 0.489\n",
      "Epoch 5385, Loss: 17.785, Final Batch Loss: 0.527\n",
      "Epoch 5386, Loss: 17.755, Final Batch Loss: 0.460\n",
      "Epoch 5387, Loss: 17.727, Final Batch Loss: 0.482\n",
      "Epoch 5388, Loss: 17.633, Final Batch Loss: 0.471\n",
      "Epoch 5389, Loss: 17.473, Final Batch Loss: 0.468\n",
      "Epoch 5390, Loss: 17.695, Final Batch Loss: 0.468\n",
      "Epoch 5391, Loss: 17.840, Final Batch Loss: 0.463\n",
      "Epoch 5392, Loss: 17.570, Final Batch Loss: 0.408\n",
      "Epoch 5393, Loss: 17.805, Final Batch Loss: 0.524\n",
      "Epoch 5394, Loss: 17.644, Final Batch Loss: 0.602\n",
      "Epoch 5395, Loss: 17.407, Final Batch Loss: 0.458\n",
      "Epoch 5396, Loss: 17.703, Final Batch Loss: 0.421\n",
      "Epoch 5397, Loss: 17.614, Final Batch Loss: 0.433\n",
      "Epoch 5398, Loss: 17.812, Final Batch Loss: 0.490\n",
      "Epoch 5399, Loss: 17.894, Final Batch Loss: 0.537\n",
      "Epoch 5400, Loss: 17.632, Final Batch Loss: 0.469\n",
      "Epoch 5401, Loss: 17.589, Final Batch Loss: 0.507\n",
      "Epoch 5402, Loss: 17.615, Final Batch Loss: 0.470\n",
      "Epoch 5403, Loss: 17.747, Final Batch Loss: 0.504\n",
      "Epoch 5404, Loss: 17.683, Final Batch Loss: 0.384\n",
      "Epoch 5405, Loss: 17.870, Final Batch Loss: 0.460\n",
      "Epoch 5406, Loss: 17.777, Final Batch Loss: 0.490\n",
      "Epoch 5407, Loss: 17.756, Final Batch Loss: 0.537\n",
      "Epoch 5408, Loss: 17.688, Final Batch Loss: 0.556\n",
      "Epoch 5409, Loss: 17.617, Final Batch Loss: 0.524\n",
      "Epoch 5410, Loss: 17.414, Final Batch Loss: 0.611\n",
      "Epoch 5411, Loss: 17.626, Final Batch Loss: 0.490\n",
      "Epoch 5412, Loss: 17.688, Final Batch Loss: 0.603\n",
      "Epoch 5413, Loss: 17.439, Final Batch Loss: 0.540\n",
      "Epoch 5414, Loss: 17.765, Final Batch Loss: 0.506\n",
      "Epoch 5415, Loss: 17.494, Final Batch Loss: 0.427\n",
      "Epoch 5416, Loss: 17.786, Final Batch Loss: 0.456\n",
      "Epoch 5417, Loss: 17.602, Final Batch Loss: 0.469\n",
      "Epoch 5418, Loss: 17.609, Final Batch Loss: 0.495\n",
      "Epoch 5419, Loss: 17.728, Final Batch Loss: 0.436\n",
      "Epoch 5420, Loss: 17.620, Final Batch Loss: 0.435\n",
      "Epoch 5421, Loss: 17.644, Final Batch Loss: 0.598\n",
      "Epoch 5422, Loss: 17.703, Final Batch Loss: 0.421\n",
      "Epoch 5423, Loss: 17.693, Final Batch Loss: 0.364\n",
      "Epoch 5424, Loss: 17.652, Final Batch Loss: 0.491\n",
      "Epoch 5425, Loss: 17.744, Final Batch Loss: 0.594\n",
      "Epoch 5426, Loss: 17.636, Final Batch Loss: 0.466\n",
      "Epoch 5427, Loss: 17.772, Final Batch Loss: 0.551\n",
      "Epoch 5428, Loss: 17.679, Final Batch Loss: 0.412\n",
      "Epoch 5429, Loss: 17.745, Final Batch Loss: 0.513\n",
      "Epoch 5430, Loss: 17.756, Final Batch Loss: 0.527\n",
      "Epoch 5431, Loss: 17.801, Final Batch Loss: 0.669\n",
      "Epoch 5432, Loss: 17.532, Final Batch Loss: 0.571\n",
      "Epoch 5433, Loss: 17.606, Final Batch Loss: 0.390\n",
      "Epoch 5434, Loss: 17.707, Final Batch Loss: 0.499\n",
      "Epoch 5435, Loss: 17.704, Final Batch Loss: 0.471\n",
      "Epoch 5436, Loss: 17.663, Final Batch Loss: 0.468\n",
      "Epoch 5437, Loss: 17.826, Final Batch Loss: 0.505\n",
      "Epoch 5438, Loss: 17.670, Final Batch Loss: 0.615\n",
      "Epoch 5439, Loss: 17.659, Final Batch Loss: 0.471\n",
      "Epoch 5440, Loss: 17.917, Final Batch Loss: 0.642\n",
      "Epoch 5441, Loss: 17.654, Final Batch Loss: 0.553\n",
      "Epoch 5442, Loss: 17.779, Final Batch Loss: 0.454\n",
      "Epoch 5443, Loss: 17.674, Final Batch Loss: 0.395\n",
      "Epoch 5444, Loss: 17.700, Final Batch Loss: 0.483\n",
      "Epoch 5445, Loss: 17.471, Final Batch Loss: 0.466\n",
      "Epoch 5446, Loss: 17.790, Final Batch Loss: 0.453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5447, Loss: 17.809, Final Batch Loss: 0.532\n",
      "Epoch 5448, Loss: 17.589, Final Batch Loss: 0.547\n",
      "Epoch 5449, Loss: 17.651, Final Batch Loss: 0.413\n",
      "Epoch 5450, Loss: 17.717, Final Batch Loss: 0.497\n",
      "Epoch 5451, Loss: 17.635, Final Batch Loss: 0.487\n",
      "Epoch 5452, Loss: 18.037, Final Batch Loss: 0.652\n",
      "Epoch 5453, Loss: 17.412, Final Batch Loss: 0.465\n",
      "Epoch 5454, Loss: 17.665, Final Batch Loss: 0.433\n",
      "Epoch 5455, Loss: 17.489, Final Batch Loss: 0.412\n",
      "Epoch 5456, Loss: 17.609, Final Batch Loss: 0.513\n",
      "Epoch 5457, Loss: 17.678, Final Batch Loss: 0.444\n",
      "Epoch 5458, Loss: 17.642, Final Batch Loss: 0.482\n",
      "Epoch 5459, Loss: 17.489, Final Batch Loss: 0.539\n",
      "Epoch 5460, Loss: 17.797, Final Batch Loss: 0.416\n",
      "Epoch 5461, Loss: 17.661, Final Batch Loss: 0.490\n",
      "Epoch 5462, Loss: 17.806, Final Batch Loss: 0.429\n",
      "Epoch 5463, Loss: 17.592, Final Batch Loss: 0.459\n",
      "Epoch 5464, Loss: 17.735, Final Batch Loss: 0.511\n",
      "Epoch 5465, Loss: 17.939, Final Batch Loss: 0.446\n",
      "Epoch 5466, Loss: 17.890, Final Batch Loss: 0.513\n",
      "Epoch 5467, Loss: 17.686, Final Batch Loss: 0.489\n",
      "Epoch 5468, Loss: 17.268, Final Batch Loss: 0.500\n",
      "Epoch 5469, Loss: 17.499, Final Batch Loss: 0.413\n",
      "Epoch 5470, Loss: 17.657, Final Batch Loss: 0.537\n",
      "Epoch 5471, Loss: 17.468, Final Batch Loss: 0.463\n",
      "Epoch 5472, Loss: 18.023, Final Batch Loss: 0.670\n",
      "Epoch 5473, Loss: 17.495, Final Batch Loss: 0.620\n",
      "Epoch 5474, Loss: 17.956, Final Batch Loss: 0.381\n",
      "Epoch 5475, Loss: 17.730, Final Batch Loss: 0.551\n",
      "Epoch 5476, Loss: 17.804, Final Batch Loss: 0.525\n",
      "Epoch 5477, Loss: 18.013, Final Batch Loss: 0.550\n",
      "Epoch 5478, Loss: 17.678, Final Batch Loss: 0.506\n",
      "Epoch 5479, Loss: 17.720, Final Batch Loss: 0.533\n",
      "Epoch 5480, Loss: 17.670, Final Batch Loss: 0.487\n",
      "Epoch 5481, Loss: 17.387, Final Batch Loss: 0.593\n",
      "Epoch 5482, Loss: 17.633, Final Batch Loss: 0.450\n",
      "Epoch 5483, Loss: 17.724, Final Batch Loss: 0.452\n",
      "Epoch 5484, Loss: 17.540, Final Batch Loss: 0.446\n",
      "Epoch 5485, Loss: 17.772, Final Batch Loss: 0.423\n",
      "Epoch 5486, Loss: 17.726, Final Batch Loss: 0.510\n",
      "Epoch 5487, Loss: 17.615, Final Batch Loss: 0.438\n",
      "Epoch 5488, Loss: 17.580, Final Batch Loss: 0.483\n",
      "Epoch 5489, Loss: 17.790, Final Batch Loss: 0.407\n",
      "Epoch 5490, Loss: 17.619, Final Batch Loss: 0.448\n",
      "Epoch 5491, Loss: 17.542, Final Batch Loss: 0.413\n",
      "Epoch 5492, Loss: 17.388, Final Batch Loss: 0.431\n",
      "Epoch 5493, Loss: 17.614, Final Batch Loss: 0.448\n",
      "Epoch 5494, Loss: 17.524, Final Batch Loss: 0.467\n",
      "Epoch 5495, Loss: 17.665, Final Batch Loss: 0.486\n",
      "Epoch 5496, Loss: 17.610, Final Batch Loss: 0.442\n",
      "Epoch 5497, Loss: 17.538, Final Batch Loss: 0.496\n",
      "Epoch 5498, Loss: 17.583, Final Batch Loss: 0.417\n",
      "Epoch 5499, Loss: 17.618, Final Batch Loss: 0.550\n",
      "Epoch 5500, Loss: 17.716, Final Batch Loss: 0.527\n",
      "Epoch 5501, Loss: 17.695, Final Batch Loss: 0.501\n",
      "Epoch 5502, Loss: 17.633, Final Batch Loss: 0.464\n",
      "Epoch 5503, Loss: 17.637, Final Batch Loss: 0.542\n",
      "Epoch 5504, Loss: 17.679, Final Batch Loss: 0.592\n",
      "Epoch 5505, Loss: 17.533, Final Batch Loss: 0.452\n",
      "Epoch 5506, Loss: 17.526, Final Batch Loss: 0.599\n",
      "Epoch 5507, Loss: 17.422, Final Batch Loss: 0.567\n",
      "Epoch 5508, Loss: 17.467, Final Batch Loss: 0.556\n",
      "Epoch 5509, Loss: 17.763, Final Batch Loss: 0.371\n",
      "Epoch 5510, Loss: 17.520, Final Batch Loss: 0.492\n",
      "Epoch 5511, Loss: 17.759, Final Batch Loss: 0.476\n",
      "Epoch 5512, Loss: 17.641, Final Batch Loss: 0.506\n",
      "Epoch 5513, Loss: 17.570, Final Batch Loss: 0.492\n",
      "Epoch 5514, Loss: 17.939, Final Batch Loss: 0.603\n",
      "Epoch 5515, Loss: 17.592, Final Batch Loss: 0.586\n",
      "Epoch 5516, Loss: 17.726, Final Batch Loss: 0.490\n",
      "Epoch 5517, Loss: 17.560, Final Batch Loss: 0.458\n",
      "Epoch 5518, Loss: 17.355, Final Batch Loss: 0.369\n",
      "Epoch 5519, Loss: 17.590, Final Batch Loss: 0.601\n",
      "Epoch 5520, Loss: 17.786, Final Batch Loss: 0.453\n",
      "Epoch 5521, Loss: 17.762, Final Batch Loss: 0.508\n",
      "Epoch 5522, Loss: 17.723, Final Batch Loss: 0.590\n",
      "Epoch 5523, Loss: 17.470, Final Batch Loss: 0.417\n",
      "Epoch 5524, Loss: 17.769, Final Batch Loss: 0.471\n",
      "Epoch 5525, Loss: 17.943, Final Batch Loss: 0.440\n",
      "Epoch 5526, Loss: 17.617, Final Batch Loss: 0.444\n",
      "Epoch 5527, Loss: 17.698, Final Batch Loss: 0.643\n",
      "Epoch 5528, Loss: 17.634, Final Batch Loss: 0.555\n",
      "Epoch 5529, Loss: 17.447, Final Batch Loss: 0.458\n",
      "Epoch 5530, Loss: 17.790, Final Batch Loss: 0.479\n",
      "Epoch 5531, Loss: 17.546, Final Batch Loss: 0.422\n",
      "Epoch 5532, Loss: 17.587, Final Batch Loss: 0.519\n",
      "Epoch 5533, Loss: 17.510, Final Batch Loss: 0.491\n",
      "Epoch 5534, Loss: 17.397, Final Batch Loss: 0.452\n",
      "Epoch 5535, Loss: 17.665, Final Batch Loss: 0.453\n",
      "Epoch 5536, Loss: 17.744, Final Batch Loss: 0.481\n",
      "Epoch 5537, Loss: 17.621, Final Batch Loss: 0.439\n",
      "Epoch 5538, Loss: 17.985, Final Batch Loss: 0.709\n",
      "Epoch 5539, Loss: 17.704, Final Batch Loss: 0.499\n",
      "Epoch 5540, Loss: 17.542, Final Batch Loss: 0.463\n",
      "Epoch 5541, Loss: 17.551, Final Batch Loss: 0.469\n",
      "Epoch 5542, Loss: 17.526, Final Batch Loss: 0.427\n",
      "Epoch 5543, Loss: 17.716, Final Batch Loss: 0.473\n",
      "Epoch 5544, Loss: 17.536, Final Batch Loss: 0.475\n",
      "Epoch 5545, Loss: 17.595, Final Batch Loss: 0.584\n",
      "Epoch 5546, Loss: 17.594, Final Batch Loss: 0.491\n",
      "Epoch 5547, Loss: 17.412, Final Batch Loss: 0.514\n",
      "Epoch 5548, Loss: 17.603, Final Batch Loss: 0.513\n",
      "Epoch 5549, Loss: 17.614, Final Batch Loss: 0.557\n",
      "Epoch 5550, Loss: 17.821, Final Batch Loss: 0.452\n",
      "Epoch 5551, Loss: 17.882, Final Batch Loss: 0.414\n",
      "Epoch 5552, Loss: 17.661, Final Batch Loss: 0.451\n",
      "Epoch 5553, Loss: 17.513, Final Batch Loss: 0.532\n",
      "Epoch 5554, Loss: 17.742, Final Batch Loss: 0.441\n",
      "Epoch 5555, Loss: 17.882, Final Batch Loss: 0.532\n",
      "Epoch 5556, Loss: 17.401, Final Batch Loss: 0.423\n",
      "Epoch 5557, Loss: 17.655, Final Batch Loss: 0.494\n",
      "Epoch 5558, Loss: 17.591, Final Batch Loss: 0.507\n",
      "Epoch 5559, Loss: 17.813, Final Batch Loss: 0.480\n",
      "Epoch 5560, Loss: 17.358, Final Batch Loss: 0.614\n",
      "Epoch 5561, Loss: 17.348, Final Batch Loss: 0.402\n",
      "Epoch 5562, Loss: 17.725, Final Batch Loss: 0.563\n",
      "Epoch 5563, Loss: 17.374, Final Batch Loss: 0.507\n",
      "Epoch 5564, Loss: 17.459, Final Batch Loss: 0.497\n",
      "Epoch 5565, Loss: 17.543, Final Batch Loss: 0.604\n",
      "Epoch 5566, Loss: 17.563, Final Batch Loss: 0.536\n",
      "Epoch 5567, Loss: 17.693, Final Batch Loss: 0.541\n",
      "Epoch 5568, Loss: 17.666, Final Batch Loss: 0.525\n",
      "Epoch 5569, Loss: 17.672, Final Batch Loss: 0.554\n",
      "Epoch 5570, Loss: 17.821, Final Batch Loss: 0.535\n",
      "Epoch 5571, Loss: 17.578, Final Batch Loss: 0.466\n",
      "Epoch 5572, Loss: 17.531, Final Batch Loss: 0.424\n",
      "Epoch 5573, Loss: 17.386, Final Batch Loss: 0.478\n",
      "Epoch 5574, Loss: 17.562, Final Batch Loss: 0.581\n",
      "Epoch 5575, Loss: 17.618, Final Batch Loss: 0.481\n",
      "Epoch 5576, Loss: 17.643, Final Batch Loss: 0.368\n",
      "Epoch 5577, Loss: 17.837, Final Batch Loss: 0.507\n",
      "Epoch 5578, Loss: 17.765, Final Batch Loss: 0.555\n",
      "Epoch 5579, Loss: 17.755, Final Batch Loss: 0.497\n",
      "Epoch 5580, Loss: 17.528, Final Batch Loss: 0.481\n",
      "Epoch 5581, Loss: 17.695, Final Batch Loss: 0.547\n",
      "Epoch 5582, Loss: 17.324, Final Batch Loss: 0.552\n",
      "Epoch 5583, Loss: 17.519, Final Batch Loss: 0.422\n",
      "Epoch 5584, Loss: 17.353, Final Batch Loss: 0.485\n",
      "Epoch 5585, Loss: 17.683, Final Batch Loss: 0.466\n",
      "Epoch 5586, Loss: 17.687, Final Batch Loss: 0.535\n",
      "Epoch 5587, Loss: 17.586, Final Batch Loss: 0.571\n",
      "Epoch 5588, Loss: 17.600, Final Batch Loss: 0.480\n",
      "Epoch 5589, Loss: 17.717, Final Batch Loss: 0.450\n",
      "Epoch 5590, Loss: 17.427, Final Batch Loss: 0.475\n",
      "Epoch 5591, Loss: 17.583, Final Batch Loss: 0.560\n",
      "Epoch 5592, Loss: 17.550, Final Batch Loss: 0.448\n",
      "Epoch 5593, Loss: 17.376, Final Batch Loss: 0.472\n",
      "Epoch 5594, Loss: 17.399, Final Batch Loss: 0.447\n",
      "Epoch 5595, Loss: 17.512, Final Batch Loss: 0.549\n",
      "Epoch 5596, Loss: 17.496, Final Batch Loss: 0.387\n",
      "Epoch 5597, Loss: 17.610, Final Batch Loss: 0.447\n",
      "Epoch 5598, Loss: 17.486, Final Batch Loss: 0.539\n",
      "Epoch 5599, Loss: 17.287, Final Batch Loss: 0.505\n",
      "Epoch 5600, Loss: 17.668, Final Batch Loss: 0.484\n",
      "Epoch 5601, Loss: 17.412, Final Batch Loss: 0.490\n",
      "Epoch 5602, Loss: 17.596, Final Batch Loss: 0.456\n",
      "Epoch 5603, Loss: 17.408, Final Batch Loss: 0.399\n",
      "Epoch 5604, Loss: 17.661, Final Batch Loss: 0.396\n",
      "Epoch 5605, Loss: 17.502, Final Batch Loss: 0.688\n",
      "Epoch 5606, Loss: 17.571, Final Batch Loss: 0.412\n",
      "Epoch 5607, Loss: 17.258, Final Batch Loss: 0.483\n",
      "Epoch 5608, Loss: 17.623, Final Batch Loss: 0.429\n",
      "Epoch 5609, Loss: 17.257, Final Batch Loss: 0.456\n",
      "Epoch 5610, Loss: 17.441, Final Batch Loss: 0.430\n",
      "Epoch 5611, Loss: 17.812, Final Batch Loss: 0.532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5612, Loss: 17.760, Final Batch Loss: 0.530\n",
      "Epoch 5613, Loss: 17.688, Final Batch Loss: 0.500\n",
      "Epoch 5614, Loss: 17.776, Final Batch Loss: 0.481\n",
      "Epoch 5615, Loss: 17.478, Final Batch Loss: 0.452\n",
      "Epoch 5616, Loss: 17.660, Final Batch Loss: 0.508\n",
      "Epoch 5617, Loss: 17.761, Final Batch Loss: 0.492\n",
      "Epoch 5618, Loss: 17.614, Final Batch Loss: 0.442\n",
      "Epoch 5619, Loss: 17.796, Final Batch Loss: 0.605\n",
      "Epoch 5620, Loss: 17.712, Final Batch Loss: 0.542\n",
      "Epoch 5621, Loss: 17.700, Final Batch Loss: 0.519\n",
      "Epoch 5622, Loss: 17.374, Final Batch Loss: 0.403\n",
      "Epoch 5623, Loss: 17.486, Final Batch Loss: 0.566\n",
      "Epoch 5624, Loss: 17.533, Final Batch Loss: 0.553\n",
      "Epoch 5625, Loss: 17.781, Final Batch Loss: 0.582\n",
      "Epoch 5626, Loss: 17.764, Final Batch Loss: 0.512\n",
      "Epoch 5627, Loss: 17.738, Final Batch Loss: 0.505\n",
      "Epoch 5628, Loss: 17.482, Final Batch Loss: 0.496\n",
      "Epoch 5629, Loss: 17.357, Final Batch Loss: 0.597\n",
      "Epoch 5630, Loss: 17.555, Final Batch Loss: 0.352\n",
      "Epoch 5631, Loss: 17.476, Final Batch Loss: 0.499\n",
      "Epoch 5632, Loss: 17.612, Final Batch Loss: 0.516\n",
      "Epoch 5633, Loss: 17.439, Final Batch Loss: 0.459\n",
      "Epoch 5634, Loss: 17.495, Final Batch Loss: 0.534\n",
      "Epoch 5635, Loss: 17.405, Final Batch Loss: 0.444\n",
      "Epoch 5636, Loss: 17.764, Final Batch Loss: 0.487\n",
      "Epoch 5637, Loss: 17.588, Final Batch Loss: 0.420\n",
      "Epoch 5638, Loss: 17.598, Final Batch Loss: 0.482\n",
      "Epoch 5639, Loss: 17.650, Final Batch Loss: 0.443\n",
      "Epoch 5640, Loss: 17.676, Final Batch Loss: 0.541\n",
      "Epoch 5641, Loss: 17.864, Final Batch Loss: 0.576\n",
      "Epoch 5642, Loss: 17.430, Final Batch Loss: 0.507\n",
      "Epoch 5643, Loss: 17.674, Final Batch Loss: 0.606\n",
      "Epoch 5644, Loss: 17.686, Final Batch Loss: 0.678\n",
      "Epoch 5645, Loss: 17.362, Final Batch Loss: 0.336\n",
      "Epoch 5646, Loss: 17.597, Final Batch Loss: 0.421\n",
      "Epoch 5647, Loss: 17.538, Final Batch Loss: 0.408\n",
      "Epoch 5648, Loss: 17.602, Final Batch Loss: 0.544\n",
      "Epoch 5649, Loss: 17.552, Final Batch Loss: 0.483\n",
      "Epoch 5650, Loss: 17.470, Final Batch Loss: 0.524\n",
      "Epoch 5651, Loss: 17.574, Final Batch Loss: 0.387\n",
      "Epoch 5652, Loss: 17.668, Final Batch Loss: 0.493\n",
      "Epoch 5653, Loss: 17.593, Final Batch Loss: 0.418\n",
      "Epoch 5654, Loss: 17.235, Final Batch Loss: 0.487\n",
      "Epoch 5655, Loss: 17.665, Final Batch Loss: 0.603\n",
      "Epoch 5656, Loss: 17.621, Final Batch Loss: 0.556\n",
      "Epoch 5657, Loss: 17.598, Final Batch Loss: 0.568\n",
      "Epoch 5658, Loss: 17.697, Final Batch Loss: 0.489\n",
      "Epoch 5659, Loss: 17.734, Final Batch Loss: 0.496\n",
      "Epoch 5660, Loss: 17.399, Final Batch Loss: 0.433\n",
      "Epoch 5661, Loss: 17.757, Final Batch Loss: 0.536\n",
      "Epoch 5662, Loss: 17.477, Final Batch Loss: 0.475\n",
      "Epoch 5663, Loss: 17.347, Final Batch Loss: 0.471\n",
      "Epoch 5664, Loss: 17.892, Final Batch Loss: 0.568\n",
      "Epoch 5665, Loss: 17.490, Final Batch Loss: 0.547\n",
      "Epoch 5666, Loss: 17.495, Final Batch Loss: 0.471\n",
      "Epoch 5667, Loss: 17.799, Final Batch Loss: 0.531\n",
      "Epoch 5668, Loss: 17.719, Final Batch Loss: 0.462\n",
      "Epoch 5669, Loss: 17.512, Final Batch Loss: 0.467\n",
      "Epoch 5670, Loss: 17.483, Final Batch Loss: 0.426\n",
      "Epoch 5671, Loss: 17.525, Final Batch Loss: 0.452\n",
      "Epoch 5672, Loss: 17.580, Final Batch Loss: 0.438\n",
      "Epoch 5673, Loss: 17.620, Final Batch Loss: 0.505\n",
      "Epoch 5674, Loss: 17.528, Final Batch Loss: 0.417\n",
      "Epoch 5675, Loss: 17.438, Final Batch Loss: 0.472\n",
      "Epoch 5676, Loss: 17.549, Final Batch Loss: 0.486\n",
      "Epoch 5677, Loss: 17.415, Final Batch Loss: 0.471\n",
      "Epoch 5678, Loss: 17.785, Final Batch Loss: 0.652\n",
      "Epoch 5679, Loss: 17.681, Final Batch Loss: 0.569\n",
      "Epoch 5680, Loss: 17.544, Final Batch Loss: 0.482\n",
      "Epoch 5681, Loss: 17.532, Final Batch Loss: 0.566\n",
      "Epoch 5682, Loss: 17.534, Final Batch Loss: 0.389\n",
      "Epoch 5683, Loss: 17.277, Final Batch Loss: 0.543\n",
      "Epoch 5684, Loss: 17.297, Final Batch Loss: 0.470\n",
      "Epoch 5685, Loss: 17.622, Final Batch Loss: 0.402\n",
      "Epoch 5686, Loss: 17.355, Final Batch Loss: 0.468\n",
      "Epoch 5687, Loss: 17.569, Final Batch Loss: 0.428\n",
      "Epoch 5688, Loss: 17.559, Final Batch Loss: 0.394\n",
      "Epoch 5689, Loss: 17.398, Final Batch Loss: 0.577\n",
      "Epoch 5690, Loss: 17.820, Final Batch Loss: 0.411\n",
      "Epoch 5691, Loss: 17.616, Final Batch Loss: 0.447\n",
      "Epoch 5692, Loss: 17.728, Final Batch Loss: 0.494\n",
      "Epoch 5693, Loss: 17.260, Final Batch Loss: 0.584\n",
      "Epoch 5694, Loss: 17.383, Final Batch Loss: 0.616\n",
      "Epoch 5695, Loss: 17.520, Final Batch Loss: 0.541\n",
      "Epoch 5696, Loss: 17.394, Final Batch Loss: 0.399\n",
      "Epoch 5697, Loss: 17.524, Final Batch Loss: 0.426\n",
      "Epoch 5698, Loss: 17.708, Final Batch Loss: 0.421\n",
      "Epoch 5699, Loss: 17.447, Final Batch Loss: 0.422\n",
      "Epoch 5700, Loss: 17.425, Final Batch Loss: 0.529\n",
      "Epoch 5701, Loss: 17.651, Final Batch Loss: 0.469\n",
      "Epoch 5702, Loss: 17.755, Final Batch Loss: 0.426\n",
      "Epoch 5703, Loss: 17.491, Final Batch Loss: 0.426\n",
      "Epoch 5704, Loss: 17.509, Final Batch Loss: 0.487\n",
      "Epoch 5705, Loss: 17.495, Final Batch Loss: 0.522\n",
      "Epoch 5706, Loss: 17.633, Final Batch Loss: 0.523\n",
      "Epoch 5707, Loss: 17.524, Final Batch Loss: 0.562\n",
      "Epoch 5708, Loss: 17.644, Final Batch Loss: 0.407\n",
      "Epoch 5709, Loss: 17.619, Final Batch Loss: 0.527\n",
      "Epoch 5710, Loss: 17.477, Final Batch Loss: 0.454\n",
      "Epoch 5711, Loss: 17.406, Final Batch Loss: 0.425\n",
      "Epoch 5712, Loss: 17.897, Final Batch Loss: 0.453\n",
      "Epoch 5713, Loss: 17.631, Final Batch Loss: 0.478\n",
      "Epoch 5714, Loss: 17.770, Final Batch Loss: 0.399\n",
      "Epoch 5715, Loss: 17.388, Final Batch Loss: 0.424\n",
      "Epoch 5716, Loss: 17.629, Final Batch Loss: 0.435\n",
      "Epoch 5717, Loss: 17.790, Final Batch Loss: 0.527\n",
      "Epoch 5718, Loss: 17.991, Final Batch Loss: 0.563\n",
      "Epoch 5719, Loss: 17.632, Final Batch Loss: 0.526\n",
      "Epoch 5720, Loss: 17.633, Final Batch Loss: 0.500\n",
      "Epoch 5721, Loss: 17.518, Final Batch Loss: 0.528\n",
      "Epoch 5722, Loss: 17.621, Final Batch Loss: 0.522\n",
      "Epoch 5723, Loss: 17.427, Final Batch Loss: 0.372\n",
      "Epoch 5724, Loss: 17.440, Final Batch Loss: 0.447\n",
      "Epoch 5725, Loss: 17.694, Final Batch Loss: 0.419\n",
      "Epoch 5726, Loss: 17.379, Final Batch Loss: 0.430\n",
      "Epoch 5727, Loss: 17.371, Final Batch Loss: 0.482\n",
      "Epoch 5728, Loss: 17.165, Final Batch Loss: 0.404\n",
      "Epoch 5729, Loss: 17.527, Final Batch Loss: 0.509\n",
      "Epoch 5730, Loss: 17.550, Final Batch Loss: 0.606\n",
      "Epoch 5731, Loss: 17.608, Final Batch Loss: 0.533\n",
      "Epoch 5732, Loss: 17.359, Final Batch Loss: 0.526\n",
      "Epoch 5733, Loss: 17.541, Final Batch Loss: 0.484\n",
      "Epoch 5734, Loss: 17.627, Final Batch Loss: 0.523\n",
      "Epoch 5735, Loss: 17.584, Final Batch Loss: 0.462\n",
      "Epoch 5736, Loss: 17.545, Final Batch Loss: 0.428\n",
      "Epoch 5737, Loss: 17.688, Final Batch Loss: 0.538\n",
      "Epoch 5738, Loss: 17.675, Final Batch Loss: 0.519\n",
      "Epoch 5739, Loss: 17.512, Final Batch Loss: 0.473\n",
      "Epoch 5740, Loss: 17.709, Final Batch Loss: 0.459\n",
      "Epoch 5741, Loss: 17.573, Final Batch Loss: 0.471\n",
      "Epoch 5742, Loss: 17.496, Final Batch Loss: 0.477\n",
      "Epoch 5743, Loss: 17.442, Final Batch Loss: 0.500\n",
      "Epoch 5744, Loss: 17.357, Final Batch Loss: 0.443\n",
      "Epoch 5745, Loss: 17.837, Final Batch Loss: 0.656\n",
      "Epoch 5746, Loss: 17.346, Final Batch Loss: 0.429\n",
      "Epoch 5747, Loss: 17.523, Final Batch Loss: 0.466\n",
      "Epoch 5748, Loss: 17.521, Final Batch Loss: 0.527\n",
      "Epoch 5749, Loss: 17.575, Final Batch Loss: 0.491\n",
      "Epoch 5750, Loss: 17.598, Final Batch Loss: 0.520\n",
      "Epoch 5751, Loss: 17.418, Final Batch Loss: 0.426\n",
      "Epoch 5752, Loss: 17.582, Final Batch Loss: 0.500\n",
      "Epoch 5753, Loss: 17.302, Final Batch Loss: 0.407\n",
      "Epoch 5754, Loss: 17.542, Final Batch Loss: 0.480\n",
      "Epoch 5755, Loss: 17.396, Final Batch Loss: 0.515\n",
      "Epoch 5756, Loss: 17.514, Final Batch Loss: 0.461\n",
      "Epoch 5757, Loss: 17.474, Final Batch Loss: 0.423\n",
      "Epoch 5758, Loss: 17.286, Final Batch Loss: 0.422\n",
      "Epoch 5759, Loss: 17.532, Final Batch Loss: 0.463\n",
      "Epoch 5760, Loss: 17.393, Final Batch Loss: 0.583\n",
      "Epoch 5761, Loss: 17.644, Final Batch Loss: 0.478\n",
      "Epoch 5762, Loss: 17.519, Final Batch Loss: 0.515\n",
      "Epoch 5763, Loss: 17.418, Final Batch Loss: 0.427\n",
      "Epoch 5764, Loss: 17.344, Final Batch Loss: 0.438\n",
      "Epoch 5765, Loss: 17.253, Final Batch Loss: 0.510\n",
      "Epoch 5766, Loss: 17.466, Final Batch Loss: 0.540\n",
      "Epoch 5767, Loss: 17.682, Final Batch Loss: 0.492\n",
      "Epoch 5768, Loss: 17.584, Final Batch Loss: 0.553\n",
      "Epoch 5769, Loss: 17.775, Final Batch Loss: 0.448\n",
      "Epoch 5770, Loss: 17.218, Final Batch Loss: 0.458\n",
      "Epoch 5771, Loss: 17.622, Final Batch Loss: 0.446\n",
      "Epoch 5772, Loss: 17.431, Final Batch Loss: 0.524\n",
      "Epoch 5773, Loss: 17.595, Final Batch Loss: 0.564\n",
      "Epoch 5774, Loss: 17.430, Final Batch Loss: 0.489\n",
      "Epoch 5775, Loss: 17.567, Final Batch Loss: 0.419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5776, Loss: 17.293, Final Batch Loss: 0.535\n",
      "Epoch 5777, Loss: 17.467, Final Batch Loss: 0.372\n",
      "Epoch 5778, Loss: 17.364, Final Batch Loss: 0.472\n",
      "Epoch 5779, Loss: 17.557, Final Batch Loss: 0.615\n",
      "Epoch 5780, Loss: 17.124, Final Batch Loss: 0.405\n",
      "Epoch 5781, Loss: 17.226, Final Batch Loss: 0.513\n",
      "Epoch 5782, Loss: 17.800, Final Batch Loss: 0.570\n",
      "Epoch 5783, Loss: 17.388, Final Batch Loss: 0.517\n",
      "Epoch 5784, Loss: 17.582, Final Batch Loss: 0.466\n",
      "Epoch 5785, Loss: 17.323, Final Batch Loss: 0.490\n",
      "Epoch 5786, Loss: 17.445, Final Batch Loss: 0.420\n",
      "Epoch 5787, Loss: 17.555, Final Batch Loss: 0.526\n",
      "Epoch 5788, Loss: 17.584, Final Batch Loss: 0.445\n",
      "Epoch 5789, Loss: 17.376, Final Batch Loss: 0.339\n",
      "Epoch 5790, Loss: 17.565, Final Batch Loss: 0.419\n",
      "Epoch 5791, Loss: 17.558, Final Batch Loss: 0.581\n",
      "Epoch 5792, Loss: 17.375, Final Batch Loss: 0.478\n",
      "Epoch 5793, Loss: 17.653, Final Batch Loss: 0.567\n",
      "Epoch 5794, Loss: 17.377, Final Batch Loss: 0.521\n",
      "Epoch 5795, Loss: 17.619, Final Batch Loss: 0.486\n",
      "Epoch 5796, Loss: 17.527, Final Batch Loss: 0.592\n",
      "Epoch 5797, Loss: 17.383, Final Batch Loss: 0.495\n",
      "Epoch 5798, Loss: 17.553, Final Batch Loss: 0.454\n",
      "Epoch 5799, Loss: 17.729, Final Batch Loss: 0.545\n",
      "Epoch 5800, Loss: 17.303, Final Batch Loss: 0.482\n",
      "Epoch 5801, Loss: 17.410, Final Batch Loss: 0.533\n",
      "Epoch 5802, Loss: 17.457, Final Batch Loss: 0.537\n",
      "Epoch 5803, Loss: 17.677, Final Batch Loss: 0.448\n",
      "Epoch 5804, Loss: 17.473, Final Batch Loss: 0.496\n",
      "Epoch 5805, Loss: 17.568, Final Batch Loss: 0.482\n",
      "Epoch 5806, Loss: 17.641, Final Batch Loss: 0.444\n",
      "Epoch 5807, Loss: 17.510, Final Batch Loss: 0.455\n",
      "Epoch 5808, Loss: 17.362, Final Batch Loss: 0.431\n",
      "Epoch 5809, Loss: 17.687, Final Batch Loss: 0.491\n",
      "Epoch 5810, Loss: 17.800, Final Batch Loss: 0.511\n",
      "Epoch 5811, Loss: 17.674, Final Batch Loss: 0.527\n",
      "Epoch 5812, Loss: 17.465, Final Batch Loss: 0.447\n",
      "Epoch 5813, Loss: 17.370, Final Batch Loss: 0.489\n",
      "Epoch 5814, Loss: 17.779, Final Batch Loss: 0.514\n",
      "Epoch 5815, Loss: 17.531, Final Batch Loss: 0.562\n",
      "Epoch 5816, Loss: 17.537, Final Batch Loss: 0.536\n",
      "Epoch 5817, Loss: 17.411, Final Batch Loss: 0.431\n",
      "Epoch 5818, Loss: 17.592, Final Batch Loss: 0.545\n",
      "Epoch 5819, Loss: 17.382, Final Batch Loss: 0.504\n",
      "Epoch 5820, Loss: 17.573, Final Batch Loss: 0.474\n",
      "Epoch 5821, Loss: 17.249, Final Batch Loss: 0.483\n",
      "Epoch 5822, Loss: 17.719, Final Batch Loss: 0.536\n",
      "Epoch 5823, Loss: 17.381, Final Batch Loss: 0.544\n",
      "Epoch 5824, Loss: 17.518, Final Batch Loss: 0.451\n",
      "Epoch 5825, Loss: 17.432, Final Batch Loss: 0.444\n",
      "Epoch 5826, Loss: 17.493, Final Batch Loss: 0.504\n",
      "Epoch 5827, Loss: 17.604, Final Batch Loss: 0.596\n",
      "Epoch 5828, Loss: 17.398, Final Batch Loss: 0.382\n",
      "Epoch 5829, Loss: 17.238, Final Batch Loss: 0.406\n",
      "Epoch 5830, Loss: 17.236, Final Batch Loss: 0.506\n",
      "Epoch 5831, Loss: 17.630, Final Batch Loss: 0.572\n",
      "Epoch 5832, Loss: 17.466, Final Batch Loss: 0.521\n",
      "Epoch 5833, Loss: 17.699, Final Batch Loss: 0.769\n",
      "Epoch 5834, Loss: 17.809, Final Batch Loss: 0.548\n",
      "Epoch 5835, Loss: 17.377, Final Batch Loss: 0.433\n",
      "Epoch 5836, Loss: 17.781, Final Batch Loss: 0.718\n",
      "Epoch 5837, Loss: 17.503, Final Batch Loss: 0.519\n",
      "Epoch 5838, Loss: 17.435, Final Batch Loss: 0.539\n",
      "Epoch 5839, Loss: 17.599, Final Batch Loss: 0.442\n",
      "Epoch 5840, Loss: 17.759, Final Batch Loss: 0.458\n",
      "Epoch 5841, Loss: 17.612, Final Batch Loss: 0.522\n",
      "Epoch 5842, Loss: 17.266, Final Batch Loss: 0.430\n",
      "Epoch 5843, Loss: 17.483, Final Batch Loss: 0.514\n",
      "Epoch 5844, Loss: 17.513, Final Batch Loss: 0.491\n",
      "Epoch 5845, Loss: 17.462, Final Batch Loss: 0.447\n",
      "Epoch 5846, Loss: 17.553, Final Batch Loss: 0.450\n",
      "Epoch 5847, Loss: 17.690, Final Batch Loss: 0.581\n",
      "Epoch 5848, Loss: 17.604, Final Batch Loss: 0.521\n",
      "Epoch 5849, Loss: 17.486, Final Batch Loss: 0.468\n",
      "Epoch 5850, Loss: 17.526, Final Batch Loss: 0.441\n",
      "Epoch 5851, Loss: 17.364, Final Batch Loss: 0.502\n",
      "Epoch 5852, Loss: 17.349, Final Batch Loss: 0.513\n",
      "Epoch 5853, Loss: 17.377, Final Batch Loss: 0.367\n",
      "Epoch 5854, Loss: 17.384, Final Batch Loss: 0.443\n",
      "Epoch 5855, Loss: 17.456, Final Batch Loss: 0.497\n",
      "Epoch 5856, Loss: 17.411, Final Batch Loss: 0.517\n",
      "Epoch 5857, Loss: 17.428, Final Batch Loss: 0.507\n",
      "Epoch 5858, Loss: 17.602, Final Batch Loss: 0.403\n",
      "Epoch 5859, Loss: 17.519, Final Batch Loss: 0.425\n",
      "Epoch 5860, Loss: 17.635, Final Batch Loss: 0.520\n",
      "Epoch 5861, Loss: 17.587, Final Batch Loss: 0.517\n",
      "Epoch 5862, Loss: 17.632, Final Batch Loss: 0.424\n",
      "Epoch 5863, Loss: 17.357, Final Batch Loss: 0.496\n",
      "Epoch 5864, Loss: 17.301, Final Batch Loss: 0.323\n",
      "Epoch 5865, Loss: 17.569, Final Batch Loss: 0.499\n",
      "Epoch 5866, Loss: 17.382, Final Batch Loss: 0.501\n",
      "Epoch 5867, Loss: 17.496, Final Batch Loss: 0.429\n",
      "Epoch 5868, Loss: 17.223, Final Batch Loss: 0.372\n",
      "Epoch 5869, Loss: 17.442, Final Batch Loss: 0.467\n",
      "Epoch 5870, Loss: 17.601, Final Batch Loss: 0.510\n",
      "Epoch 5871, Loss: 17.196, Final Batch Loss: 0.514\n",
      "Epoch 5872, Loss: 17.286, Final Batch Loss: 0.510\n",
      "Epoch 5873, Loss: 17.521, Final Batch Loss: 0.506\n",
      "Epoch 5874, Loss: 17.675, Final Batch Loss: 0.429\n",
      "Epoch 5875, Loss: 17.465, Final Batch Loss: 0.535\n",
      "Epoch 5876, Loss: 17.577, Final Batch Loss: 0.434\n",
      "Epoch 5877, Loss: 17.590, Final Batch Loss: 0.435\n",
      "Epoch 5878, Loss: 17.622, Final Batch Loss: 0.498\n",
      "Epoch 5879, Loss: 17.551, Final Batch Loss: 0.573\n",
      "Epoch 5880, Loss: 17.671, Final Batch Loss: 0.515\n",
      "Epoch 5881, Loss: 17.600, Final Batch Loss: 0.547\n",
      "Epoch 5882, Loss: 17.164, Final Batch Loss: 0.428\n",
      "Epoch 5883, Loss: 17.341, Final Batch Loss: 0.488\n",
      "Epoch 5884, Loss: 17.430, Final Batch Loss: 0.427\n",
      "Epoch 5885, Loss: 17.697, Final Batch Loss: 0.442\n",
      "Epoch 5886, Loss: 17.482, Final Batch Loss: 0.387\n",
      "Epoch 5887, Loss: 17.451, Final Batch Loss: 0.498\n",
      "Epoch 5888, Loss: 17.646, Final Batch Loss: 0.499\n",
      "Epoch 5889, Loss: 17.769, Final Batch Loss: 0.475\n",
      "Epoch 5890, Loss: 17.432, Final Batch Loss: 0.433\n",
      "Epoch 5891, Loss: 17.318, Final Batch Loss: 0.431\n",
      "Epoch 5892, Loss: 17.774, Final Batch Loss: 0.531\n",
      "Epoch 5893, Loss: 17.483, Final Batch Loss: 0.424\n",
      "Epoch 5894, Loss: 17.622, Final Batch Loss: 0.509\n",
      "Epoch 5895, Loss: 17.558, Final Batch Loss: 0.485\n",
      "Epoch 5896, Loss: 17.457, Final Batch Loss: 0.489\n",
      "Epoch 5897, Loss: 17.260, Final Batch Loss: 0.425\n",
      "Epoch 5898, Loss: 17.356, Final Batch Loss: 0.481\n",
      "Epoch 5899, Loss: 17.372, Final Batch Loss: 0.445\n",
      "Epoch 5900, Loss: 17.211, Final Batch Loss: 0.424\n",
      "Epoch 5901, Loss: 17.521, Final Batch Loss: 0.492\n",
      "Epoch 5902, Loss: 17.384, Final Batch Loss: 0.565\n",
      "Epoch 5903, Loss: 17.175, Final Batch Loss: 0.547\n",
      "Epoch 5904, Loss: 17.594, Final Batch Loss: 0.490\n",
      "Epoch 5905, Loss: 17.696, Final Batch Loss: 0.417\n",
      "Epoch 5906, Loss: 17.660, Final Batch Loss: 0.434\n",
      "Epoch 5907, Loss: 17.556, Final Batch Loss: 0.390\n",
      "Epoch 5908, Loss: 17.863, Final Batch Loss: 0.527\n",
      "Epoch 5909, Loss: 17.619, Final Batch Loss: 0.491\n",
      "Epoch 5910, Loss: 17.543, Final Batch Loss: 0.409\n",
      "Epoch 5911, Loss: 17.394, Final Batch Loss: 0.451\n",
      "Epoch 5912, Loss: 17.469, Final Batch Loss: 0.522\n",
      "Epoch 5913, Loss: 17.700, Final Batch Loss: 0.409\n",
      "Epoch 5914, Loss: 17.731, Final Batch Loss: 0.517\n",
      "Epoch 5915, Loss: 17.654, Final Batch Loss: 0.475\n",
      "Epoch 5916, Loss: 17.380, Final Batch Loss: 0.497\n",
      "Epoch 5917, Loss: 17.437, Final Batch Loss: 0.523\n",
      "Epoch 5918, Loss: 17.617, Final Batch Loss: 0.575\n",
      "Epoch 5919, Loss: 17.806, Final Batch Loss: 0.473\n",
      "Epoch 5920, Loss: 17.526, Final Batch Loss: 0.451\n",
      "Epoch 5921, Loss: 17.047, Final Batch Loss: 0.421\n",
      "Epoch 5922, Loss: 17.579, Final Batch Loss: 0.416\n",
      "Epoch 5923, Loss: 17.409, Final Batch Loss: 0.479\n",
      "Epoch 5924, Loss: 17.209, Final Batch Loss: 0.368\n",
      "Epoch 5925, Loss: 17.282, Final Batch Loss: 0.475\n",
      "Epoch 5926, Loss: 17.329, Final Batch Loss: 0.500\n",
      "Epoch 5927, Loss: 17.244, Final Batch Loss: 0.396\n",
      "Epoch 5928, Loss: 17.609, Final Batch Loss: 0.436\n",
      "Epoch 5929, Loss: 17.679, Final Batch Loss: 0.467\n",
      "Epoch 5930, Loss: 17.541, Final Batch Loss: 0.540\n",
      "Epoch 5931, Loss: 17.463, Final Batch Loss: 0.469\n",
      "Epoch 5932, Loss: 17.520, Final Batch Loss: 0.499\n",
      "Epoch 5933, Loss: 17.417, Final Batch Loss: 0.420\n",
      "Epoch 5934, Loss: 17.578, Final Batch Loss: 0.539\n",
      "Epoch 5935, Loss: 17.156, Final Batch Loss: 0.488\n",
      "Epoch 5936, Loss: 17.488, Final Batch Loss: 0.429\n",
      "Epoch 5937, Loss: 17.892, Final Batch Loss: 0.506\n",
      "Epoch 5938, Loss: 17.376, Final Batch Loss: 0.564\n",
      "Epoch 5939, Loss: 17.391, Final Batch Loss: 0.545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5940, Loss: 17.462, Final Batch Loss: 0.543\n",
      "Epoch 5941, Loss: 17.260, Final Batch Loss: 0.427\n",
      "Epoch 5942, Loss: 17.649, Final Batch Loss: 0.463\n",
      "Epoch 5943, Loss: 17.691, Final Batch Loss: 0.481\n",
      "Epoch 5944, Loss: 17.194, Final Batch Loss: 0.433\n",
      "Epoch 5945, Loss: 17.496, Final Batch Loss: 0.489\n",
      "Epoch 5946, Loss: 17.414, Final Batch Loss: 0.449\n",
      "Epoch 5947, Loss: 17.338, Final Batch Loss: 0.438\n",
      "Epoch 5948, Loss: 17.464, Final Batch Loss: 0.593\n",
      "Epoch 5949, Loss: 17.307, Final Batch Loss: 0.492\n",
      "Epoch 5950, Loss: 17.518, Final Batch Loss: 0.516\n",
      "Epoch 5951, Loss: 17.356, Final Batch Loss: 0.403\n",
      "Epoch 5952, Loss: 17.600, Final Batch Loss: 0.375\n",
      "Epoch 5953, Loss: 17.366, Final Batch Loss: 0.452\n",
      "Epoch 5954, Loss: 17.507, Final Batch Loss: 0.461\n",
      "Epoch 5955, Loss: 17.596, Final Batch Loss: 0.585\n",
      "Epoch 5956, Loss: 17.393, Final Batch Loss: 0.571\n",
      "Epoch 5957, Loss: 17.618, Final Batch Loss: 0.435\n",
      "Epoch 5958, Loss: 17.809, Final Batch Loss: 0.530\n",
      "Epoch 5959, Loss: 17.206, Final Batch Loss: 0.404\n",
      "Epoch 5960, Loss: 17.339, Final Batch Loss: 0.450\n",
      "Epoch 5961, Loss: 17.462, Final Batch Loss: 0.504\n",
      "Epoch 5962, Loss: 17.606, Final Batch Loss: 0.496\n",
      "Epoch 5963, Loss: 17.530, Final Batch Loss: 0.510\n",
      "Epoch 5964, Loss: 17.356, Final Batch Loss: 0.493\n",
      "Epoch 5965, Loss: 17.479, Final Batch Loss: 0.455\n",
      "Epoch 5966, Loss: 17.285, Final Batch Loss: 0.462\n",
      "Epoch 5967, Loss: 17.841, Final Batch Loss: 0.582\n",
      "Epoch 5968, Loss: 17.417, Final Batch Loss: 0.405\n",
      "Epoch 5969, Loss: 17.388, Final Batch Loss: 0.426\n",
      "Epoch 5970, Loss: 17.498, Final Batch Loss: 0.384\n",
      "Epoch 5971, Loss: 17.403, Final Batch Loss: 0.478\n",
      "Epoch 5972, Loss: 17.453, Final Batch Loss: 0.474\n",
      "Epoch 5973, Loss: 17.399, Final Batch Loss: 0.503\n",
      "Epoch 5974, Loss: 17.211, Final Batch Loss: 0.371\n",
      "Epoch 5975, Loss: 17.484, Final Batch Loss: 0.598\n",
      "Epoch 5976, Loss: 17.331, Final Batch Loss: 0.447\n",
      "Epoch 5977, Loss: 17.446, Final Batch Loss: 0.548\n",
      "Epoch 5978, Loss: 17.264, Final Batch Loss: 0.406\n",
      "Epoch 5979, Loss: 17.282, Final Batch Loss: 0.415\n",
      "Epoch 5980, Loss: 17.775, Final Batch Loss: 0.490\n",
      "Epoch 5981, Loss: 17.443, Final Batch Loss: 0.487\n",
      "Epoch 5982, Loss: 17.156, Final Batch Loss: 0.535\n",
      "Epoch 5983, Loss: 17.502, Final Batch Loss: 0.492\n",
      "Epoch 5984, Loss: 17.452, Final Batch Loss: 0.505\n",
      "Epoch 5985, Loss: 17.675, Final Batch Loss: 0.587\n",
      "Epoch 5986, Loss: 17.646, Final Batch Loss: 0.504\n",
      "Epoch 5987, Loss: 17.640, Final Batch Loss: 0.519\n",
      "Epoch 5988, Loss: 17.520, Final Batch Loss: 0.473\n",
      "Epoch 5989, Loss: 17.344, Final Batch Loss: 0.408\n",
      "Epoch 5990, Loss: 17.415, Final Batch Loss: 0.500\n",
      "Epoch 5991, Loss: 17.511, Final Batch Loss: 0.485\n",
      "Epoch 5992, Loss: 17.752, Final Batch Loss: 0.542\n",
      "Epoch 5993, Loss: 17.230, Final Batch Loss: 0.438\n",
      "Epoch 5994, Loss: 17.518, Final Batch Loss: 0.521\n",
      "Epoch 5995, Loss: 17.213, Final Batch Loss: 0.425\n",
      "Epoch 5996, Loss: 17.604, Final Batch Loss: 0.441\n",
      "Epoch 5997, Loss: 17.659, Final Batch Loss: 0.490\n",
      "Epoch 5998, Loss: 17.422, Final Batch Loss: 0.520\n",
      "Epoch 5999, Loss: 17.392, Final Batch Loss: 0.390\n",
      "Epoch 6000, Loss: 17.638, Final Batch Loss: 0.441\n",
      "Epoch 6001, Loss: 17.213, Final Batch Loss: 0.553\n",
      "Epoch 6002, Loss: 17.564, Final Batch Loss: 0.573\n",
      "Epoch 6003, Loss: 17.499, Final Batch Loss: 0.542\n",
      "Epoch 6004, Loss: 17.502, Final Batch Loss: 0.451\n",
      "Epoch 6005, Loss: 17.382, Final Batch Loss: 0.419\n",
      "Epoch 6006, Loss: 17.220, Final Batch Loss: 0.478\n",
      "Epoch 6007, Loss: 17.247, Final Batch Loss: 0.454\n",
      "Epoch 6008, Loss: 17.441, Final Batch Loss: 0.473\n",
      "Epoch 6009, Loss: 17.638, Final Batch Loss: 0.493\n",
      "Epoch 6010, Loss: 16.995, Final Batch Loss: 0.423\n",
      "Epoch 6011, Loss: 17.458, Final Batch Loss: 0.456\n",
      "Epoch 6012, Loss: 17.262, Final Batch Loss: 0.477\n",
      "Epoch 6013, Loss: 17.212, Final Batch Loss: 0.411\n",
      "Epoch 6014, Loss: 17.335, Final Batch Loss: 0.394\n",
      "Epoch 6015, Loss: 17.630, Final Batch Loss: 0.517\n",
      "Epoch 6016, Loss: 17.618, Final Batch Loss: 0.565\n",
      "Epoch 6017, Loss: 17.313, Final Batch Loss: 0.424\n",
      "Epoch 6018, Loss: 17.436, Final Batch Loss: 0.471\n",
      "Epoch 6019, Loss: 17.680, Final Batch Loss: 0.350\n",
      "Epoch 6020, Loss: 17.653, Final Batch Loss: 0.545\n",
      "Epoch 6021, Loss: 17.558, Final Batch Loss: 0.500\n",
      "Epoch 6022, Loss: 17.018, Final Batch Loss: 0.471\n",
      "Epoch 6023, Loss: 17.173, Final Batch Loss: 0.476\n",
      "Epoch 6024, Loss: 17.182, Final Batch Loss: 0.426\n",
      "Epoch 6025, Loss: 17.506, Final Batch Loss: 0.651\n",
      "Epoch 6026, Loss: 17.343, Final Batch Loss: 0.473\n",
      "Epoch 6027, Loss: 17.500, Final Batch Loss: 0.431\n",
      "Epoch 6028, Loss: 17.505, Final Batch Loss: 0.478\n",
      "Epoch 6029, Loss: 17.262, Final Batch Loss: 0.542\n",
      "Epoch 6030, Loss: 17.286, Final Batch Loss: 0.440\n",
      "Epoch 6031, Loss: 17.430, Final Batch Loss: 0.480\n",
      "Epoch 6032, Loss: 17.299, Final Batch Loss: 0.455\n",
      "Epoch 6033, Loss: 17.413, Final Batch Loss: 0.485\n",
      "Epoch 6034, Loss: 17.268, Final Batch Loss: 0.407\n",
      "Epoch 6035, Loss: 17.571, Final Batch Loss: 0.347\n",
      "Epoch 6036, Loss: 17.338, Final Batch Loss: 0.411\n",
      "Epoch 6037, Loss: 17.424, Final Batch Loss: 0.538\n",
      "Epoch 6038, Loss: 17.385, Final Batch Loss: 0.469\n",
      "Epoch 6039, Loss: 17.377, Final Batch Loss: 0.543\n",
      "Epoch 6040, Loss: 17.380, Final Batch Loss: 0.467\n",
      "Epoch 6041, Loss: 17.633, Final Batch Loss: 0.605\n",
      "Epoch 6042, Loss: 17.236, Final Batch Loss: 0.514\n",
      "Epoch 6043, Loss: 17.665, Final Batch Loss: 0.461\n",
      "Epoch 6044, Loss: 17.456, Final Batch Loss: 0.447\n",
      "Epoch 6045, Loss: 17.324, Final Batch Loss: 0.477\n",
      "Epoch 6046, Loss: 17.683, Final Batch Loss: 0.506\n",
      "Epoch 6047, Loss: 17.429, Final Batch Loss: 0.494\n",
      "Epoch 6048, Loss: 17.455, Final Batch Loss: 0.526\n",
      "Epoch 6049, Loss: 17.219, Final Batch Loss: 0.554\n",
      "Epoch 6050, Loss: 17.091, Final Batch Loss: 0.457\n",
      "Epoch 6051, Loss: 17.318, Final Batch Loss: 0.408\n",
      "Epoch 6052, Loss: 17.484, Final Batch Loss: 0.441\n",
      "Epoch 6053, Loss: 17.385, Final Batch Loss: 0.625\n",
      "Epoch 6054, Loss: 17.587, Final Batch Loss: 0.567\n",
      "Epoch 6055, Loss: 17.351, Final Batch Loss: 0.469\n",
      "Epoch 6056, Loss: 17.383, Final Batch Loss: 0.475\n",
      "Epoch 6057, Loss: 17.650, Final Batch Loss: 0.526\n",
      "Epoch 6058, Loss: 17.365, Final Batch Loss: 0.401\n",
      "Epoch 6059, Loss: 17.227, Final Batch Loss: 0.353\n",
      "Epoch 6060, Loss: 17.439, Final Batch Loss: 0.437\n",
      "Epoch 6061, Loss: 17.468, Final Batch Loss: 0.509\n",
      "Epoch 6062, Loss: 17.367, Final Batch Loss: 0.415\n",
      "Epoch 6063, Loss: 17.417, Final Batch Loss: 0.448\n",
      "Epoch 6064, Loss: 17.401, Final Batch Loss: 0.538\n",
      "Epoch 6065, Loss: 17.435, Final Batch Loss: 0.375\n",
      "Epoch 6066, Loss: 17.470, Final Batch Loss: 0.441\n",
      "Epoch 6067, Loss: 17.441, Final Batch Loss: 0.504\n",
      "Epoch 6068, Loss: 17.312, Final Batch Loss: 0.458\n",
      "Epoch 6069, Loss: 17.306, Final Batch Loss: 0.398\n",
      "Epoch 6070, Loss: 17.350, Final Batch Loss: 0.514\n",
      "Epoch 6071, Loss: 17.390, Final Batch Loss: 0.438\n",
      "Epoch 6072, Loss: 17.471, Final Batch Loss: 0.496\n",
      "Epoch 6073, Loss: 17.588, Final Batch Loss: 0.478\n",
      "Epoch 6074, Loss: 17.425, Final Batch Loss: 0.502\n",
      "Epoch 6075, Loss: 17.198, Final Batch Loss: 0.384\n",
      "Epoch 6076, Loss: 17.121, Final Batch Loss: 0.403\n",
      "Epoch 6077, Loss: 17.565, Final Batch Loss: 0.416\n",
      "Epoch 6078, Loss: 17.338, Final Batch Loss: 0.525\n",
      "Epoch 6079, Loss: 17.416, Final Batch Loss: 0.468\n",
      "Epoch 6080, Loss: 17.521, Final Batch Loss: 0.556\n",
      "Epoch 6081, Loss: 17.612, Final Batch Loss: 0.518\n",
      "Epoch 6082, Loss: 17.532, Final Batch Loss: 0.520\n",
      "Epoch 6083, Loss: 17.275, Final Batch Loss: 0.454\n",
      "Epoch 6084, Loss: 17.498, Final Batch Loss: 0.572\n",
      "Epoch 6085, Loss: 17.559, Final Batch Loss: 0.539\n",
      "Epoch 6086, Loss: 17.373, Final Batch Loss: 0.406\n",
      "Epoch 6087, Loss: 17.692, Final Batch Loss: 0.585\n",
      "Epoch 6088, Loss: 17.605, Final Batch Loss: 0.575\n",
      "Epoch 6089, Loss: 17.433, Final Batch Loss: 0.454\n",
      "Epoch 6090, Loss: 17.506, Final Batch Loss: 0.384\n",
      "Epoch 6091, Loss: 17.221, Final Batch Loss: 0.513\n",
      "Epoch 6092, Loss: 17.491, Final Batch Loss: 0.374\n",
      "Epoch 6093, Loss: 17.430, Final Batch Loss: 0.415\n",
      "Epoch 6094, Loss: 17.492, Final Batch Loss: 0.474\n",
      "Epoch 6095, Loss: 17.572, Final Batch Loss: 0.571\n",
      "Epoch 6096, Loss: 17.360, Final Batch Loss: 0.487\n",
      "Epoch 6097, Loss: 17.132, Final Batch Loss: 0.545\n",
      "Epoch 6098, Loss: 17.476, Final Batch Loss: 0.537\n",
      "Epoch 6099, Loss: 17.217, Final Batch Loss: 0.470\n",
      "Epoch 6100, Loss: 17.570, Final Batch Loss: 0.404\n",
      "Epoch 6101, Loss: 17.326, Final Batch Loss: 0.451\n",
      "Epoch 6102, Loss: 17.529, Final Batch Loss: 0.562\n",
      "Epoch 6103, Loss: 17.239, Final Batch Loss: 0.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6104, Loss: 17.712, Final Batch Loss: 0.519\n",
      "Epoch 6105, Loss: 17.320, Final Batch Loss: 0.483\n",
      "Epoch 6106, Loss: 17.384, Final Batch Loss: 0.456\n",
      "Epoch 6107, Loss: 17.306, Final Batch Loss: 0.440\n",
      "Epoch 6108, Loss: 17.459, Final Batch Loss: 0.415\n",
      "Epoch 6109, Loss: 17.690, Final Batch Loss: 0.505\n",
      "Epoch 6110, Loss: 17.328, Final Batch Loss: 0.519\n",
      "Epoch 6111, Loss: 17.517, Final Batch Loss: 0.398\n",
      "Epoch 6112, Loss: 17.316, Final Batch Loss: 0.561\n",
      "Epoch 6113, Loss: 17.506, Final Batch Loss: 0.513\n",
      "Epoch 6114, Loss: 17.682, Final Batch Loss: 0.521\n",
      "Epoch 6115, Loss: 17.701, Final Batch Loss: 0.500\n",
      "Epoch 6116, Loss: 17.209, Final Batch Loss: 0.408\n",
      "Epoch 6117, Loss: 17.207, Final Batch Loss: 0.413\n",
      "Epoch 6118, Loss: 17.221, Final Batch Loss: 0.445\n",
      "Epoch 6119, Loss: 17.448, Final Batch Loss: 0.560\n",
      "Epoch 6120, Loss: 17.565, Final Batch Loss: 0.480\n",
      "Epoch 6121, Loss: 17.352, Final Batch Loss: 0.479\n",
      "Epoch 6122, Loss: 17.454, Final Batch Loss: 0.509\n",
      "Epoch 6123, Loss: 17.461, Final Batch Loss: 0.449\n",
      "Epoch 6124, Loss: 17.230, Final Batch Loss: 0.434\n",
      "Epoch 6125, Loss: 17.659, Final Batch Loss: 0.502\n",
      "Epoch 6126, Loss: 17.350, Final Batch Loss: 0.501\n",
      "Epoch 6127, Loss: 17.372, Final Batch Loss: 0.509\n",
      "Epoch 6128, Loss: 17.707, Final Batch Loss: 0.450\n",
      "Epoch 6129, Loss: 17.195, Final Batch Loss: 0.517\n",
      "Epoch 6130, Loss: 17.248, Final Batch Loss: 0.497\n",
      "Epoch 6131, Loss: 17.252, Final Batch Loss: 0.401\n",
      "Epoch 6132, Loss: 17.475, Final Batch Loss: 0.479\n",
      "Epoch 6133, Loss: 17.471, Final Batch Loss: 0.492\n",
      "Epoch 6134, Loss: 17.654, Final Batch Loss: 0.575\n",
      "Epoch 6135, Loss: 17.821, Final Batch Loss: 0.523\n",
      "Epoch 6136, Loss: 17.448, Final Batch Loss: 0.483\n",
      "Epoch 6137, Loss: 17.580, Final Batch Loss: 0.426\n",
      "Epoch 6138, Loss: 17.595, Final Batch Loss: 0.559\n",
      "Epoch 6139, Loss: 17.132, Final Batch Loss: 0.464\n",
      "Epoch 6140, Loss: 17.560, Final Batch Loss: 0.551\n",
      "Epoch 6141, Loss: 17.438, Final Batch Loss: 0.511\n",
      "Epoch 6142, Loss: 17.247, Final Batch Loss: 0.496\n",
      "Epoch 6143, Loss: 17.320, Final Batch Loss: 0.489\n",
      "Epoch 6144, Loss: 17.835, Final Batch Loss: 0.375\n",
      "Epoch 6145, Loss: 17.271, Final Batch Loss: 0.504\n",
      "Epoch 6146, Loss: 17.415, Final Batch Loss: 0.445\n",
      "Epoch 6147, Loss: 17.613, Final Batch Loss: 0.475\n",
      "Epoch 6148, Loss: 17.556, Final Batch Loss: 0.573\n",
      "Epoch 6149, Loss: 17.438, Final Batch Loss: 0.463\n",
      "Epoch 6150, Loss: 17.264, Final Batch Loss: 0.512\n",
      "Epoch 6151, Loss: 17.536, Final Batch Loss: 0.439\n",
      "Epoch 6152, Loss: 17.564, Final Batch Loss: 0.513\n",
      "Epoch 6153, Loss: 17.814, Final Batch Loss: 0.468\n",
      "Epoch 6154, Loss: 17.392, Final Batch Loss: 0.477\n",
      "Epoch 6155, Loss: 17.672, Final Batch Loss: 0.462\n",
      "Epoch 6156, Loss: 17.556, Final Batch Loss: 0.511\n",
      "Epoch 6157, Loss: 17.340, Final Batch Loss: 0.458\n",
      "Epoch 6158, Loss: 17.623, Final Batch Loss: 0.547\n",
      "Epoch 6159, Loss: 17.665, Final Batch Loss: 0.607\n",
      "Epoch 6160, Loss: 17.475, Final Batch Loss: 0.548\n",
      "Epoch 6161, Loss: 17.678, Final Batch Loss: 0.549\n",
      "Epoch 6162, Loss: 17.408, Final Batch Loss: 0.416\n",
      "Epoch 6163, Loss: 17.667, Final Batch Loss: 0.437\n",
      "Epoch 6164, Loss: 17.678, Final Batch Loss: 0.455\n",
      "Epoch 6165, Loss: 17.538, Final Batch Loss: 0.527\n",
      "Epoch 6166, Loss: 17.510, Final Batch Loss: 0.488\n",
      "Epoch 6167, Loss: 17.389, Final Batch Loss: 0.500\n",
      "Epoch 6168, Loss: 17.022, Final Batch Loss: 0.366\n",
      "Epoch 6169, Loss: 17.239, Final Batch Loss: 0.455\n",
      "Epoch 6170, Loss: 17.631, Final Batch Loss: 0.493\n",
      "Epoch 6171, Loss: 17.364, Final Batch Loss: 0.442\n",
      "Epoch 6172, Loss: 17.459, Final Batch Loss: 0.392\n",
      "Epoch 6173, Loss: 17.526, Final Batch Loss: 0.473\n",
      "Epoch 6174, Loss: 17.413, Final Batch Loss: 0.548\n",
      "Epoch 6175, Loss: 17.333, Final Batch Loss: 0.465\n",
      "Epoch 6176, Loss: 17.423, Final Batch Loss: 0.517\n",
      "Epoch 6177, Loss: 17.389, Final Batch Loss: 0.500\n",
      "Epoch 6178, Loss: 17.282, Final Batch Loss: 0.427\n",
      "Epoch 6179, Loss: 17.461, Final Batch Loss: 0.419\n",
      "Epoch 6180, Loss: 17.177, Final Batch Loss: 0.461\n",
      "Epoch 6181, Loss: 17.441, Final Batch Loss: 0.554\n",
      "Epoch 6182, Loss: 17.350, Final Batch Loss: 0.464\n",
      "Epoch 6183, Loss: 17.400, Final Batch Loss: 0.425\n",
      "Epoch 6184, Loss: 17.454, Final Batch Loss: 0.368\n",
      "Epoch 6185, Loss: 17.536, Final Batch Loss: 0.544\n",
      "Epoch 6186, Loss: 17.411, Final Batch Loss: 0.506\n",
      "Epoch 6187, Loss: 17.321, Final Batch Loss: 0.345\n",
      "Epoch 6188, Loss: 17.375, Final Batch Loss: 0.462\n",
      "Epoch 6189, Loss: 17.273, Final Batch Loss: 0.534\n",
      "Epoch 6190, Loss: 17.401, Final Batch Loss: 0.391\n",
      "Epoch 6191, Loss: 17.513, Final Batch Loss: 0.482\n",
      "Epoch 6192, Loss: 17.284, Final Batch Loss: 0.543\n",
      "Epoch 6193, Loss: 17.609, Final Batch Loss: 0.506\n",
      "Epoch 6194, Loss: 17.410, Final Batch Loss: 0.402\n",
      "Epoch 6195, Loss: 17.705, Final Batch Loss: 0.556\n",
      "Epoch 6196, Loss: 17.526, Final Batch Loss: 0.463\n",
      "Epoch 6197, Loss: 17.311, Final Batch Loss: 0.481\n",
      "Epoch 6198, Loss: 17.415, Final Batch Loss: 0.458\n",
      "Epoch 6199, Loss: 17.381, Final Batch Loss: 0.448\n",
      "Epoch 6200, Loss: 17.340, Final Batch Loss: 0.498\n",
      "Epoch 6201, Loss: 17.471, Final Batch Loss: 0.617\n",
      "Epoch 6202, Loss: 17.617, Final Batch Loss: 0.511\n",
      "Epoch 6203, Loss: 17.183, Final Batch Loss: 0.477\n",
      "Epoch 6204, Loss: 17.328, Final Batch Loss: 0.532\n",
      "Epoch 6205, Loss: 17.183, Final Batch Loss: 0.465\n",
      "Epoch 6206, Loss: 17.587, Final Batch Loss: 0.446\n",
      "Epoch 6207, Loss: 17.311, Final Batch Loss: 0.364\n",
      "Epoch 6208, Loss: 17.717, Final Batch Loss: 0.749\n",
      "Epoch 6209, Loss: 17.487, Final Batch Loss: 0.498\n",
      "Epoch 6210, Loss: 17.652, Final Batch Loss: 0.628\n",
      "Epoch 6211, Loss: 17.071, Final Batch Loss: 0.433\n",
      "Epoch 6212, Loss: 17.406, Final Batch Loss: 0.467\n",
      "Epoch 6213, Loss: 17.219, Final Batch Loss: 0.447\n",
      "Epoch 6214, Loss: 17.439, Final Batch Loss: 0.588\n",
      "Epoch 6215, Loss: 17.261, Final Batch Loss: 0.463\n",
      "Epoch 6216, Loss: 17.390, Final Batch Loss: 0.451\n",
      "Epoch 6217, Loss: 17.485, Final Batch Loss: 0.465\n",
      "Epoch 6218, Loss: 17.627, Final Batch Loss: 0.548\n",
      "Epoch 6219, Loss: 17.596, Final Batch Loss: 0.490\n",
      "Epoch 6220, Loss: 17.611, Final Batch Loss: 0.571\n",
      "Epoch 6221, Loss: 17.451, Final Batch Loss: 0.410\n",
      "Epoch 6222, Loss: 17.328, Final Batch Loss: 0.415\n",
      "Epoch 6223, Loss: 17.774, Final Batch Loss: 0.451\n",
      "Epoch 6224, Loss: 17.298, Final Batch Loss: 0.360\n",
      "Epoch 6225, Loss: 17.477, Final Batch Loss: 0.494\n",
      "Epoch 6226, Loss: 17.415, Final Batch Loss: 0.481\n",
      "Epoch 6227, Loss: 17.390, Final Batch Loss: 0.448\n",
      "Epoch 6228, Loss: 17.460, Final Batch Loss: 0.510\n",
      "Epoch 6229, Loss: 17.237, Final Batch Loss: 0.527\n",
      "Epoch 6230, Loss: 17.399, Final Batch Loss: 0.522\n",
      "Epoch 6231, Loss: 17.393, Final Batch Loss: 0.399\n",
      "Epoch 6232, Loss: 17.377, Final Batch Loss: 0.414\n",
      "Epoch 6233, Loss: 17.306, Final Batch Loss: 0.499\n",
      "Epoch 6234, Loss: 17.262, Final Batch Loss: 0.465\n",
      "Epoch 6235, Loss: 17.384, Final Batch Loss: 0.411\n",
      "Epoch 6236, Loss: 17.478, Final Batch Loss: 0.504\n",
      "Epoch 6237, Loss: 17.333, Final Batch Loss: 0.501\n",
      "Epoch 6238, Loss: 17.418, Final Batch Loss: 0.535\n",
      "Epoch 6239, Loss: 17.337, Final Batch Loss: 0.430\n",
      "Epoch 6240, Loss: 17.449, Final Batch Loss: 0.521\n",
      "Epoch 6241, Loss: 17.556, Final Batch Loss: 0.577\n",
      "Epoch 6242, Loss: 17.273, Final Batch Loss: 0.439\n",
      "Epoch 6243, Loss: 17.375, Final Batch Loss: 0.542\n",
      "Epoch 6244, Loss: 17.337, Final Batch Loss: 0.437\n",
      "Epoch 6245, Loss: 17.735, Final Batch Loss: 0.520\n",
      "Epoch 6246, Loss: 17.227, Final Batch Loss: 0.532\n",
      "Epoch 6247, Loss: 17.405, Final Batch Loss: 0.401\n",
      "Epoch 6248, Loss: 17.287, Final Batch Loss: 0.482\n",
      "Epoch 6249, Loss: 17.285, Final Batch Loss: 0.602\n",
      "Epoch 6250, Loss: 17.444, Final Batch Loss: 0.389\n",
      "Epoch 6251, Loss: 17.551, Final Batch Loss: 0.539\n",
      "Epoch 6252, Loss: 17.600, Final Batch Loss: 0.467\n",
      "Epoch 6253, Loss: 17.352, Final Batch Loss: 0.509\n",
      "Epoch 6254, Loss: 17.455, Final Batch Loss: 0.489\n",
      "Epoch 6255, Loss: 17.218, Final Batch Loss: 0.436\n",
      "Epoch 6256, Loss: 17.328, Final Batch Loss: 0.460\n",
      "Epoch 6257, Loss: 17.630, Final Batch Loss: 0.447\n",
      "Epoch 6258, Loss: 17.193, Final Batch Loss: 0.452\n",
      "Epoch 6259, Loss: 17.367, Final Batch Loss: 0.448\n",
      "Epoch 6260, Loss: 17.208, Final Batch Loss: 0.412\n",
      "Epoch 6261, Loss: 17.528, Final Batch Loss: 0.583\n",
      "Epoch 6262, Loss: 17.327, Final Batch Loss: 0.505\n",
      "Epoch 6263, Loss: 17.326, Final Batch Loss: 0.473\n",
      "Epoch 6264, Loss: 17.344, Final Batch Loss: 0.398\n",
      "Epoch 6265, Loss: 17.332, Final Batch Loss: 0.470\n",
      "Epoch 6266, Loss: 17.136, Final Batch Loss: 0.410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6267, Loss: 17.479, Final Batch Loss: 0.496\n",
      "Epoch 6268, Loss: 17.461, Final Batch Loss: 0.451\n",
      "Epoch 6269, Loss: 17.404, Final Batch Loss: 0.542\n",
      "Epoch 6270, Loss: 17.470, Final Batch Loss: 0.449\n",
      "Epoch 6271, Loss: 17.512, Final Batch Loss: 0.499\n",
      "Epoch 6272, Loss: 17.392, Final Batch Loss: 0.388\n",
      "Epoch 6273, Loss: 17.417, Final Batch Loss: 0.439\n",
      "Epoch 6274, Loss: 17.410, Final Batch Loss: 0.540\n",
      "Epoch 6275, Loss: 17.265, Final Batch Loss: 0.488\n",
      "Epoch 6276, Loss: 17.557, Final Batch Loss: 0.570\n",
      "Epoch 6277, Loss: 17.507, Final Batch Loss: 0.420\n",
      "Epoch 6278, Loss: 17.439, Final Batch Loss: 0.433\n",
      "Epoch 6279, Loss: 17.286, Final Batch Loss: 0.459\n",
      "Epoch 6280, Loss: 17.164, Final Batch Loss: 0.448\n",
      "Epoch 6281, Loss: 17.317, Final Batch Loss: 0.538\n",
      "Epoch 6282, Loss: 17.396, Final Batch Loss: 0.429\n",
      "Epoch 6283, Loss: 17.654, Final Batch Loss: 0.459\n",
      "Epoch 6284, Loss: 17.428, Final Batch Loss: 0.434\n",
      "Epoch 6285, Loss: 17.311, Final Batch Loss: 0.552\n",
      "Epoch 6286, Loss: 17.361, Final Batch Loss: 0.453\n",
      "Epoch 6287, Loss: 17.318, Final Batch Loss: 0.462\n",
      "Epoch 6288, Loss: 17.554, Final Batch Loss: 0.409\n",
      "Epoch 6289, Loss: 17.523, Final Batch Loss: 0.592\n",
      "Epoch 6290, Loss: 17.453, Final Batch Loss: 0.457\n",
      "Epoch 6291, Loss: 17.428, Final Batch Loss: 0.555\n",
      "Epoch 6292, Loss: 17.148, Final Batch Loss: 0.395\n",
      "Epoch 6293, Loss: 17.542, Final Batch Loss: 0.552\n",
      "Epoch 6294, Loss: 17.296, Final Batch Loss: 0.509\n",
      "Epoch 6295, Loss: 17.228, Final Batch Loss: 0.518\n",
      "Epoch 6296, Loss: 17.212, Final Batch Loss: 0.464\n",
      "Epoch 6297, Loss: 17.372, Final Batch Loss: 0.487\n",
      "Epoch 6298, Loss: 17.452, Final Batch Loss: 0.504\n",
      "Epoch 6299, Loss: 17.448, Final Batch Loss: 0.532\n",
      "Epoch 6300, Loss: 17.822, Final Batch Loss: 0.622\n",
      "Epoch 6301, Loss: 17.236, Final Batch Loss: 0.533\n",
      "Epoch 6302, Loss: 17.386, Final Batch Loss: 0.479\n",
      "Epoch 6303, Loss: 17.534, Final Batch Loss: 0.506\n",
      "Epoch 6304, Loss: 17.279, Final Batch Loss: 0.384\n",
      "Epoch 6305, Loss: 17.479, Final Batch Loss: 0.487\n",
      "Epoch 6306, Loss: 17.287, Final Batch Loss: 0.391\n",
      "Epoch 6307, Loss: 17.259, Final Batch Loss: 0.634\n",
      "Epoch 6308, Loss: 17.420, Final Batch Loss: 0.473\n",
      "Epoch 6309, Loss: 17.274, Final Batch Loss: 0.513\n",
      "Epoch 6310, Loss: 17.483, Final Batch Loss: 0.604\n",
      "Epoch 6311, Loss: 17.592, Final Batch Loss: 0.475\n",
      "Epoch 6312, Loss: 17.014, Final Batch Loss: 0.455\n",
      "Epoch 6313, Loss: 17.276, Final Batch Loss: 0.478\n",
      "Epoch 6314, Loss: 17.415, Final Batch Loss: 0.382\n",
      "Epoch 6315, Loss: 17.526, Final Batch Loss: 0.442\n",
      "Epoch 6316, Loss: 17.761, Final Batch Loss: 0.635\n",
      "Epoch 6317, Loss: 17.285, Final Batch Loss: 0.418\n",
      "Epoch 6318, Loss: 17.284, Final Batch Loss: 0.464\n",
      "Epoch 6319, Loss: 17.412, Final Batch Loss: 0.483\n",
      "Epoch 6320, Loss: 17.388, Final Batch Loss: 0.484\n",
      "Epoch 6321, Loss: 17.656, Final Batch Loss: 0.491\n",
      "Epoch 6322, Loss: 17.465, Final Batch Loss: 0.523\n",
      "Epoch 6323, Loss: 17.486, Final Batch Loss: 0.451\n",
      "Epoch 6324, Loss: 17.180, Final Batch Loss: 0.498\n",
      "Epoch 6325, Loss: 17.210, Final Batch Loss: 0.361\n",
      "Epoch 6326, Loss: 17.445, Final Batch Loss: 0.573\n",
      "Epoch 6327, Loss: 17.388, Final Batch Loss: 0.539\n",
      "Epoch 6328, Loss: 17.319, Final Batch Loss: 0.491\n",
      "Epoch 6329, Loss: 17.382, Final Batch Loss: 0.434\n",
      "Epoch 6330, Loss: 17.301, Final Batch Loss: 0.570\n",
      "Epoch 6331, Loss: 17.537, Final Batch Loss: 0.477\n",
      "Epoch 6332, Loss: 17.208, Final Batch Loss: 0.527\n",
      "Epoch 6333, Loss: 17.655, Final Batch Loss: 0.585\n",
      "Epoch 6334, Loss: 17.458, Final Batch Loss: 0.506\n",
      "Epoch 6335, Loss: 17.328, Final Batch Loss: 0.489\n",
      "Epoch 6336, Loss: 17.183, Final Batch Loss: 0.488\n",
      "Epoch 6337, Loss: 17.243, Final Batch Loss: 0.483\n",
      "Epoch 6338, Loss: 17.540, Final Batch Loss: 0.475\n",
      "Epoch 6339, Loss: 17.450, Final Batch Loss: 0.549\n",
      "Epoch 6340, Loss: 17.305, Final Batch Loss: 0.549\n",
      "Epoch 6341, Loss: 17.287, Final Batch Loss: 0.437\n",
      "Epoch 6342, Loss: 17.591, Final Batch Loss: 0.499\n",
      "Epoch 6343, Loss: 17.591, Final Batch Loss: 0.550\n",
      "Epoch 6344, Loss: 17.349, Final Batch Loss: 0.562\n",
      "Epoch 6345, Loss: 17.284, Final Batch Loss: 0.494\n",
      "Epoch 6346, Loss: 17.446, Final Batch Loss: 0.537\n",
      "Epoch 6347, Loss: 17.567, Final Batch Loss: 0.615\n",
      "Epoch 6348, Loss: 17.483, Final Batch Loss: 0.510\n",
      "Epoch 6349, Loss: 17.367, Final Batch Loss: 0.499\n",
      "Epoch 6350, Loss: 17.417, Final Batch Loss: 0.571\n",
      "Epoch 6351, Loss: 17.570, Final Batch Loss: 0.470\n",
      "Epoch 6352, Loss: 17.038, Final Batch Loss: 0.527\n",
      "Epoch 6353, Loss: 17.379, Final Batch Loss: 0.510\n",
      "Epoch 6354, Loss: 17.289, Final Batch Loss: 0.439\n",
      "Epoch 6355, Loss: 17.561, Final Batch Loss: 0.512\n",
      "Epoch 6356, Loss: 17.407, Final Batch Loss: 0.494\n",
      "Epoch 6357, Loss: 17.588, Final Batch Loss: 0.485\n",
      "Epoch 6358, Loss: 17.268, Final Batch Loss: 0.597\n",
      "Epoch 6359, Loss: 17.566, Final Batch Loss: 0.558\n",
      "Epoch 6360, Loss: 17.237, Final Batch Loss: 0.538\n",
      "Epoch 6361, Loss: 17.468, Final Batch Loss: 0.527\n",
      "Epoch 6362, Loss: 17.463, Final Batch Loss: 0.492\n",
      "Epoch 6363, Loss: 17.277, Final Batch Loss: 0.448\n",
      "Epoch 6364, Loss: 17.494, Final Batch Loss: 0.498\n",
      "Epoch 6365, Loss: 17.410, Final Batch Loss: 0.529\n",
      "Epoch 6366, Loss: 17.196, Final Batch Loss: 0.450\n",
      "Epoch 6367, Loss: 17.235, Final Batch Loss: 0.485\n",
      "Epoch 6368, Loss: 17.387, Final Batch Loss: 0.419\n",
      "Epoch 6369, Loss: 17.243, Final Batch Loss: 0.526\n",
      "Epoch 6370, Loss: 17.370, Final Batch Loss: 0.532\n",
      "Epoch 6371, Loss: 17.315, Final Batch Loss: 0.433\n",
      "Epoch 6372, Loss: 17.389, Final Batch Loss: 0.478\n",
      "Epoch 6373, Loss: 17.177, Final Batch Loss: 0.363\n",
      "Epoch 6374, Loss: 17.449, Final Batch Loss: 0.449\n",
      "Epoch 6375, Loss: 17.308, Final Batch Loss: 0.506\n",
      "Epoch 6376, Loss: 17.574, Final Batch Loss: 0.456\n",
      "Epoch 6377, Loss: 17.293, Final Batch Loss: 0.451\n",
      "Epoch 6378, Loss: 17.425, Final Batch Loss: 0.489\n",
      "Epoch 6379, Loss: 17.023, Final Batch Loss: 0.516\n",
      "Epoch 6380, Loss: 17.394, Final Batch Loss: 0.481\n",
      "Epoch 6381, Loss: 17.430, Final Batch Loss: 0.591\n",
      "Epoch 6382, Loss: 17.370, Final Batch Loss: 0.452\n",
      "Epoch 6383, Loss: 17.177, Final Batch Loss: 0.397\n",
      "Epoch 6384, Loss: 17.312, Final Batch Loss: 0.395\n",
      "Epoch 6385, Loss: 17.395, Final Batch Loss: 0.552\n",
      "Epoch 6386, Loss: 17.042, Final Batch Loss: 0.361\n",
      "Epoch 6387, Loss: 17.473, Final Batch Loss: 0.377\n",
      "Epoch 6388, Loss: 17.502, Final Batch Loss: 0.517\n",
      "Epoch 6389, Loss: 17.386, Final Batch Loss: 0.445\n",
      "Epoch 6390, Loss: 17.193, Final Batch Loss: 0.550\n",
      "Epoch 6391, Loss: 17.474, Final Batch Loss: 0.463\n",
      "Epoch 6392, Loss: 17.402, Final Batch Loss: 0.445\n",
      "Epoch 6393, Loss: 17.312, Final Batch Loss: 0.395\n",
      "Epoch 6394, Loss: 17.324, Final Batch Loss: 0.496\n",
      "Epoch 6395, Loss: 17.177, Final Batch Loss: 0.390\n",
      "Epoch 6396, Loss: 17.369, Final Batch Loss: 0.369\n",
      "Epoch 6397, Loss: 17.172, Final Batch Loss: 0.497\n",
      "Epoch 6398, Loss: 17.250, Final Batch Loss: 0.504\n",
      "Epoch 6399, Loss: 17.321, Final Batch Loss: 0.494\n",
      "Epoch 6400, Loss: 17.394, Final Batch Loss: 0.486\n",
      "Epoch 6401, Loss: 17.340, Final Batch Loss: 0.503\n",
      "Epoch 6402, Loss: 17.399, Final Batch Loss: 0.487\n",
      "Epoch 6403, Loss: 17.427, Final Batch Loss: 0.393\n",
      "Epoch 6404, Loss: 17.151, Final Batch Loss: 0.446\n",
      "Epoch 6405, Loss: 17.206, Final Batch Loss: 0.458\n",
      "Epoch 6406, Loss: 17.292, Final Batch Loss: 0.408\n",
      "Epoch 6407, Loss: 17.566, Final Batch Loss: 0.527\n",
      "Epoch 6408, Loss: 17.397, Final Batch Loss: 0.500\n",
      "Epoch 6409, Loss: 17.195, Final Batch Loss: 0.489\n",
      "Epoch 6410, Loss: 17.157, Final Batch Loss: 0.501\n",
      "Epoch 6411, Loss: 17.378, Final Batch Loss: 0.491\n",
      "Epoch 6412, Loss: 17.134, Final Batch Loss: 0.467\n",
      "Epoch 6413, Loss: 17.420, Final Batch Loss: 0.423\n",
      "Epoch 6414, Loss: 17.373, Final Batch Loss: 0.428\n",
      "Epoch 6415, Loss: 17.560, Final Batch Loss: 0.470\n",
      "Epoch 6416, Loss: 17.281, Final Batch Loss: 0.461\n",
      "Epoch 6417, Loss: 17.586, Final Batch Loss: 0.711\n",
      "Epoch 6418, Loss: 17.208, Final Batch Loss: 0.434\n",
      "Epoch 6419, Loss: 17.140, Final Batch Loss: 0.511\n",
      "Epoch 6420, Loss: 17.206, Final Batch Loss: 0.402\n",
      "Epoch 6421, Loss: 17.593, Final Batch Loss: 0.480\n",
      "Epoch 6422, Loss: 17.432, Final Batch Loss: 0.422\n",
      "Epoch 6423, Loss: 17.154, Final Batch Loss: 0.524\n",
      "Epoch 6424, Loss: 17.143, Final Batch Loss: 0.502\n",
      "Epoch 6425, Loss: 17.205, Final Batch Loss: 0.465\n",
      "Epoch 6426, Loss: 17.163, Final Batch Loss: 0.447\n",
      "Epoch 6427, Loss: 17.447, Final Batch Loss: 0.433\n",
      "Epoch 6428, Loss: 17.380, Final Batch Loss: 0.446\n",
      "Epoch 6429, Loss: 17.153, Final Batch Loss: 0.446\n",
      "Epoch 6430, Loss: 17.101, Final Batch Loss: 0.413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6431, Loss: 17.504, Final Batch Loss: 0.520\n",
      "Epoch 6432, Loss: 17.501, Final Batch Loss: 0.618\n",
      "Epoch 6433, Loss: 17.277, Final Batch Loss: 0.401\n",
      "Epoch 6434, Loss: 17.580, Final Batch Loss: 0.484\n",
      "Epoch 6435, Loss: 17.277, Final Batch Loss: 0.496\n",
      "Epoch 6436, Loss: 17.070, Final Batch Loss: 0.371\n",
      "Epoch 6437, Loss: 17.277, Final Batch Loss: 0.494\n",
      "Epoch 6438, Loss: 17.472, Final Batch Loss: 0.642\n",
      "Epoch 6439, Loss: 17.142, Final Batch Loss: 0.467\n",
      "Epoch 6440, Loss: 17.471, Final Batch Loss: 0.370\n",
      "Epoch 6441, Loss: 17.150, Final Batch Loss: 0.503\n",
      "Epoch 6442, Loss: 17.183, Final Batch Loss: 0.392\n",
      "Epoch 6443, Loss: 17.481, Final Batch Loss: 0.460\n",
      "Epoch 6444, Loss: 17.160, Final Batch Loss: 0.560\n",
      "Epoch 6445, Loss: 17.280, Final Batch Loss: 0.553\n",
      "Epoch 6446, Loss: 17.279, Final Batch Loss: 0.436\n",
      "Epoch 6447, Loss: 17.496, Final Batch Loss: 0.448\n",
      "Epoch 6448, Loss: 17.179, Final Batch Loss: 0.422\n",
      "Epoch 6449, Loss: 17.250, Final Batch Loss: 0.509\n",
      "Epoch 6450, Loss: 17.521, Final Batch Loss: 0.420\n",
      "Epoch 6451, Loss: 17.172, Final Batch Loss: 0.396\n",
      "Epoch 6452, Loss: 17.548, Final Batch Loss: 0.676\n",
      "Epoch 6453, Loss: 17.496, Final Batch Loss: 0.434\n",
      "Epoch 6454, Loss: 17.234, Final Batch Loss: 0.521\n",
      "Epoch 6455, Loss: 17.265, Final Batch Loss: 0.429\n",
      "Epoch 6456, Loss: 17.195, Final Batch Loss: 0.454\n",
      "Epoch 6457, Loss: 17.323, Final Batch Loss: 0.420\n",
      "Epoch 6458, Loss: 17.392, Final Batch Loss: 0.529\n",
      "Epoch 6459, Loss: 17.383, Final Batch Loss: 0.529\n",
      "Epoch 6460, Loss: 17.400, Final Batch Loss: 0.514\n",
      "Epoch 6461, Loss: 17.547, Final Batch Loss: 0.477\n",
      "Epoch 6462, Loss: 17.457, Final Batch Loss: 0.518\n",
      "Epoch 6463, Loss: 17.284, Final Batch Loss: 0.445\n",
      "Epoch 6464, Loss: 17.321, Final Batch Loss: 0.522\n",
      "Epoch 6465, Loss: 17.315, Final Batch Loss: 0.526\n",
      "Epoch 6466, Loss: 17.027, Final Batch Loss: 0.494\n",
      "Epoch 6467, Loss: 17.183, Final Batch Loss: 0.539\n",
      "Epoch 6468, Loss: 17.314, Final Batch Loss: 0.467\n",
      "Epoch 6469, Loss: 17.594, Final Batch Loss: 0.483\n",
      "Epoch 6470, Loss: 17.376, Final Batch Loss: 0.371\n",
      "Epoch 6471, Loss: 17.536, Final Batch Loss: 0.513\n",
      "Epoch 6472, Loss: 17.341, Final Batch Loss: 0.568\n",
      "Epoch 6473, Loss: 17.349, Final Batch Loss: 0.511\n",
      "Epoch 6474, Loss: 17.438, Final Batch Loss: 0.483\n",
      "Epoch 6475, Loss: 17.266, Final Batch Loss: 0.506\n",
      "Epoch 6476, Loss: 17.352, Final Batch Loss: 0.530\n",
      "Epoch 6477, Loss: 17.167, Final Batch Loss: 0.394\n",
      "Epoch 6478, Loss: 17.138, Final Batch Loss: 0.488\n",
      "Epoch 6479, Loss: 17.464, Final Batch Loss: 0.547\n",
      "Epoch 6480, Loss: 17.235, Final Batch Loss: 0.387\n",
      "Epoch 6481, Loss: 17.315, Final Batch Loss: 0.417\n",
      "Epoch 6482, Loss: 17.265, Final Batch Loss: 0.583\n",
      "Epoch 6483, Loss: 17.352, Final Batch Loss: 0.536\n",
      "Epoch 6484, Loss: 17.421, Final Batch Loss: 0.458\n",
      "Epoch 6485, Loss: 17.415, Final Batch Loss: 0.497\n",
      "Epoch 6486, Loss: 17.332, Final Batch Loss: 0.449\n",
      "Epoch 6487, Loss: 17.220, Final Batch Loss: 0.477\n",
      "Epoch 6488, Loss: 17.401, Final Batch Loss: 0.448\n",
      "Epoch 6489, Loss: 17.348, Final Batch Loss: 0.632\n",
      "Epoch 6490, Loss: 17.196, Final Batch Loss: 0.437\n",
      "Epoch 6491, Loss: 17.229, Final Batch Loss: 0.364\n",
      "Epoch 6492, Loss: 17.553, Final Batch Loss: 0.436\n",
      "Epoch 6493, Loss: 17.243, Final Batch Loss: 0.418\n",
      "Epoch 6494, Loss: 17.444, Final Batch Loss: 0.424\n",
      "Epoch 6495, Loss: 17.543, Final Batch Loss: 0.470\n",
      "Epoch 6496, Loss: 17.295, Final Batch Loss: 0.476\n",
      "Epoch 6497, Loss: 17.364, Final Batch Loss: 0.432\n",
      "Epoch 6498, Loss: 17.085, Final Batch Loss: 0.372\n",
      "Epoch 6499, Loss: 17.211, Final Batch Loss: 0.444\n",
      "Epoch 6500, Loss: 17.414, Final Batch Loss: 0.505\n",
      "Epoch 6501, Loss: 17.173, Final Batch Loss: 0.522\n",
      "Epoch 6502, Loss: 17.360, Final Batch Loss: 0.456\n",
      "Epoch 6503, Loss: 17.328, Final Batch Loss: 0.369\n",
      "Epoch 6504, Loss: 17.478, Final Batch Loss: 0.442\n",
      "Epoch 6505, Loss: 17.162, Final Batch Loss: 0.532\n",
      "Epoch 6506, Loss: 17.274, Final Batch Loss: 0.460\n",
      "Epoch 6507, Loss: 17.622, Final Batch Loss: 0.402\n",
      "Epoch 6508, Loss: 17.524, Final Batch Loss: 0.478\n",
      "Epoch 6509, Loss: 17.135, Final Batch Loss: 0.490\n",
      "Epoch 6510, Loss: 17.512, Final Batch Loss: 0.502\n",
      "Epoch 6511, Loss: 17.454, Final Batch Loss: 0.505\n",
      "Epoch 6512, Loss: 17.366, Final Batch Loss: 0.506\n",
      "Epoch 6513, Loss: 17.572, Final Batch Loss: 0.523\n",
      "Epoch 6514, Loss: 17.023, Final Batch Loss: 0.480\n",
      "Epoch 6515, Loss: 17.206, Final Batch Loss: 0.395\n",
      "Epoch 6516, Loss: 17.435, Final Batch Loss: 0.450\n",
      "Epoch 6517, Loss: 17.467, Final Batch Loss: 0.454\n",
      "Epoch 6518, Loss: 17.174, Final Batch Loss: 0.544\n",
      "Epoch 6519, Loss: 17.459, Final Batch Loss: 0.567\n",
      "Epoch 6520, Loss: 17.257, Final Batch Loss: 0.433\n",
      "Epoch 6521, Loss: 17.265, Final Batch Loss: 0.429\n",
      "Epoch 6522, Loss: 17.492, Final Batch Loss: 0.598\n",
      "Epoch 6523, Loss: 17.122, Final Batch Loss: 0.400\n",
      "Epoch 6524, Loss: 17.279, Final Batch Loss: 0.498\n",
      "Epoch 6525, Loss: 17.251, Final Batch Loss: 0.481\n",
      "Epoch 6526, Loss: 17.228, Final Batch Loss: 0.607\n",
      "Epoch 6527, Loss: 17.363, Final Batch Loss: 0.489\n",
      "Epoch 6528, Loss: 17.284, Final Batch Loss: 0.494\n",
      "Epoch 6529, Loss: 17.546, Final Batch Loss: 0.532\n",
      "Epoch 6530, Loss: 17.309, Final Batch Loss: 0.501\n",
      "Epoch 6531, Loss: 17.362, Final Batch Loss: 0.577\n",
      "Epoch 6532, Loss: 17.334, Final Batch Loss: 0.462\n",
      "Epoch 6533, Loss: 17.231, Final Batch Loss: 0.475\n",
      "Epoch 6534, Loss: 17.195, Final Batch Loss: 0.553\n",
      "Epoch 6535, Loss: 17.325, Final Batch Loss: 0.527\n",
      "Epoch 6536, Loss: 17.212, Final Batch Loss: 0.383\n",
      "Epoch 6537, Loss: 17.342, Final Batch Loss: 0.586\n",
      "Epoch 6538, Loss: 17.491, Final Batch Loss: 0.503\n",
      "Epoch 6539, Loss: 17.418, Final Batch Loss: 0.522\n",
      "Epoch 6540, Loss: 17.084, Final Batch Loss: 0.376\n",
      "Epoch 6541, Loss: 17.332, Final Batch Loss: 0.602\n",
      "Epoch 6542, Loss: 17.372, Final Batch Loss: 0.464\n",
      "Epoch 6543, Loss: 17.507, Final Batch Loss: 0.511\n",
      "Epoch 6544, Loss: 17.353, Final Batch Loss: 0.546\n",
      "Epoch 6545, Loss: 17.387, Final Batch Loss: 0.552\n",
      "Epoch 6546, Loss: 17.387, Final Batch Loss: 0.582\n",
      "Epoch 6547, Loss: 17.562, Final Batch Loss: 0.485\n",
      "Epoch 6548, Loss: 17.479, Final Batch Loss: 0.563\n",
      "Epoch 6549, Loss: 17.544, Final Batch Loss: 0.576\n",
      "Epoch 6550, Loss: 17.236, Final Batch Loss: 0.381\n",
      "Epoch 6551, Loss: 17.233, Final Batch Loss: 0.564\n",
      "Epoch 6552, Loss: 17.068, Final Batch Loss: 0.520\n",
      "Epoch 6553, Loss: 17.292, Final Batch Loss: 0.476\n",
      "Epoch 6554, Loss: 17.207, Final Batch Loss: 0.574\n",
      "Epoch 6555, Loss: 17.231, Final Batch Loss: 0.394\n",
      "Epoch 6556, Loss: 17.346, Final Batch Loss: 0.446\n",
      "Epoch 6557, Loss: 17.372, Final Batch Loss: 0.478\n",
      "Epoch 6558, Loss: 17.331, Final Batch Loss: 0.465\n",
      "Epoch 6559, Loss: 17.348, Final Batch Loss: 0.540\n",
      "Epoch 6560, Loss: 17.284, Final Batch Loss: 0.468\n",
      "Epoch 6561, Loss: 17.299, Final Batch Loss: 0.508\n",
      "Epoch 6562, Loss: 17.123, Final Batch Loss: 0.514\n",
      "Epoch 6563, Loss: 17.141, Final Batch Loss: 0.426\n",
      "Epoch 6564, Loss: 17.592, Final Batch Loss: 0.540\n",
      "Epoch 6565, Loss: 17.284, Final Batch Loss: 0.492\n",
      "Epoch 6566, Loss: 17.212, Final Batch Loss: 0.438\n",
      "Epoch 6567, Loss: 17.331, Final Batch Loss: 0.465\n",
      "Epoch 6568, Loss: 17.230, Final Batch Loss: 0.460\n",
      "Epoch 6569, Loss: 17.526, Final Batch Loss: 0.442\n",
      "Epoch 6570, Loss: 17.314, Final Batch Loss: 0.451\n",
      "Epoch 6571, Loss: 16.953, Final Batch Loss: 0.338\n",
      "Epoch 6572, Loss: 17.437, Final Batch Loss: 0.373\n",
      "Epoch 6573, Loss: 17.298, Final Batch Loss: 0.499\n",
      "Epoch 6574, Loss: 17.166, Final Batch Loss: 0.433\n",
      "Epoch 6575, Loss: 17.230, Final Batch Loss: 0.558\n",
      "Epoch 6576, Loss: 17.276, Final Batch Loss: 0.423\n",
      "Epoch 6577, Loss: 17.305, Final Batch Loss: 0.512\n",
      "Epoch 6578, Loss: 17.383, Final Batch Loss: 0.489\n",
      "Epoch 6579, Loss: 17.222, Final Batch Loss: 0.469\n",
      "Epoch 6580, Loss: 17.568, Final Batch Loss: 0.576\n",
      "Epoch 6581, Loss: 17.341, Final Batch Loss: 0.526\n",
      "Epoch 6582, Loss: 17.174, Final Batch Loss: 0.494\n",
      "Epoch 6583, Loss: 17.322, Final Batch Loss: 0.448\n",
      "Epoch 6584, Loss: 17.152, Final Batch Loss: 0.448\n",
      "Epoch 6585, Loss: 16.679, Final Batch Loss: 0.509\n",
      "Epoch 6586, Loss: 17.189, Final Batch Loss: 0.424\n",
      "Epoch 6587, Loss: 17.069, Final Batch Loss: 0.532\n",
      "Epoch 6588, Loss: 17.435, Final Batch Loss: 0.519\n",
      "Epoch 6589, Loss: 17.061, Final Batch Loss: 0.373\n",
      "Epoch 6590, Loss: 17.361, Final Batch Loss: 0.468\n",
      "Epoch 6591, Loss: 17.377, Final Batch Loss: 0.572\n",
      "Epoch 6592, Loss: 17.167, Final Batch Loss: 0.482\n",
      "Epoch 6593, Loss: 17.439, Final Batch Loss: 0.597\n",
      "Epoch 6594, Loss: 17.062, Final Batch Loss: 0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6595, Loss: 17.307, Final Batch Loss: 0.448\n",
      "Epoch 6596, Loss: 17.331, Final Batch Loss: 0.515\n",
      "Epoch 6597, Loss: 17.388, Final Batch Loss: 0.531\n",
      "Epoch 6598, Loss: 17.151, Final Batch Loss: 0.479\n",
      "Epoch 6599, Loss: 17.421, Final Batch Loss: 0.463\n",
      "Epoch 6600, Loss: 17.372, Final Batch Loss: 0.410\n",
      "Epoch 6601, Loss: 17.344, Final Batch Loss: 0.331\n",
      "Epoch 6602, Loss: 17.192, Final Batch Loss: 0.537\n",
      "Epoch 6603, Loss: 17.196, Final Batch Loss: 0.527\n",
      "Epoch 6604, Loss: 17.147, Final Batch Loss: 0.478\n",
      "Epoch 6605, Loss: 17.172, Final Batch Loss: 0.417\n",
      "Epoch 6606, Loss: 17.383, Final Batch Loss: 0.593\n",
      "Epoch 6607, Loss: 17.255, Final Batch Loss: 0.480\n",
      "Epoch 6608, Loss: 17.554, Final Batch Loss: 0.438\n",
      "Epoch 6609, Loss: 17.181, Final Batch Loss: 0.492\n",
      "Epoch 6610, Loss: 17.355, Final Batch Loss: 0.439\n",
      "Epoch 6611, Loss: 17.115, Final Batch Loss: 0.369\n",
      "Epoch 6612, Loss: 17.355, Final Batch Loss: 0.460\n",
      "Epoch 6613, Loss: 17.331, Final Batch Loss: 0.554\n",
      "Epoch 6614, Loss: 17.024, Final Batch Loss: 0.337\n",
      "Epoch 6615, Loss: 17.355, Final Batch Loss: 0.396\n",
      "Epoch 6616, Loss: 17.222, Final Batch Loss: 0.491\n",
      "Epoch 6617, Loss: 17.291, Final Batch Loss: 0.506\n",
      "Epoch 6618, Loss: 17.190, Final Batch Loss: 0.482\n",
      "Epoch 6619, Loss: 17.477, Final Batch Loss: 0.405\n",
      "Epoch 6620, Loss: 17.584, Final Batch Loss: 0.565\n",
      "Epoch 6621, Loss: 17.292, Final Batch Loss: 0.523\n",
      "Epoch 6622, Loss: 17.342, Final Batch Loss: 0.449\n",
      "Epoch 6623, Loss: 17.381, Final Batch Loss: 0.564\n",
      "Epoch 6624, Loss: 17.178, Final Batch Loss: 0.407\n",
      "Epoch 6625, Loss: 17.347, Final Batch Loss: 0.496\n",
      "Epoch 6626, Loss: 17.113, Final Batch Loss: 0.472\n",
      "Epoch 6627, Loss: 17.535, Final Batch Loss: 0.608\n",
      "Epoch 6628, Loss: 17.226, Final Batch Loss: 0.556\n",
      "Epoch 6629, Loss: 17.246, Final Batch Loss: 0.489\n",
      "Epoch 6630, Loss: 17.423, Final Batch Loss: 0.600\n",
      "Epoch 6631, Loss: 17.363, Final Batch Loss: 0.495\n",
      "Epoch 6632, Loss: 17.226, Final Batch Loss: 0.508\n",
      "Epoch 6633, Loss: 17.182, Final Batch Loss: 0.408\n",
      "Epoch 6634, Loss: 17.374, Final Batch Loss: 0.614\n",
      "Epoch 6635, Loss: 17.220, Final Batch Loss: 0.512\n",
      "Epoch 6636, Loss: 17.103, Final Batch Loss: 0.570\n",
      "Epoch 6637, Loss: 17.286, Final Batch Loss: 0.393\n",
      "Epoch 6638, Loss: 17.213, Final Batch Loss: 0.400\n",
      "Epoch 6639, Loss: 17.173, Final Batch Loss: 0.542\n",
      "Epoch 6640, Loss: 17.145, Final Batch Loss: 0.465\n",
      "Epoch 6641, Loss: 17.301, Final Batch Loss: 0.497\n",
      "Epoch 6642, Loss: 17.283, Final Batch Loss: 0.488\n",
      "Epoch 6643, Loss: 17.283, Final Batch Loss: 0.444\n",
      "Epoch 6644, Loss: 17.288, Final Batch Loss: 0.500\n",
      "Epoch 6645, Loss: 17.263, Final Batch Loss: 0.389\n",
      "Epoch 6646, Loss: 17.195, Final Batch Loss: 0.546\n",
      "Epoch 6647, Loss: 17.365, Final Batch Loss: 0.461\n",
      "Epoch 6648, Loss: 17.008, Final Batch Loss: 0.438\n",
      "Epoch 6649, Loss: 17.493, Final Batch Loss: 0.460\n",
      "Epoch 6650, Loss: 17.418, Final Batch Loss: 0.479\n",
      "Epoch 6651, Loss: 17.438, Final Batch Loss: 0.431\n",
      "Epoch 6652, Loss: 17.299, Final Batch Loss: 0.449\n",
      "Epoch 6653, Loss: 17.057, Final Batch Loss: 0.522\n",
      "Epoch 6654, Loss: 17.270, Final Batch Loss: 0.528\n",
      "Epoch 6655, Loss: 17.177, Final Batch Loss: 0.533\n",
      "Epoch 6656, Loss: 17.391, Final Batch Loss: 0.436\n",
      "Epoch 6657, Loss: 17.265, Final Batch Loss: 0.465\n",
      "Epoch 6658, Loss: 17.234, Final Batch Loss: 0.591\n",
      "Epoch 6659, Loss: 17.429, Final Batch Loss: 0.450\n",
      "Epoch 6660, Loss: 17.193, Final Batch Loss: 0.502\n",
      "Epoch 6661, Loss: 17.912, Final Batch Loss: 0.549\n",
      "Epoch 6662, Loss: 17.449, Final Batch Loss: 0.407\n",
      "Epoch 6663, Loss: 17.229, Final Batch Loss: 0.396\n",
      "Epoch 6664, Loss: 16.933, Final Batch Loss: 0.411\n",
      "Epoch 6665, Loss: 17.211, Final Batch Loss: 0.527\n",
      "Epoch 6666, Loss: 17.521, Final Batch Loss: 0.584\n",
      "Epoch 6667, Loss: 17.003, Final Batch Loss: 0.464\n",
      "Epoch 6668, Loss: 17.289, Final Batch Loss: 0.492\n",
      "Epoch 6669, Loss: 17.371, Final Batch Loss: 0.497\n",
      "Epoch 6670, Loss: 17.322, Final Batch Loss: 0.483\n",
      "Epoch 6671, Loss: 17.203, Final Batch Loss: 0.479\n",
      "Epoch 6672, Loss: 17.290, Final Batch Loss: 0.361\n",
      "Epoch 6673, Loss: 17.256, Final Batch Loss: 0.464\n",
      "Epoch 6674, Loss: 17.300, Final Batch Loss: 0.540\n",
      "Epoch 6675, Loss: 17.465, Final Batch Loss: 0.534\n",
      "Epoch 6676, Loss: 17.381, Final Batch Loss: 0.413\n",
      "Epoch 6677, Loss: 17.015, Final Batch Loss: 0.511\n",
      "Epoch 6678, Loss: 17.246, Final Batch Loss: 0.443\n",
      "Epoch 6679, Loss: 17.085, Final Batch Loss: 0.469\n",
      "Epoch 6680, Loss: 17.044, Final Batch Loss: 0.472\n",
      "Epoch 6681, Loss: 17.340, Final Batch Loss: 0.530\n",
      "Epoch 6682, Loss: 17.523, Final Batch Loss: 0.497\n",
      "Epoch 6683, Loss: 17.398, Final Batch Loss: 0.570\n",
      "Epoch 6684, Loss: 17.573, Final Batch Loss: 0.534\n",
      "Epoch 6685, Loss: 17.206, Final Batch Loss: 0.418\n",
      "Epoch 6686, Loss: 17.428, Final Batch Loss: 0.511\n",
      "Epoch 6687, Loss: 17.033, Final Batch Loss: 0.397\n",
      "Epoch 6688, Loss: 17.513, Final Batch Loss: 0.423\n",
      "Epoch 6689, Loss: 17.783, Final Batch Loss: 0.591\n",
      "Epoch 6690, Loss: 17.233, Final Batch Loss: 0.436\n",
      "Epoch 6691, Loss: 17.445, Final Batch Loss: 0.449\n",
      "Epoch 6692, Loss: 17.401, Final Batch Loss: 0.435\n",
      "Epoch 6693, Loss: 17.360, Final Batch Loss: 0.514\n",
      "Epoch 6694, Loss: 17.419, Final Batch Loss: 0.434\n",
      "Epoch 6695, Loss: 17.279, Final Batch Loss: 0.417\n",
      "Epoch 6696, Loss: 17.436, Final Batch Loss: 0.427\n",
      "Epoch 6697, Loss: 17.199, Final Batch Loss: 0.398\n",
      "Epoch 6698, Loss: 17.162, Final Batch Loss: 0.466\n",
      "Epoch 6699, Loss: 17.366, Final Batch Loss: 0.530\n",
      "Epoch 6700, Loss: 17.322, Final Batch Loss: 0.427\n",
      "Epoch 6701, Loss: 17.147, Final Batch Loss: 0.369\n",
      "Epoch 6702, Loss: 17.473, Final Batch Loss: 0.545\n",
      "Epoch 6703, Loss: 17.313, Final Batch Loss: 0.453\n",
      "Epoch 6704, Loss: 17.369, Final Batch Loss: 0.453\n",
      "Epoch 6705, Loss: 17.051, Final Batch Loss: 0.458\n",
      "Epoch 6706, Loss: 17.343, Final Batch Loss: 0.536\n",
      "Epoch 6707, Loss: 17.266, Final Batch Loss: 0.436\n",
      "Epoch 6708, Loss: 17.026, Final Batch Loss: 0.407\n",
      "Epoch 6709, Loss: 17.295, Final Batch Loss: 0.416\n",
      "Epoch 6710, Loss: 17.383, Final Batch Loss: 0.441\n",
      "Epoch 6711, Loss: 17.383, Final Batch Loss: 0.468\n",
      "Epoch 6712, Loss: 17.243, Final Batch Loss: 0.430\n",
      "Epoch 6713, Loss: 17.166, Final Batch Loss: 0.494\n",
      "Epoch 6714, Loss: 17.152, Final Batch Loss: 0.441\n",
      "Epoch 6715, Loss: 17.005, Final Batch Loss: 0.417\n",
      "Epoch 6716, Loss: 17.483, Final Batch Loss: 0.565\n",
      "Epoch 6717, Loss: 17.065, Final Batch Loss: 0.456\n",
      "Epoch 6718, Loss: 17.218, Final Batch Loss: 0.451\n",
      "Epoch 6719, Loss: 17.352, Final Batch Loss: 0.518\n",
      "Epoch 6720, Loss: 17.152, Final Batch Loss: 0.458\n",
      "Epoch 6721, Loss: 16.998, Final Batch Loss: 0.414\n",
      "Epoch 6722, Loss: 17.165, Final Batch Loss: 0.453\n",
      "Epoch 6723, Loss: 17.216, Final Batch Loss: 0.390\n",
      "Epoch 6724, Loss: 17.465, Final Batch Loss: 0.474\n",
      "Epoch 6725, Loss: 17.160, Final Batch Loss: 0.399\n",
      "Epoch 6726, Loss: 17.389, Final Batch Loss: 0.494\n",
      "Epoch 6727, Loss: 17.460, Final Batch Loss: 0.456\n",
      "Epoch 6728, Loss: 17.255, Final Batch Loss: 0.481\n",
      "Epoch 6729, Loss: 17.311, Final Batch Loss: 0.459\n",
      "Epoch 6730, Loss: 17.143, Final Batch Loss: 0.519\n",
      "Epoch 6731, Loss: 17.442, Final Batch Loss: 0.462\n",
      "Epoch 6732, Loss: 17.548, Final Batch Loss: 0.512\n",
      "Epoch 6733, Loss: 17.286, Final Batch Loss: 0.401\n",
      "Epoch 6734, Loss: 17.053, Final Batch Loss: 0.465\n",
      "Epoch 6735, Loss: 17.447, Final Batch Loss: 0.537\n",
      "Epoch 6736, Loss: 17.641, Final Batch Loss: 0.397\n",
      "Epoch 6737, Loss: 17.261, Final Batch Loss: 0.365\n",
      "Epoch 6738, Loss: 17.366, Final Batch Loss: 0.514\n",
      "Epoch 6739, Loss: 17.586, Final Batch Loss: 0.534\n",
      "Epoch 6740, Loss: 17.123, Final Batch Loss: 0.502\n",
      "Epoch 6741, Loss: 17.136, Final Batch Loss: 0.505\n",
      "Epoch 6742, Loss: 17.339, Final Batch Loss: 0.554\n",
      "Epoch 6743, Loss: 17.523, Final Batch Loss: 0.514\n",
      "Epoch 6744, Loss: 17.322, Final Batch Loss: 0.518\n",
      "Epoch 6745, Loss: 17.223, Final Batch Loss: 0.424\n",
      "Epoch 6746, Loss: 17.204, Final Batch Loss: 0.529\n",
      "Epoch 6747, Loss: 17.046, Final Batch Loss: 0.460\n",
      "Epoch 6748, Loss: 16.997, Final Batch Loss: 0.441\n",
      "Epoch 6749, Loss: 17.350, Final Batch Loss: 0.463\n",
      "Epoch 6750, Loss: 17.297, Final Batch Loss: 0.482\n",
      "Epoch 6751, Loss: 17.564, Final Batch Loss: 0.429\n",
      "Epoch 6752, Loss: 17.518, Final Batch Loss: 0.473\n",
      "Epoch 6753, Loss: 17.502, Final Batch Loss: 0.486\n",
      "Epoch 6754, Loss: 17.512, Final Batch Loss: 0.500\n",
      "Epoch 6755, Loss: 16.901, Final Batch Loss: 0.410\n",
      "Epoch 6756, Loss: 17.401, Final Batch Loss: 0.549\n",
      "Epoch 6757, Loss: 17.326, Final Batch Loss: 0.473\n",
      "Epoch 6758, Loss: 17.421, Final Batch Loss: 0.483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6759, Loss: 17.354, Final Batch Loss: 0.585\n",
      "Epoch 6760, Loss: 17.318, Final Batch Loss: 0.516\n",
      "Epoch 6761, Loss: 17.179, Final Batch Loss: 0.469\n",
      "Epoch 6762, Loss: 17.311, Final Batch Loss: 0.444\n",
      "Epoch 6763, Loss: 17.589, Final Batch Loss: 0.435\n",
      "Epoch 6764, Loss: 17.293, Final Batch Loss: 0.566\n",
      "Epoch 6765, Loss: 17.100, Final Batch Loss: 0.463\n",
      "Epoch 6766, Loss: 17.146, Final Batch Loss: 0.503\n",
      "Epoch 6767, Loss: 17.322, Final Batch Loss: 0.504\n",
      "Epoch 6768, Loss: 17.264, Final Batch Loss: 0.451\n",
      "Epoch 6769, Loss: 17.455, Final Batch Loss: 0.483\n",
      "Epoch 6770, Loss: 17.335, Final Batch Loss: 0.530\n",
      "Epoch 6771, Loss: 17.164, Final Batch Loss: 0.427\n",
      "Epoch 6772, Loss: 17.291, Final Batch Loss: 0.375\n",
      "Epoch 6773, Loss: 17.299, Final Batch Loss: 0.485\n",
      "Epoch 6774, Loss: 17.226, Final Batch Loss: 0.581\n",
      "Epoch 6775, Loss: 16.981, Final Batch Loss: 0.426\n",
      "Epoch 6776, Loss: 16.984, Final Batch Loss: 0.403\n",
      "Epoch 6777, Loss: 17.182, Final Batch Loss: 0.481\n",
      "Epoch 6778, Loss: 16.906, Final Batch Loss: 0.503\n",
      "Epoch 6779, Loss: 17.163, Final Batch Loss: 0.469\n",
      "Epoch 6780, Loss: 17.077, Final Batch Loss: 0.465\n",
      "Epoch 6781, Loss: 17.068, Final Batch Loss: 0.437\n",
      "Epoch 6782, Loss: 17.029, Final Batch Loss: 0.413\n",
      "Epoch 6783, Loss: 17.063, Final Batch Loss: 0.468\n",
      "Epoch 6784, Loss: 17.492, Final Batch Loss: 0.573\n",
      "Epoch 6785, Loss: 17.495, Final Batch Loss: 0.520\n",
      "Epoch 6786, Loss: 17.208, Final Batch Loss: 0.542\n",
      "Epoch 6787, Loss: 16.886, Final Batch Loss: 0.401\n",
      "Epoch 6788, Loss: 17.190, Final Batch Loss: 0.421\n",
      "Epoch 6789, Loss: 17.000, Final Batch Loss: 0.432\n",
      "Epoch 6790, Loss: 17.451, Final Batch Loss: 0.453\n",
      "Epoch 6791, Loss: 17.072, Final Batch Loss: 0.476\n",
      "Epoch 6792, Loss: 17.251, Final Batch Loss: 0.446\n",
      "Epoch 6793, Loss: 17.275, Final Batch Loss: 0.431\n",
      "Epoch 6794, Loss: 17.003, Final Batch Loss: 0.402\n",
      "Epoch 6795, Loss: 17.282, Final Batch Loss: 0.436\n",
      "Epoch 6796, Loss: 17.372, Final Batch Loss: 0.515\n",
      "Epoch 6797, Loss: 17.257, Final Batch Loss: 0.447\n",
      "Epoch 6798, Loss: 17.326, Final Batch Loss: 0.401\n",
      "Epoch 6799, Loss: 17.144, Final Batch Loss: 0.451\n",
      "Epoch 6800, Loss: 17.361, Final Batch Loss: 0.507\n",
      "Epoch 6801, Loss: 17.161, Final Batch Loss: 0.499\n",
      "Epoch 6802, Loss: 17.212, Final Batch Loss: 0.478\n",
      "Epoch 6803, Loss: 17.378, Final Batch Loss: 0.461\n",
      "Epoch 6804, Loss: 17.171, Final Batch Loss: 0.494\n",
      "Epoch 6805, Loss: 17.192, Final Batch Loss: 0.553\n",
      "Epoch 6806, Loss: 17.138, Final Batch Loss: 0.567\n",
      "Epoch 6807, Loss: 17.167, Final Batch Loss: 0.414\n",
      "Epoch 6808, Loss: 16.975, Final Batch Loss: 0.431\n",
      "Epoch 6809, Loss: 17.377, Final Batch Loss: 0.455\n",
      "Epoch 6810, Loss: 17.244, Final Batch Loss: 0.468\n",
      "Epoch 6811, Loss: 16.973, Final Batch Loss: 0.453\n",
      "Epoch 6812, Loss: 17.009, Final Batch Loss: 0.438\n",
      "Epoch 6813, Loss: 17.232, Final Batch Loss: 0.580\n",
      "Epoch 6814, Loss: 17.126, Final Batch Loss: 0.479\n",
      "Epoch 6815, Loss: 16.980, Final Batch Loss: 0.549\n",
      "Epoch 6816, Loss: 17.193, Final Batch Loss: 0.462\n",
      "Epoch 6817, Loss: 17.401, Final Batch Loss: 0.450\n",
      "Epoch 6818, Loss: 17.160, Final Batch Loss: 0.418\n",
      "Epoch 6819, Loss: 17.333, Final Batch Loss: 0.499\n",
      "Epoch 6820, Loss: 17.139, Final Batch Loss: 0.422\n",
      "Epoch 6821, Loss: 17.198, Final Batch Loss: 0.495\n",
      "Epoch 6822, Loss: 17.067, Final Batch Loss: 0.471\n",
      "Epoch 6823, Loss: 17.221, Final Batch Loss: 0.447\n",
      "Epoch 6824, Loss: 17.458, Final Batch Loss: 0.597\n",
      "Epoch 6825, Loss: 17.018, Final Batch Loss: 0.492\n",
      "Epoch 6826, Loss: 17.304, Final Batch Loss: 0.490\n",
      "Epoch 6827, Loss: 17.127, Final Batch Loss: 0.495\n",
      "Epoch 6828, Loss: 17.270, Final Batch Loss: 0.416\n",
      "Epoch 6829, Loss: 17.081, Final Batch Loss: 0.491\n",
      "Epoch 6830, Loss: 17.141, Final Batch Loss: 0.436\n",
      "Epoch 6831, Loss: 17.361, Final Batch Loss: 0.413\n",
      "Epoch 6832, Loss: 17.356, Final Batch Loss: 0.522\n",
      "Epoch 6833, Loss: 17.378, Final Batch Loss: 0.465\n",
      "Epoch 6834, Loss: 17.107, Final Batch Loss: 0.373\n",
      "Epoch 6835, Loss: 17.293, Final Batch Loss: 0.465\n",
      "Epoch 6836, Loss: 16.997, Final Batch Loss: 0.385\n",
      "Epoch 6837, Loss: 17.289, Final Batch Loss: 0.482\n",
      "Epoch 6838, Loss: 17.082, Final Batch Loss: 0.476\n",
      "Epoch 6839, Loss: 17.592, Final Batch Loss: 0.453\n",
      "Epoch 6840, Loss: 17.084, Final Batch Loss: 0.404\n",
      "Epoch 6841, Loss: 17.234, Final Batch Loss: 0.489\n",
      "Epoch 6842, Loss: 17.098, Final Batch Loss: 0.484\n",
      "Epoch 6843, Loss: 17.323, Final Batch Loss: 0.517\n",
      "Epoch 6844, Loss: 17.324, Final Batch Loss: 0.558\n",
      "Epoch 6845, Loss: 17.082, Final Batch Loss: 0.475\n",
      "Epoch 6846, Loss: 17.210, Final Batch Loss: 0.418\n",
      "Epoch 6847, Loss: 17.480, Final Batch Loss: 0.483\n",
      "Epoch 6848, Loss: 17.218, Final Batch Loss: 0.538\n",
      "Epoch 6849, Loss: 17.113, Final Batch Loss: 0.490\n",
      "Epoch 6850, Loss: 17.249, Final Batch Loss: 0.475\n",
      "Epoch 6851, Loss: 17.375, Final Batch Loss: 0.488\n",
      "Epoch 6852, Loss: 17.378, Final Batch Loss: 0.515\n",
      "Epoch 6853, Loss: 17.430, Final Batch Loss: 0.490\n",
      "Epoch 6854, Loss: 16.984, Final Batch Loss: 0.469\n",
      "Epoch 6855, Loss: 17.510, Final Batch Loss: 0.591\n",
      "Epoch 6856, Loss: 17.139, Final Batch Loss: 0.478\n",
      "Epoch 6857, Loss: 17.396, Final Batch Loss: 0.515\n",
      "Epoch 6858, Loss: 17.108, Final Batch Loss: 0.429\n",
      "Epoch 6859, Loss: 17.282, Final Batch Loss: 0.619\n",
      "Epoch 6860, Loss: 17.138, Final Batch Loss: 0.423\n",
      "Epoch 6861, Loss: 17.307, Final Batch Loss: 0.582\n",
      "Epoch 6862, Loss: 16.931, Final Batch Loss: 0.381\n",
      "Epoch 6863, Loss: 17.208, Final Batch Loss: 0.461\n",
      "Epoch 6864, Loss: 16.928, Final Batch Loss: 0.507\n",
      "Epoch 6865, Loss: 17.381, Final Batch Loss: 0.572\n",
      "Epoch 6866, Loss: 17.185, Final Batch Loss: 0.466\n",
      "Epoch 6867, Loss: 17.344, Final Batch Loss: 0.499\n",
      "Epoch 6868, Loss: 17.442, Final Batch Loss: 0.458\n",
      "Epoch 6869, Loss: 17.037, Final Batch Loss: 0.385\n",
      "Epoch 6870, Loss: 17.179, Final Batch Loss: 0.432\n",
      "Epoch 6871, Loss: 17.228, Final Batch Loss: 0.399\n",
      "Epoch 6872, Loss: 17.251, Final Batch Loss: 0.523\n",
      "Epoch 6873, Loss: 17.310, Final Batch Loss: 0.461\n",
      "Epoch 6874, Loss: 17.180, Final Batch Loss: 0.498\n",
      "Epoch 6875, Loss: 17.181, Final Batch Loss: 0.436\n",
      "Epoch 6876, Loss: 17.248, Final Batch Loss: 0.538\n",
      "Epoch 6877, Loss: 17.118, Final Batch Loss: 0.467\n",
      "Epoch 6878, Loss: 17.394, Final Batch Loss: 0.504\n",
      "Epoch 6879, Loss: 17.398, Final Batch Loss: 0.500\n",
      "Epoch 6880, Loss: 16.914, Final Batch Loss: 0.438\n",
      "Epoch 6881, Loss: 16.768, Final Batch Loss: 0.442\n",
      "Epoch 6882, Loss: 17.381, Final Batch Loss: 0.430\n",
      "Epoch 6883, Loss: 17.273, Final Batch Loss: 0.442\n",
      "Epoch 6884, Loss: 17.296, Final Batch Loss: 0.454\n",
      "Epoch 6885, Loss: 17.448, Final Batch Loss: 0.510\n",
      "Epoch 6886, Loss: 17.334, Final Batch Loss: 0.543\n",
      "Epoch 6887, Loss: 17.590, Final Batch Loss: 0.539\n",
      "Epoch 6888, Loss: 17.153, Final Batch Loss: 0.428\n",
      "Epoch 6889, Loss: 17.078, Final Batch Loss: 0.493\n",
      "Epoch 6890, Loss: 17.115, Final Batch Loss: 0.511\n",
      "Epoch 6891, Loss: 17.178, Final Batch Loss: 0.505\n",
      "Epoch 6892, Loss: 17.308, Final Batch Loss: 0.456\n",
      "Epoch 6893, Loss: 17.237, Final Batch Loss: 0.401\n",
      "Epoch 6894, Loss: 17.378, Final Batch Loss: 0.452\n",
      "Epoch 6895, Loss: 17.320, Final Batch Loss: 0.452\n",
      "Epoch 6896, Loss: 17.291, Final Batch Loss: 0.533\n",
      "Epoch 6897, Loss: 17.054, Final Batch Loss: 0.418\n",
      "Epoch 6898, Loss: 17.089, Final Batch Loss: 0.441\n",
      "Epoch 6899, Loss: 17.061, Final Batch Loss: 0.403\n",
      "Epoch 6900, Loss: 17.388, Final Batch Loss: 0.418\n",
      "Epoch 6901, Loss: 17.106, Final Batch Loss: 0.456\n",
      "Epoch 6902, Loss: 17.142, Final Batch Loss: 0.417\n",
      "Epoch 6903, Loss: 17.374, Final Batch Loss: 0.493\n",
      "Epoch 6904, Loss: 17.222, Final Batch Loss: 0.460\n",
      "Epoch 6905, Loss: 17.302, Final Batch Loss: 0.615\n",
      "Epoch 6906, Loss: 17.311, Final Batch Loss: 0.473\n",
      "Epoch 6907, Loss: 17.098, Final Batch Loss: 0.478\n",
      "Epoch 6908, Loss: 17.088, Final Batch Loss: 0.573\n",
      "Epoch 6909, Loss: 17.428, Final Batch Loss: 0.565\n",
      "Epoch 6910, Loss: 17.149, Final Batch Loss: 0.519\n",
      "Epoch 6911, Loss: 17.373, Final Batch Loss: 0.428\n",
      "Epoch 6912, Loss: 17.282, Final Batch Loss: 0.509\n",
      "Epoch 6913, Loss: 17.337, Final Batch Loss: 0.495\n",
      "Epoch 6914, Loss: 17.242, Final Batch Loss: 0.465\n",
      "Epoch 6915, Loss: 17.525, Final Batch Loss: 0.439\n",
      "Epoch 6916, Loss: 16.904, Final Batch Loss: 0.396\n",
      "Epoch 6917, Loss: 17.194, Final Batch Loss: 0.469\n",
      "Epoch 6918, Loss: 17.229, Final Batch Loss: 0.556\n",
      "Epoch 6919, Loss: 17.186, Final Batch Loss: 0.522\n",
      "Epoch 6920, Loss: 17.405, Final Batch Loss: 0.415\n",
      "Epoch 6921, Loss: 17.124, Final Batch Loss: 0.565\n",
      "Epoch 6922, Loss: 17.204, Final Batch Loss: 0.561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6923, Loss: 17.235, Final Batch Loss: 0.492\n",
      "Epoch 6924, Loss: 17.072, Final Batch Loss: 0.399\n",
      "Epoch 6925, Loss: 17.052, Final Batch Loss: 0.363\n",
      "Epoch 6926, Loss: 17.150, Final Batch Loss: 0.440\n",
      "Epoch 6927, Loss: 17.023, Final Batch Loss: 0.436\n",
      "Epoch 6928, Loss: 17.019, Final Batch Loss: 0.454\n",
      "Epoch 6929, Loss: 17.146, Final Batch Loss: 0.423\n",
      "Epoch 6930, Loss: 17.234, Final Batch Loss: 0.421\n",
      "Epoch 6931, Loss: 17.404, Final Batch Loss: 0.474\n",
      "Epoch 6932, Loss: 17.124, Final Batch Loss: 0.353\n",
      "Epoch 6933, Loss: 17.175, Final Batch Loss: 0.386\n",
      "Epoch 6934, Loss: 16.869, Final Batch Loss: 0.469\n",
      "Epoch 6935, Loss: 17.401, Final Batch Loss: 0.523\n",
      "Epoch 6936, Loss: 17.115, Final Batch Loss: 0.463\n",
      "Epoch 6937, Loss: 17.295, Final Batch Loss: 0.480\n",
      "Epoch 6938, Loss: 17.321, Final Batch Loss: 0.546\n",
      "Epoch 6939, Loss: 17.389, Final Batch Loss: 0.489\n",
      "Epoch 6940, Loss: 17.379, Final Batch Loss: 0.502\n",
      "Epoch 6941, Loss: 17.301, Final Batch Loss: 0.460\n",
      "Epoch 6942, Loss: 17.207, Final Batch Loss: 0.506\n",
      "Epoch 6943, Loss: 17.326, Final Batch Loss: 0.522\n",
      "Epoch 6944, Loss: 17.393, Final Batch Loss: 0.498\n",
      "Epoch 6945, Loss: 16.984, Final Batch Loss: 0.522\n",
      "Epoch 6946, Loss: 17.334, Final Batch Loss: 0.499\n",
      "Epoch 6947, Loss: 16.846, Final Batch Loss: 0.418\n",
      "Epoch 6948, Loss: 16.993, Final Batch Loss: 0.440\n",
      "Epoch 6949, Loss: 17.054, Final Batch Loss: 0.540\n",
      "Epoch 6950, Loss: 17.234, Final Batch Loss: 0.390\n",
      "Epoch 6951, Loss: 17.373, Final Batch Loss: 0.475\n",
      "Epoch 6952, Loss: 17.150, Final Batch Loss: 0.437\n",
      "Epoch 6953, Loss: 17.259, Final Batch Loss: 0.478\n",
      "Epoch 6954, Loss: 17.112, Final Batch Loss: 0.379\n",
      "Epoch 6955, Loss: 17.246, Final Batch Loss: 0.488\n",
      "Epoch 6956, Loss: 17.044, Final Batch Loss: 0.477\n",
      "Epoch 6957, Loss: 17.319, Final Batch Loss: 0.584\n",
      "Epoch 6958, Loss: 17.306, Final Batch Loss: 0.473\n",
      "Epoch 6959, Loss: 17.144, Final Batch Loss: 0.482\n",
      "Epoch 6960, Loss: 17.033, Final Batch Loss: 0.467\n",
      "Epoch 6961, Loss: 17.232, Final Batch Loss: 0.516\n",
      "Epoch 6962, Loss: 17.124, Final Batch Loss: 0.426\n",
      "Epoch 6963, Loss: 17.365, Final Batch Loss: 0.567\n",
      "Epoch 6964, Loss: 17.175, Final Batch Loss: 0.622\n",
      "Epoch 6965, Loss: 17.281, Final Batch Loss: 0.403\n",
      "Epoch 6966, Loss: 17.083, Final Batch Loss: 0.438\n",
      "Epoch 6967, Loss: 17.264, Final Batch Loss: 0.512\n",
      "Epoch 6968, Loss: 16.977, Final Batch Loss: 0.396\n",
      "Epoch 6969, Loss: 17.278, Final Batch Loss: 0.510\n",
      "Epoch 6970, Loss: 17.143, Final Batch Loss: 0.522\n",
      "Epoch 6971, Loss: 17.039, Final Batch Loss: 0.419\n",
      "Epoch 6972, Loss: 17.314, Final Batch Loss: 0.581\n",
      "Epoch 6973, Loss: 17.448, Final Batch Loss: 0.490\n",
      "Epoch 6974, Loss: 17.428, Final Batch Loss: 0.501\n",
      "Epoch 6975, Loss: 16.959, Final Batch Loss: 0.404\n",
      "Epoch 6976, Loss: 16.875, Final Batch Loss: 0.475\n",
      "Epoch 6977, Loss: 17.218, Final Batch Loss: 0.519\n",
      "Epoch 6978, Loss: 17.163, Final Batch Loss: 0.564\n",
      "Epoch 6979, Loss: 16.976, Final Batch Loss: 0.409\n",
      "Epoch 6980, Loss: 16.913, Final Batch Loss: 0.561\n",
      "Epoch 6981, Loss: 17.002, Final Batch Loss: 0.402\n",
      "Epoch 6982, Loss: 17.090, Final Batch Loss: 0.370\n",
      "Epoch 6983, Loss: 17.406, Final Batch Loss: 0.457\n",
      "Epoch 6984, Loss: 17.105, Final Batch Loss: 0.443\n",
      "Epoch 6985, Loss: 17.308, Final Batch Loss: 0.413\n",
      "Epoch 6986, Loss: 17.171, Final Batch Loss: 0.560\n",
      "Epoch 6987, Loss: 17.178, Final Batch Loss: 0.458\n",
      "Epoch 6988, Loss: 17.273, Final Batch Loss: 0.462\n",
      "Epoch 6989, Loss: 17.195, Final Batch Loss: 0.516\n",
      "Epoch 6990, Loss: 17.299, Final Batch Loss: 0.497\n",
      "Epoch 6991, Loss: 16.741, Final Batch Loss: 0.432\n",
      "Epoch 6992, Loss: 17.003, Final Batch Loss: 0.456\n",
      "Epoch 6993, Loss: 16.999, Final Batch Loss: 0.372\n",
      "Epoch 6994, Loss: 17.118, Final Batch Loss: 0.558\n",
      "Epoch 6995, Loss: 17.214, Final Batch Loss: 0.574\n",
      "Epoch 6996, Loss: 17.118, Final Batch Loss: 0.547\n",
      "Epoch 6997, Loss: 17.251, Final Batch Loss: 0.476\n",
      "Epoch 6998, Loss: 17.287, Final Batch Loss: 0.571\n",
      "Epoch 6999, Loss: 17.100, Final Batch Loss: 0.497\n",
      "Epoch 7000, Loss: 17.062, Final Batch Loss: 0.400\n",
      "Epoch 7001, Loss: 16.671, Final Batch Loss: 0.401\n",
      "Epoch 7002, Loss: 17.199, Final Batch Loss: 0.643\n",
      "Epoch 7003, Loss: 16.973, Final Batch Loss: 0.474\n",
      "Epoch 7004, Loss: 17.125, Final Batch Loss: 0.493\n",
      "Epoch 7005, Loss: 17.143, Final Batch Loss: 0.469\n",
      "Epoch 7006, Loss: 17.246, Final Batch Loss: 0.350\n",
      "Epoch 7007, Loss: 17.179, Final Batch Loss: 0.467\n",
      "Epoch 7008, Loss: 16.893, Final Batch Loss: 0.438\n",
      "Epoch 7009, Loss: 16.916, Final Batch Loss: 0.472\n",
      "Epoch 7010, Loss: 17.304, Final Batch Loss: 0.587\n",
      "Epoch 7011, Loss: 17.184, Final Batch Loss: 0.484\n",
      "Epoch 7012, Loss: 17.157, Final Batch Loss: 0.533\n",
      "Epoch 7013, Loss: 16.996, Final Batch Loss: 0.454\n",
      "Epoch 7014, Loss: 17.148, Final Batch Loss: 0.416\n",
      "Epoch 7015, Loss: 17.156, Final Batch Loss: 0.397\n",
      "Epoch 7016, Loss: 17.042, Final Batch Loss: 0.473\n",
      "Epoch 7017, Loss: 17.066, Final Batch Loss: 0.414\n",
      "Epoch 7018, Loss: 17.189, Final Batch Loss: 0.495\n",
      "Epoch 7019, Loss: 17.575, Final Batch Loss: 0.521\n",
      "Epoch 7020, Loss: 17.310, Final Batch Loss: 0.492\n",
      "Epoch 7021, Loss: 17.414, Final Batch Loss: 0.490\n",
      "Epoch 7022, Loss: 17.021, Final Batch Loss: 0.414\n",
      "Epoch 7023, Loss: 17.456, Final Batch Loss: 0.505\n",
      "Epoch 7024, Loss: 17.281, Final Batch Loss: 0.429\n",
      "Epoch 7025, Loss: 17.089, Final Batch Loss: 0.592\n",
      "Epoch 7026, Loss: 17.266, Final Batch Loss: 0.479\n",
      "Epoch 7027, Loss: 17.531, Final Batch Loss: 0.418\n",
      "Epoch 7028, Loss: 17.195, Final Batch Loss: 0.457\n",
      "Epoch 7029, Loss: 17.313, Final Batch Loss: 0.483\n",
      "Epoch 7030, Loss: 17.086, Final Batch Loss: 0.480\n",
      "Epoch 7031, Loss: 17.310, Final Batch Loss: 0.431\n",
      "Epoch 7032, Loss: 17.388, Final Batch Loss: 0.562\n",
      "Epoch 7033, Loss: 17.244, Final Batch Loss: 0.482\n",
      "Epoch 7034, Loss: 17.133, Final Batch Loss: 0.495\n",
      "Epoch 7035, Loss: 17.347, Final Batch Loss: 0.422\n",
      "Epoch 7036, Loss: 17.113, Final Batch Loss: 0.429\n",
      "Epoch 7037, Loss: 17.151, Final Batch Loss: 0.424\n",
      "Epoch 7038, Loss: 16.916, Final Batch Loss: 0.555\n",
      "Epoch 7039, Loss: 17.284, Final Batch Loss: 0.470\n",
      "Epoch 7040, Loss: 17.139, Final Batch Loss: 0.585\n",
      "Epoch 7041, Loss: 17.166, Final Batch Loss: 0.478\n",
      "Epoch 7042, Loss: 17.167, Final Batch Loss: 0.532\n",
      "Epoch 7043, Loss: 17.116, Final Batch Loss: 0.524\n",
      "Epoch 7044, Loss: 17.014, Final Batch Loss: 0.409\n",
      "Epoch 7045, Loss: 17.347, Final Batch Loss: 0.533\n",
      "Epoch 7046, Loss: 17.038, Final Batch Loss: 0.587\n",
      "Epoch 7047, Loss: 17.099, Final Batch Loss: 0.462\n",
      "Epoch 7048, Loss: 16.883, Final Batch Loss: 0.467\n",
      "Epoch 7049, Loss: 17.376, Final Batch Loss: 0.442\n",
      "Epoch 7050, Loss: 17.302, Final Batch Loss: 0.532\n",
      "Epoch 7051, Loss: 17.234, Final Batch Loss: 0.492\n",
      "Epoch 7052, Loss: 17.355, Final Batch Loss: 0.508\n",
      "Epoch 7053, Loss: 17.080, Final Batch Loss: 0.422\n",
      "Epoch 7054, Loss: 17.350, Final Batch Loss: 0.483\n",
      "Epoch 7055, Loss: 17.009, Final Batch Loss: 0.595\n",
      "Epoch 7056, Loss: 17.172, Final Batch Loss: 0.421\n",
      "Epoch 7057, Loss: 17.098, Final Batch Loss: 0.448\n",
      "Epoch 7058, Loss: 16.951, Final Batch Loss: 0.539\n",
      "Epoch 7059, Loss: 17.360, Final Batch Loss: 0.423\n",
      "Epoch 7060, Loss: 17.062, Final Batch Loss: 0.491\n",
      "Epoch 7061, Loss: 17.106, Final Batch Loss: 0.674\n",
      "Epoch 7062, Loss: 17.256, Final Batch Loss: 0.394\n",
      "Epoch 7063, Loss: 17.038, Final Batch Loss: 0.418\n",
      "Epoch 7064, Loss: 17.097, Final Batch Loss: 0.514\n",
      "Epoch 7065, Loss: 17.217, Final Batch Loss: 0.613\n",
      "Epoch 7066, Loss: 17.020, Final Batch Loss: 0.476\n",
      "Epoch 7067, Loss: 16.990, Final Batch Loss: 0.498\n",
      "Epoch 7068, Loss: 17.057, Final Batch Loss: 0.443\n",
      "Epoch 7069, Loss: 17.401, Final Batch Loss: 0.528\n",
      "Epoch 7070, Loss: 17.014, Final Batch Loss: 0.437\n",
      "Epoch 7071, Loss: 17.085, Final Batch Loss: 0.462\n",
      "Epoch 7072, Loss: 17.143, Final Batch Loss: 0.583\n",
      "Epoch 7073, Loss: 17.255, Final Batch Loss: 0.558\n",
      "Epoch 7074, Loss: 17.108, Final Batch Loss: 0.370\n",
      "Epoch 7075, Loss: 17.208, Final Batch Loss: 0.429\n",
      "Epoch 7076, Loss: 17.204, Final Batch Loss: 0.563\n",
      "Epoch 7077, Loss: 17.347, Final Batch Loss: 0.428\n",
      "Epoch 7078, Loss: 17.423, Final Batch Loss: 0.426\n",
      "Epoch 7079, Loss: 17.272, Final Batch Loss: 0.560\n",
      "Epoch 7080, Loss: 17.290, Final Batch Loss: 0.503\n",
      "Epoch 7081, Loss: 17.226, Final Batch Loss: 0.460\n",
      "Epoch 7082, Loss: 17.164, Final Batch Loss: 0.471\n",
      "Epoch 7083, Loss: 17.351, Final Batch Loss: 0.562\n",
      "Epoch 7084, Loss: 17.131, Final Batch Loss: 0.521\n",
      "Epoch 7085, Loss: 16.996, Final Batch Loss: 0.458\n",
      "Epoch 7086, Loss: 17.152, Final Batch Loss: 0.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7087, Loss: 17.081, Final Batch Loss: 0.471\n",
      "Epoch 7088, Loss: 17.132, Final Batch Loss: 0.433\n",
      "Epoch 7089, Loss: 16.981, Final Batch Loss: 0.404\n",
      "Epoch 7090, Loss: 17.266, Final Batch Loss: 0.507\n",
      "Epoch 7091, Loss: 17.561, Final Batch Loss: 0.434\n",
      "Epoch 7092, Loss: 17.189, Final Batch Loss: 0.395\n",
      "Epoch 7093, Loss: 17.556, Final Batch Loss: 0.562\n",
      "Epoch 7094, Loss: 17.291, Final Batch Loss: 0.463\n",
      "Epoch 7095, Loss: 17.215, Final Batch Loss: 0.507\n",
      "Epoch 7096, Loss: 17.240, Final Batch Loss: 0.496\n",
      "Epoch 7097, Loss: 17.107, Final Batch Loss: 0.399\n",
      "Epoch 7098, Loss: 17.063, Final Batch Loss: 0.433\n",
      "Epoch 7099, Loss: 17.203, Final Batch Loss: 0.424\n",
      "Epoch 7100, Loss: 17.220, Final Batch Loss: 0.502\n",
      "Epoch 7101, Loss: 17.163, Final Batch Loss: 0.465\n",
      "Epoch 7102, Loss: 17.070, Final Batch Loss: 0.430\n",
      "Epoch 7103, Loss: 17.309, Final Batch Loss: 0.578\n",
      "Epoch 7104, Loss: 17.298, Final Batch Loss: 0.420\n",
      "Epoch 7105, Loss: 17.461, Final Batch Loss: 0.495\n",
      "Epoch 7106, Loss: 17.276, Final Batch Loss: 0.581\n",
      "Epoch 7107, Loss: 17.012, Final Batch Loss: 0.399\n",
      "Epoch 7108, Loss: 17.246, Final Batch Loss: 0.464\n",
      "Epoch 7109, Loss: 17.264, Final Batch Loss: 0.516\n",
      "Epoch 7110, Loss: 17.084, Final Batch Loss: 0.391\n",
      "Epoch 7111, Loss: 17.150, Final Batch Loss: 0.446\n",
      "Epoch 7112, Loss: 17.301, Final Batch Loss: 0.470\n",
      "Epoch 7113, Loss: 17.051, Final Batch Loss: 0.471\n",
      "Epoch 7114, Loss: 16.990, Final Batch Loss: 0.521\n",
      "Epoch 7115, Loss: 17.160, Final Batch Loss: 0.452\n",
      "Epoch 7116, Loss: 17.033, Final Batch Loss: 0.516\n",
      "Epoch 7117, Loss: 17.186, Final Batch Loss: 0.487\n",
      "Epoch 7118, Loss: 17.140, Final Batch Loss: 0.458\n",
      "Epoch 7119, Loss: 17.063, Final Batch Loss: 0.469\n",
      "Epoch 7120, Loss: 17.429, Final Batch Loss: 0.539\n",
      "Epoch 7121, Loss: 17.339, Final Batch Loss: 0.466\n",
      "Epoch 7122, Loss: 17.586, Final Batch Loss: 0.451\n",
      "Epoch 7123, Loss: 17.114, Final Batch Loss: 0.372\n",
      "Epoch 7124, Loss: 16.971, Final Batch Loss: 0.479\n",
      "Epoch 7125, Loss: 17.333, Final Batch Loss: 0.461\n",
      "Epoch 7126, Loss: 16.988, Final Batch Loss: 0.438\n",
      "Epoch 7127, Loss: 16.972, Final Batch Loss: 0.516\n",
      "Epoch 7128, Loss: 17.117, Final Batch Loss: 0.385\n",
      "Epoch 7129, Loss: 17.288, Final Batch Loss: 0.546\n",
      "Epoch 7130, Loss: 17.126, Final Batch Loss: 0.441\n",
      "Epoch 7131, Loss: 17.056, Final Batch Loss: 0.486\n",
      "Epoch 7132, Loss: 17.044, Final Batch Loss: 0.394\n",
      "Epoch 7133, Loss: 17.109, Final Batch Loss: 0.514\n",
      "Epoch 7134, Loss: 17.141, Final Batch Loss: 0.404\n",
      "Epoch 7135, Loss: 17.225, Final Batch Loss: 0.451\n",
      "Epoch 7136, Loss: 16.964, Final Batch Loss: 0.581\n",
      "Epoch 7137, Loss: 16.913, Final Batch Loss: 0.544\n",
      "Epoch 7138, Loss: 17.316, Final Batch Loss: 0.492\n",
      "Epoch 7139, Loss: 17.058, Final Batch Loss: 0.431\n",
      "Epoch 7140, Loss: 17.269, Final Batch Loss: 0.423\n",
      "Epoch 7141, Loss: 17.433, Final Batch Loss: 0.568\n",
      "Epoch 7142, Loss: 17.272, Final Batch Loss: 0.578\n",
      "Epoch 7143, Loss: 17.426, Final Batch Loss: 0.505\n",
      "Epoch 7144, Loss: 17.518, Final Batch Loss: 0.468\n",
      "Epoch 7145, Loss: 17.013, Final Batch Loss: 0.559\n",
      "Epoch 7146, Loss: 17.270, Final Batch Loss: 0.577\n",
      "Epoch 7147, Loss: 17.216, Final Batch Loss: 0.417\n",
      "Epoch 7148, Loss: 17.235, Final Batch Loss: 0.338\n",
      "Epoch 7149, Loss: 17.051, Final Batch Loss: 0.557\n",
      "Epoch 7150, Loss: 17.285, Final Batch Loss: 0.400\n",
      "Epoch 7151, Loss: 17.403, Final Batch Loss: 0.504\n",
      "Epoch 7152, Loss: 17.370, Final Batch Loss: 0.416\n",
      "Epoch 7153, Loss: 16.932, Final Batch Loss: 0.473\n",
      "Epoch 7154, Loss: 16.868, Final Batch Loss: 0.373\n",
      "Epoch 7155, Loss: 17.305, Final Batch Loss: 0.512\n",
      "Epoch 7156, Loss: 17.262, Final Batch Loss: 0.474\n",
      "Epoch 7157, Loss: 17.306, Final Batch Loss: 0.520\n",
      "Epoch 7158, Loss: 17.486, Final Batch Loss: 0.537\n",
      "Epoch 7159, Loss: 16.839, Final Batch Loss: 0.329\n",
      "Epoch 7160, Loss: 17.124, Final Batch Loss: 0.484\n",
      "Epoch 7161, Loss: 17.322, Final Batch Loss: 0.519\n",
      "Epoch 7162, Loss: 17.254, Final Batch Loss: 0.416\n",
      "Epoch 7163, Loss: 17.266, Final Batch Loss: 0.436\n",
      "Epoch 7164, Loss: 17.076, Final Batch Loss: 0.532\n",
      "Epoch 7165, Loss: 17.044, Final Batch Loss: 0.422\n",
      "Epoch 7166, Loss: 17.433, Final Batch Loss: 0.395\n",
      "Epoch 7167, Loss: 17.088, Final Batch Loss: 0.495\n",
      "Epoch 7168, Loss: 16.929, Final Batch Loss: 0.372\n",
      "Epoch 7169, Loss: 17.204, Final Batch Loss: 0.432\n",
      "Epoch 7170, Loss: 17.163, Final Batch Loss: 0.585\n",
      "Epoch 7171, Loss: 16.909, Final Batch Loss: 0.403\n",
      "Epoch 7172, Loss: 17.023, Final Batch Loss: 0.479\n",
      "Epoch 7173, Loss: 17.134, Final Batch Loss: 0.392\n",
      "Epoch 7174, Loss: 17.077, Final Batch Loss: 0.465\n",
      "Epoch 7175, Loss: 17.070, Final Batch Loss: 0.432\n",
      "Epoch 7176, Loss: 17.175, Final Batch Loss: 0.422\n",
      "Epoch 7177, Loss: 17.515, Final Batch Loss: 0.537\n",
      "Epoch 7178, Loss: 17.179, Final Batch Loss: 0.515\n",
      "Epoch 7179, Loss: 17.225, Final Batch Loss: 0.489\n",
      "Epoch 7180, Loss: 17.305, Final Batch Loss: 0.493\n",
      "Epoch 7181, Loss: 17.137, Final Batch Loss: 0.405\n",
      "Epoch 7182, Loss: 17.103, Final Batch Loss: 0.554\n",
      "Epoch 7183, Loss: 17.335, Final Batch Loss: 0.536\n",
      "Epoch 7184, Loss: 17.151, Final Batch Loss: 0.478\n",
      "Epoch 7185, Loss: 17.098, Final Batch Loss: 0.568\n",
      "Epoch 7186, Loss: 17.002, Final Batch Loss: 0.431\n",
      "Epoch 7187, Loss: 17.179, Final Batch Loss: 0.491\n",
      "Epoch 7188, Loss: 17.249, Final Batch Loss: 0.565\n",
      "Epoch 7189, Loss: 17.002, Final Batch Loss: 0.455\n",
      "Epoch 7190, Loss: 17.171, Final Batch Loss: 0.430\n",
      "Epoch 7191, Loss: 16.946, Final Batch Loss: 0.437\n",
      "Epoch 7192, Loss: 16.998, Final Batch Loss: 0.480\n",
      "Epoch 7193, Loss: 17.249, Final Batch Loss: 0.461\n",
      "Epoch 7194, Loss: 17.101, Final Batch Loss: 0.560\n",
      "Epoch 7195, Loss: 17.361, Final Batch Loss: 0.469\n",
      "Epoch 7196, Loss: 17.420, Final Batch Loss: 0.621\n",
      "Epoch 7197, Loss: 17.096, Final Batch Loss: 0.500\n",
      "Epoch 7198, Loss: 17.276, Final Batch Loss: 0.620\n",
      "Epoch 7199, Loss: 17.215, Final Batch Loss: 0.582\n",
      "Epoch 7200, Loss: 17.029, Final Batch Loss: 0.453\n",
      "Epoch 7201, Loss: 17.227, Final Batch Loss: 0.470\n",
      "Epoch 7202, Loss: 17.218, Final Batch Loss: 0.553\n",
      "Epoch 7203, Loss: 17.215, Final Batch Loss: 0.457\n",
      "Epoch 7204, Loss: 17.000, Final Batch Loss: 0.415\n",
      "Epoch 7205, Loss: 17.039, Final Batch Loss: 0.399\n",
      "Epoch 7206, Loss: 17.258, Final Batch Loss: 0.394\n",
      "Epoch 7207, Loss: 17.150, Final Batch Loss: 0.462\n",
      "Epoch 7208, Loss: 17.336, Final Batch Loss: 0.635\n",
      "Epoch 7209, Loss: 17.107, Final Batch Loss: 0.508\n",
      "Epoch 7210, Loss: 16.938, Final Batch Loss: 0.558\n",
      "Epoch 7211, Loss: 17.123, Final Batch Loss: 0.374\n",
      "Epoch 7212, Loss: 17.180, Final Batch Loss: 0.499\n",
      "Epoch 7213, Loss: 17.198, Final Batch Loss: 0.498\n",
      "Epoch 7214, Loss: 17.294, Final Batch Loss: 0.520\n",
      "Epoch 7215, Loss: 16.896, Final Batch Loss: 0.509\n",
      "Epoch 7216, Loss: 16.867, Final Batch Loss: 0.377\n",
      "Epoch 7217, Loss: 17.128, Final Batch Loss: 0.529\n",
      "Epoch 7218, Loss: 17.189, Final Batch Loss: 0.442\n",
      "Epoch 7219, Loss: 17.312, Final Batch Loss: 0.395\n",
      "Epoch 7220, Loss: 17.107, Final Batch Loss: 0.392\n",
      "Epoch 7221, Loss: 17.177, Final Batch Loss: 0.434\n",
      "Epoch 7222, Loss: 17.252, Final Batch Loss: 0.445\n",
      "Epoch 7223, Loss: 17.044, Final Batch Loss: 0.410\n",
      "Epoch 7224, Loss: 16.863, Final Batch Loss: 0.412\n",
      "Epoch 7225, Loss: 17.302, Final Batch Loss: 0.415\n",
      "Epoch 7226, Loss: 17.082, Final Batch Loss: 0.491\n",
      "Epoch 7227, Loss: 17.294, Final Batch Loss: 0.522\n",
      "Epoch 7228, Loss: 17.186, Final Batch Loss: 0.509\n",
      "Epoch 7229, Loss: 16.757, Final Batch Loss: 0.379\n",
      "Epoch 7230, Loss: 17.485, Final Batch Loss: 0.550\n",
      "Epoch 7231, Loss: 17.070, Final Batch Loss: 0.430\n",
      "Epoch 7232, Loss: 17.146, Final Batch Loss: 0.577\n",
      "Epoch 7233, Loss: 17.148, Final Batch Loss: 0.496\n",
      "Epoch 7234, Loss: 17.231, Final Batch Loss: 0.447\n",
      "Epoch 7235, Loss: 17.101, Final Batch Loss: 0.384\n",
      "Epoch 7236, Loss: 17.205, Final Batch Loss: 0.480\n",
      "Epoch 7237, Loss: 17.145, Final Batch Loss: 0.587\n",
      "Epoch 7238, Loss: 17.166, Final Batch Loss: 0.514\n",
      "Epoch 7239, Loss: 17.300, Final Batch Loss: 0.452\n",
      "Epoch 7240, Loss: 17.313, Final Batch Loss: 0.404\n",
      "Epoch 7241, Loss: 17.056, Final Batch Loss: 0.377\n",
      "Epoch 7242, Loss: 17.158, Final Batch Loss: 0.365\n",
      "Epoch 7243, Loss: 17.272, Final Batch Loss: 0.378\n",
      "Epoch 7244, Loss: 16.851, Final Batch Loss: 0.443\n",
      "Epoch 7245, Loss: 17.146, Final Batch Loss: 0.549\n",
      "Epoch 7246, Loss: 17.104, Final Batch Loss: 0.663\n",
      "Epoch 7247, Loss: 17.195, Final Batch Loss: 0.465\n",
      "Epoch 7248, Loss: 17.219, Final Batch Loss: 0.457\n",
      "Epoch 7249, Loss: 17.213, Final Batch Loss: 0.479\n",
      "Epoch 7250, Loss: 17.212, Final Batch Loss: 0.448\n",
      "Epoch 7251, Loss: 17.209, Final Batch Loss: 0.539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7252, Loss: 17.212, Final Batch Loss: 0.451\n",
      "Epoch 7253, Loss: 17.236, Final Batch Loss: 0.381\n",
      "Epoch 7254, Loss: 16.988, Final Batch Loss: 0.528\n",
      "Epoch 7255, Loss: 17.118, Final Batch Loss: 0.434\n",
      "Epoch 7256, Loss: 17.134, Final Batch Loss: 0.484\n",
      "Epoch 7257, Loss: 17.274, Final Batch Loss: 0.437\n",
      "Epoch 7258, Loss: 17.156, Final Batch Loss: 0.482\n",
      "Epoch 7259, Loss: 17.228, Final Batch Loss: 0.427\n",
      "Epoch 7260, Loss: 17.391, Final Batch Loss: 0.401\n",
      "Epoch 7261, Loss: 17.040, Final Batch Loss: 0.482\n",
      "Epoch 7262, Loss: 17.003, Final Batch Loss: 0.575\n",
      "Epoch 7263, Loss: 17.282, Final Batch Loss: 0.560\n",
      "Epoch 7264, Loss: 17.016, Final Batch Loss: 0.505\n",
      "Epoch 7265, Loss: 16.958, Final Batch Loss: 0.477\n",
      "Epoch 7266, Loss: 17.417, Final Batch Loss: 0.593\n",
      "Epoch 7267, Loss: 17.072, Final Batch Loss: 0.412\n",
      "Epoch 7268, Loss: 17.081, Final Batch Loss: 0.403\n",
      "Epoch 7269, Loss: 17.189, Final Batch Loss: 0.473\n",
      "Epoch 7270, Loss: 17.061, Final Batch Loss: 0.451\n",
      "Epoch 7271, Loss: 17.432, Final Batch Loss: 0.485\n",
      "Epoch 7272, Loss: 16.728, Final Batch Loss: 0.423\n",
      "Epoch 7273, Loss: 17.085, Final Batch Loss: 0.404\n",
      "Epoch 7274, Loss: 17.200, Final Batch Loss: 0.474\n",
      "Epoch 7275, Loss: 17.224, Final Batch Loss: 0.457\n",
      "Epoch 7276, Loss: 17.167, Final Batch Loss: 0.393\n",
      "Epoch 7277, Loss: 17.097, Final Batch Loss: 0.481\n",
      "Epoch 7278, Loss: 16.902, Final Batch Loss: 0.459\n",
      "Epoch 7279, Loss: 17.438, Final Batch Loss: 0.579\n",
      "Epoch 7280, Loss: 17.060, Final Batch Loss: 0.506\n",
      "Epoch 7281, Loss: 17.257, Final Batch Loss: 0.495\n",
      "Epoch 7282, Loss: 17.228, Final Batch Loss: 0.504\n",
      "Epoch 7283, Loss: 16.987, Final Batch Loss: 0.466\n",
      "Epoch 7284, Loss: 17.196, Final Batch Loss: 0.464\n",
      "Epoch 7285, Loss: 17.111, Final Batch Loss: 0.414\n",
      "Epoch 7286, Loss: 17.247, Final Batch Loss: 0.485\n",
      "Epoch 7287, Loss: 17.177, Final Batch Loss: 0.474\n",
      "Epoch 7288, Loss: 17.051, Final Batch Loss: 0.433\n",
      "Epoch 7289, Loss: 17.331, Final Batch Loss: 0.516\n",
      "Epoch 7290, Loss: 17.047, Final Batch Loss: 0.474\n",
      "Epoch 7291, Loss: 17.228, Final Batch Loss: 0.396\n",
      "Epoch 7292, Loss: 17.158, Final Batch Loss: 0.434\n",
      "Epoch 7293, Loss: 17.238, Final Batch Loss: 0.484\n",
      "Epoch 7294, Loss: 17.142, Final Batch Loss: 0.502\n",
      "Epoch 7295, Loss: 17.119, Final Batch Loss: 0.554\n",
      "Epoch 7296, Loss: 17.322, Final Batch Loss: 0.432\n",
      "Epoch 7297, Loss: 17.106, Final Batch Loss: 0.548\n",
      "Epoch 7298, Loss: 17.218, Final Batch Loss: 0.435\n",
      "Epoch 7299, Loss: 17.321, Final Batch Loss: 0.494\n",
      "Epoch 7300, Loss: 17.203, Final Batch Loss: 0.432\n",
      "Epoch 7301, Loss: 17.202, Final Batch Loss: 0.509\n",
      "Epoch 7302, Loss: 17.099, Final Batch Loss: 0.460\n",
      "Epoch 7303, Loss: 17.019, Final Batch Loss: 0.576\n",
      "Epoch 7304, Loss: 17.360, Final Batch Loss: 0.400\n",
      "Epoch 7305, Loss: 17.194, Final Batch Loss: 0.500\n",
      "Epoch 7306, Loss: 16.991, Final Batch Loss: 0.399\n",
      "Epoch 7307, Loss: 16.967, Final Batch Loss: 0.506\n",
      "Epoch 7308, Loss: 17.287, Final Batch Loss: 0.437\n",
      "Epoch 7309, Loss: 17.069, Final Batch Loss: 0.436\n",
      "Epoch 7310, Loss: 17.211, Final Batch Loss: 0.432\n",
      "Epoch 7311, Loss: 17.184, Final Batch Loss: 0.478\n",
      "Epoch 7312, Loss: 16.972, Final Batch Loss: 0.522\n",
      "Epoch 7313, Loss: 17.286, Final Batch Loss: 0.675\n",
      "Epoch 7314, Loss: 16.928, Final Batch Loss: 0.537\n",
      "Epoch 7315, Loss: 17.059, Final Batch Loss: 0.397\n",
      "Epoch 7316, Loss: 17.235, Final Batch Loss: 0.443\n",
      "Epoch 7317, Loss: 17.217, Final Batch Loss: 0.529\n",
      "Epoch 7318, Loss: 17.186, Final Batch Loss: 0.517\n",
      "Epoch 7319, Loss: 17.115, Final Batch Loss: 0.474\n",
      "Epoch 7320, Loss: 17.170, Final Batch Loss: 0.448\n",
      "Epoch 7321, Loss: 16.909, Final Batch Loss: 0.562\n",
      "Epoch 7322, Loss: 17.134, Final Batch Loss: 0.546\n",
      "Epoch 7323, Loss: 17.203, Final Batch Loss: 0.466\n",
      "Epoch 7324, Loss: 16.982, Final Batch Loss: 0.401\n",
      "Epoch 7325, Loss: 16.893, Final Batch Loss: 0.498\n",
      "Epoch 7326, Loss: 17.237, Final Batch Loss: 0.511\n",
      "Epoch 7327, Loss: 17.120, Final Batch Loss: 0.615\n",
      "Epoch 7328, Loss: 16.986, Final Batch Loss: 0.475\n",
      "Epoch 7329, Loss: 17.016, Final Batch Loss: 0.424\n",
      "Epoch 7330, Loss: 17.213, Final Batch Loss: 0.463\n",
      "Epoch 7331, Loss: 17.081, Final Batch Loss: 0.358\n",
      "Epoch 7332, Loss: 17.373, Final Batch Loss: 0.495\n",
      "Epoch 7333, Loss: 17.220, Final Batch Loss: 0.439\n",
      "Epoch 7334, Loss: 17.157, Final Batch Loss: 0.462\n",
      "Epoch 7335, Loss: 16.831, Final Batch Loss: 0.426\n",
      "Epoch 7336, Loss: 17.095, Final Batch Loss: 0.428\n",
      "Epoch 7337, Loss: 17.187, Final Batch Loss: 0.518\n",
      "Epoch 7338, Loss: 17.170, Final Batch Loss: 0.363\n",
      "Epoch 7339, Loss: 17.303, Final Batch Loss: 0.451\n",
      "Epoch 7340, Loss: 16.828, Final Batch Loss: 0.427\n",
      "Epoch 7341, Loss: 17.049, Final Batch Loss: 0.565\n",
      "Epoch 7342, Loss: 17.342, Final Batch Loss: 0.415\n",
      "Epoch 7343, Loss: 17.153, Final Batch Loss: 0.432\n",
      "Epoch 7344, Loss: 16.912, Final Batch Loss: 0.602\n",
      "Epoch 7345, Loss: 17.094, Final Batch Loss: 0.458\n",
      "Epoch 7346, Loss: 16.902, Final Batch Loss: 0.429\n",
      "Epoch 7347, Loss: 17.381, Final Batch Loss: 0.520\n",
      "Epoch 7348, Loss: 17.360, Final Batch Loss: 0.450\n",
      "Epoch 7349, Loss: 17.411, Final Batch Loss: 0.476\n",
      "Epoch 7350, Loss: 16.993, Final Batch Loss: 0.445\n",
      "Epoch 7351, Loss: 17.199, Final Batch Loss: 0.441\n",
      "Epoch 7352, Loss: 17.180, Final Batch Loss: 0.549\n",
      "Epoch 7353, Loss: 17.012, Final Batch Loss: 0.479\n",
      "Epoch 7354, Loss: 17.276, Final Batch Loss: 0.540\n",
      "Epoch 7355, Loss: 17.104, Final Batch Loss: 0.414\n",
      "Epoch 7356, Loss: 17.037, Final Batch Loss: 0.422\n",
      "Epoch 7357, Loss: 17.163, Final Batch Loss: 0.499\n",
      "Epoch 7358, Loss: 17.191, Final Batch Loss: 0.493\n",
      "Epoch 7359, Loss: 17.242, Final Batch Loss: 0.440\n",
      "Epoch 7360, Loss: 17.319, Final Batch Loss: 0.510\n",
      "Epoch 7361, Loss: 16.800, Final Batch Loss: 0.394\n",
      "Epoch 7362, Loss: 17.014, Final Batch Loss: 0.560\n",
      "Epoch 7363, Loss: 17.047, Final Batch Loss: 0.463\n",
      "Epoch 7364, Loss: 16.981, Final Batch Loss: 0.483\n",
      "Epoch 7365, Loss: 17.127, Final Batch Loss: 0.505\n",
      "Epoch 7366, Loss: 16.701, Final Batch Loss: 0.550\n",
      "Epoch 7367, Loss: 16.937, Final Batch Loss: 0.374\n",
      "Epoch 7368, Loss: 17.426, Final Batch Loss: 0.579\n",
      "Epoch 7369, Loss: 17.282, Final Batch Loss: 0.543\n",
      "Epoch 7370, Loss: 16.910, Final Batch Loss: 0.344\n",
      "Epoch 7371, Loss: 16.895, Final Batch Loss: 0.500\n",
      "Epoch 7372, Loss: 17.494, Final Batch Loss: 0.588\n",
      "Epoch 7373, Loss: 17.253, Final Batch Loss: 0.347\n",
      "Epoch 7374, Loss: 17.061, Final Batch Loss: 0.435\n",
      "Epoch 7375, Loss: 17.065, Final Batch Loss: 0.553\n",
      "Epoch 7376, Loss: 16.832, Final Batch Loss: 0.610\n",
      "Epoch 7377, Loss: 17.123, Final Batch Loss: 0.484\n",
      "Epoch 7378, Loss: 17.221, Final Batch Loss: 0.450\n",
      "Epoch 7379, Loss: 17.274, Final Batch Loss: 0.475\n",
      "Epoch 7380, Loss: 17.229, Final Batch Loss: 0.577\n",
      "Epoch 7381, Loss: 17.234, Final Batch Loss: 0.468\n",
      "Epoch 7382, Loss: 17.336, Final Batch Loss: 0.443\n",
      "Epoch 7383, Loss: 17.018, Final Batch Loss: 0.433\n",
      "Epoch 7384, Loss: 16.981, Final Batch Loss: 0.460\n",
      "Epoch 7385, Loss: 17.069, Final Batch Loss: 0.464\n",
      "Epoch 7386, Loss: 17.118, Final Batch Loss: 0.440\n",
      "Epoch 7387, Loss: 17.114, Final Batch Loss: 0.576\n",
      "Epoch 7388, Loss: 17.245, Final Batch Loss: 0.540\n",
      "Epoch 7389, Loss: 16.903, Final Batch Loss: 0.406\n",
      "Epoch 7390, Loss: 17.017, Final Batch Loss: 0.448\n",
      "Epoch 7391, Loss: 16.698, Final Batch Loss: 0.496\n",
      "Epoch 7392, Loss: 16.975, Final Batch Loss: 0.472\n",
      "Epoch 7393, Loss: 17.287, Final Batch Loss: 0.470\n",
      "Epoch 7394, Loss: 17.150, Final Batch Loss: 0.538\n",
      "Epoch 7395, Loss: 16.884, Final Batch Loss: 0.473\n",
      "Epoch 7396, Loss: 17.163, Final Batch Loss: 0.602\n",
      "Epoch 7397, Loss: 16.956, Final Batch Loss: 0.455\n",
      "Epoch 7398, Loss: 17.015, Final Batch Loss: 0.398\n",
      "Epoch 7399, Loss: 16.880, Final Batch Loss: 0.454\n",
      "Epoch 7400, Loss: 17.303, Final Batch Loss: 0.425\n",
      "Epoch 7401, Loss: 17.237, Final Batch Loss: 0.456\n",
      "Epoch 7402, Loss: 17.038, Final Batch Loss: 0.526\n",
      "Epoch 7403, Loss: 17.113, Final Batch Loss: 0.430\n",
      "Epoch 7404, Loss: 16.995, Final Batch Loss: 0.386\n",
      "Epoch 7405, Loss: 17.179, Final Batch Loss: 0.585\n",
      "Epoch 7406, Loss: 17.177, Final Batch Loss: 0.491\n",
      "Epoch 7407, Loss: 16.897, Final Batch Loss: 0.479\n",
      "Epoch 7408, Loss: 17.146, Final Batch Loss: 0.532\n",
      "Epoch 7409, Loss: 17.062, Final Batch Loss: 0.413\n",
      "Epoch 7410, Loss: 17.108, Final Batch Loss: 0.503\n",
      "Epoch 7411, Loss: 17.443, Final Batch Loss: 0.551\n",
      "Epoch 7412, Loss: 17.116, Final Batch Loss: 0.576\n",
      "Epoch 7413, Loss: 17.100, Final Batch Loss: 0.435\n",
      "Epoch 7414, Loss: 17.181, Final Batch Loss: 0.392\n",
      "Epoch 7415, Loss: 17.150, Final Batch Loss: 0.437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7416, Loss: 17.046, Final Batch Loss: 0.583\n",
      "Epoch 7417, Loss: 17.003, Final Batch Loss: 0.468\n",
      "Epoch 7418, Loss: 17.463, Final Batch Loss: 0.568\n",
      "Epoch 7419, Loss: 17.043, Final Batch Loss: 0.510\n",
      "Epoch 7420, Loss: 16.913, Final Batch Loss: 0.458\n",
      "Epoch 7421, Loss: 17.251, Final Batch Loss: 0.469\n",
      "Epoch 7422, Loss: 17.359, Final Batch Loss: 0.588\n",
      "Epoch 7423, Loss: 17.233, Final Batch Loss: 0.534\n",
      "Epoch 7424, Loss: 16.788, Final Batch Loss: 0.437\n",
      "Epoch 7425, Loss: 16.850, Final Batch Loss: 0.478\n",
      "Epoch 7426, Loss: 17.173, Final Batch Loss: 0.497\n",
      "Epoch 7427, Loss: 17.027, Final Batch Loss: 0.506\n",
      "Epoch 7428, Loss: 17.134, Final Batch Loss: 0.432\n",
      "Epoch 7429, Loss: 17.081, Final Batch Loss: 0.542\n",
      "Epoch 7430, Loss: 17.076, Final Batch Loss: 0.383\n",
      "Epoch 7431, Loss: 17.003, Final Batch Loss: 0.479\n",
      "Epoch 7432, Loss: 17.252, Final Batch Loss: 0.444\n",
      "Epoch 7433, Loss: 16.878, Final Batch Loss: 0.480\n",
      "Epoch 7434, Loss: 17.075, Final Batch Loss: 0.421\n",
      "Epoch 7435, Loss: 17.009, Final Batch Loss: 0.386\n",
      "Epoch 7436, Loss: 17.250, Final Batch Loss: 0.443\n",
      "Epoch 7437, Loss: 16.890, Final Batch Loss: 0.463\n",
      "Epoch 7438, Loss: 17.066, Final Batch Loss: 0.449\n",
      "Epoch 7439, Loss: 17.209, Final Batch Loss: 0.449\n",
      "Epoch 7440, Loss: 17.034, Final Batch Loss: 0.402\n",
      "Epoch 7441, Loss: 16.912, Final Batch Loss: 0.414\n",
      "Epoch 7442, Loss: 17.081, Final Batch Loss: 0.619\n",
      "Epoch 7443, Loss: 17.436, Final Batch Loss: 0.519\n",
      "Epoch 7444, Loss: 16.718, Final Batch Loss: 0.481\n",
      "Epoch 7445, Loss: 16.990, Final Batch Loss: 0.409\n",
      "Epoch 7446, Loss: 17.027, Final Batch Loss: 0.600\n",
      "Epoch 7447, Loss: 17.255, Final Batch Loss: 0.500\n",
      "Epoch 7448, Loss: 17.232, Final Batch Loss: 0.467\n",
      "Epoch 7449, Loss: 17.140, Final Batch Loss: 0.401\n",
      "Epoch 7450, Loss: 17.154, Final Batch Loss: 0.426\n",
      "Epoch 7451, Loss: 17.063, Final Batch Loss: 0.437\n",
      "Epoch 7452, Loss: 16.969, Final Batch Loss: 0.485\n",
      "Epoch 7453, Loss: 17.223, Final Batch Loss: 0.661\n",
      "Epoch 7454, Loss: 17.103, Final Batch Loss: 0.456\n",
      "Epoch 7455, Loss: 16.932, Final Batch Loss: 0.383\n",
      "Epoch 7456, Loss: 17.119, Final Batch Loss: 0.501\n",
      "Epoch 7457, Loss: 16.859, Final Batch Loss: 0.406\n",
      "Epoch 7458, Loss: 17.130, Final Batch Loss: 0.527\n",
      "Epoch 7459, Loss: 16.964, Final Batch Loss: 0.525\n",
      "Epoch 7460, Loss: 16.848, Final Batch Loss: 0.610\n",
      "Epoch 7461, Loss: 17.099, Final Batch Loss: 0.523\n",
      "Epoch 7462, Loss: 17.132, Final Batch Loss: 0.430\n",
      "Epoch 7463, Loss: 16.848, Final Batch Loss: 0.578\n",
      "Epoch 7464, Loss: 17.120, Final Batch Loss: 0.461\n",
      "Epoch 7465, Loss: 17.192, Final Batch Loss: 0.408\n",
      "Epoch 7466, Loss: 17.036, Final Batch Loss: 0.466\n",
      "Epoch 7467, Loss: 17.215, Final Batch Loss: 0.510\n",
      "Epoch 7468, Loss: 17.230, Final Batch Loss: 0.397\n",
      "Epoch 7469, Loss: 17.023, Final Batch Loss: 0.461\n",
      "Epoch 7470, Loss: 17.302, Final Batch Loss: 0.513\n",
      "Epoch 7471, Loss: 17.297, Final Batch Loss: 0.435\n",
      "Epoch 7472, Loss: 17.250, Final Batch Loss: 0.546\n",
      "Epoch 7473, Loss: 17.356, Final Batch Loss: 0.461\n",
      "Epoch 7474, Loss: 17.271, Final Batch Loss: 0.468\n",
      "Epoch 7475, Loss: 17.224, Final Batch Loss: 0.446\n",
      "Epoch 7476, Loss: 17.070, Final Batch Loss: 0.538\n",
      "Epoch 7477, Loss: 17.057, Final Batch Loss: 0.520\n",
      "Epoch 7478, Loss: 16.920, Final Batch Loss: 0.461\n",
      "Epoch 7479, Loss: 17.154, Final Batch Loss: 0.418\n",
      "Epoch 7480, Loss: 17.052, Final Batch Loss: 0.452\n",
      "Epoch 7481, Loss: 17.146, Final Batch Loss: 0.538\n",
      "Epoch 7482, Loss: 16.834, Final Batch Loss: 0.508\n",
      "Epoch 7483, Loss: 17.190, Final Batch Loss: 0.425\n",
      "Epoch 7484, Loss: 17.312, Final Batch Loss: 0.482\n",
      "Epoch 7485, Loss: 16.893, Final Batch Loss: 0.399\n",
      "Epoch 7486, Loss: 17.238, Final Batch Loss: 0.489\n",
      "Epoch 7487, Loss: 17.183, Final Batch Loss: 0.480\n",
      "Epoch 7488, Loss: 16.994, Final Batch Loss: 0.425\n",
      "Epoch 7489, Loss: 17.028, Final Batch Loss: 0.383\n",
      "Epoch 7490, Loss: 16.777, Final Batch Loss: 0.401\n",
      "Epoch 7491, Loss: 17.102, Final Batch Loss: 0.441\n",
      "Epoch 7492, Loss: 17.155, Final Batch Loss: 0.462\n",
      "Epoch 7493, Loss: 17.284, Final Batch Loss: 0.507\n",
      "Epoch 7494, Loss: 17.063, Final Batch Loss: 0.444\n",
      "Epoch 7495, Loss: 17.058, Final Batch Loss: 0.435\n",
      "Epoch 7496, Loss: 17.179, Final Batch Loss: 0.451\n",
      "Epoch 7497, Loss: 17.153, Final Batch Loss: 0.399\n",
      "Epoch 7498, Loss: 16.929, Final Batch Loss: 0.483\n",
      "Epoch 7499, Loss: 16.963, Final Batch Loss: 0.448\n",
      "Epoch 7500, Loss: 17.121, Final Batch Loss: 0.432\n",
      "Epoch 7501, Loss: 17.325, Final Batch Loss: 0.478\n",
      "Epoch 7502, Loss: 17.226, Final Batch Loss: 0.493\n",
      "Epoch 7503, Loss: 17.017, Final Batch Loss: 0.468\n",
      "Epoch 7504, Loss: 17.114, Final Batch Loss: 0.526\n",
      "Epoch 7505, Loss: 17.145, Final Batch Loss: 0.394\n",
      "Epoch 7506, Loss: 16.937, Final Batch Loss: 0.540\n",
      "Epoch 7507, Loss: 17.217, Final Batch Loss: 0.409\n",
      "Epoch 7508, Loss: 17.086, Final Batch Loss: 0.390\n",
      "Epoch 7509, Loss: 17.250, Final Batch Loss: 0.442\n",
      "Epoch 7510, Loss: 16.998, Final Batch Loss: 0.518\n",
      "Epoch 7511, Loss: 17.091, Final Batch Loss: 0.457\n",
      "Epoch 7512, Loss: 17.275, Final Batch Loss: 0.451\n",
      "Epoch 7513, Loss: 17.037, Final Batch Loss: 0.417\n",
      "Epoch 7514, Loss: 17.144, Final Batch Loss: 0.430\n",
      "Epoch 7515, Loss: 17.103, Final Batch Loss: 0.567\n",
      "Epoch 7516, Loss: 16.934, Final Batch Loss: 0.476\n",
      "Epoch 7517, Loss: 16.946, Final Batch Loss: 0.497\n",
      "Epoch 7518, Loss: 17.174, Final Batch Loss: 0.449\n",
      "Epoch 7519, Loss: 17.131, Final Batch Loss: 0.526\n",
      "Epoch 7520, Loss: 17.027, Final Batch Loss: 0.488\n",
      "Epoch 7521, Loss: 17.183, Final Batch Loss: 0.466\n",
      "Epoch 7522, Loss: 17.231, Final Batch Loss: 0.483\n",
      "Epoch 7523, Loss: 17.263, Final Batch Loss: 0.464\n",
      "Epoch 7524, Loss: 16.999, Final Batch Loss: 0.513\n",
      "Epoch 7525, Loss: 17.086, Final Batch Loss: 0.477\n",
      "Epoch 7526, Loss: 17.064, Final Batch Loss: 0.388\n",
      "Epoch 7527, Loss: 17.008, Final Batch Loss: 0.525\n",
      "Epoch 7528, Loss: 16.945, Final Batch Loss: 0.464\n",
      "Epoch 7529, Loss: 16.737, Final Batch Loss: 0.372\n",
      "Epoch 7530, Loss: 17.400, Final Batch Loss: 0.439\n",
      "Epoch 7531, Loss: 17.311, Final Batch Loss: 0.521\n",
      "Epoch 7532, Loss: 17.084, Final Batch Loss: 0.500\n",
      "Epoch 7533, Loss: 17.046, Final Batch Loss: 0.476\n",
      "Epoch 7534, Loss: 17.054, Final Batch Loss: 0.525\n",
      "Epoch 7535, Loss: 17.248, Final Batch Loss: 0.469\n",
      "Epoch 7536, Loss: 17.198, Final Batch Loss: 0.497\n",
      "Epoch 7537, Loss: 16.969, Final Batch Loss: 0.429\n",
      "Epoch 7538, Loss: 17.172, Final Batch Loss: 0.446\n",
      "Epoch 7539, Loss: 16.957, Final Batch Loss: 0.506\n",
      "Epoch 7540, Loss: 16.712, Final Batch Loss: 0.468\n",
      "Epoch 7541, Loss: 17.032, Final Batch Loss: 0.465\n",
      "Epoch 7542, Loss: 16.952, Final Batch Loss: 0.579\n",
      "Epoch 7543, Loss: 17.160, Final Batch Loss: 0.398\n",
      "Epoch 7544, Loss: 17.188, Final Batch Loss: 0.508\n",
      "Epoch 7545, Loss: 17.152, Final Batch Loss: 0.408\n",
      "Epoch 7546, Loss: 17.025, Final Batch Loss: 0.529\n",
      "Epoch 7547, Loss: 17.002, Final Batch Loss: 0.411\n",
      "Epoch 7548, Loss: 17.070, Final Batch Loss: 0.491\n",
      "Epoch 7549, Loss: 16.866, Final Batch Loss: 0.422\n",
      "Epoch 7550, Loss: 16.907, Final Batch Loss: 0.465\n",
      "Epoch 7551, Loss: 17.341, Final Batch Loss: 0.524\n",
      "Epoch 7552, Loss: 17.135, Final Batch Loss: 0.516\n",
      "Epoch 7553, Loss: 17.127, Final Batch Loss: 0.488\n",
      "Epoch 7554, Loss: 16.884, Final Batch Loss: 0.520\n",
      "Epoch 7555, Loss: 17.235, Final Batch Loss: 0.499\n",
      "Epoch 7556, Loss: 17.178, Final Batch Loss: 0.506\n",
      "Epoch 7557, Loss: 17.120, Final Batch Loss: 0.492\n",
      "Epoch 7558, Loss: 17.100, Final Batch Loss: 0.513\n",
      "Epoch 7559, Loss: 17.117, Final Batch Loss: 0.426\n",
      "Epoch 7560, Loss: 17.165, Final Batch Loss: 0.477\n",
      "Epoch 7561, Loss: 17.187, Final Batch Loss: 0.528\n",
      "Epoch 7562, Loss: 16.896, Final Batch Loss: 0.559\n",
      "Epoch 7563, Loss: 17.205, Final Batch Loss: 0.462\n",
      "Epoch 7564, Loss: 17.111, Final Batch Loss: 0.451\n",
      "Epoch 7565, Loss: 17.095, Final Batch Loss: 0.484\n",
      "Epoch 7566, Loss: 16.904, Final Batch Loss: 0.458\n",
      "Epoch 7567, Loss: 17.019, Final Batch Loss: 0.392\n",
      "Epoch 7568, Loss: 16.920, Final Batch Loss: 0.598\n",
      "Epoch 7569, Loss: 16.876, Final Batch Loss: 0.444\n",
      "Epoch 7570, Loss: 17.176, Final Batch Loss: 0.443\n",
      "Epoch 7571, Loss: 17.128, Final Batch Loss: 0.529\n",
      "Epoch 7572, Loss: 16.950, Final Batch Loss: 0.502\n",
      "Epoch 7573, Loss: 17.121, Final Batch Loss: 0.532\n",
      "Epoch 7574, Loss: 16.827, Final Batch Loss: 0.389\n",
      "Epoch 7575, Loss: 17.065, Final Batch Loss: 0.490\n",
      "Epoch 7576, Loss: 17.023, Final Batch Loss: 0.505\n",
      "Epoch 7577, Loss: 16.880, Final Batch Loss: 0.493\n",
      "Epoch 7578, Loss: 17.017, Final Batch Loss: 0.579\n",
      "Epoch 7579, Loss: 17.295, Final Batch Loss: 0.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7580, Loss: 17.125, Final Batch Loss: 0.571\n",
      "Epoch 7581, Loss: 17.096, Final Batch Loss: 0.523\n",
      "Epoch 7582, Loss: 17.112, Final Batch Loss: 0.431\n",
      "Epoch 7583, Loss: 17.309, Final Batch Loss: 0.465\n",
      "Epoch 7584, Loss: 17.271, Final Batch Loss: 0.456\n",
      "Epoch 7585, Loss: 17.176, Final Batch Loss: 0.483\n",
      "Epoch 7586, Loss: 16.924, Final Batch Loss: 0.410\n",
      "Epoch 7587, Loss: 17.115, Final Batch Loss: 0.564\n",
      "Epoch 7588, Loss: 16.796, Final Batch Loss: 0.439\n",
      "Epoch 7589, Loss: 16.916, Final Batch Loss: 0.499\n",
      "Epoch 7590, Loss: 17.155, Final Batch Loss: 0.400\n",
      "Epoch 7591, Loss: 16.804, Final Batch Loss: 0.425\n",
      "Epoch 7592, Loss: 16.972, Final Batch Loss: 0.436\n",
      "Epoch 7593, Loss: 16.746, Final Batch Loss: 0.458\n",
      "Epoch 7594, Loss: 16.954, Final Batch Loss: 0.466\n",
      "Epoch 7595, Loss: 17.037, Final Batch Loss: 0.400\n",
      "Epoch 7596, Loss: 17.221, Final Batch Loss: 0.585\n",
      "Epoch 7597, Loss: 17.205, Final Batch Loss: 0.481\n",
      "Epoch 7598, Loss: 17.163, Final Batch Loss: 0.464\n",
      "Epoch 7599, Loss: 17.188, Final Batch Loss: 0.491\n",
      "Epoch 7600, Loss: 17.046, Final Batch Loss: 0.445\n",
      "Epoch 7601, Loss: 16.793, Final Batch Loss: 0.504\n",
      "Epoch 7602, Loss: 17.176, Final Batch Loss: 0.465\n",
      "Epoch 7603, Loss: 17.028, Final Batch Loss: 0.450\n",
      "Epoch 7604, Loss: 16.972, Final Batch Loss: 0.507\n",
      "Epoch 7605, Loss: 17.050, Final Batch Loss: 0.509\n",
      "Epoch 7606, Loss: 17.349, Final Batch Loss: 0.519\n",
      "Epoch 7607, Loss: 17.234, Final Batch Loss: 0.524\n",
      "Epoch 7608, Loss: 17.052, Final Batch Loss: 0.384\n",
      "Epoch 7609, Loss: 17.112, Final Batch Loss: 0.532\n",
      "Epoch 7610, Loss: 16.959, Final Batch Loss: 0.484\n",
      "Epoch 7611, Loss: 17.121, Final Batch Loss: 0.421\n",
      "Epoch 7612, Loss: 17.487, Final Batch Loss: 0.557\n",
      "Epoch 7613, Loss: 17.018, Final Batch Loss: 0.567\n",
      "Epoch 7614, Loss: 17.157, Final Batch Loss: 0.536\n",
      "Epoch 7615, Loss: 16.978, Final Batch Loss: 0.408\n",
      "Epoch 7616, Loss: 17.160, Final Batch Loss: 0.530\n",
      "Epoch 7617, Loss: 16.883, Final Batch Loss: 0.422\n",
      "Epoch 7618, Loss: 16.956, Final Batch Loss: 0.405\n",
      "Epoch 7619, Loss: 17.137, Final Batch Loss: 0.545\n",
      "Epoch 7620, Loss: 16.896, Final Batch Loss: 0.405\n",
      "Epoch 7621, Loss: 16.977, Final Batch Loss: 0.417\n",
      "Epoch 7622, Loss: 17.050, Final Batch Loss: 0.358\n",
      "Epoch 7623, Loss: 17.034, Final Batch Loss: 0.409\n",
      "Epoch 7624, Loss: 17.077, Final Batch Loss: 0.547\n",
      "Epoch 7625, Loss: 16.929, Final Batch Loss: 0.493\n",
      "Epoch 7626, Loss: 17.085, Final Batch Loss: 0.544\n",
      "Epoch 7627, Loss: 17.019, Final Batch Loss: 0.392\n",
      "Epoch 7628, Loss: 16.888, Final Batch Loss: 0.479\n",
      "Epoch 7629, Loss: 17.116, Final Batch Loss: 0.536\n",
      "Epoch 7630, Loss: 17.160, Final Batch Loss: 0.498\n",
      "Epoch 7631, Loss: 16.827, Final Batch Loss: 0.516\n",
      "Epoch 7632, Loss: 17.089, Final Batch Loss: 0.521\n",
      "Epoch 7633, Loss: 16.964, Final Batch Loss: 0.366\n",
      "Epoch 7634, Loss: 17.267, Final Batch Loss: 0.445\n",
      "Epoch 7635, Loss: 16.679, Final Batch Loss: 0.466\n",
      "Epoch 7636, Loss: 17.061, Final Batch Loss: 0.369\n",
      "Epoch 7637, Loss: 17.213, Final Batch Loss: 0.407\n",
      "Epoch 7638, Loss: 16.830, Final Batch Loss: 0.407\n",
      "Epoch 7639, Loss: 16.879, Final Batch Loss: 0.473\n",
      "Epoch 7640, Loss: 16.858, Final Batch Loss: 0.442\n",
      "Epoch 7641, Loss: 16.892, Final Batch Loss: 0.458\n",
      "Epoch 7642, Loss: 17.069, Final Batch Loss: 0.506\n",
      "Epoch 7643, Loss: 17.045, Final Batch Loss: 0.426\n",
      "Epoch 7644, Loss: 16.710, Final Batch Loss: 0.404\n",
      "Epoch 7645, Loss: 17.236, Final Batch Loss: 0.514\n",
      "Epoch 7646, Loss: 17.096, Final Batch Loss: 0.416\n",
      "Epoch 7647, Loss: 16.919, Final Batch Loss: 0.373\n",
      "Epoch 7648, Loss: 17.336, Final Batch Loss: 0.387\n",
      "Epoch 7649, Loss: 16.926, Final Batch Loss: 0.463\n",
      "Epoch 7650, Loss: 17.105, Final Batch Loss: 0.445\n",
      "Epoch 7651, Loss: 17.235, Final Batch Loss: 0.583\n",
      "Epoch 7652, Loss: 16.905, Final Batch Loss: 0.475\n",
      "Epoch 7653, Loss: 16.826, Final Batch Loss: 0.572\n",
      "Epoch 7654, Loss: 17.066, Final Batch Loss: 0.423\n",
      "Epoch 7655, Loss: 17.144, Final Batch Loss: 0.511\n",
      "Epoch 7656, Loss: 17.162, Final Batch Loss: 0.529\n",
      "Epoch 7657, Loss: 16.945, Final Batch Loss: 0.440\n",
      "Epoch 7658, Loss: 16.947, Final Batch Loss: 0.432\n",
      "Epoch 7659, Loss: 17.313, Final Batch Loss: 0.614\n",
      "Epoch 7660, Loss: 17.056, Final Batch Loss: 0.455\n",
      "Epoch 7661, Loss: 17.287, Final Batch Loss: 0.501\n",
      "Epoch 7662, Loss: 16.958, Final Batch Loss: 0.402\n",
      "Epoch 7663, Loss: 16.790, Final Batch Loss: 0.400\n",
      "Epoch 7664, Loss: 17.060, Final Batch Loss: 0.518\n",
      "Epoch 7665, Loss: 16.960, Final Batch Loss: 0.417\n",
      "Epoch 7666, Loss: 16.930, Final Batch Loss: 0.474\n",
      "Epoch 7667, Loss: 17.160, Final Batch Loss: 0.621\n",
      "Epoch 7668, Loss: 16.959, Final Batch Loss: 0.441\n",
      "Epoch 7669, Loss: 16.807, Final Batch Loss: 0.512\n",
      "Epoch 7670, Loss: 17.069, Final Batch Loss: 0.459\n",
      "Epoch 7671, Loss: 16.708, Final Batch Loss: 0.442\n",
      "Epoch 7672, Loss: 17.307, Final Batch Loss: 0.597\n",
      "Epoch 7673, Loss: 16.936, Final Batch Loss: 0.465\n",
      "Epoch 7674, Loss: 16.772, Final Batch Loss: 0.440\n",
      "Epoch 7675, Loss: 16.946, Final Batch Loss: 0.363\n",
      "Epoch 7676, Loss: 16.932, Final Batch Loss: 0.443\n",
      "Epoch 7677, Loss: 17.350, Final Batch Loss: 0.430\n",
      "Epoch 7678, Loss: 17.128, Final Batch Loss: 0.440\n",
      "Epoch 7679, Loss: 16.888, Final Batch Loss: 0.496\n",
      "Epoch 7680, Loss: 16.941, Final Batch Loss: 0.442\n",
      "Epoch 7681, Loss: 17.179, Final Batch Loss: 0.430\n",
      "Epoch 7682, Loss: 16.987, Final Batch Loss: 0.481\n",
      "Epoch 7683, Loss: 17.027, Final Batch Loss: 0.390\n",
      "Epoch 7684, Loss: 17.056, Final Batch Loss: 0.473\n",
      "Epoch 7685, Loss: 16.983, Final Batch Loss: 0.413\n",
      "Epoch 7686, Loss: 17.269, Final Batch Loss: 0.417\n",
      "Epoch 7687, Loss: 17.158, Final Batch Loss: 0.528\n",
      "Epoch 7688, Loss: 16.997, Final Batch Loss: 0.550\n",
      "Epoch 7689, Loss: 16.953, Final Batch Loss: 0.417\n",
      "Epoch 7690, Loss: 17.111, Final Batch Loss: 0.533\n",
      "Epoch 7691, Loss: 17.101, Final Batch Loss: 0.454\n",
      "Epoch 7692, Loss: 16.878, Final Batch Loss: 0.548\n",
      "Epoch 7693, Loss: 17.119, Final Batch Loss: 0.589\n",
      "Epoch 7694, Loss: 17.018, Final Batch Loss: 0.587\n",
      "Epoch 7695, Loss: 17.217, Final Batch Loss: 0.391\n",
      "Epoch 7696, Loss: 16.832, Final Batch Loss: 0.381\n",
      "Epoch 7697, Loss: 16.885, Final Batch Loss: 0.438\n",
      "Epoch 7698, Loss: 17.326, Final Batch Loss: 0.514\n",
      "Epoch 7699, Loss: 17.114, Final Batch Loss: 0.488\n",
      "Epoch 7700, Loss: 17.080, Final Batch Loss: 0.446\n",
      "Epoch 7701, Loss: 17.126, Final Batch Loss: 0.542\n",
      "Epoch 7702, Loss: 17.126, Final Batch Loss: 0.467\n",
      "Epoch 7703, Loss: 16.975, Final Batch Loss: 0.449\n",
      "Epoch 7704, Loss: 16.773, Final Batch Loss: 0.555\n",
      "Epoch 7705, Loss: 17.264, Final Batch Loss: 0.511\n",
      "Epoch 7706, Loss: 16.977, Final Batch Loss: 0.517\n",
      "Epoch 7707, Loss: 17.108, Final Batch Loss: 0.417\n",
      "Epoch 7708, Loss: 16.859, Final Batch Loss: 0.340\n",
      "Epoch 7709, Loss: 16.967, Final Batch Loss: 0.405\n",
      "Epoch 7710, Loss: 16.942, Final Batch Loss: 0.508\n",
      "Epoch 7711, Loss: 16.993, Final Batch Loss: 0.378\n",
      "Epoch 7712, Loss: 17.133, Final Batch Loss: 0.479\n",
      "Epoch 7713, Loss: 17.004, Final Batch Loss: 0.349\n",
      "Epoch 7714, Loss: 17.297, Final Batch Loss: 0.526\n",
      "Epoch 7715, Loss: 16.945, Final Batch Loss: 0.417\n",
      "Epoch 7716, Loss: 16.927, Final Batch Loss: 0.474\n",
      "Epoch 7717, Loss: 17.239, Final Batch Loss: 0.515\n",
      "Epoch 7718, Loss: 16.914, Final Batch Loss: 0.543\n",
      "Epoch 7719, Loss: 17.016, Final Batch Loss: 0.526\n",
      "Epoch 7720, Loss: 17.098, Final Batch Loss: 0.525\n",
      "Epoch 7721, Loss: 16.977, Final Batch Loss: 0.441\n",
      "Epoch 7722, Loss: 16.892, Final Batch Loss: 0.366\n",
      "Epoch 7723, Loss: 17.250, Final Batch Loss: 0.583\n",
      "Epoch 7724, Loss: 17.017, Final Batch Loss: 0.454\n",
      "Epoch 7725, Loss: 16.996, Final Batch Loss: 0.473\n",
      "Epoch 7726, Loss: 16.894, Final Batch Loss: 0.476\n",
      "Epoch 7727, Loss: 17.083, Final Batch Loss: 0.519\n",
      "Epoch 7728, Loss: 17.020, Final Batch Loss: 0.498\n",
      "Epoch 7729, Loss: 17.274, Final Batch Loss: 0.495\n",
      "Epoch 7730, Loss: 16.979, Final Batch Loss: 0.514\n",
      "Epoch 7731, Loss: 17.205, Final Batch Loss: 0.506\n",
      "Epoch 7732, Loss: 16.978, Final Batch Loss: 0.500\n",
      "Epoch 7733, Loss: 16.879, Final Batch Loss: 0.490\n",
      "Epoch 7734, Loss: 16.911, Final Batch Loss: 0.436\n",
      "Epoch 7735, Loss: 16.903, Final Batch Loss: 0.480\n",
      "Epoch 7736, Loss: 16.940, Final Batch Loss: 0.542\n",
      "Epoch 7737, Loss: 17.313, Final Batch Loss: 0.519\n",
      "Epoch 7738, Loss: 17.192, Final Batch Loss: 0.529\n",
      "Epoch 7739, Loss: 17.001, Final Batch Loss: 0.426\n",
      "Epoch 7740, Loss: 17.117, Final Batch Loss: 0.553\n",
      "Epoch 7741, Loss: 17.067, Final Batch Loss: 0.512\n",
      "Epoch 7742, Loss: 17.270, Final Batch Loss: 0.444\n",
      "Epoch 7743, Loss: 17.135, Final Batch Loss: 0.409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7744, Loss: 17.105, Final Batch Loss: 0.491\n",
      "Epoch 7745, Loss: 16.934, Final Batch Loss: 0.507\n",
      "Epoch 7746, Loss: 17.114, Final Batch Loss: 0.439\n",
      "Epoch 7747, Loss: 16.925, Final Batch Loss: 0.438\n",
      "Epoch 7748, Loss: 16.855, Final Batch Loss: 0.444\n",
      "Epoch 7749, Loss: 16.875, Final Batch Loss: 0.422\n",
      "Epoch 7750, Loss: 16.834, Final Batch Loss: 0.450\n",
      "Epoch 7751, Loss: 17.100, Final Batch Loss: 0.359\n",
      "Epoch 7752, Loss: 17.213, Final Batch Loss: 0.400\n",
      "Epoch 7753, Loss: 17.101, Final Batch Loss: 0.439\n",
      "Epoch 7754, Loss: 17.098, Final Batch Loss: 0.447\n",
      "Epoch 7755, Loss: 17.256, Final Batch Loss: 0.517\n",
      "Epoch 7756, Loss: 16.748, Final Batch Loss: 0.413\n",
      "Epoch 7757, Loss: 17.129, Final Batch Loss: 0.615\n",
      "Epoch 7758, Loss: 16.633, Final Batch Loss: 0.424\n",
      "Epoch 7759, Loss: 17.141, Final Batch Loss: 0.456\n",
      "Epoch 7760, Loss: 17.001, Final Batch Loss: 0.569\n",
      "Epoch 7761, Loss: 17.111, Final Batch Loss: 0.451\n",
      "Epoch 7762, Loss: 16.805, Final Batch Loss: 0.404\n",
      "Epoch 7763, Loss: 17.235, Final Batch Loss: 0.499\n",
      "Epoch 7764, Loss: 17.010, Final Batch Loss: 0.474\n",
      "Epoch 7765, Loss: 17.276, Final Batch Loss: 0.497\n",
      "Epoch 7766, Loss: 17.196, Final Batch Loss: 0.514\n",
      "Epoch 7767, Loss: 17.167, Final Batch Loss: 0.392\n",
      "Epoch 7768, Loss: 17.031, Final Batch Loss: 0.422\n",
      "Epoch 7769, Loss: 17.056, Final Batch Loss: 0.452\n",
      "Epoch 7770, Loss: 16.991, Final Batch Loss: 0.443\n",
      "Epoch 7771, Loss: 17.061, Final Batch Loss: 0.508\n",
      "Epoch 7772, Loss: 17.217, Final Batch Loss: 0.526\n",
      "Epoch 7773, Loss: 16.835, Final Batch Loss: 0.489\n",
      "Epoch 7774, Loss: 17.087, Final Batch Loss: 0.534\n",
      "Epoch 7775, Loss: 17.192, Final Batch Loss: 0.551\n",
      "Epoch 7776, Loss: 17.209, Final Batch Loss: 0.568\n",
      "Epoch 7777, Loss: 17.082, Final Batch Loss: 0.492\n",
      "Epoch 7778, Loss: 17.012, Final Batch Loss: 0.456\n",
      "Epoch 7779, Loss: 17.108, Final Batch Loss: 0.392\n",
      "Epoch 7780, Loss: 17.094, Final Batch Loss: 0.337\n",
      "Epoch 7781, Loss: 16.954, Final Batch Loss: 0.514\n",
      "Epoch 7782, Loss: 17.115, Final Batch Loss: 0.596\n",
      "Epoch 7783, Loss: 17.162, Final Batch Loss: 0.482\n",
      "Epoch 7784, Loss: 16.708, Final Batch Loss: 0.514\n",
      "Epoch 7785, Loss: 16.720, Final Batch Loss: 0.392\n",
      "Epoch 7786, Loss: 17.056, Final Batch Loss: 0.512\n",
      "Epoch 7787, Loss: 17.431, Final Batch Loss: 0.519\n",
      "Epoch 7788, Loss: 17.120, Final Batch Loss: 0.526\n",
      "Epoch 7789, Loss: 17.140, Final Batch Loss: 0.469\n",
      "Epoch 7790, Loss: 17.084, Final Batch Loss: 0.373\n",
      "Epoch 7791, Loss: 17.306, Final Batch Loss: 0.385\n",
      "Epoch 7792, Loss: 16.922, Final Batch Loss: 0.480\n",
      "Epoch 7793, Loss: 17.315, Final Batch Loss: 0.485\n",
      "Epoch 7794, Loss: 17.121, Final Batch Loss: 0.533\n",
      "Epoch 7795, Loss: 16.934, Final Batch Loss: 0.423\n",
      "Epoch 7796, Loss: 16.789, Final Batch Loss: 0.473\n",
      "Epoch 7797, Loss: 17.020, Final Batch Loss: 0.524\n",
      "Epoch 7798, Loss: 17.046, Final Batch Loss: 0.424\n",
      "Epoch 7799, Loss: 16.927, Final Batch Loss: 0.497\n",
      "Epoch 7800, Loss: 17.274, Final Batch Loss: 0.442\n",
      "Epoch 7801, Loss: 17.120, Final Batch Loss: 0.488\n",
      "Epoch 7802, Loss: 16.851, Final Batch Loss: 0.432\n",
      "Epoch 7803, Loss: 17.073, Final Batch Loss: 0.474\n",
      "Epoch 7804, Loss: 17.006, Final Batch Loss: 0.478\n",
      "Epoch 7805, Loss: 17.099, Final Batch Loss: 0.587\n",
      "Epoch 7806, Loss: 17.058, Final Batch Loss: 0.470\n",
      "Epoch 7807, Loss: 17.169, Final Batch Loss: 0.499\n",
      "Epoch 7808, Loss: 17.109, Final Batch Loss: 0.344\n",
      "Epoch 7809, Loss: 16.946, Final Batch Loss: 0.520\n",
      "Epoch 7810, Loss: 16.905, Final Batch Loss: 0.385\n",
      "Epoch 7811, Loss: 17.002, Final Batch Loss: 0.433\n",
      "Epoch 7812, Loss: 16.832, Final Batch Loss: 0.387\n",
      "Epoch 7813, Loss: 16.891, Final Batch Loss: 0.450\n",
      "Epoch 7814, Loss: 17.168, Final Batch Loss: 0.601\n",
      "Epoch 7815, Loss: 16.904, Final Batch Loss: 0.496\n",
      "Epoch 7816, Loss: 16.933, Final Batch Loss: 0.531\n",
      "Epoch 7817, Loss: 16.995, Final Batch Loss: 0.473\n",
      "Epoch 7818, Loss: 17.199, Final Batch Loss: 0.540\n",
      "Epoch 7819, Loss: 16.924, Final Batch Loss: 0.479\n",
      "Epoch 7820, Loss: 17.015, Final Batch Loss: 0.414\n",
      "Epoch 7821, Loss: 17.001, Final Batch Loss: 0.484\n",
      "Epoch 7822, Loss: 16.940, Final Batch Loss: 0.487\n",
      "Epoch 7823, Loss: 17.155, Final Batch Loss: 0.518\n",
      "Epoch 7824, Loss: 17.473, Final Batch Loss: 0.540\n",
      "Epoch 7825, Loss: 17.074, Final Batch Loss: 0.480\n",
      "Epoch 7826, Loss: 16.795, Final Batch Loss: 0.443\n",
      "Epoch 7827, Loss: 16.821, Final Batch Loss: 0.465\n",
      "Epoch 7828, Loss: 17.222, Final Batch Loss: 0.500\n",
      "Epoch 7829, Loss: 16.948, Final Batch Loss: 0.537\n",
      "Epoch 7830, Loss: 17.032, Final Batch Loss: 0.528\n",
      "Epoch 7831, Loss: 16.996, Final Batch Loss: 0.441\n",
      "Epoch 7832, Loss: 17.106, Final Batch Loss: 0.432\n",
      "Epoch 7833, Loss: 17.152, Final Batch Loss: 0.490\n",
      "Epoch 7834, Loss: 16.939, Final Batch Loss: 0.522\n",
      "Epoch 7835, Loss: 17.174, Final Batch Loss: 0.452\n",
      "Epoch 7836, Loss: 17.218, Final Batch Loss: 0.487\n",
      "Epoch 7837, Loss: 17.161, Final Batch Loss: 0.491\n",
      "Epoch 7838, Loss: 17.048, Final Batch Loss: 0.440\n",
      "Epoch 7839, Loss: 17.169, Final Batch Loss: 0.556\n",
      "Epoch 7840, Loss: 17.027, Final Batch Loss: 0.462\n",
      "Epoch 7841, Loss: 17.092, Final Batch Loss: 0.489\n",
      "Epoch 7842, Loss: 17.128, Final Batch Loss: 0.483\n",
      "Epoch 7843, Loss: 16.958, Final Batch Loss: 0.514\n",
      "Epoch 7844, Loss: 16.956, Final Batch Loss: 0.442\n",
      "Epoch 7845, Loss: 16.915, Final Batch Loss: 0.466\n",
      "Epoch 7846, Loss: 17.203, Final Batch Loss: 0.418\n",
      "Epoch 7847, Loss: 17.029, Final Batch Loss: 0.499\n",
      "Epoch 7848, Loss: 16.976, Final Batch Loss: 0.407\n",
      "Epoch 7849, Loss: 16.873, Final Batch Loss: 0.345\n",
      "Epoch 7850, Loss: 16.990, Final Batch Loss: 0.574\n",
      "Epoch 7851, Loss: 17.170, Final Batch Loss: 0.536\n",
      "Epoch 7852, Loss: 16.950, Final Batch Loss: 0.495\n",
      "Epoch 7853, Loss: 17.207, Final Batch Loss: 0.532\n",
      "Epoch 7854, Loss: 16.861, Final Batch Loss: 0.418\n",
      "Epoch 7855, Loss: 16.968, Final Batch Loss: 0.494\n",
      "Epoch 7856, Loss: 17.167, Final Batch Loss: 0.451\n",
      "Epoch 7857, Loss: 17.268, Final Batch Loss: 0.485\n",
      "Epoch 7858, Loss: 16.983, Final Batch Loss: 0.466\n",
      "Epoch 7859, Loss: 17.011, Final Batch Loss: 0.482\n",
      "Epoch 7860, Loss: 17.298, Final Batch Loss: 0.591\n",
      "Epoch 7861, Loss: 17.203, Final Batch Loss: 0.590\n",
      "Epoch 7862, Loss: 16.839, Final Batch Loss: 0.392\n",
      "Epoch 7863, Loss: 16.905, Final Batch Loss: 0.547\n",
      "Epoch 7864, Loss: 17.083, Final Batch Loss: 0.462\n",
      "Epoch 7865, Loss: 16.971, Final Batch Loss: 0.472\n",
      "Epoch 7866, Loss: 17.101, Final Batch Loss: 0.475\n",
      "Epoch 7867, Loss: 17.029, Final Batch Loss: 0.433\n",
      "Epoch 7868, Loss: 16.930, Final Batch Loss: 0.326\n",
      "Epoch 7869, Loss: 16.971, Final Batch Loss: 0.450\n",
      "Epoch 7870, Loss: 17.159, Final Batch Loss: 0.635\n",
      "Epoch 7871, Loss: 16.929, Final Batch Loss: 0.455\n",
      "Epoch 7872, Loss: 17.097, Final Batch Loss: 0.537\n",
      "Epoch 7873, Loss: 17.159, Final Batch Loss: 0.490\n",
      "Epoch 7874, Loss: 16.943, Final Batch Loss: 0.493\n",
      "Epoch 7875, Loss: 17.134, Final Batch Loss: 0.498\n",
      "Epoch 7876, Loss: 16.737, Final Batch Loss: 0.380\n",
      "Epoch 7877, Loss: 17.069, Final Batch Loss: 0.385\n",
      "Epoch 7878, Loss: 16.976, Final Batch Loss: 0.465\n",
      "Epoch 7879, Loss: 17.113, Final Batch Loss: 0.480\n",
      "Epoch 7880, Loss: 17.307, Final Batch Loss: 0.373\n",
      "Epoch 7881, Loss: 16.776, Final Batch Loss: 0.375\n",
      "Epoch 7882, Loss: 17.086, Final Batch Loss: 0.438\n",
      "Epoch 7883, Loss: 16.641, Final Batch Loss: 0.393\n",
      "Epoch 7884, Loss: 17.029, Final Batch Loss: 0.469\n",
      "Epoch 7885, Loss: 17.096, Final Batch Loss: 0.464\n",
      "Epoch 7886, Loss: 16.771, Final Batch Loss: 0.391\n",
      "Epoch 7887, Loss: 16.811, Final Batch Loss: 0.474\n",
      "Epoch 7888, Loss: 17.054, Final Batch Loss: 0.467\n",
      "Epoch 7889, Loss: 17.112, Final Batch Loss: 0.428\n",
      "Epoch 7890, Loss: 16.887, Final Batch Loss: 0.502\n",
      "Epoch 7891, Loss: 17.194, Final Batch Loss: 0.578\n",
      "Epoch 7892, Loss: 17.033, Final Batch Loss: 0.466\n",
      "Epoch 7893, Loss: 16.965, Final Batch Loss: 0.461\n",
      "Epoch 7894, Loss: 17.038, Final Batch Loss: 0.506\n",
      "Epoch 7895, Loss: 16.969, Final Batch Loss: 0.394\n",
      "Epoch 7896, Loss: 16.951, Final Batch Loss: 0.454\n",
      "Epoch 7897, Loss: 16.930, Final Batch Loss: 0.406\n",
      "Epoch 7898, Loss: 16.926, Final Batch Loss: 0.493\n",
      "Epoch 7899, Loss: 16.981, Final Batch Loss: 0.559\n",
      "Epoch 7900, Loss: 16.959, Final Batch Loss: 0.443\n",
      "Epoch 7901, Loss: 16.948, Final Batch Loss: 0.494\n",
      "Epoch 7902, Loss: 17.033, Final Batch Loss: 0.441\n",
      "Epoch 7903, Loss: 17.066, Final Batch Loss: 0.498\n",
      "Epoch 7904, Loss: 17.027, Final Batch Loss: 0.454\n",
      "Epoch 7905, Loss: 17.176, Final Batch Loss: 0.459\n",
      "Epoch 7906, Loss: 17.021, Final Batch Loss: 0.492\n",
      "Epoch 7907, Loss: 16.866, Final Batch Loss: 0.438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7908, Loss: 17.109, Final Batch Loss: 0.422\n",
      "Epoch 7909, Loss: 17.158, Final Batch Loss: 0.426\n",
      "Epoch 7910, Loss: 17.155, Final Batch Loss: 0.580\n",
      "Epoch 7911, Loss: 17.114, Final Batch Loss: 0.469\n",
      "Epoch 7912, Loss: 17.137, Final Batch Loss: 0.545\n",
      "Epoch 7913, Loss: 16.793, Final Batch Loss: 0.467\n",
      "Epoch 7914, Loss: 16.875, Final Batch Loss: 0.526\n",
      "Epoch 7915, Loss: 17.108, Final Batch Loss: 0.468\n",
      "Epoch 7916, Loss: 16.971, Final Batch Loss: 0.436\n",
      "Epoch 7917, Loss: 16.617, Final Batch Loss: 0.358\n",
      "Epoch 7918, Loss: 16.930, Final Batch Loss: 0.580\n",
      "Epoch 7919, Loss: 16.692, Final Batch Loss: 0.495\n",
      "Epoch 7920, Loss: 16.990, Final Batch Loss: 0.400\n",
      "Epoch 7921, Loss: 16.868, Final Batch Loss: 0.477\n",
      "Epoch 7922, Loss: 17.117, Final Batch Loss: 0.535\n",
      "Epoch 7923, Loss: 17.097, Final Batch Loss: 0.463\n",
      "Epoch 7924, Loss: 16.938, Final Batch Loss: 0.448\n",
      "Epoch 7925, Loss: 16.790, Final Batch Loss: 0.441\n",
      "Epoch 7926, Loss: 17.078, Final Batch Loss: 0.545\n",
      "Epoch 7927, Loss: 17.104, Final Batch Loss: 0.569\n",
      "Epoch 7928, Loss: 17.174, Final Batch Loss: 0.438\n",
      "Epoch 7929, Loss: 17.091, Final Batch Loss: 0.483\n",
      "Epoch 7930, Loss: 16.809, Final Batch Loss: 0.467\n",
      "Epoch 7931, Loss: 16.822, Final Batch Loss: 0.454\n",
      "Epoch 7932, Loss: 16.602, Final Batch Loss: 0.506\n",
      "Epoch 7933, Loss: 17.168, Final Batch Loss: 0.480\n",
      "Epoch 7934, Loss: 17.425, Final Batch Loss: 0.451\n",
      "Epoch 7935, Loss: 16.885, Final Batch Loss: 0.340\n",
      "Epoch 7936, Loss: 17.111, Final Batch Loss: 0.473\n",
      "Epoch 7937, Loss: 17.154, Final Batch Loss: 0.398\n",
      "Epoch 7938, Loss: 16.989, Final Batch Loss: 0.458\n",
      "Epoch 7939, Loss: 16.757, Final Batch Loss: 0.549\n",
      "Epoch 7940, Loss: 16.996, Final Batch Loss: 0.420\n",
      "Epoch 7941, Loss: 17.106, Final Batch Loss: 0.364\n",
      "Epoch 7942, Loss: 16.926, Final Batch Loss: 0.479\n",
      "Epoch 7943, Loss: 16.920, Final Batch Loss: 0.428\n",
      "Epoch 7944, Loss: 16.846, Final Batch Loss: 0.491\n",
      "Epoch 7945, Loss: 17.035, Final Batch Loss: 0.513\n",
      "Epoch 7946, Loss: 16.931, Final Batch Loss: 0.475\n",
      "Epoch 7947, Loss: 16.874, Final Batch Loss: 0.445\n",
      "Epoch 7948, Loss: 17.195, Final Batch Loss: 0.531\n",
      "Epoch 7949, Loss: 17.073, Final Batch Loss: 0.472\n",
      "Epoch 7950, Loss: 16.938, Final Batch Loss: 0.514\n",
      "Epoch 7951, Loss: 17.221, Final Batch Loss: 0.503\n",
      "Epoch 7952, Loss: 17.038, Final Batch Loss: 0.432\n",
      "Epoch 7953, Loss: 16.925, Final Batch Loss: 0.580\n",
      "Epoch 7954, Loss: 17.133, Final Batch Loss: 0.351\n",
      "Epoch 7955, Loss: 16.546, Final Batch Loss: 0.527\n",
      "Epoch 7956, Loss: 17.041, Final Batch Loss: 0.596\n",
      "Epoch 7957, Loss: 16.898, Final Batch Loss: 0.523\n",
      "Epoch 7958, Loss: 16.937, Final Batch Loss: 0.362\n",
      "Epoch 7959, Loss: 17.018, Final Batch Loss: 0.529\n",
      "Epoch 7960, Loss: 16.949, Final Batch Loss: 0.560\n",
      "Epoch 7961, Loss: 17.393, Final Batch Loss: 0.464\n",
      "Epoch 7962, Loss: 16.862, Final Batch Loss: 0.426\n",
      "Epoch 7963, Loss: 17.095, Final Batch Loss: 0.569\n",
      "Epoch 7964, Loss: 17.192, Final Batch Loss: 0.469\n",
      "Epoch 7965, Loss: 16.924, Final Batch Loss: 0.415\n",
      "Epoch 7966, Loss: 17.207, Final Batch Loss: 0.590\n",
      "Epoch 7967, Loss: 16.912, Final Batch Loss: 0.525\n",
      "Epoch 7968, Loss: 16.898, Final Batch Loss: 0.488\n",
      "Epoch 7969, Loss: 17.142, Final Batch Loss: 0.450\n",
      "Epoch 7970, Loss: 16.922, Final Batch Loss: 0.445\n",
      "Epoch 7971, Loss: 16.930, Final Batch Loss: 0.456\n",
      "Epoch 7972, Loss: 17.078, Final Batch Loss: 0.494\n",
      "Epoch 7973, Loss: 16.825, Final Batch Loss: 0.441\n",
      "Epoch 7974, Loss: 16.979, Final Batch Loss: 0.382\n",
      "Epoch 7975, Loss: 17.376, Final Batch Loss: 0.549\n",
      "Epoch 7976, Loss: 16.831, Final Batch Loss: 0.414\n",
      "Epoch 7977, Loss: 17.127, Final Batch Loss: 0.440\n",
      "Epoch 7978, Loss: 16.918, Final Batch Loss: 0.541\n",
      "Epoch 7979, Loss: 17.042, Final Batch Loss: 0.574\n",
      "Epoch 7980, Loss: 17.329, Final Batch Loss: 0.422\n",
      "Epoch 7981, Loss: 16.864, Final Batch Loss: 0.431\n",
      "Epoch 7982, Loss: 16.901, Final Batch Loss: 0.507\n",
      "Epoch 7983, Loss: 17.211, Final Batch Loss: 0.545\n",
      "Epoch 7984, Loss: 16.957, Final Batch Loss: 0.519\n",
      "Epoch 7985, Loss: 16.811, Final Batch Loss: 0.380\n",
      "Epoch 7986, Loss: 16.737, Final Batch Loss: 0.498\n",
      "Epoch 7987, Loss: 17.092, Final Batch Loss: 0.408\n",
      "Epoch 7988, Loss: 17.017, Final Batch Loss: 0.523\n",
      "Epoch 7989, Loss: 17.056, Final Batch Loss: 0.454\n",
      "Epoch 7990, Loss: 16.989, Final Batch Loss: 0.533\n",
      "Epoch 7991, Loss: 17.299, Final Batch Loss: 0.465\n",
      "Epoch 7992, Loss: 16.815, Final Batch Loss: 0.458\n",
      "Epoch 7993, Loss: 17.153, Final Batch Loss: 0.489\n",
      "Epoch 7994, Loss: 16.927, Final Batch Loss: 0.370\n",
      "Epoch 7995, Loss: 17.104, Final Batch Loss: 0.508\n",
      "Epoch 7996, Loss: 17.229, Final Batch Loss: 0.511\n",
      "Epoch 7997, Loss: 16.917, Final Batch Loss: 0.449\n",
      "Epoch 7998, Loss: 17.167, Final Batch Loss: 0.495\n",
      "Epoch 7999, Loss: 17.063, Final Batch Loss: 0.447\n",
      "Epoch 8000, Loss: 17.034, Final Batch Loss: 0.472\n",
      "Epoch 8001, Loss: 16.952, Final Batch Loss: 0.431\n",
      "Epoch 8002, Loss: 16.867, Final Batch Loss: 0.342\n",
      "Epoch 8003, Loss: 16.998, Final Batch Loss: 0.443\n",
      "Epoch 8004, Loss: 17.369, Final Batch Loss: 0.504\n",
      "Epoch 8005, Loss: 17.099, Final Batch Loss: 0.516\n",
      "Epoch 8006, Loss: 17.002, Final Batch Loss: 0.522\n",
      "Epoch 8007, Loss: 17.032, Final Batch Loss: 0.642\n",
      "Epoch 8008, Loss: 16.875, Final Batch Loss: 0.418\n",
      "Epoch 8009, Loss: 16.960, Final Batch Loss: 0.557\n",
      "Epoch 8010, Loss: 17.024, Final Batch Loss: 0.587\n",
      "Epoch 8011, Loss: 17.143, Final Batch Loss: 0.471\n",
      "Epoch 8012, Loss: 17.156, Final Batch Loss: 0.527\n",
      "Epoch 8013, Loss: 16.975, Final Batch Loss: 0.438\n",
      "Epoch 8014, Loss: 16.995, Final Batch Loss: 0.415\n",
      "Epoch 8015, Loss: 17.053, Final Batch Loss: 0.551\n",
      "Epoch 8016, Loss: 16.711, Final Batch Loss: 0.467\n",
      "Epoch 8017, Loss: 16.736, Final Batch Loss: 0.480\n",
      "Epoch 8018, Loss: 16.873, Final Batch Loss: 0.561\n",
      "Epoch 8019, Loss: 17.254, Final Batch Loss: 0.561\n",
      "Epoch 8020, Loss: 17.152, Final Batch Loss: 0.494\n",
      "Epoch 8021, Loss: 17.392, Final Batch Loss: 0.556\n",
      "Epoch 8022, Loss: 16.968, Final Batch Loss: 0.482\n",
      "Epoch 8023, Loss: 17.046, Final Batch Loss: 0.512\n",
      "Epoch 8024, Loss: 16.992, Final Batch Loss: 0.533\n",
      "Epoch 8025, Loss: 16.861, Final Batch Loss: 0.396\n",
      "Epoch 8026, Loss: 17.021, Final Batch Loss: 0.496\n",
      "Epoch 8027, Loss: 16.992, Final Batch Loss: 0.463\n",
      "Epoch 8028, Loss: 16.812, Final Batch Loss: 0.504\n",
      "Epoch 8029, Loss: 16.972, Final Batch Loss: 0.478\n",
      "Epoch 8030, Loss: 16.976, Final Batch Loss: 0.389\n",
      "Epoch 8031, Loss: 16.922, Final Batch Loss: 0.480\n",
      "Epoch 8032, Loss: 17.088, Final Batch Loss: 0.565\n",
      "Epoch 8033, Loss: 17.338, Final Batch Loss: 0.552\n",
      "Epoch 8034, Loss: 17.188, Final Batch Loss: 0.477\n",
      "Epoch 8035, Loss: 16.779, Final Batch Loss: 0.636\n",
      "Epoch 8036, Loss: 17.245, Final Batch Loss: 0.583\n",
      "Epoch 8037, Loss: 17.051, Final Batch Loss: 0.460\n",
      "Epoch 8038, Loss: 17.013, Final Batch Loss: 0.428\n",
      "Epoch 8039, Loss: 16.947, Final Batch Loss: 0.469\n",
      "Epoch 8040, Loss: 17.202, Final Batch Loss: 0.543\n",
      "Epoch 8041, Loss: 16.904, Final Batch Loss: 0.441\n",
      "Epoch 8042, Loss: 17.254, Final Batch Loss: 0.568\n",
      "Epoch 8043, Loss: 16.770, Final Batch Loss: 0.504\n",
      "Epoch 8044, Loss: 16.737, Final Batch Loss: 0.390\n",
      "Epoch 8045, Loss: 16.904, Final Batch Loss: 0.425\n",
      "Epoch 8046, Loss: 17.127, Final Batch Loss: 0.572\n",
      "Epoch 8047, Loss: 17.057, Final Batch Loss: 0.651\n",
      "Epoch 8048, Loss: 16.902, Final Batch Loss: 0.520\n",
      "Epoch 8049, Loss: 17.056, Final Batch Loss: 0.620\n",
      "Epoch 8050, Loss: 17.067, Final Batch Loss: 0.523\n",
      "Epoch 8051, Loss: 16.948, Final Batch Loss: 0.515\n",
      "Epoch 8052, Loss: 16.836, Final Batch Loss: 0.476\n",
      "Epoch 8053, Loss: 17.005, Final Batch Loss: 0.491\n",
      "Epoch 8054, Loss: 16.955, Final Batch Loss: 0.506\n",
      "Epoch 8055, Loss: 17.040, Final Batch Loss: 0.474\n",
      "Epoch 8056, Loss: 16.929, Final Batch Loss: 0.366\n",
      "Epoch 8057, Loss: 16.765, Final Batch Loss: 0.400\n",
      "Epoch 8058, Loss: 16.837, Final Batch Loss: 0.521\n",
      "Epoch 8059, Loss: 16.990, Final Batch Loss: 0.453\n",
      "Epoch 8060, Loss: 16.964, Final Batch Loss: 0.474\n",
      "Epoch 8061, Loss: 17.249, Final Batch Loss: 0.515\n",
      "Epoch 8062, Loss: 17.076, Final Batch Loss: 0.466\n",
      "Epoch 8063, Loss: 17.135, Final Batch Loss: 0.406\n",
      "Epoch 8064, Loss: 16.734, Final Batch Loss: 0.593\n",
      "Epoch 8065, Loss: 17.037, Final Batch Loss: 0.499\n",
      "Epoch 8066, Loss: 16.935, Final Batch Loss: 0.459\n",
      "Epoch 8067, Loss: 16.953, Final Batch Loss: 0.531\n",
      "Epoch 8068, Loss: 16.997, Final Batch Loss: 0.469\n",
      "Epoch 8069, Loss: 16.897, Final Batch Loss: 0.458\n",
      "Epoch 8070, Loss: 16.591, Final Batch Loss: 0.376\n",
      "Epoch 8071, Loss: 17.189, Final Batch Loss: 0.407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8072, Loss: 17.074, Final Batch Loss: 0.514\n",
      "Epoch 8073, Loss: 16.804, Final Batch Loss: 0.375\n",
      "Epoch 8074, Loss: 16.987, Final Batch Loss: 0.499\n",
      "Epoch 8075, Loss: 17.249, Final Batch Loss: 0.496\n",
      "Epoch 8076, Loss: 16.824, Final Batch Loss: 0.448\n",
      "Epoch 8077, Loss: 16.896, Final Batch Loss: 0.450\n",
      "Epoch 8078, Loss: 17.066, Final Batch Loss: 0.453\n",
      "Epoch 8079, Loss: 16.704, Final Batch Loss: 0.439\n",
      "Epoch 8080, Loss: 17.095, Final Batch Loss: 0.404\n",
      "Epoch 8081, Loss: 16.946, Final Batch Loss: 0.467\n",
      "Epoch 8082, Loss: 17.034, Final Batch Loss: 0.525\n",
      "Epoch 8083, Loss: 16.949, Final Batch Loss: 0.435\n",
      "Epoch 8084, Loss: 16.851, Final Batch Loss: 0.502\n",
      "Epoch 8085, Loss: 17.078, Final Batch Loss: 0.445\n",
      "Epoch 8086, Loss: 16.953, Final Batch Loss: 0.433\n",
      "Epoch 8087, Loss: 17.111, Final Batch Loss: 0.429\n",
      "Epoch 8088, Loss: 17.118, Final Batch Loss: 0.553\n",
      "Epoch 8089, Loss: 16.547, Final Batch Loss: 0.460\n",
      "Epoch 8090, Loss: 17.130, Final Batch Loss: 0.474\n",
      "Epoch 8091, Loss: 17.086, Final Batch Loss: 0.477\n",
      "Epoch 8092, Loss: 17.157, Final Batch Loss: 0.553\n",
      "Epoch 8093, Loss: 17.067, Final Batch Loss: 0.442\n",
      "Epoch 8094, Loss: 16.792, Final Batch Loss: 0.379\n",
      "Epoch 8095, Loss: 16.569, Final Batch Loss: 0.461\n",
      "Epoch 8096, Loss: 16.644, Final Batch Loss: 0.416\n",
      "Epoch 8097, Loss: 16.887, Final Batch Loss: 0.571\n",
      "Epoch 8098, Loss: 16.823, Final Batch Loss: 0.419\n",
      "Epoch 8099, Loss: 16.760, Final Batch Loss: 0.364\n",
      "Epoch 8100, Loss: 16.883, Final Batch Loss: 0.519\n",
      "Epoch 8101, Loss: 16.847, Final Batch Loss: 0.425\n",
      "Epoch 8102, Loss: 16.686, Final Batch Loss: 0.428\n",
      "Epoch 8103, Loss: 17.064, Final Batch Loss: 0.404\n",
      "Epoch 8104, Loss: 17.095, Final Batch Loss: 0.501\n",
      "Epoch 8105, Loss: 17.077, Final Batch Loss: 0.532\n",
      "Epoch 8106, Loss: 16.731, Final Batch Loss: 0.496\n",
      "Epoch 8107, Loss: 17.118, Final Batch Loss: 0.462\n",
      "Epoch 8108, Loss: 16.666, Final Batch Loss: 0.462\n",
      "Epoch 8109, Loss: 16.919, Final Batch Loss: 0.459\n",
      "Epoch 8110, Loss: 16.974, Final Batch Loss: 0.528\n",
      "Epoch 8111, Loss: 16.852, Final Batch Loss: 0.344\n",
      "Epoch 8112, Loss: 17.011, Final Batch Loss: 0.474\n",
      "Epoch 8113, Loss: 17.157, Final Batch Loss: 0.512\n",
      "Epoch 8114, Loss: 16.854, Final Batch Loss: 0.449\n",
      "Epoch 8115, Loss: 17.104, Final Batch Loss: 0.424\n",
      "Epoch 8116, Loss: 16.923, Final Batch Loss: 0.448\n",
      "Epoch 8117, Loss: 16.956, Final Batch Loss: 0.433\n",
      "Epoch 8118, Loss: 17.164, Final Batch Loss: 0.388\n",
      "Epoch 8119, Loss: 16.897, Final Batch Loss: 0.526\n",
      "Epoch 8120, Loss: 16.938, Final Batch Loss: 0.536\n",
      "Epoch 8121, Loss: 16.911, Final Batch Loss: 0.458\n",
      "Epoch 8122, Loss: 16.894, Final Batch Loss: 0.421\n",
      "Epoch 8123, Loss: 16.792, Final Batch Loss: 0.442\n",
      "Epoch 8124, Loss: 17.182, Final Batch Loss: 0.435\n",
      "Epoch 8125, Loss: 16.941, Final Batch Loss: 0.473\n",
      "Epoch 8126, Loss: 17.011, Final Batch Loss: 0.413\n",
      "Epoch 8127, Loss: 17.030, Final Batch Loss: 0.423\n",
      "Epoch 8128, Loss: 17.111, Final Batch Loss: 0.535\n",
      "Epoch 8129, Loss: 17.041, Final Batch Loss: 0.615\n",
      "Epoch 8130, Loss: 17.253, Final Batch Loss: 0.453\n",
      "Epoch 8131, Loss: 16.557, Final Batch Loss: 0.381\n",
      "Epoch 8132, Loss: 17.196, Final Batch Loss: 0.538\n",
      "Epoch 8133, Loss: 16.987, Final Batch Loss: 0.385\n",
      "Epoch 8134, Loss: 17.023, Final Batch Loss: 0.570\n",
      "Epoch 8135, Loss: 17.062, Final Batch Loss: 0.486\n",
      "Epoch 8136, Loss: 17.085, Final Batch Loss: 0.529\n",
      "Epoch 8137, Loss: 16.940, Final Batch Loss: 0.433\n",
      "Epoch 8138, Loss: 16.712, Final Batch Loss: 0.460\n",
      "Epoch 8139, Loss: 16.931, Final Batch Loss: 0.441\n",
      "Epoch 8140, Loss: 16.859, Final Batch Loss: 0.439\n",
      "Epoch 8141, Loss: 17.080, Final Batch Loss: 0.514\n",
      "Epoch 8142, Loss: 17.188, Final Batch Loss: 0.480\n",
      "Epoch 8143, Loss: 17.294, Final Batch Loss: 0.704\n",
      "Epoch 8144, Loss: 17.100, Final Batch Loss: 0.470\n",
      "Epoch 8145, Loss: 16.972, Final Batch Loss: 0.396\n",
      "Epoch 8146, Loss: 16.883, Final Batch Loss: 0.547\n",
      "Epoch 8147, Loss: 16.981, Final Batch Loss: 0.440\n",
      "Epoch 8148, Loss: 16.838, Final Batch Loss: 0.401\n",
      "Epoch 8149, Loss: 16.918, Final Batch Loss: 0.531\n",
      "Epoch 8150, Loss: 16.881, Final Batch Loss: 0.507\n",
      "Epoch 8151, Loss: 16.875, Final Batch Loss: 0.362\n",
      "Epoch 8152, Loss: 16.844, Final Batch Loss: 0.383\n",
      "Epoch 8153, Loss: 16.713, Final Batch Loss: 0.483\n",
      "Epoch 8154, Loss: 17.267, Final Batch Loss: 0.482\n",
      "Epoch 8155, Loss: 16.917, Final Batch Loss: 0.415\n",
      "Epoch 8156, Loss: 16.943, Final Batch Loss: 0.411\n",
      "Epoch 8157, Loss: 16.900, Final Batch Loss: 0.510\n",
      "Epoch 8158, Loss: 16.837, Final Batch Loss: 0.420\n",
      "Epoch 8159, Loss: 16.995, Final Batch Loss: 0.529\n",
      "Epoch 8160, Loss: 17.168, Final Batch Loss: 0.417\n",
      "Epoch 8161, Loss: 16.649, Final Batch Loss: 0.405\n",
      "Epoch 8162, Loss: 16.948, Final Batch Loss: 0.405\n",
      "Epoch 8163, Loss: 17.074, Final Batch Loss: 0.475\n",
      "Epoch 8164, Loss: 17.037, Final Batch Loss: 0.528\n",
      "Epoch 8165, Loss: 16.826, Final Batch Loss: 0.422\n",
      "Epoch 8166, Loss: 16.674, Final Batch Loss: 0.431\n",
      "Epoch 8167, Loss: 17.098, Final Batch Loss: 0.513\n",
      "Epoch 8168, Loss: 17.234, Final Batch Loss: 0.471\n",
      "Epoch 8169, Loss: 17.073, Final Batch Loss: 0.374\n",
      "Epoch 8170, Loss: 16.695, Final Batch Loss: 0.469\n",
      "Epoch 8171, Loss: 17.094, Final Batch Loss: 0.474\n",
      "Epoch 8172, Loss: 16.810, Final Batch Loss: 0.443\n",
      "Epoch 8173, Loss: 16.844, Final Batch Loss: 0.447\n",
      "Epoch 8174, Loss: 16.983, Final Batch Loss: 0.403\n",
      "Epoch 8175, Loss: 16.776, Final Batch Loss: 0.456\n",
      "Epoch 8176, Loss: 16.837, Final Batch Loss: 0.370\n",
      "Epoch 8177, Loss: 16.890, Final Batch Loss: 0.589\n",
      "Epoch 8178, Loss: 16.736, Final Batch Loss: 0.488\n",
      "Epoch 8179, Loss: 17.032, Final Batch Loss: 0.466\n",
      "Epoch 8180, Loss: 16.607, Final Batch Loss: 0.471\n",
      "Epoch 8181, Loss: 17.016, Final Batch Loss: 0.542\n",
      "Epoch 8182, Loss: 17.016, Final Batch Loss: 0.637\n",
      "Epoch 8183, Loss: 17.069, Final Batch Loss: 0.529\n",
      "Epoch 8184, Loss: 17.135, Final Batch Loss: 0.520\n",
      "Epoch 8185, Loss: 16.946, Final Batch Loss: 0.440\n",
      "Epoch 8186, Loss: 17.148, Final Batch Loss: 0.546\n",
      "Epoch 8187, Loss: 16.847, Final Batch Loss: 0.370\n",
      "Epoch 8188, Loss: 17.141, Final Batch Loss: 0.456\n",
      "Epoch 8189, Loss: 16.697, Final Batch Loss: 0.393\n",
      "Epoch 8190, Loss: 16.916, Final Batch Loss: 0.369\n",
      "Epoch 8191, Loss: 16.829, Final Batch Loss: 0.482\n",
      "Epoch 8192, Loss: 16.896, Final Batch Loss: 0.443\n",
      "Epoch 8193, Loss: 16.874, Final Batch Loss: 0.434\n",
      "Epoch 8194, Loss: 16.963, Final Batch Loss: 0.430\n",
      "Epoch 8195, Loss: 16.578, Final Batch Loss: 0.454\n",
      "Epoch 8196, Loss: 16.837, Final Batch Loss: 0.436\n",
      "Epoch 8197, Loss: 17.203, Final Batch Loss: 0.494\n",
      "Epoch 8198, Loss: 16.920, Final Batch Loss: 0.524\n",
      "Epoch 8199, Loss: 16.993, Final Batch Loss: 0.446\n",
      "Epoch 8200, Loss: 17.060, Final Batch Loss: 0.416\n",
      "Epoch 8201, Loss: 16.819, Final Batch Loss: 0.552\n",
      "Epoch 8202, Loss: 16.845, Final Batch Loss: 0.504\n",
      "Epoch 8203, Loss: 16.840, Final Batch Loss: 0.472\n",
      "Epoch 8204, Loss: 16.909, Final Batch Loss: 0.374\n",
      "Epoch 8205, Loss: 17.225, Final Batch Loss: 0.458\n",
      "Epoch 8206, Loss: 17.011, Final Batch Loss: 0.497\n",
      "Epoch 8207, Loss: 17.162, Final Batch Loss: 0.537\n",
      "Epoch 8208, Loss: 17.065, Final Batch Loss: 0.423\n",
      "Epoch 8209, Loss: 17.081, Final Batch Loss: 0.517\n",
      "Epoch 8210, Loss: 16.876, Final Batch Loss: 0.410\n",
      "Epoch 8211, Loss: 17.076, Final Batch Loss: 0.374\n",
      "Epoch 8212, Loss: 16.790, Final Batch Loss: 0.547\n",
      "Epoch 8213, Loss: 16.982, Final Batch Loss: 0.525\n",
      "Epoch 8214, Loss: 17.044, Final Batch Loss: 0.425\n",
      "Epoch 8215, Loss: 17.159, Final Batch Loss: 0.547\n",
      "Epoch 8216, Loss: 17.058, Final Batch Loss: 0.447\n",
      "Epoch 8217, Loss: 17.105, Final Batch Loss: 0.488\n",
      "Epoch 8218, Loss: 16.834, Final Batch Loss: 0.539\n",
      "Epoch 8219, Loss: 17.158, Final Batch Loss: 0.473\n",
      "Epoch 8220, Loss: 16.967, Final Batch Loss: 0.437\n",
      "Epoch 8221, Loss: 16.866, Final Batch Loss: 0.361\n",
      "Epoch 8222, Loss: 17.050, Final Batch Loss: 0.560\n",
      "Epoch 8223, Loss: 16.750, Final Batch Loss: 0.459\n",
      "Epoch 8224, Loss: 16.880, Final Batch Loss: 0.453\n",
      "Epoch 8225, Loss: 16.787, Final Batch Loss: 0.493\n",
      "Epoch 8226, Loss: 17.124, Final Batch Loss: 0.538\n",
      "Epoch 8227, Loss: 16.922, Final Batch Loss: 0.391\n",
      "Epoch 8228, Loss: 16.738, Final Batch Loss: 0.569\n",
      "Epoch 8229, Loss: 16.892, Final Batch Loss: 0.345\n",
      "Epoch 8230, Loss: 16.831, Final Batch Loss: 0.546\n",
      "Epoch 8231, Loss: 16.795, Final Batch Loss: 0.539\n",
      "Epoch 8232, Loss: 17.127, Final Batch Loss: 0.479\n",
      "Epoch 8233, Loss: 16.735, Final Batch Loss: 0.410\n",
      "Epoch 8234, Loss: 16.768, Final Batch Loss: 0.398\n",
      "Epoch 8235, Loss: 17.103, Final Batch Loss: 0.528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8236, Loss: 16.980, Final Batch Loss: 0.452\n",
      "Epoch 8237, Loss: 17.280, Final Batch Loss: 0.506\n",
      "Epoch 8238, Loss: 16.993, Final Batch Loss: 0.443\n",
      "Epoch 8239, Loss: 17.029, Final Batch Loss: 0.446\n",
      "Epoch 8240, Loss: 17.086, Final Batch Loss: 0.526\n",
      "Epoch 8241, Loss: 16.699, Final Batch Loss: 0.493\n",
      "Epoch 8242, Loss: 16.976, Final Batch Loss: 0.570\n",
      "Epoch 8243, Loss: 17.032, Final Batch Loss: 0.435\n",
      "Epoch 8244, Loss: 17.003, Final Batch Loss: 0.434\n",
      "Epoch 8245, Loss: 16.780, Final Batch Loss: 0.390\n",
      "Epoch 8246, Loss: 17.130, Final Batch Loss: 0.594\n",
      "Epoch 8247, Loss: 16.962, Final Batch Loss: 0.511\n",
      "Epoch 8248, Loss: 16.719, Final Batch Loss: 0.424\n",
      "Epoch 8249, Loss: 16.961, Final Batch Loss: 0.332\n",
      "Epoch 8250, Loss: 16.786, Final Batch Loss: 0.407\n",
      "Epoch 8251, Loss: 16.964, Final Batch Loss: 0.468\n",
      "Epoch 8252, Loss: 16.782, Final Batch Loss: 0.563\n",
      "Epoch 8253, Loss: 16.828, Final Batch Loss: 0.392\n",
      "Epoch 8254, Loss: 16.913, Final Batch Loss: 0.493\n",
      "Epoch 8255, Loss: 16.961, Final Batch Loss: 0.442\n",
      "Epoch 8256, Loss: 16.768, Final Batch Loss: 0.540\n",
      "Epoch 8257, Loss: 16.987, Final Batch Loss: 0.460\n",
      "Epoch 8258, Loss: 16.831, Final Batch Loss: 0.499\n",
      "Epoch 8259, Loss: 17.040, Final Batch Loss: 0.435\n",
      "Epoch 8260, Loss: 16.796, Final Batch Loss: 0.450\n",
      "Epoch 8261, Loss: 16.848, Final Batch Loss: 0.422\n",
      "Epoch 8262, Loss: 17.179, Final Batch Loss: 0.466\n",
      "Epoch 8263, Loss: 16.651, Final Batch Loss: 0.372\n",
      "Epoch 8264, Loss: 17.007, Final Batch Loss: 0.536\n",
      "Epoch 8265, Loss: 16.897, Final Batch Loss: 0.585\n",
      "Epoch 8266, Loss: 16.833, Final Batch Loss: 0.508\n",
      "Epoch 8267, Loss: 16.825, Final Batch Loss: 0.457\n",
      "Epoch 8268, Loss: 16.935, Final Batch Loss: 0.522\n",
      "Epoch 8269, Loss: 16.646, Final Batch Loss: 0.433\n",
      "Epoch 8270, Loss: 16.995, Final Batch Loss: 0.470\n",
      "Epoch 8271, Loss: 16.939, Final Batch Loss: 0.464\n",
      "Epoch 8272, Loss: 16.747, Final Batch Loss: 0.401\n",
      "Epoch 8273, Loss: 17.050, Final Batch Loss: 0.535\n",
      "Epoch 8274, Loss: 16.762, Final Batch Loss: 0.430\n",
      "Epoch 8275, Loss: 17.065, Final Batch Loss: 0.463\n",
      "Epoch 8276, Loss: 16.983, Final Batch Loss: 0.533\n",
      "Epoch 8277, Loss: 16.857, Final Batch Loss: 0.435\n",
      "Epoch 8278, Loss: 16.624, Final Batch Loss: 0.419\n",
      "Epoch 8279, Loss: 16.763, Final Batch Loss: 0.481\n",
      "Epoch 8280, Loss: 16.840, Final Batch Loss: 0.302\n",
      "Epoch 8281, Loss: 16.774, Final Batch Loss: 0.310\n",
      "Epoch 8282, Loss: 17.132, Final Batch Loss: 0.533\n",
      "Epoch 8283, Loss: 16.932, Final Batch Loss: 0.437\n",
      "Epoch 8284, Loss: 16.649, Final Batch Loss: 0.501\n",
      "Epoch 8285, Loss: 16.966, Final Batch Loss: 0.491\n",
      "Epoch 8286, Loss: 16.917, Final Batch Loss: 0.411\n",
      "Epoch 8287, Loss: 17.100, Final Batch Loss: 0.488\n",
      "Epoch 8288, Loss: 16.969, Final Batch Loss: 0.528\n",
      "Epoch 8289, Loss: 17.056, Final Batch Loss: 0.540\n",
      "Epoch 8290, Loss: 17.035, Final Batch Loss: 0.421\n",
      "Epoch 8291, Loss: 17.055, Final Batch Loss: 0.497\n",
      "Epoch 8292, Loss: 16.822, Final Batch Loss: 0.423\n",
      "Epoch 8293, Loss: 16.720, Final Batch Loss: 0.461\n",
      "Epoch 8294, Loss: 16.983, Final Batch Loss: 0.501\n",
      "Epoch 8295, Loss: 16.983, Final Batch Loss: 0.351\n",
      "Epoch 8296, Loss: 17.241, Final Batch Loss: 0.499\n",
      "Epoch 8297, Loss: 17.067, Final Batch Loss: 0.564\n",
      "Epoch 8298, Loss: 16.762, Final Batch Loss: 0.507\n",
      "Epoch 8299, Loss: 16.810, Final Batch Loss: 0.471\n",
      "Epoch 8300, Loss: 16.844, Final Batch Loss: 0.468\n",
      "Epoch 8301, Loss: 16.958, Final Batch Loss: 0.594\n",
      "Epoch 8302, Loss: 17.220, Final Batch Loss: 0.491\n",
      "Epoch 8303, Loss: 16.839, Final Batch Loss: 0.476\n",
      "Epoch 8304, Loss: 16.963, Final Batch Loss: 0.462\n",
      "Epoch 8305, Loss: 16.837, Final Batch Loss: 0.342\n",
      "Epoch 8306, Loss: 17.143, Final Batch Loss: 0.397\n",
      "Epoch 8307, Loss: 16.978, Final Batch Loss: 0.443\n",
      "Epoch 8308, Loss: 16.720, Final Batch Loss: 0.516\n",
      "Epoch 8309, Loss: 17.165, Final Batch Loss: 0.497\n",
      "Epoch 8310, Loss: 17.003, Final Batch Loss: 0.426\n",
      "Epoch 8311, Loss: 16.952, Final Batch Loss: 0.553\n",
      "Epoch 8312, Loss: 16.906, Final Batch Loss: 0.561\n",
      "Epoch 8313, Loss: 16.904, Final Batch Loss: 0.487\n",
      "Epoch 8314, Loss: 17.087, Final Batch Loss: 0.479\n",
      "Epoch 8315, Loss: 16.885, Final Batch Loss: 0.557\n",
      "Epoch 8316, Loss: 17.125, Final Batch Loss: 0.514\n",
      "Epoch 8317, Loss: 16.971, Final Batch Loss: 0.479\n",
      "Epoch 8318, Loss: 16.979, Final Batch Loss: 0.508\n",
      "Epoch 8319, Loss: 16.884, Final Batch Loss: 0.461\n",
      "Epoch 8320, Loss: 16.835, Final Batch Loss: 0.483\n",
      "Epoch 8321, Loss: 17.003, Final Batch Loss: 0.385\n",
      "Epoch 8322, Loss: 16.646, Final Batch Loss: 0.526\n",
      "Epoch 8323, Loss: 17.071, Final Batch Loss: 0.475\n",
      "Epoch 8324, Loss: 16.874, Final Batch Loss: 0.514\n",
      "Epoch 8325, Loss: 16.892, Final Batch Loss: 0.505\n",
      "Epoch 8326, Loss: 16.885, Final Batch Loss: 0.533\n",
      "Epoch 8327, Loss: 16.992, Final Batch Loss: 0.462\n",
      "Epoch 8328, Loss: 16.936, Final Batch Loss: 0.533\n",
      "Epoch 8329, Loss: 16.769, Final Batch Loss: 0.534\n",
      "Epoch 8330, Loss: 16.825, Final Batch Loss: 0.411\n",
      "Epoch 8331, Loss: 17.069, Final Batch Loss: 0.458\n",
      "Epoch 8332, Loss: 16.913, Final Batch Loss: 0.532\n",
      "Epoch 8333, Loss: 17.019, Final Batch Loss: 0.397\n",
      "Epoch 8334, Loss: 16.907, Final Batch Loss: 0.495\n",
      "Epoch 8335, Loss: 16.807, Final Batch Loss: 0.450\n",
      "Epoch 8336, Loss: 16.772, Final Batch Loss: 0.566\n",
      "Epoch 8337, Loss: 16.782, Final Batch Loss: 0.493\n",
      "Epoch 8338, Loss: 17.084, Final Batch Loss: 0.512\n",
      "Epoch 8339, Loss: 16.996, Final Batch Loss: 0.527\n",
      "Epoch 8340, Loss: 16.884, Final Batch Loss: 0.431\n",
      "Epoch 8341, Loss: 16.739, Final Batch Loss: 0.524\n",
      "Epoch 8342, Loss: 16.791, Final Batch Loss: 0.430\n",
      "Epoch 8343, Loss: 17.122, Final Batch Loss: 0.477\n",
      "Epoch 8344, Loss: 16.722, Final Batch Loss: 0.396\n",
      "Epoch 8345, Loss: 16.801, Final Batch Loss: 0.478\n",
      "Epoch 8346, Loss: 16.818, Final Batch Loss: 0.559\n",
      "Epoch 8347, Loss: 17.075, Final Batch Loss: 0.516\n",
      "Epoch 8348, Loss: 17.128, Final Batch Loss: 0.523\n",
      "Epoch 8349, Loss: 17.247, Final Batch Loss: 0.403\n",
      "Epoch 8350, Loss: 17.126, Final Batch Loss: 0.570\n",
      "Epoch 8351, Loss: 17.009, Final Batch Loss: 0.455\n",
      "Epoch 8352, Loss: 16.739, Final Batch Loss: 0.481\n",
      "Epoch 8353, Loss: 16.865, Final Batch Loss: 0.423\n",
      "Epoch 8354, Loss: 16.797, Final Batch Loss: 0.501\n",
      "Epoch 8355, Loss: 16.936, Final Batch Loss: 0.411\n",
      "Epoch 8356, Loss: 16.904, Final Batch Loss: 0.507\n",
      "Epoch 8357, Loss: 16.522, Final Batch Loss: 0.470\n",
      "Epoch 8358, Loss: 17.003, Final Batch Loss: 0.476\n",
      "Epoch 8359, Loss: 16.677, Final Batch Loss: 0.425\n",
      "Epoch 8360, Loss: 16.841, Final Batch Loss: 0.457\n",
      "Epoch 8361, Loss: 16.940, Final Batch Loss: 0.520\n",
      "Epoch 8362, Loss: 16.772, Final Batch Loss: 0.480\n",
      "Epoch 8363, Loss: 16.744, Final Batch Loss: 0.468\n",
      "Epoch 8364, Loss: 16.957, Final Batch Loss: 0.521\n",
      "Epoch 8365, Loss: 16.823, Final Batch Loss: 0.449\n",
      "Epoch 8366, Loss: 16.729, Final Batch Loss: 0.462\n",
      "Epoch 8367, Loss: 16.741, Final Batch Loss: 0.417\n",
      "Epoch 8368, Loss: 16.982, Final Batch Loss: 0.602\n",
      "Epoch 8369, Loss: 16.875, Final Batch Loss: 0.546\n",
      "Epoch 8370, Loss: 16.517, Final Batch Loss: 0.479\n",
      "Epoch 8371, Loss: 16.995, Final Batch Loss: 0.512\n",
      "Epoch 8372, Loss: 16.902, Final Batch Loss: 0.361\n",
      "Epoch 8373, Loss: 16.838, Final Batch Loss: 0.469\n",
      "Epoch 8374, Loss: 16.746, Final Batch Loss: 0.580\n",
      "Epoch 8375, Loss: 16.788, Final Batch Loss: 0.529\n",
      "Epoch 8376, Loss: 16.597, Final Batch Loss: 0.510\n",
      "Epoch 8377, Loss: 16.818, Final Batch Loss: 0.424\n",
      "Epoch 8378, Loss: 16.940, Final Batch Loss: 0.373\n",
      "Epoch 8379, Loss: 17.139, Final Batch Loss: 0.663\n",
      "Epoch 8380, Loss: 16.760, Final Batch Loss: 0.380\n",
      "Epoch 8381, Loss: 16.983, Final Batch Loss: 0.516\n",
      "Epoch 8382, Loss: 17.091, Final Batch Loss: 0.448\n",
      "Epoch 8383, Loss: 16.589, Final Batch Loss: 0.370\n",
      "Epoch 8384, Loss: 16.918, Final Batch Loss: 0.448\n",
      "Epoch 8385, Loss: 16.818, Final Batch Loss: 0.435\n",
      "Epoch 8386, Loss: 16.859, Final Batch Loss: 0.529\n",
      "Epoch 8387, Loss: 16.911, Final Batch Loss: 0.405\n",
      "Epoch 8388, Loss: 16.867, Final Batch Loss: 0.441\n",
      "Epoch 8389, Loss: 17.089, Final Batch Loss: 0.353\n",
      "Epoch 8390, Loss: 16.896, Final Batch Loss: 0.464\n",
      "Epoch 8391, Loss: 16.831, Final Batch Loss: 0.443\n",
      "Epoch 8392, Loss: 16.991, Final Batch Loss: 0.518\n",
      "Epoch 8393, Loss: 16.913, Final Batch Loss: 0.426\n",
      "Epoch 8394, Loss: 16.766, Final Batch Loss: 0.375\n",
      "Epoch 8395, Loss: 16.912, Final Batch Loss: 0.470\n",
      "Epoch 8396, Loss: 16.596, Final Batch Loss: 0.502\n",
      "Epoch 8397, Loss: 17.127, Final Batch Loss: 0.551\n",
      "Epoch 8398, Loss: 16.795, Final Batch Loss: 0.446\n",
      "Epoch 8399, Loss: 16.912, Final Batch Loss: 0.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8400, Loss: 16.876, Final Batch Loss: 0.407\n",
      "Epoch 8401, Loss: 17.018, Final Batch Loss: 0.521\n",
      "Epoch 8402, Loss: 16.821, Final Batch Loss: 0.469\n",
      "Epoch 8403, Loss: 17.003, Final Batch Loss: 0.553\n",
      "Epoch 8404, Loss: 16.631, Final Batch Loss: 0.445\n",
      "Epoch 8405, Loss: 16.710, Final Batch Loss: 0.364\n",
      "Epoch 8406, Loss: 17.144, Final Batch Loss: 0.471\n",
      "Epoch 8407, Loss: 16.718, Final Batch Loss: 0.473\n",
      "Epoch 8408, Loss: 16.894, Final Batch Loss: 0.414\n",
      "Epoch 8409, Loss: 16.876, Final Batch Loss: 0.546\n",
      "Epoch 8410, Loss: 16.847, Final Batch Loss: 0.449\n",
      "Epoch 8411, Loss: 16.832, Final Batch Loss: 0.435\n",
      "Epoch 8412, Loss: 16.898, Final Batch Loss: 0.540\n",
      "Epoch 8413, Loss: 16.906, Final Batch Loss: 0.425\n",
      "Epoch 8414, Loss: 16.739, Final Batch Loss: 0.460\n",
      "Epoch 8415, Loss: 16.953, Final Batch Loss: 0.588\n",
      "Epoch 8416, Loss: 16.877, Final Batch Loss: 0.442\n",
      "Epoch 8417, Loss: 16.869, Final Batch Loss: 0.425\n",
      "Epoch 8418, Loss: 16.885, Final Batch Loss: 0.445\n",
      "Epoch 8419, Loss: 16.945, Final Batch Loss: 0.479\n",
      "Epoch 8420, Loss: 16.805, Final Batch Loss: 0.312\n",
      "Epoch 8421, Loss: 16.978, Final Batch Loss: 0.615\n",
      "Epoch 8422, Loss: 16.819, Final Batch Loss: 0.469\n",
      "Epoch 8423, Loss: 16.805, Final Batch Loss: 0.454\n",
      "Epoch 8424, Loss: 16.937, Final Batch Loss: 0.498\n",
      "Epoch 8425, Loss: 16.804, Final Batch Loss: 0.455\n",
      "Epoch 8426, Loss: 16.951, Final Batch Loss: 0.546\n",
      "Epoch 8427, Loss: 16.896, Final Batch Loss: 0.495\n",
      "Epoch 8428, Loss: 16.836, Final Batch Loss: 0.446\n",
      "Epoch 8429, Loss: 16.832, Final Batch Loss: 0.400\n",
      "Epoch 8430, Loss: 17.123, Final Batch Loss: 0.474\n",
      "Epoch 8431, Loss: 16.926, Final Batch Loss: 0.362\n",
      "Epoch 8432, Loss: 16.711, Final Batch Loss: 0.464\n",
      "Epoch 8433, Loss: 16.836, Final Batch Loss: 0.363\n",
      "Epoch 8434, Loss: 16.806, Final Batch Loss: 0.515\n",
      "Epoch 8435, Loss: 17.124, Final Batch Loss: 0.401\n",
      "Epoch 8436, Loss: 16.810, Final Batch Loss: 0.526\n",
      "Epoch 8437, Loss: 16.936, Final Batch Loss: 0.463\n",
      "Epoch 8438, Loss: 16.837, Final Batch Loss: 0.333\n",
      "Epoch 8439, Loss: 16.906, Final Batch Loss: 0.354\n",
      "Epoch 8440, Loss: 16.872, Final Batch Loss: 0.430\n",
      "Epoch 8441, Loss: 16.568, Final Batch Loss: 0.381\n",
      "Epoch 8442, Loss: 16.781, Final Batch Loss: 0.493\n",
      "Epoch 8443, Loss: 16.653, Final Batch Loss: 0.425\n",
      "Epoch 8444, Loss: 16.801, Final Batch Loss: 0.379\n",
      "Epoch 8445, Loss: 17.058, Final Batch Loss: 0.555\n",
      "Epoch 8446, Loss: 16.653, Final Batch Loss: 0.411\n",
      "Epoch 8447, Loss: 16.815, Final Batch Loss: 0.387\n",
      "Epoch 8448, Loss: 16.992, Final Batch Loss: 0.498\n",
      "Epoch 8449, Loss: 17.315, Final Batch Loss: 0.518\n",
      "Epoch 8450, Loss: 16.997, Final Batch Loss: 0.403\n",
      "Epoch 8451, Loss: 16.699, Final Batch Loss: 0.456\n",
      "Epoch 8452, Loss: 17.080, Final Batch Loss: 0.441\n",
      "Epoch 8453, Loss: 17.014, Final Batch Loss: 0.419\n",
      "Epoch 8454, Loss: 16.858, Final Batch Loss: 0.424\n",
      "Epoch 8455, Loss: 16.661, Final Batch Loss: 0.465\n",
      "Epoch 8456, Loss: 16.963, Final Batch Loss: 0.543\n",
      "Epoch 8457, Loss: 17.053, Final Batch Loss: 0.473\n",
      "Epoch 8458, Loss: 16.914, Final Batch Loss: 0.470\n",
      "Epoch 8459, Loss: 16.835, Final Batch Loss: 0.453\n",
      "Epoch 8460, Loss: 17.002, Final Batch Loss: 0.434\n",
      "Epoch 8461, Loss: 16.979, Final Batch Loss: 0.501\n",
      "Epoch 8462, Loss: 17.052, Final Batch Loss: 0.460\n",
      "Epoch 8463, Loss: 16.955, Final Batch Loss: 0.516\n",
      "Epoch 8464, Loss: 16.814, Final Batch Loss: 0.474\n",
      "Epoch 8465, Loss: 16.972, Final Batch Loss: 0.624\n",
      "Epoch 8466, Loss: 17.057, Final Batch Loss: 0.551\n",
      "Epoch 8467, Loss: 16.935, Final Batch Loss: 0.424\n",
      "Epoch 8468, Loss: 17.134, Final Batch Loss: 0.561\n",
      "Epoch 8469, Loss: 17.083, Final Batch Loss: 0.518\n",
      "Epoch 8470, Loss: 16.932, Final Batch Loss: 0.486\n",
      "Epoch 8471, Loss: 16.853, Final Batch Loss: 0.373\n",
      "Epoch 8472, Loss: 16.851, Final Batch Loss: 0.464\n",
      "Epoch 8473, Loss: 16.862, Final Batch Loss: 0.518\n",
      "Epoch 8474, Loss: 17.049, Final Batch Loss: 0.464\n",
      "Epoch 8475, Loss: 17.036, Final Batch Loss: 0.553\n",
      "Epoch 8476, Loss: 16.952, Final Batch Loss: 0.434\n",
      "Epoch 8477, Loss: 16.836, Final Batch Loss: 0.487\n",
      "Epoch 8478, Loss: 17.013, Final Batch Loss: 0.483\n",
      "Epoch 8479, Loss: 16.786, Final Batch Loss: 0.445\n",
      "Epoch 8480, Loss: 16.976, Final Batch Loss: 0.346\n",
      "Epoch 8481, Loss: 16.807, Final Batch Loss: 0.433\n",
      "Epoch 8482, Loss: 17.019, Final Batch Loss: 0.560\n",
      "Epoch 8483, Loss: 16.865, Final Batch Loss: 0.400\n",
      "Epoch 8484, Loss: 16.652, Final Batch Loss: 0.515\n",
      "Epoch 8485, Loss: 16.709, Final Batch Loss: 0.485\n",
      "Epoch 8486, Loss: 17.000, Final Batch Loss: 0.405\n",
      "Epoch 8487, Loss: 16.993, Final Batch Loss: 0.435\n",
      "Epoch 8488, Loss: 17.046, Final Batch Loss: 0.539\n",
      "Epoch 8489, Loss: 17.029, Final Batch Loss: 0.443\n",
      "Epoch 8490, Loss: 17.076, Final Batch Loss: 0.554\n",
      "Epoch 8491, Loss: 16.737, Final Batch Loss: 0.387\n",
      "Epoch 8492, Loss: 16.821, Final Batch Loss: 0.421\n",
      "Epoch 8493, Loss: 16.839, Final Batch Loss: 0.564\n",
      "Epoch 8494, Loss: 16.630, Final Batch Loss: 0.489\n",
      "Epoch 8495, Loss: 16.800, Final Batch Loss: 0.471\n",
      "Epoch 8496, Loss: 16.739, Final Batch Loss: 0.495\n",
      "Epoch 8497, Loss: 16.940, Final Batch Loss: 0.455\n",
      "Epoch 8498, Loss: 16.858, Final Batch Loss: 0.467\n",
      "Epoch 8499, Loss: 16.989, Final Batch Loss: 0.435\n",
      "Epoch 8500, Loss: 16.871, Final Batch Loss: 0.472\n",
      "Epoch 8501, Loss: 16.827, Final Batch Loss: 0.445\n",
      "Epoch 8502, Loss: 16.787, Final Batch Loss: 0.424\n",
      "Epoch 8503, Loss: 17.334, Final Batch Loss: 0.592\n",
      "Epoch 8504, Loss: 16.765, Final Batch Loss: 0.354\n",
      "Epoch 8505, Loss: 16.751, Final Batch Loss: 0.493\n",
      "Epoch 8506, Loss: 16.830, Final Batch Loss: 0.404\n",
      "Epoch 8507, Loss: 17.095, Final Batch Loss: 0.451\n",
      "Epoch 8508, Loss: 17.051, Final Batch Loss: 0.484\n",
      "Epoch 8509, Loss: 16.847, Final Batch Loss: 0.491\n",
      "Epoch 8510, Loss: 16.794, Final Batch Loss: 0.475\n",
      "Epoch 8511, Loss: 17.161, Final Batch Loss: 0.524\n",
      "Epoch 8512, Loss: 17.196, Final Batch Loss: 0.396\n",
      "Epoch 8513, Loss: 16.816, Final Batch Loss: 0.524\n",
      "Epoch 8514, Loss: 16.928, Final Batch Loss: 0.575\n",
      "Epoch 8515, Loss: 17.097, Final Batch Loss: 0.534\n",
      "Epoch 8516, Loss: 16.975, Final Batch Loss: 0.508\n",
      "Epoch 8517, Loss: 17.067, Final Batch Loss: 0.538\n",
      "Epoch 8518, Loss: 16.702, Final Batch Loss: 0.601\n",
      "Epoch 8519, Loss: 17.268, Final Batch Loss: 0.478\n",
      "Epoch 8520, Loss: 16.818, Final Batch Loss: 0.484\n",
      "Epoch 8521, Loss: 16.851, Final Batch Loss: 0.363\n",
      "Epoch 8522, Loss: 17.103, Final Batch Loss: 0.470\n",
      "Epoch 8523, Loss: 16.821, Final Batch Loss: 0.557\n",
      "Epoch 8524, Loss: 16.763, Final Batch Loss: 0.373\n",
      "Epoch 8525, Loss: 16.924, Final Batch Loss: 0.371\n",
      "Epoch 8526, Loss: 16.864, Final Batch Loss: 0.458\n",
      "Epoch 8527, Loss: 17.004, Final Batch Loss: 0.407\n",
      "Epoch 8528, Loss: 16.871, Final Batch Loss: 0.359\n",
      "Epoch 8529, Loss: 16.886, Final Batch Loss: 0.432\n",
      "Epoch 8530, Loss: 16.645, Final Batch Loss: 0.480\n",
      "Epoch 8531, Loss: 16.999, Final Batch Loss: 0.379\n",
      "Epoch 8532, Loss: 16.856, Final Batch Loss: 0.320\n",
      "Epoch 8533, Loss: 17.030, Final Batch Loss: 0.432\n",
      "Epoch 8534, Loss: 16.817, Final Batch Loss: 0.454\n",
      "Epoch 8535, Loss: 17.021, Final Batch Loss: 0.523\n",
      "Epoch 8536, Loss: 17.079, Final Batch Loss: 0.537\n",
      "Epoch 8537, Loss: 17.109, Final Batch Loss: 0.441\n",
      "Epoch 8538, Loss: 16.922, Final Batch Loss: 0.486\n",
      "Epoch 8539, Loss: 16.886, Final Batch Loss: 0.437\n",
      "Epoch 8540, Loss: 16.878, Final Batch Loss: 0.576\n",
      "Epoch 8541, Loss: 16.912, Final Batch Loss: 0.412\n",
      "Epoch 8542, Loss: 16.902, Final Batch Loss: 0.492\n",
      "Epoch 8543, Loss: 16.866, Final Batch Loss: 0.420\n",
      "Epoch 8544, Loss: 17.047, Final Batch Loss: 0.445\n",
      "Epoch 8545, Loss: 16.905, Final Batch Loss: 0.372\n",
      "Epoch 8546, Loss: 16.870, Final Batch Loss: 0.427\n",
      "Epoch 8547, Loss: 16.684, Final Batch Loss: 0.400\n",
      "Epoch 8548, Loss: 17.107, Final Batch Loss: 0.491\n",
      "Epoch 8549, Loss: 16.789, Final Batch Loss: 0.446\n",
      "Epoch 8550, Loss: 16.848, Final Batch Loss: 0.509\n",
      "Epoch 8551, Loss: 16.860, Final Batch Loss: 0.540\n",
      "Epoch 8552, Loss: 16.848, Final Batch Loss: 0.510\n",
      "Epoch 8553, Loss: 16.830, Final Batch Loss: 0.399\n",
      "Epoch 8554, Loss: 16.973, Final Batch Loss: 0.449\n",
      "Epoch 8555, Loss: 16.860, Final Batch Loss: 0.478\n",
      "Epoch 8556, Loss: 16.742, Final Batch Loss: 0.457\n",
      "Epoch 8557, Loss: 16.626, Final Batch Loss: 0.443\n",
      "Epoch 8558, Loss: 16.967, Final Batch Loss: 0.531\n",
      "Epoch 8559, Loss: 16.763, Final Batch Loss: 0.456\n",
      "Epoch 8560, Loss: 16.980, Final Batch Loss: 0.486\n",
      "Epoch 8561, Loss: 16.803, Final Batch Loss: 0.567\n",
      "Epoch 8562, Loss: 17.097, Final Batch Loss: 0.488\n",
      "Epoch 8563, Loss: 16.770, Final Batch Loss: 0.439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8564, Loss: 17.030, Final Batch Loss: 0.508\n",
      "Epoch 8565, Loss: 16.941, Final Batch Loss: 0.501\n",
      "Epoch 8566, Loss: 17.166, Final Batch Loss: 0.438\n",
      "Epoch 8567, Loss: 16.782, Final Batch Loss: 0.490\n",
      "Epoch 8568, Loss: 16.972, Final Batch Loss: 0.470\n",
      "Epoch 8569, Loss: 16.623, Final Batch Loss: 0.451\n",
      "Epoch 8570, Loss: 16.626, Final Batch Loss: 0.416\n",
      "Epoch 8571, Loss: 16.890, Final Batch Loss: 0.493\n",
      "Epoch 8572, Loss: 16.948, Final Batch Loss: 0.418\n",
      "Epoch 8573, Loss: 16.859, Final Batch Loss: 0.463\n",
      "Epoch 8574, Loss: 16.761, Final Batch Loss: 0.412\n",
      "Epoch 8575, Loss: 16.806, Final Batch Loss: 0.443\n",
      "Epoch 8576, Loss: 17.016, Final Batch Loss: 0.501\n",
      "Epoch 8577, Loss: 16.665, Final Batch Loss: 0.417\n",
      "Epoch 8578, Loss: 16.876, Final Batch Loss: 0.456\n",
      "Epoch 8579, Loss: 17.118, Final Batch Loss: 0.522\n",
      "Epoch 8580, Loss: 16.942, Final Batch Loss: 0.441\n",
      "Epoch 8581, Loss: 16.781, Final Batch Loss: 0.429\n",
      "Epoch 8582, Loss: 17.219, Final Batch Loss: 0.554\n",
      "Epoch 8583, Loss: 16.856, Final Batch Loss: 0.463\n",
      "Epoch 8584, Loss: 16.814, Final Batch Loss: 0.458\n",
      "Epoch 8585, Loss: 16.873, Final Batch Loss: 0.396\n",
      "Epoch 8586, Loss: 17.132, Final Batch Loss: 0.597\n",
      "Epoch 8587, Loss: 16.980, Final Batch Loss: 0.448\n",
      "Epoch 8588, Loss: 16.927, Final Batch Loss: 0.455\n",
      "Epoch 8589, Loss: 16.645, Final Batch Loss: 0.382\n",
      "Epoch 8590, Loss: 16.856, Final Batch Loss: 0.475\n",
      "Epoch 8591, Loss: 16.876, Final Batch Loss: 0.410\n",
      "Epoch 8592, Loss: 16.912, Final Batch Loss: 0.524\n",
      "Epoch 8593, Loss: 16.711, Final Batch Loss: 0.463\n",
      "Epoch 8594, Loss: 16.885, Final Batch Loss: 0.532\n",
      "Epoch 8595, Loss: 16.883, Final Batch Loss: 0.443\n",
      "Epoch 8596, Loss: 16.994, Final Batch Loss: 0.397\n",
      "Epoch 8597, Loss: 16.768, Final Batch Loss: 0.421\n",
      "Epoch 8598, Loss: 16.947, Final Batch Loss: 0.437\n",
      "Epoch 8599, Loss: 16.767, Final Batch Loss: 0.423\n",
      "Epoch 8600, Loss: 16.983, Final Batch Loss: 0.594\n",
      "Epoch 8601, Loss: 16.814, Final Batch Loss: 0.582\n",
      "Epoch 8602, Loss: 17.035, Final Batch Loss: 0.401\n",
      "Epoch 8603, Loss: 16.855, Final Batch Loss: 0.526\n",
      "Epoch 8604, Loss: 16.858, Final Batch Loss: 0.372\n",
      "Epoch 8605, Loss: 16.755, Final Batch Loss: 0.387\n",
      "Epoch 8606, Loss: 16.790, Final Batch Loss: 0.406\n",
      "Epoch 8607, Loss: 16.955, Final Batch Loss: 0.431\n",
      "Epoch 8608, Loss: 16.811, Final Batch Loss: 0.468\n",
      "Epoch 8609, Loss: 16.612, Final Batch Loss: 0.415\n",
      "Epoch 8610, Loss: 16.769, Final Batch Loss: 0.495\n",
      "Epoch 8611, Loss: 17.005, Final Batch Loss: 0.418\n",
      "Epoch 8612, Loss: 16.937, Final Batch Loss: 0.350\n",
      "Epoch 8613, Loss: 16.898, Final Batch Loss: 0.484\n",
      "Epoch 8614, Loss: 17.012, Final Batch Loss: 0.417\n",
      "Epoch 8615, Loss: 16.897, Final Batch Loss: 0.459\n",
      "Epoch 8616, Loss: 16.896, Final Batch Loss: 0.533\n",
      "Epoch 8617, Loss: 16.710, Final Batch Loss: 0.450\n",
      "Epoch 8618, Loss: 16.951, Final Batch Loss: 0.522\n",
      "Epoch 8619, Loss: 16.961, Final Batch Loss: 0.378\n",
      "Epoch 8620, Loss: 16.809, Final Batch Loss: 0.429\n",
      "Epoch 8621, Loss: 16.960, Final Batch Loss: 0.411\n",
      "Epoch 8622, Loss: 16.835, Final Batch Loss: 0.494\n",
      "Epoch 8623, Loss: 16.883, Final Batch Loss: 0.509\n",
      "Epoch 8624, Loss: 16.716, Final Batch Loss: 0.365\n",
      "Epoch 8625, Loss: 16.690, Final Batch Loss: 0.495\n",
      "Epoch 8626, Loss: 17.014, Final Batch Loss: 0.385\n",
      "Epoch 8627, Loss: 16.923, Final Batch Loss: 0.459\n",
      "Epoch 8628, Loss: 17.005, Final Batch Loss: 0.522\n",
      "Epoch 8629, Loss: 16.516, Final Batch Loss: 0.544\n",
      "Epoch 8630, Loss: 16.901, Final Batch Loss: 0.465\n",
      "Epoch 8631, Loss: 16.883, Final Batch Loss: 0.476\n",
      "Epoch 8632, Loss: 16.747, Final Batch Loss: 0.443\n",
      "Epoch 8633, Loss: 17.039, Final Batch Loss: 0.629\n",
      "Epoch 8634, Loss: 16.797, Final Batch Loss: 0.444\n",
      "Epoch 8635, Loss: 16.952, Final Batch Loss: 0.545\n",
      "Epoch 8636, Loss: 16.864, Final Batch Loss: 0.421\n",
      "Epoch 8637, Loss: 16.756, Final Batch Loss: 0.429\n",
      "Epoch 8638, Loss: 16.801, Final Batch Loss: 0.408\n",
      "Epoch 8639, Loss: 16.866, Final Batch Loss: 0.499\n",
      "Epoch 8640, Loss: 16.766, Final Batch Loss: 0.488\n",
      "Epoch 8641, Loss: 16.697, Final Batch Loss: 0.449\n",
      "Epoch 8642, Loss: 16.670, Final Batch Loss: 0.428\n",
      "Epoch 8643, Loss: 16.685, Final Batch Loss: 0.479\n",
      "Epoch 8644, Loss: 16.969, Final Batch Loss: 0.514\n",
      "Epoch 8645, Loss: 16.851, Final Batch Loss: 0.469\n",
      "Epoch 8646, Loss: 17.038, Final Batch Loss: 0.446\n",
      "Epoch 8647, Loss: 16.972, Final Batch Loss: 0.437\n",
      "Epoch 8648, Loss: 17.001, Final Batch Loss: 0.443\n",
      "Epoch 8649, Loss: 16.923, Final Batch Loss: 0.435\n",
      "Epoch 8650, Loss: 16.941, Final Batch Loss: 0.435\n",
      "Epoch 8651, Loss: 16.934, Final Batch Loss: 0.518\n",
      "Epoch 8652, Loss: 16.924, Final Batch Loss: 0.444\n",
      "Epoch 8653, Loss: 17.131, Final Batch Loss: 0.472\n",
      "Epoch 8654, Loss: 17.021, Final Batch Loss: 0.541\n",
      "Epoch 8655, Loss: 16.853, Final Batch Loss: 0.370\n",
      "Epoch 8656, Loss: 16.818, Final Batch Loss: 0.477\n",
      "Epoch 8657, Loss: 16.814, Final Batch Loss: 0.506\n",
      "Epoch 8658, Loss: 16.935, Final Batch Loss: 0.521\n",
      "Epoch 8659, Loss: 16.767, Final Batch Loss: 0.431\n",
      "Epoch 8660, Loss: 17.109, Final Batch Loss: 0.449\n",
      "Epoch 8661, Loss: 16.748, Final Batch Loss: 0.379\n",
      "Epoch 8662, Loss: 16.877, Final Batch Loss: 0.491\n",
      "Epoch 8663, Loss: 16.791, Final Batch Loss: 0.431\n",
      "Epoch 8664, Loss: 17.203, Final Batch Loss: 0.558\n",
      "Epoch 8665, Loss: 16.909, Final Batch Loss: 0.494\n",
      "Epoch 8666, Loss: 16.847, Final Batch Loss: 0.419\n",
      "Epoch 8667, Loss: 16.882, Final Batch Loss: 0.487\n",
      "Epoch 8668, Loss: 16.823, Final Batch Loss: 0.425\n",
      "Epoch 8669, Loss: 16.764, Final Batch Loss: 0.349\n",
      "Epoch 8670, Loss: 16.602, Final Batch Loss: 0.388\n",
      "Epoch 8671, Loss: 16.941, Final Batch Loss: 0.431\n",
      "Epoch 8672, Loss: 16.954, Final Batch Loss: 0.517\n",
      "Epoch 8673, Loss: 17.011, Final Batch Loss: 0.502\n",
      "Epoch 8674, Loss: 17.035, Final Batch Loss: 0.520\n",
      "Epoch 8675, Loss: 16.792, Final Batch Loss: 0.556\n",
      "Epoch 8676, Loss: 16.824, Final Batch Loss: 0.498\n",
      "Epoch 8677, Loss: 16.678, Final Batch Loss: 0.593\n",
      "Epoch 8678, Loss: 17.049, Final Batch Loss: 0.373\n",
      "Epoch 8679, Loss: 16.957, Final Batch Loss: 0.416\n",
      "Epoch 8680, Loss: 16.714, Final Batch Loss: 0.395\n",
      "Epoch 8681, Loss: 16.904, Final Batch Loss: 0.454\n",
      "Epoch 8682, Loss: 16.933, Final Batch Loss: 0.499\n",
      "Epoch 8683, Loss: 17.021, Final Batch Loss: 0.450\n",
      "Epoch 8684, Loss: 17.176, Final Batch Loss: 0.453\n",
      "Epoch 8685, Loss: 16.706, Final Batch Loss: 0.393\n",
      "Epoch 8686, Loss: 16.920, Final Batch Loss: 0.599\n",
      "Epoch 8687, Loss: 16.951, Final Batch Loss: 0.511\n",
      "Epoch 8688, Loss: 16.810, Final Batch Loss: 0.532\n",
      "Epoch 8689, Loss: 16.913, Final Batch Loss: 0.486\n",
      "Epoch 8690, Loss: 17.028, Final Batch Loss: 0.467\n",
      "Epoch 8691, Loss: 16.915, Final Batch Loss: 0.586\n",
      "Epoch 8692, Loss: 17.134, Final Batch Loss: 0.644\n",
      "Epoch 8693, Loss: 16.820, Final Batch Loss: 0.487\n",
      "Epoch 8694, Loss: 16.890, Final Batch Loss: 0.469\n",
      "Epoch 8695, Loss: 17.145, Final Batch Loss: 0.483\n",
      "Epoch 8696, Loss: 16.806, Final Batch Loss: 0.498\n",
      "Epoch 8697, Loss: 17.024, Final Batch Loss: 0.451\n",
      "Epoch 8698, Loss: 16.702, Final Batch Loss: 0.442\n",
      "Epoch 8699, Loss: 17.043, Final Batch Loss: 0.433\n",
      "Epoch 8700, Loss: 16.934, Final Batch Loss: 0.610\n",
      "Epoch 8701, Loss: 16.849, Final Batch Loss: 0.515\n",
      "Epoch 8702, Loss: 16.968, Final Batch Loss: 0.497\n",
      "Epoch 8703, Loss: 16.812, Final Batch Loss: 0.478\n",
      "Epoch 8704, Loss: 16.975, Final Batch Loss: 0.456\n",
      "Epoch 8705, Loss: 16.624, Final Batch Loss: 0.532\n",
      "Epoch 8706, Loss: 16.908, Final Batch Loss: 0.481\n",
      "Epoch 8707, Loss: 16.671, Final Batch Loss: 0.534\n",
      "Epoch 8708, Loss: 16.811, Final Batch Loss: 0.661\n",
      "Epoch 8709, Loss: 16.996, Final Batch Loss: 0.492\n",
      "Epoch 8710, Loss: 16.652, Final Batch Loss: 0.476\n",
      "Epoch 8711, Loss: 17.019, Final Batch Loss: 0.493\n",
      "Epoch 8712, Loss: 16.638, Final Batch Loss: 0.416\n",
      "Epoch 8713, Loss: 16.786, Final Batch Loss: 0.447\n",
      "Epoch 8714, Loss: 16.774, Final Batch Loss: 0.461\n",
      "Epoch 8715, Loss: 16.820, Final Batch Loss: 0.458\n",
      "Epoch 8716, Loss: 16.867, Final Batch Loss: 0.448\n",
      "Epoch 8717, Loss: 16.736, Final Batch Loss: 0.351\n",
      "Epoch 8718, Loss: 16.774, Final Batch Loss: 0.509\n",
      "Epoch 8719, Loss: 16.876, Final Batch Loss: 0.543\n",
      "Epoch 8720, Loss: 16.769, Final Batch Loss: 0.467\n",
      "Epoch 8721, Loss: 16.870, Final Batch Loss: 0.394\n",
      "Epoch 8722, Loss: 17.090, Final Batch Loss: 0.436\n",
      "Epoch 8723, Loss: 16.780, Final Batch Loss: 0.400\n",
      "Epoch 8724, Loss: 16.692, Final Batch Loss: 0.462\n",
      "Epoch 8725, Loss: 16.735, Final Batch Loss: 0.434\n",
      "Epoch 8726, Loss: 16.776, Final Batch Loss: 0.431\n",
      "Epoch 8727, Loss: 16.687, Final Batch Loss: 0.461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8728, Loss: 17.134, Final Batch Loss: 0.479\n",
      "Epoch 8729, Loss: 16.589, Final Batch Loss: 0.469\n",
      "Epoch 8730, Loss: 16.790, Final Batch Loss: 0.437\n",
      "Epoch 8731, Loss: 16.795, Final Batch Loss: 0.346\n",
      "Epoch 8732, Loss: 16.535, Final Batch Loss: 0.430\n",
      "Epoch 8733, Loss: 16.733, Final Batch Loss: 0.376\n",
      "Epoch 8734, Loss: 16.747, Final Batch Loss: 0.431\n",
      "Epoch 8735, Loss: 16.849, Final Batch Loss: 0.461\n",
      "Epoch 8736, Loss: 17.046, Final Batch Loss: 0.505\n",
      "Epoch 8737, Loss: 16.845, Final Batch Loss: 0.478\n",
      "Epoch 8738, Loss: 17.044, Final Batch Loss: 0.501\n",
      "Epoch 8739, Loss: 16.965, Final Batch Loss: 0.433\n",
      "Epoch 8740, Loss: 16.988, Final Batch Loss: 0.401\n",
      "Epoch 8741, Loss: 16.606, Final Batch Loss: 0.506\n",
      "Epoch 8742, Loss: 16.864, Final Batch Loss: 0.599\n",
      "Epoch 8743, Loss: 16.759, Final Batch Loss: 0.430\n",
      "Epoch 8744, Loss: 17.028, Final Batch Loss: 0.542\n",
      "Epoch 8745, Loss: 16.692, Final Batch Loss: 0.466\n",
      "Epoch 8746, Loss: 17.005, Final Batch Loss: 0.494\n",
      "Epoch 8747, Loss: 16.815, Final Batch Loss: 0.441\n",
      "Epoch 8748, Loss: 16.788, Final Batch Loss: 0.537\n",
      "Epoch 8749, Loss: 16.814, Final Batch Loss: 0.474\n",
      "Epoch 8750, Loss: 17.011, Final Batch Loss: 0.434\n",
      "Epoch 8751, Loss: 16.618, Final Batch Loss: 0.457\n",
      "Epoch 8752, Loss: 16.914, Final Batch Loss: 0.526\n",
      "Epoch 8753, Loss: 16.840, Final Batch Loss: 0.447\n",
      "Epoch 8754, Loss: 16.953, Final Batch Loss: 0.460\n",
      "Epoch 8755, Loss: 16.934, Final Batch Loss: 0.505\n",
      "Epoch 8756, Loss: 16.849, Final Batch Loss: 0.458\n",
      "Epoch 8757, Loss: 16.957, Final Batch Loss: 0.451\n",
      "Epoch 8758, Loss: 16.925, Final Batch Loss: 0.365\n",
      "Epoch 8759, Loss: 17.008, Final Batch Loss: 0.529\n",
      "Epoch 8760, Loss: 16.822, Final Batch Loss: 0.469\n",
      "Epoch 8761, Loss: 16.725, Final Batch Loss: 0.523\n",
      "Epoch 8762, Loss: 16.786, Final Batch Loss: 0.374\n",
      "Epoch 8763, Loss: 16.804, Final Batch Loss: 0.410\n",
      "Epoch 8764, Loss: 17.018, Final Batch Loss: 0.495\n",
      "Epoch 8765, Loss: 16.895, Final Batch Loss: 0.573\n",
      "Epoch 8766, Loss: 16.782, Final Batch Loss: 0.536\n",
      "Epoch 8767, Loss: 16.809, Final Batch Loss: 0.413\n",
      "Epoch 8768, Loss: 16.930, Final Batch Loss: 0.543\n",
      "Epoch 8769, Loss: 16.876, Final Batch Loss: 0.421\n",
      "Epoch 8770, Loss: 16.864, Final Batch Loss: 0.464\n",
      "Epoch 8771, Loss: 16.775, Final Batch Loss: 0.396\n",
      "Epoch 8772, Loss: 16.853, Final Batch Loss: 0.429\n",
      "Epoch 8773, Loss: 16.909, Final Batch Loss: 0.440\n",
      "Epoch 8774, Loss: 16.990, Final Batch Loss: 0.549\n",
      "Epoch 8775, Loss: 16.688, Final Batch Loss: 0.424\n",
      "Epoch 8776, Loss: 16.996, Final Batch Loss: 0.454\n",
      "Epoch 8777, Loss: 16.791, Final Batch Loss: 0.476\n",
      "Epoch 8778, Loss: 16.657, Final Batch Loss: 0.503\n",
      "Epoch 8779, Loss: 17.213, Final Batch Loss: 0.457\n",
      "Epoch 8780, Loss: 16.694, Final Batch Loss: 0.450\n",
      "Epoch 8781, Loss: 16.806, Final Batch Loss: 0.372\n",
      "Epoch 8782, Loss: 16.834, Final Batch Loss: 0.468\n",
      "Epoch 8783, Loss: 16.968, Final Batch Loss: 0.410\n",
      "Epoch 8784, Loss: 16.671, Final Batch Loss: 0.361\n",
      "Epoch 8785, Loss: 16.675, Final Batch Loss: 0.386\n",
      "Epoch 8786, Loss: 16.741, Final Batch Loss: 0.464\n",
      "Epoch 8787, Loss: 16.989, Final Batch Loss: 0.439\n",
      "Epoch 8788, Loss: 16.535, Final Batch Loss: 0.487\n",
      "Epoch 8789, Loss: 16.623, Final Batch Loss: 0.474\n",
      "Epoch 8790, Loss: 17.258, Final Batch Loss: 0.420\n",
      "Epoch 8791, Loss: 16.939, Final Batch Loss: 0.451\n",
      "Epoch 8792, Loss: 16.924, Final Batch Loss: 0.444\n",
      "Epoch 8793, Loss: 16.770, Final Batch Loss: 0.443\n",
      "Epoch 8794, Loss: 16.827, Final Batch Loss: 0.443\n",
      "Epoch 8795, Loss: 16.870, Final Batch Loss: 0.457\n",
      "Epoch 8796, Loss: 16.772, Final Batch Loss: 0.429\n",
      "Epoch 8797, Loss: 16.891, Final Batch Loss: 0.482\n",
      "Epoch 8798, Loss: 16.663, Final Batch Loss: 0.370\n",
      "Epoch 8799, Loss: 16.828, Final Batch Loss: 0.392\n",
      "Epoch 8800, Loss: 16.620, Final Batch Loss: 0.459\n",
      "Epoch 8801, Loss: 16.756, Final Batch Loss: 0.356\n",
      "Epoch 8802, Loss: 16.783, Final Batch Loss: 0.589\n",
      "Epoch 8803, Loss: 16.955, Final Batch Loss: 0.478\n",
      "Epoch 8804, Loss: 16.773, Final Batch Loss: 0.364\n",
      "Epoch 8805, Loss: 16.785, Final Batch Loss: 0.600\n",
      "Epoch 8806, Loss: 16.669, Final Batch Loss: 0.425\n",
      "Epoch 8807, Loss: 17.004, Final Batch Loss: 0.398\n",
      "Epoch 8808, Loss: 16.936, Final Batch Loss: 0.474\n",
      "Epoch 8809, Loss: 16.775, Final Batch Loss: 0.372\n",
      "Epoch 8810, Loss: 16.589, Final Batch Loss: 0.525\n",
      "Epoch 8811, Loss: 16.770, Final Batch Loss: 0.524\n",
      "Epoch 8812, Loss: 17.226, Final Batch Loss: 0.537\n",
      "Epoch 8813, Loss: 16.847, Final Batch Loss: 0.490\n",
      "Epoch 8814, Loss: 16.789, Final Batch Loss: 0.353\n",
      "Epoch 8815, Loss: 16.797, Final Batch Loss: 0.489\n",
      "Epoch 8816, Loss: 16.731, Final Batch Loss: 0.375\n",
      "Epoch 8817, Loss: 16.942, Final Batch Loss: 0.538\n",
      "Epoch 8818, Loss: 16.758, Final Batch Loss: 0.546\n",
      "Epoch 8819, Loss: 16.677, Final Batch Loss: 0.445\n",
      "Epoch 8820, Loss: 16.739, Final Batch Loss: 0.572\n",
      "Epoch 8821, Loss: 16.911, Final Batch Loss: 0.556\n",
      "Epoch 8822, Loss: 16.780, Final Batch Loss: 0.380\n",
      "Epoch 8823, Loss: 16.622, Final Batch Loss: 0.491\n",
      "Epoch 8824, Loss: 16.832, Final Batch Loss: 0.471\n",
      "Epoch 8825, Loss: 16.836, Final Batch Loss: 0.578\n",
      "Epoch 8826, Loss: 16.939, Final Batch Loss: 0.557\n",
      "Epoch 8827, Loss: 16.704, Final Batch Loss: 0.452\n",
      "Epoch 8828, Loss: 16.688, Final Batch Loss: 0.468\n",
      "Epoch 8829, Loss: 16.669, Final Batch Loss: 0.449\n",
      "Epoch 8830, Loss: 16.609, Final Batch Loss: 0.324\n",
      "Epoch 8831, Loss: 16.817, Final Batch Loss: 0.484\n",
      "Epoch 8832, Loss: 16.844, Final Batch Loss: 0.597\n",
      "Epoch 8833, Loss: 16.829, Final Batch Loss: 0.419\n",
      "Epoch 8834, Loss: 16.991, Final Batch Loss: 0.472\n",
      "Epoch 8835, Loss: 16.809, Final Batch Loss: 0.464\n",
      "Epoch 8836, Loss: 16.841, Final Batch Loss: 0.505\n",
      "Epoch 8837, Loss: 17.065, Final Batch Loss: 0.516\n",
      "Epoch 8838, Loss: 16.737, Final Batch Loss: 0.435\n",
      "Epoch 8839, Loss: 16.816, Final Batch Loss: 0.534\n",
      "Epoch 8840, Loss: 16.778, Final Batch Loss: 0.485\n",
      "Epoch 8841, Loss: 16.989, Final Batch Loss: 0.504\n",
      "Epoch 8842, Loss: 16.891, Final Batch Loss: 0.377\n",
      "Epoch 8843, Loss: 16.657, Final Batch Loss: 0.447\n",
      "Epoch 8844, Loss: 16.677, Final Batch Loss: 0.481\n",
      "Epoch 8845, Loss: 16.789, Final Batch Loss: 0.492\n",
      "Epoch 8846, Loss: 16.789, Final Batch Loss: 0.477\n",
      "Epoch 8847, Loss: 16.686, Final Batch Loss: 0.389\n",
      "Epoch 8848, Loss: 16.759, Final Batch Loss: 0.449\n",
      "Epoch 8849, Loss: 17.235, Final Batch Loss: 0.561\n",
      "Epoch 8850, Loss: 16.864, Final Batch Loss: 0.535\n",
      "Epoch 8851, Loss: 16.827, Final Batch Loss: 0.516\n",
      "Epoch 8852, Loss: 17.010, Final Batch Loss: 0.466\n",
      "Epoch 8853, Loss: 16.635, Final Batch Loss: 0.401\n",
      "Epoch 8854, Loss: 16.567, Final Batch Loss: 0.438\n",
      "Epoch 8855, Loss: 16.920, Final Batch Loss: 0.528\n",
      "Epoch 8856, Loss: 17.015, Final Batch Loss: 0.460\n",
      "Epoch 8857, Loss: 16.842, Final Batch Loss: 0.414\n",
      "Epoch 8858, Loss: 16.697, Final Batch Loss: 0.422\n",
      "Epoch 8859, Loss: 16.887, Final Batch Loss: 0.435\n",
      "Epoch 8860, Loss: 16.918, Final Batch Loss: 0.500\n",
      "Epoch 8861, Loss: 16.755, Final Batch Loss: 0.466\n",
      "Epoch 8862, Loss: 16.941, Final Batch Loss: 0.400\n",
      "Epoch 8863, Loss: 16.923, Final Batch Loss: 0.494\n",
      "Epoch 8864, Loss: 17.054, Final Batch Loss: 0.490\n",
      "Epoch 8865, Loss: 17.042, Final Batch Loss: 0.419\n",
      "Epoch 8866, Loss: 16.918, Final Batch Loss: 0.372\n",
      "Epoch 8867, Loss: 16.963, Final Batch Loss: 0.491\n",
      "Epoch 8868, Loss: 16.685, Final Batch Loss: 0.540\n",
      "Epoch 8869, Loss: 16.974, Final Batch Loss: 0.527\n",
      "Epoch 8870, Loss: 16.876, Final Batch Loss: 0.415\n",
      "Epoch 8871, Loss: 16.752, Final Batch Loss: 0.336\n",
      "Epoch 8872, Loss: 16.724, Final Batch Loss: 0.578\n",
      "Epoch 8873, Loss: 16.841, Final Batch Loss: 0.325\n",
      "Epoch 8874, Loss: 17.021, Final Batch Loss: 0.615\n",
      "Epoch 8875, Loss: 16.914, Final Batch Loss: 0.449\n",
      "Epoch 8876, Loss: 16.978, Final Batch Loss: 0.462\n",
      "Epoch 8877, Loss: 16.900, Final Batch Loss: 0.459\n",
      "Epoch 8878, Loss: 17.081, Final Batch Loss: 0.392\n",
      "Epoch 8879, Loss: 16.927, Final Batch Loss: 0.520\n",
      "Epoch 8880, Loss: 16.956, Final Batch Loss: 0.456\n",
      "Epoch 8881, Loss: 16.761, Final Batch Loss: 0.537\n",
      "Epoch 8882, Loss: 16.737, Final Batch Loss: 0.429\n",
      "Epoch 8883, Loss: 16.622, Final Batch Loss: 0.446\n",
      "Epoch 8884, Loss: 16.839, Final Batch Loss: 0.447\n",
      "Epoch 8885, Loss: 16.891, Final Batch Loss: 0.493\n",
      "Epoch 8886, Loss: 16.901, Final Batch Loss: 0.406\n",
      "Epoch 8887, Loss: 16.874, Final Batch Loss: 0.540\n",
      "Epoch 8888, Loss: 16.926, Final Batch Loss: 0.511\n",
      "Epoch 8889, Loss: 17.108, Final Batch Loss: 0.465\n",
      "Epoch 8890, Loss: 16.811, Final Batch Loss: 0.461\n",
      "Epoch 8891, Loss: 16.645, Final Batch Loss: 0.316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8892, Loss: 16.811, Final Batch Loss: 0.403\n",
      "Epoch 8893, Loss: 16.867, Final Batch Loss: 0.470\n",
      "Epoch 8894, Loss: 16.762, Final Batch Loss: 0.424\n",
      "Epoch 8895, Loss: 16.997, Final Batch Loss: 0.399\n",
      "Epoch 8896, Loss: 16.803, Final Batch Loss: 0.484\n",
      "Epoch 8897, Loss: 17.029, Final Batch Loss: 0.554\n",
      "Epoch 8898, Loss: 16.960, Final Batch Loss: 0.454\n",
      "Epoch 8899, Loss: 16.726, Final Batch Loss: 0.470\n",
      "Epoch 8900, Loss: 16.889, Final Batch Loss: 0.543\n",
      "Epoch 8901, Loss: 16.865, Final Batch Loss: 0.421\n",
      "Epoch 8902, Loss: 16.895, Final Batch Loss: 0.593\n",
      "Epoch 8903, Loss: 16.812, Final Batch Loss: 0.434\n",
      "Epoch 8904, Loss: 16.749, Final Batch Loss: 0.401\n",
      "Epoch 8905, Loss: 16.497, Final Batch Loss: 0.445\n",
      "Epoch 8906, Loss: 16.684, Final Batch Loss: 0.443\n",
      "Epoch 8907, Loss: 17.007, Final Batch Loss: 0.503\n",
      "Epoch 8908, Loss: 17.171, Final Batch Loss: 0.484\n",
      "Epoch 8909, Loss: 16.968, Final Batch Loss: 0.449\n",
      "Epoch 8910, Loss: 16.936, Final Batch Loss: 0.596\n",
      "Epoch 8911, Loss: 16.855, Final Batch Loss: 0.458\n",
      "Epoch 8912, Loss: 16.878, Final Batch Loss: 0.507\n",
      "Epoch 8913, Loss: 16.953, Final Batch Loss: 0.422\n",
      "Epoch 8914, Loss: 16.836, Final Batch Loss: 0.424\n",
      "Epoch 8915, Loss: 16.871, Final Batch Loss: 0.556\n",
      "Epoch 8916, Loss: 16.801, Final Batch Loss: 0.423\n",
      "Epoch 8917, Loss: 16.924, Final Batch Loss: 0.455\n",
      "Epoch 8918, Loss: 16.658, Final Batch Loss: 0.490\n",
      "Epoch 8919, Loss: 16.870, Final Batch Loss: 0.511\n",
      "Epoch 8920, Loss: 16.474, Final Batch Loss: 0.333\n",
      "Epoch 8921, Loss: 17.186, Final Batch Loss: 0.633\n",
      "Epoch 8922, Loss: 16.822, Final Batch Loss: 0.541\n",
      "Epoch 8923, Loss: 16.735, Final Batch Loss: 0.497\n",
      "Epoch 8924, Loss: 16.957, Final Batch Loss: 0.377\n",
      "Epoch 8925, Loss: 16.726, Final Batch Loss: 0.413\n",
      "Epoch 8926, Loss: 16.950, Final Batch Loss: 0.412\n",
      "Epoch 8927, Loss: 16.991, Final Batch Loss: 0.450\n",
      "Epoch 8928, Loss: 16.881, Final Batch Loss: 0.502\n",
      "Epoch 8929, Loss: 16.899, Final Batch Loss: 0.470\n",
      "Epoch 8930, Loss: 17.049, Final Batch Loss: 0.545\n",
      "Epoch 8931, Loss: 16.864, Final Batch Loss: 0.501\n",
      "Epoch 8932, Loss: 16.997, Final Batch Loss: 0.479\n",
      "Epoch 8933, Loss: 16.906, Final Batch Loss: 0.519\n",
      "Epoch 8934, Loss: 16.948, Final Batch Loss: 0.499\n",
      "Epoch 8935, Loss: 16.956, Final Batch Loss: 0.477\n",
      "Epoch 8936, Loss: 16.753, Final Batch Loss: 0.338\n",
      "Epoch 8937, Loss: 16.860, Final Batch Loss: 0.386\n",
      "Epoch 8938, Loss: 16.687, Final Batch Loss: 0.394\n",
      "Epoch 8939, Loss: 17.001, Final Batch Loss: 0.569\n",
      "Epoch 8940, Loss: 17.021, Final Batch Loss: 0.407\n",
      "Epoch 8941, Loss: 16.840, Final Batch Loss: 0.420\n",
      "Epoch 8942, Loss: 16.522, Final Batch Loss: 0.517\n",
      "Epoch 8943, Loss: 17.003, Final Batch Loss: 0.512\n",
      "Epoch 8944, Loss: 16.910, Final Batch Loss: 0.502\n",
      "Epoch 8945, Loss: 16.622, Final Batch Loss: 0.440\n",
      "Epoch 8946, Loss: 16.953, Final Batch Loss: 0.516\n",
      "Epoch 8947, Loss: 17.122, Final Batch Loss: 0.601\n",
      "Epoch 8948, Loss: 16.490, Final Batch Loss: 0.537\n",
      "Epoch 8949, Loss: 16.824, Final Batch Loss: 0.488\n",
      "Epoch 8950, Loss: 16.744, Final Batch Loss: 0.555\n",
      "Epoch 8951, Loss: 17.086, Final Batch Loss: 0.494\n",
      "Epoch 8952, Loss: 16.865, Final Batch Loss: 0.483\n",
      "Epoch 8953, Loss: 17.012, Final Batch Loss: 0.477\n",
      "Epoch 8954, Loss: 16.673, Final Batch Loss: 0.388\n",
      "Epoch 8955, Loss: 16.382, Final Batch Loss: 0.394\n",
      "Epoch 8956, Loss: 16.617, Final Batch Loss: 0.392\n",
      "Epoch 8957, Loss: 16.862, Final Batch Loss: 0.428\n",
      "Epoch 8958, Loss: 16.956, Final Batch Loss: 0.457\n",
      "Epoch 8959, Loss: 16.740, Final Batch Loss: 0.419\n",
      "Epoch 8960, Loss: 16.883, Final Batch Loss: 0.556\n",
      "Epoch 8961, Loss: 17.055, Final Batch Loss: 0.503\n",
      "Epoch 8962, Loss: 16.742, Final Batch Loss: 0.489\n",
      "Epoch 8963, Loss: 16.736, Final Batch Loss: 0.514\n",
      "Epoch 8964, Loss: 17.020, Final Batch Loss: 0.386\n",
      "Epoch 8965, Loss: 16.509, Final Batch Loss: 0.457\n",
      "Epoch 8966, Loss: 16.924, Final Batch Loss: 0.582\n",
      "Epoch 8967, Loss: 17.111, Final Batch Loss: 0.447\n",
      "Epoch 8968, Loss: 16.837, Final Batch Loss: 0.366\n",
      "Epoch 8969, Loss: 16.699, Final Batch Loss: 0.401\n",
      "Epoch 8970, Loss: 16.895, Final Batch Loss: 0.530\n",
      "Epoch 8971, Loss: 16.524, Final Batch Loss: 0.450\n",
      "Epoch 8972, Loss: 16.872, Final Batch Loss: 0.483\n",
      "Epoch 8973, Loss: 16.980, Final Batch Loss: 0.455\n",
      "Epoch 8974, Loss: 16.925, Final Batch Loss: 0.509\n",
      "Epoch 8975, Loss: 17.070, Final Batch Loss: 0.499\n",
      "Epoch 8976, Loss: 17.084, Final Batch Loss: 0.538\n",
      "Epoch 8977, Loss: 16.756, Final Batch Loss: 0.434\n",
      "Epoch 8978, Loss: 16.935, Final Batch Loss: 0.546\n",
      "Epoch 8979, Loss: 17.295, Final Batch Loss: 0.480\n",
      "Epoch 8980, Loss: 16.984, Final Batch Loss: 0.481\n",
      "Epoch 8981, Loss: 16.740, Final Batch Loss: 0.476\n",
      "Epoch 8982, Loss: 16.737, Final Batch Loss: 0.380\n",
      "Epoch 8983, Loss: 17.028, Final Batch Loss: 0.530\n",
      "Epoch 8984, Loss: 16.540, Final Batch Loss: 0.529\n",
      "Epoch 8985, Loss: 17.036, Final Batch Loss: 0.463\n",
      "Epoch 8986, Loss: 16.764, Final Batch Loss: 0.452\n",
      "Epoch 8987, Loss: 16.808, Final Batch Loss: 0.378\n",
      "Epoch 8988, Loss: 16.703, Final Batch Loss: 0.353\n",
      "Epoch 8989, Loss: 16.714, Final Batch Loss: 0.367\n",
      "Epoch 8990, Loss: 16.914, Final Batch Loss: 0.411\n",
      "Epoch 8991, Loss: 16.910, Final Batch Loss: 0.626\n",
      "Epoch 8992, Loss: 16.914, Final Batch Loss: 0.430\n",
      "Epoch 8993, Loss: 16.892, Final Batch Loss: 0.451\n",
      "Epoch 8994, Loss: 16.887, Final Batch Loss: 0.489\n",
      "Epoch 8995, Loss: 16.917, Final Batch Loss: 0.414\n",
      "Epoch 8996, Loss: 16.605, Final Batch Loss: 0.471\n",
      "Epoch 8997, Loss: 16.634, Final Batch Loss: 0.476\n",
      "Epoch 8998, Loss: 16.811, Final Batch Loss: 0.426\n",
      "Epoch 8999, Loss: 16.909, Final Batch Loss: 0.563\n",
      "Epoch 9000, Loss: 16.794, Final Batch Loss: 0.417\n",
      "Epoch 9001, Loss: 16.900, Final Batch Loss: 0.503\n",
      "Epoch 9002, Loss: 16.789, Final Batch Loss: 0.385\n",
      "Epoch 9003, Loss: 16.945, Final Batch Loss: 0.427\n",
      "Epoch 9004, Loss: 17.009, Final Batch Loss: 0.501\n",
      "Epoch 9005, Loss: 16.926, Final Batch Loss: 0.467\n",
      "Epoch 9006, Loss: 16.731, Final Batch Loss: 0.497\n",
      "Epoch 9007, Loss: 16.894, Final Batch Loss: 0.512\n",
      "Epoch 9008, Loss: 16.710, Final Batch Loss: 0.491\n",
      "Epoch 9009, Loss: 17.121, Final Batch Loss: 0.509\n",
      "Epoch 9010, Loss: 16.936, Final Batch Loss: 0.453\n",
      "Epoch 9011, Loss: 16.812, Final Batch Loss: 0.472\n",
      "Epoch 9012, Loss: 16.785, Final Batch Loss: 0.424\n",
      "Epoch 9013, Loss: 16.978, Final Batch Loss: 0.466\n",
      "Epoch 9014, Loss: 17.284, Final Batch Loss: 0.504\n",
      "Epoch 9015, Loss: 16.818, Final Batch Loss: 0.424\n",
      "Epoch 9016, Loss: 17.145, Final Batch Loss: 0.461\n",
      "Epoch 9017, Loss: 17.168, Final Batch Loss: 0.584\n",
      "Epoch 9018, Loss: 17.037, Final Batch Loss: 0.405\n",
      "Epoch 9019, Loss: 16.920, Final Batch Loss: 0.498\n",
      "Epoch 9020, Loss: 16.897, Final Batch Loss: 0.591\n",
      "Epoch 9021, Loss: 16.745, Final Batch Loss: 0.379\n",
      "Epoch 9022, Loss: 16.941, Final Batch Loss: 0.501\n",
      "Epoch 9023, Loss: 16.756, Final Batch Loss: 0.439\n",
      "Epoch 9024, Loss: 16.537, Final Batch Loss: 0.499\n",
      "Epoch 9025, Loss: 16.861, Final Batch Loss: 0.559\n",
      "Epoch 9026, Loss: 16.867, Final Batch Loss: 0.459\n",
      "Epoch 9027, Loss: 16.755, Final Batch Loss: 0.330\n",
      "Epoch 9028, Loss: 16.941, Final Batch Loss: 0.516\n",
      "Epoch 9029, Loss: 16.850, Final Batch Loss: 0.498\n",
      "Epoch 9030, Loss: 16.818, Final Batch Loss: 0.526\n",
      "Epoch 9031, Loss: 17.092, Final Batch Loss: 0.556\n",
      "Epoch 9032, Loss: 16.906, Final Batch Loss: 0.432\n",
      "Epoch 9033, Loss: 16.911, Final Batch Loss: 0.526\n",
      "Epoch 9034, Loss: 16.566, Final Batch Loss: 0.501\n",
      "Epoch 9035, Loss: 16.718, Final Batch Loss: 0.437\n",
      "Epoch 9036, Loss: 16.919, Final Batch Loss: 0.480\n",
      "Epoch 9037, Loss: 16.763, Final Batch Loss: 0.409\n",
      "Epoch 9038, Loss: 16.697, Final Batch Loss: 0.453\n",
      "Epoch 9039, Loss: 16.995, Final Batch Loss: 0.491\n",
      "Epoch 9040, Loss: 16.814, Final Batch Loss: 0.522\n",
      "Epoch 9041, Loss: 17.125, Final Batch Loss: 0.516\n",
      "Epoch 9042, Loss: 16.882, Final Batch Loss: 0.570\n",
      "Epoch 9043, Loss: 16.810, Final Batch Loss: 0.531\n",
      "Epoch 9044, Loss: 16.770, Final Batch Loss: 0.405\n",
      "Epoch 9045, Loss: 16.679, Final Batch Loss: 0.419\n",
      "Epoch 9046, Loss: 16.618, Final Batch Loss: 0.494\n",
      "Epoch 9047, Loss: 16.786, Final Batch Loss: 0.586\n",
      "Epoch 9048, Loss: 16.791, Final Batch Loss: 0.425\n",
      "Epoch 9049, Loss: 16.924, Final Batch Loss: 0.458\n",
      "Epoch 9050, Loss: 16.855, Final Batch Loss: 0.405\n",
      "Epoch 9051, Loss: 16.829, Final Batch Loss: 0.523\n",
      "Epoch 9052, Loss: 16.593, Final Batch Loss: 0.442\n",
      "Epoch 9053, Loss: 16.715, Final Batch Loss: 0.447\n",
      "Epoch 9054, Loss: 16.916, Final Batch Loss: 0.502\n",
      "Epoch 9055, Loss: 16.500, Final Batch Loss: 0.437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9056, Loss: 16.620, Final Batch Loss: 0.458\n",
      "Epoch 9057, Loss: 16.781, Final Batch Loss: 0.473\n",
      "Epoch 9058, Loss: 16.865, Final Batch Loss: 0.485\n",
      "Epoch 9059, Loss: 16.776, Final Batch Loss: 0.466\n",
      "Epoch 9060, Loss: 16.705, Final Batch Loss: 0.383\n",
      "Epoch 9061, Loss: 16.855, Final Batch Loss: 0.433\n",
      "Epoch 9062, Loss: 16.690, Final Batch Loss: 0.399\n",
      "Epoch 9063, Loss: 17.089, Final Batch Loss: 0.519\n",
      "Epoch 9064, Loss: 16.748, Final Batch Loss: 0.491\n",
      "Epoch 9065, Loss: 16.769, Final Batch Loss: 0.450\n",
      "Epoch 9066, Loss: 16.941, Final Batch Loss: 0.527\n",
      "Epoch 9067, Loss: 16.935, Final Batch Loss: 0.524\n",
      "Epoch 9068, Loss: 16.878, Final Batch Loss: 0.542\n",
      "Epoch 9069, Loss: 17.050, Final Batch Loss: 0.460\n",
      "Epoch 9070, Loss: 16.767, Final Batch Loss: 0.483\n",
      "Epoch 9071, Loss: 17.011, Final Batch Loss: 0.433\n",
      "Epoch 9072, Loss: 16.780, Final Batch Loss: 0.598\n",
      "Epoch 9073, Loss: 16.746, Final Batch Loss: 0.362\n",
      "Epoch 9074, Loss: 16.960, Final Batch Loss: 0.472\n",
      "Epoch 9075, Loss: 16.547, Final Batch Loss: 0.451\n",
      "Epoch 9076, Loss: 16.597, Final Batch Loss: 0.386\n",
      "Epoch 9077, Loss: 16.942, Final Batch Loss: 0.552\n",
      "Epoch 9078, Loss: 16.693, Final Batch Loss: 0.467\n",
      "Epoch 9079, Loss: 16.765, Final Batch Loss: 0.501\n",
      "Epoch 9080, Loss: 17.018, Final Batch Loss: 0.431\n",
      "Epoch 9081, Loss: 16.577, Final Batch Loss: 0.428\n",
      "Epoch 9082, Loss: 17.040, Final Batch Loss: 0.388\n",
      "Epoch 9083, Loss: 16.731, Final Batch Loss: 0.471\n",
      "Epoch 9084, Loss: 16.846, Final Batch Loss: 0.420\n",
      "Epoch 9085, Loss: 16.929, Final Batch Loss: 0.615\n",
      "Epoch 9086, Loss: 16.782, Final Batch Loss: 0.454\n",
      "Epoch 9087, Loss: 16.913, Final Batch Loss: 0.663\n",
      "Epoch 9088, Loss: 16.893, Final Batch Loss: 0.475\n",
      "Epoch 9089, Loss: 16.920, Final Batch Loss: 0.477\n",
      "Epoch 9090, Loss: 16.832, Final Batch Loss: 0.555\n",
      "Epoch 9091, Loss: 17.107, Final Batch Loss: 0.470\n",
      "Epoch 9092, Loss: 16.772, Final Batch Loss: 0.522\n",
      "Epoch 9093, Loss: 17.043, Final Batch Loss: 0.416\n",
      "Epoch 9094, Loss: 16.909, Final Batch Loss: 0.439\n",
      "Epoch 9095, Loss: 16.985, Final Batch Loss: 0.572\n",
      "Epoch 9096, Loss: 17.069, Final Batch Loss: 0.453\n",
      "Epoch 9097, Loss: 16.863, Final Batch Loss: 0.470\n",
      "Epoch 9098, Loss: 16.828, Final Batch Loss: 0.471\n",
      "Epoch 9099, Loss: 16.696, Final Batch Loss: 0.555\n",
      "Epoch 9100, Loss: 16.815, Final Batch Loss: 0.562\n",
      "Epoch 9101, Loss: 16.778, Final Batch Loss: 0.537\n",
      "Epoch 9102, Loss: 16.758, Final Batch Loss: 0.417\n",
      "Epoch 9103, Loss: 16.610, Final Batch Loss: 0.440\n",
      "Epoch 9104, Loss: 16.625, Final Batch Loss: 0.408\n",
      "Epoch 9105, Loss: 16.937, Final Batch Loss: 0.459\n",
      "Epoch 9106, Loss: 16.388, Final Batch Loss: 0.519\n",
      "Epoch 9107, Loss: 16.853, Final Batch Loss: 0.468\n",
      "Epoch 9108, Loss: 16.797, Final Batch Loss: 0.395\n",
      "Epoch 9109, Loss: 16.790, Final Batch Loss: 0.388\n",
      "Epoch 9110, Loss: 16.859, Final Batch Loss: 0.457\n",
      "Epoch 9111, Loss: 16.829, Final Batch Loss: 0.435\n",
      "Epoch 9112, Loss: 16.942, Final Batch Loss: 0.492\n",
      "Epoch 9113, Loss: 16.923, Final Batch Loss: 0.395\n",
      "Epoch 9114, Loss: 16.660, Final Batch Loss: 0.470\n",
      "Epoch 9115, Loss: 16.872, Final Batch Loss: 0.542\n",
      "Epoch 9116, Loss: 16.877, Final Batch Loss: 0.441\n",
      "Epoch 9117, Loss: 17.013, Final Batch Loss: 0.478\n",
      "Epoch 9118, Loss: 16.986, Final Batch Loss: 0.567\n",
      "Epoch 9119, Loss: 16.897, Final Batch Loss: 0.529\n",
      "Epoch 9120, Loss: 16.850, Final Batch Loss: 0.441\n",
      "Epoch 9121, Loss: 16.655, Final Batch Loss: 0.515\n",
      "Epoch 9122, Loss: 17.008, Final Batch Loss: 0.487\n",
      "Epoch 9123, Loss: 16.791, Final Batch Loss: 0.450\n",
      "Epoch 9124, Loss: 16.816, Final Batch Loss: 0.546\n",
      "Epoch 9125, Loss: 16.718, Final Batch Loss: 0.344\n",
      "Epoch 9126, Loss: 17.018, Final Batch Loss: 0.458\n",
      "Epoch 9127, Loss: 16.811, Final Batch Loss: 0.450\n",
      "Epoch 9128, Loss: 16.589, Final Batch Loss: 0.362\n",
      "Epoch 9129, Loss: 16.714, Final Batch Loss: 0.470\n",
      "Epoch 9130, Loss: 16.287, Final Batch Loss: 0.340\n",
      "Epoch 9131, Loss: 16.663, Final Batch Loss: 0.525\n",
      "Epoch 9132, Loss: 16.836, Final Batch Loss: 0.415\n",
      "Epoch 9133, Loss: 16.544, Final Batch Loss: 0.464\n",
      "Epoch 9134, Loss: 16.836, Final Batch Loss: 0.593\n",
      "Epoch 9135, Loss: 16.764, Final Batch Loss: 0.414\n",
      "Epoch 9136, Loss: 16.611, Final Batch Loss: 0.558\n",
      "Epoch 9137, Loss: 16.750, Final Batch Loss: 0.434\n",
      "Epoch 9138, Loss: 16.457, Final Batch Loss: 0.492\n",
      "Epoch 9139, Loss: 16.621, Final Batch Loss: 0.470\n",
      "Epoch 9140, Loss: 16.814, Final Batch Loss: 0.499\n",
      "Epoch 9141, Loss: 16.814, Final Batch Loss: 0.472\n",
      "Epoch 9142, Loss: 16.741, Final Batch Loss: 0.401\n",
      "Epoch 9143, Loss: 16.616, Final Batch Loss: 0.369\n",
      "Epoch 9144, Loss: 16.758, Final Batch Loss: 0.465\n",
      "Epoch 9145, Loss: 16.951, Final Batch Loss: 0.513\n",
      "Epoch 9146, Loss: 16.786, Final Batch Loss: 0.386\n",
      "Epoch 9147, Loss: 16.658, Final Batch Loss: 0.416\n",
      "Epoch 9148, Loss: 16.935, Final Batch Loss: 0.533\n",
      "Epoch 9149, Loss: 16.870, Final Batch Loss: 0.390\n",
      "Epoch 9150, Loss: 16.949, Final Batch Loss: 0.463\n",
      "Epoch 9151, Loss: 16.895, Final Batch Loss: 0.522\n",
      "Epoch 9152, Loss: 16.721, Final Batch Loss: 0.418\n",
      "Epoch 9153, Loss: 17.162, Final Batch Loss: 0.493\n",
      "Epoch 9154, Loss: 16.642, Final Batch Loss: 0.475\n",
      "Epoch 9155, Loss: 16.748, Final Batch Loss: 0.430\n",
      "Epoch 9156, Loss: 17.107, Final Batch Loss: 0.429\n",
      "Epoch 9157, Loss: 16.968, Final Batch Loss: 0.507\n",
      "Epoch 9158, Loss: 16.780, Final Batch Loss: 0.549\n",
      "Epoch 9159, Loss: 16.548, Final Batch Loss: 0.409\n",
      "Epoch 9160, Loss: 16.705, Final Batch Loss: 0.424\n",
      "Epoch 9161, Loss: 16.902, Final Batch Loss: 0.581\n",
      "Epoch 9162, Loss: 16.517, Final Batch Loss: 0.459\n",
      "Epoch 9163, Loss: 16.845, Final Batch Loss: 0.471\n",
      "Epoch 9164, Loss: 16.707, Final Batch Loss: 0.493\n",
      "Epoch 9165, Loss: 16.765, Final Batch Loss: 0.488\n",
      "Epoch 9166, Loss: 16.791, Final Batch Loss: 0.381\n",
      "Epoch 9167, Loss: 16.710, Final Batch Loss: 0.341\n",
      "Epoch 9168, Loss: 16.771, Final Batch Loss: 0.408\n",
      "Epoch 9169, Loss: 16.518, Final Batch Loss: 0.392\n",
      "Epoch 9170, Loss: 16.875, Final Batch Loss: 0.540\n",
      "Epoch 9171, Loss: 16.817, Final Batch Loss: 0.528\n",
      "Epoch 9172, Loss: 16.651, Final Batch Loss: 0.424\n",
      "Epoch 9173, Loss: 16.826, Final Batch Loss: 0.495\n",
      "Epoch 9174, Loss: 16.884, Final Batch Loss: 0.408\n",
      "Epoch 9175, Loss: 16.855, Final Batch Loss: 0.566\n",
      "Epoch 9176, Loss: 16.867, Final Batch Loss: 0.537\n",
      "Epoch 9177, Loss: 16.861, Final Batch Loss: 0.595\n",
      "Epoch 9178, Loss: 16.530, Final Batch Loss: 0.629\n",
      "Epoch 9179, Loss: 16.881, Final Batch Loss: 0.399\n",
      "Epoch 9180, Loss: 16.982, Final Batch Loss: 0.511\n",
      "Epoch 9181, Loss: 16.805, Final Batch Loss: 0.371\n",
      "Epoch 9182, Loss: 16.724, Final Batch Loss: 0.535\n",
      "Epoch 9183, Loss: 16.983, Final Batch Loss: 0.471\n",
      "Epoch 9184, Loss: 16.867, Final Batch Loss: 0.540\n",
      "Epoch 9185, Loss: 16.993, Final Batch Loss: 0.506\n",
      "Epoch 9186, Loss: 16.718, Final Batch Loss: 0.457\n",
      "Epoch 9187, Loss: 16.809, Final Batch Loss: 0.345\n",
      "Epoch 9188, Loss: 16.790, Final Batch Loss: 0.431\n",
      "Epoch 9189, Loss: 16.962, Final Batch Loss: 0.412\n",
      "Epoch 9190, Loss: 16.808, Final Batch Loss: 0.484\n",
      "Epoch 9191, Loss: 16.689, Final Batch Loss: 0.403\n",
      "Epoch 9192, Loss: 16.902, Final Batch Loss: 0.517\n",
      "Epoch 9193, Loss: 17.099, Final Batch Loss: 0.648\n",
      "Epoch 9194, Loss: 16.591, Final Batch Loss: 0.506\n",
      "Epoch 9195, Loss: 16.924, Final Batch Loss: 0.491\n",
      "Epoch 9196, Loss: 16.407, Final Batch Loss: 0.496\n",
      "Epoch 9197, Loss: 16.855, Final Batch Loss: 0.526\n",
      "Epoch 9198, Loss: 16.868, Final Batch Loss: 0.497\n",
      "Epoch 9199, Loss: 16.658, Final Batch Loss: 0.449\n",
      "Epoch 9200, Loss: 16.710, Final Batch Loss: 0.467\n",
      "Epoch 9201, Loss: 16.499, Final Batch Loss: 0.342\n",
      "Epoch 9202, Loss: 16.912, Final Batch Loss: 0.546\n",
      "Epoch 9203, Loss: 16.997, Final Batch Loss: 0.490\n",
      "Epoch 9204, Loss: 16.643, Final Batch Loss: 0.442\n",
      "Epoch 9205, Loss: 17.281, Final Batch Loss: 0.567\n",
      "Epoch 9206, Loss: 16.528, Final Batch Loss: 0.430\n",
      "Epoch 9207, Loss: 16.962, Final Batch Loss: 0.472\n",
      "Epoch 9208, Loss: 16.759, Final Batch Loss: 0.373\n",
      "Epoch 9209, Loss: 17.006, Final Batch Loss: 0.475\n",
      "Epoch 9210, Loss: 16.894, Final Batch Loss: 0.485\n",
      "Epoch 9211, Loss: 16.844, Final Batch Loss: 0.518\n",
      "Epoch 9212, Loss: 17.063, Final Batch Loss: 0.380\n",
      "Epoch 9213, Loss: 16.631, Final Batch Loss: 0.548\n",
      "Epoch 9214, Loss: 17.034, Final Batch Loss: 0.618\n",
      "Epoch 9215, Loss: 16.888, Final Batch Loss: 0.542\n",
      "Epoch 9216, Loss: 16.858, Final Batch Loss: 0.470\n",
      "Epoch 9217, Loss: 16.925, Final Batch Loss: 0.478\n",
      "Epoch 9218, Loss: 17.085, Final Batch Loss: 0.396\n",
      "Epoch 9219, Loss: 16.958, Final Batch Loss: 0.437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9220, Loss: 16.592, Final Batch Loss: 0.376\n",
      "Epoch 9221, Loss: 16.922, Final Batch Loss: 0.420\n",
      "Epoch 9222, Loss: 16.796, Final Batch Loss: 0.420\n",
      "Epoch 9223, Loss: 16.900, Final Batch Loss: 0.487\n",
      "Epoch 9224, Loss: 16.873, Final Batch Loss: 0.560\n",
      "Epoch 9225, Loss: 16.990, Final Batch Loss: 0.589\n",
      "Epoch 9226, Loss: 16.806, Final Batch Loss: 0.503\n",
      "Epoch 9227, Loss: 16.806, Final Batch Loss: 0.486\n",
      "Epoch 9228, Loss: 16.647, Final Batch Loss: 0.417\n",
      "Epoch 9229, Loss: 16.899, Final Batch Loss: 0.546\n",
      "Epoch 9230, Loss: 16.616, Final Batch Loss: 0.433\n",
      "Epoch 9231, Loss: 16.634, Final Batch Loss: 0.472\n",
      "Epoch 9232, Loss: 16.966, Final Batch Loss: 0.496\n",
      "Epoch 9233, Loss: 16.788, Final Batch Loss: 0.411\n",
      "Epoch 9234, Loss: 16.876, Final Batch Loss: 0.469\n",
      "Epoch 9235, Loss: 16.399, Final Batch Loss: 0.401\n",
      "Epoch 9236, Loss: 16.640, Final Batch Loss: 0.404\n",
      "Epoch 9237, Loss: 16.773, Final Batch Loss: 0.558\n",
      "Epoch 9238, Loss: 16.634, Final Batch Loss: 0.425\n",
      "Epoch 9239, Loss: 17.017, Final Batch Loss: 0.538\n",
      "Epoch 9240, Loss: 16.781, Final Batch Loss: 0.558\n",
      "Epoch 9241, Loss: 16.628, Final Batch Loss: 0.462\n",
      "Epoch 9242, Loss: 16.562, Final Batch Loss: 0.416\n",
      "Epoch 9243, Loss: 16.761, Final Batch Loss: 0.520\n",
      "Epoch 9244, Loss: 16.545, Final Batch Loss: 0.461\n",
      "Epoch 9245, Loss: 16.773, Final Batch Loss: 0.539\n",
      "Epoch 9246, Loss: 17.017, Final Batch Loss: 0.363\n",
      "Epoch 9247, Loss: 16.973, Final Batch Loss: 0.575\n",
      "Epoch 9248, Loss: 16.807, Final Batch Loss: 0.461\n",
      "Epoch 9249, Loss: 16.903, Final Batch Loss: 0.525\n",
      "Epoch 9250, Loss: 16.692, Final Batch Loss: 0.492\n",
      "Epoch 9251, Loss: 16.437, Final Batch Loss: 0.546\n",
      "Epoch 9252, Loss: 16.471, Final Batch Loss: 0.425\n",
      "Epoch 9253, Loss: 16.821, Final Batch Loss: 0.512\n",
      "Epoch 9254, Loss: 16.838, Final Batch Loss: 0.434\n",
      "Epoch 9255, Loss: 16.972, Final Batch Loss: 0.585\n",
      "Epoch 9256, Loss: 16.733, Final Batch Loss: 0.385\n",
      "Epoch 9257, Loss: 16.561, Final Batch Loss: 0.421\n",
      "Epoch 9258, Loss: 17.103, Final Batch Loss: 0.504\n",
      "Epoch 9259, Loss: 16.469, Final Batch Loss: 0.368\n",
      "Epoch 9260, Loss: 16.791, Final Batch Loss: 0.394\n",
      "Epoch 9261, Loss: 16.848, Final Batch Loss: 0.467\n",
      "Epoch 9262, Loss: 16.548, Final Batch Loss: 0.443\n",
      "Epoch 9263, Loss: 16.617, Final Batch Loss: 0.448\n",
      "Epoch 9264, Loss: 16.994, Final Batch Loss: 0.514\n",
      "Epoch 9265, Loss: 16.630, Final Batch Loss: 0.395\n",
      "Epoch 9266, Loss: 16.953, Final Batch Loss: 0.461\n",
      "Epoch 9267, Loss: 17.124, Final Batch Loss: 0.389\n",
      "Epoch 9268, Loss: 16.892, Final Batch Loss: 0.476\n",
      "Epoch 9269, Loss: 16.770, Final Batch Loss: 0.575\n",
      "Epoch 9270, Loss: 16.678, Final Batch Loss: 0.491\n",
      "Epoch 9271, Loss: 16.877, Final Batch Loss: 0.469\n",
      "Epoch 9272, Loss: 16.713, Final Batch Loss: 0.506\n",
      "Epoch 9273, Loss: 16.915, Final Batch Loss: 0.549\n",
      "Epoch 9274, Loss: 17.001, Final Batch Loss: 0.530\n",
      "Epoch 9275, Loss: 16.727, Final Batch Loss: 0.425\n",
      "Epoch 9276, Loss: 16.975, Final Batch Loss: 0.541\n",
      "Epoch 9277, Loss: 16.763, Final Batch Loss: 0.529\n",
      "Epoch 9278, Loss: 16.373, Final Batch Loss: 0.523\n",
      "Epoch 9279, Loss: 16.670, Final Batch Loss: 0.420\n",
      "Epoch 9280, Loss: 17.138, Final Batch Loss: 0.579\n",
      "Epoch 9281, Loss: 16.828, Final Batch Loss: 0.489\n",
      "Epoch 9282, Loss: 16.631, Final Batch Loss: 0.389\n",
      "Epoch 9283, Loss: 16.660, Final Batch Loss: 0.482\n",
      "Epoch 9284, Loss: 16.551, Final Batch Loss: 0.339\n",
      "Epoch 9285, Loss: 16.679, Final Batch Loss: 0.447\n",
      "Epoch 9286, Loss: 16.934, Final Batch Loss: 0.431\n",
      "Epoch 9287, Loss: 16.623, Final Batch Loss: 0.503\n",
      "Epoch 9288, Loss: 16.478, Final Batch Loss: 0.394\n",
      "Epoch 9289, Loss: 16.667, Final Batch Loss: 0.510\n",
      "Epoch 9290, Loss: 16.583, Final Batch Loss: 0.450\n",
      "Epoch 9291, Loss: 16.711, Final Batch Loss: 0.478\n",
      "Epoch 9292, Loss: 16.774, Final Batch Loss: 0.434\n",
      "Epoch 9293, Loss: 16.886, Final Batch Loss: 0.416\n",
      "Epoch 9294, Loss: 16.866, Final Batch Loss: 0.395\n",
      "Epoch 9295, Loss: 16.707, Final Batch Loss: 0.482\n",
      "Epoch 9296, Loss: 16.627, Final Batch Loss: 0.457\n",
      "Epoch 9297, Loss: 17.007, Final Batch Loss: 0.555\n",
      "Epoch 9298, Loss: 16.854, Final Batch Loss: 0.571\n",
      "Epoch 9299, Loss: 16.722, Final Batch Loss: 0.491\n",
      "Epoch 9300, Loss: 16.402, Final Batch Loss: 0.430\n",
      "Epoch 9301, Loss: 16.710, Final Batch Loss: 0.486\n",
      "Epoch 9302, Loss: 16.682, Final Batch Loss: 0.471\n",
      "Epoch 9303, Loss: 16.566, Final Batch Loss: 0.437\n",
      "Epoch 9304, Loss: 17.129, Final Batch Loss: 0.523\n",
      "Epoch 9305, Loss: 17.171, Final Batch Loss: 0.496\n",
      "Epoch 9306, Loss: 16.784, Final Batch Loss: 0.500\n",
      "Epoch 9307, Loss: 16.772, Final Batch Loss: 0.605\n",
      "Epoch 9308, Loss: 16.725, Final Batch Loss: 0.513\n",
      "Epoch 9309, Loss: 16.949, Final Batch Loss: 0.442\n",
      "Epoch 9310, Loss: 16.643, Final Batch Loss: 0.379\n",
      "Epoch 9311, Loss: 16.775, Final Batch Loss: 0.521\n",
      "Epoch 9312, Loss: 16.618, Final Batch Loss: 0.380\n",
      "Epoch 9313, Loss: 16.691, Final Batch Loss: 0.492\n",
      "Epoch 9314, Loss: 16.945, Final Batch Loss: 0.540\n",
      "Epoch 9315, Loss: 16.993, Final Batch Loss: 0.425\n",
      "Epoch 9316, Loss: 16.745, Final Batch Loss: 0.488\n",
      "Epoch 9317, Loss: 16.771, Final Batch Loss: 0.457\n",
      "Epoch 9318, Loss: 16.734, Final Batch Loss: 0.376\n",
      "Epoch 9319, Loss: 17.009, Final Batch Loss: 0.396\n",
      "Epoch 9320, Loss: 16.571, Final Batch Loss: 0.406\n",
      "Epoch 9321, Loss: 16.766, Final Batch Loss: 0.538\n",
      "Epoch 9322, Loss: 16.691, Final Batch Loss: 0.423\n",
      "Epoch 9323, Loss: 16.674, Final Batch Loss: 0.460\n",
      "Epoch 9324, Loss: 16.603, Final Batch Loss: 0.397\n",
      "Epoch 9325, Loss: 16.334, Final Batch Loss: 0.376\n",
      "Epoch 9326, Loss: 16.443, Final Batch Loss: 0.424\n",
      "Epoch 9327, Loss: 16.853, Final Batch Loss: 0.592\n",
      "Epoch 9328, Loss: 16.598, Final Batch Loss: 0.462\n",
      "Epoch 9329, Loss: 16.669, Final Batch Loss: 0.511\n",
      "Epoch 9330, Loss: 16.736, Final Batch Loss: 0.525\n",
      "Epoch 9331, Loss: 16.929, Final Batch Loss: 0.452\n",
      "Epoch 9332, Loss: 16.642, Final Batch Loss: 0.479\n",
      "Epoch 9333, Loss: 16.561, Final Batch Loss: 0.429\n",
      "Epoch 9334, Loss: 16.947, Final Batch Loss: 0.457\n",
      "Epoch 9335, Loss: 16.923, Final Batch Loss: 0.406\n",
      "Epoch 9336, Loss: 16.516, Final Batch Loss: 0.576\n",
      "Epoch 9337, Loss: 16.908, Final Batch Loss: 0.409\n",
      "Epoch 9338, Loss: 16.877, Final Batch Loss: 0.495\n",
      "Epoch 9339, Loss: 16.685, Final Batch Loss: 0.483\n",
      "Epoch 9340, Loss: 16.895, Final Batch Loss: 0.493\n",
      "Epoch 9341, Loss: 16.823, Final Batch Loss: 0.493\n",
      "Epoch 9342, Loss: 16.667, Final Batch Loss: 0.363\n",
      "Epoch 9343, Loss: 16.394, Final Batch Loss: 0.417\n",
      "Epoch 9344, Loss: 17.021, Final Batch Loss: 0.596\n",
      "Epoch 9345, Loss: 16.897, Final Batch Loss: 0.447\n",
      "Epoch 9346, Loss: 16.922, Final Batch Loss: 0.481\n",
      "Epoch 9347, Loss: 16.612, Final Batch Loss: 0.486\n",
      "Epoch 9348, Loss: 16.817, Final Batch Loss: 0.464\n",
      "Epoch 9349, Loss: 16.859, Final Batch Loss: 0.382\n",
      "Epoch 9350, Loss: 16.605, Final Batch Loss: 0.408\n",
      "Epoch 9351, Loss: 16.704, Final Batch Loss: 0.424\n",
      "Epoch 9352, Loss: 16.970, Final Batch Loss: 0.500\n",
      "Epoch 9353, Loss: 16.909, Final Batch Loss: 0.364\n",
      "Epoch 9354, Loss: 16.846, Final Batch Loss: 0.468\n",
      "Epoch 9355, Loss: 16.988, Final Batch Loss: 0.399\n",
      "Epoch 9356, Loss: 16.954, Final Batch Loss: 0.467\n",
      "Epoch 9357, Loss: 16.968, Final Batch Loss: 0.481\n",
      "Epoch 9358, Loss: 16.577, Final Batch Loss: 0.410\n",
      "Epoch 9359, Loss: 16.625, Final Batch Loss: 0.581\n",
      "Epoch 9360, Loss: 16.769, Final Batch Loss: 0.449\n",
      "Epoch 9361, Loss: 16.600, Final Batch Loss: 0.415\n",
      "Epoch 9362, Loss: 16.731, Final Batch Loss: 0.468\n",
      "Epoch 9363, Loss: 16.634, Final Batch Loss: 0.454\n",
      "Epoch 9364, Loss: 16.588, Final Batch Loss: 0.505\n",
      "Epoch 9365, Loss: 16.874, Final Batch Loss: 0.571\n",
      "Epoch 9366, Loss: 16.690, Final Batch Loss: 0.438\n",
      "Epoch 9367, Loss: 16.806, Final Batch Loss: 0.526\n",
      "Epoch 9368, Loss: 16.470, Final Batch Loss: 0.426\n",
      "Epoch 9369, Loss: 16.640, Final Batch Loss: 0.455\n",
      "Epoch 9370, Loss: 16.999, Final Batch Loss: 0.424\n",
      "Epoch 9371, Loss: 16.647, Final Batch Loss: 0.498\n",
      "Epoch 9372, Loss: 16.700, Final Batch Loss: 0.450\n",
      "Epoch 9373, Loss: 16.663, Final Batch Loss: 0.389\n",
      "Epoch 9374, Loss: 16.741, Final Batch Loss: 0.482\n",
      "Epoch 9375, Loss: 16.807, Final Batch Loss: 0.507\n",
      "Epoch 9376, Loss: 16.835, Final Batch Loss: 0.396\n",
      "Epoch 9377, Loss: 16.894, Final Batch Loss: 0.525\n",
      "Epoch 9378, Loss: 16.937, Final Batch Loss: 0.489\n",
      "Epoch 9379, Loss: 16.810, Final Batch Loss: 0.493\n",
      "Epoch 9380, Loss: 16.598, Final Batch Loss: 0.414\n",
      "Epoch 9381, Loss: 16.796, Final Batch Loss: 0.315\n",
      "Epoch 9382, Loss: 16.627, Final Batch Loss: 0.520\n",
      "Epoch 9383, Loss: 16.869, Final Batch Loss: 0.476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9384, Loss: 17.055, Final Batch Loss: 0.572\n",
      "Epoch 9385, Loss: 16.766, Final Batch Loss: 0.539\n",
      "Epoch 9386, Loss: 17.081, Final Batch Loss: 0.418\n",
      "Epoch 9387, Loss: 16.729, Final Batch Loss: 0.369\n",
      "Epoch 9388, Loss: 16.811, Final Batch Loss: 0.467\n",
      "Epoch 9389, Loss: 16.512, Final Batch Loss: 0.495\n",
      "Epoch 9390, Loss: 16.943, Final Batch Loss: 0.479\n",
      "Epoch 9391, Loss: 16.853, Final Batch Loss: 0.450\n",
      "Epoch 9392, Loss: 16.413, Final Batch Loss: 0.434\n",
      "Epoch 9393, Loss: 17.094, Final Batch Loss: 0.487\n",
      "Epoch 9394, Loss: 16.419, Final Batch Loss: 0.488\n",
      "Epoch 9395, Loss: 16.816, Final Batch Loss: 0.422\n",
      "Epoch 9396, Loss: 16.744, Final Batch Loss: 0.491\n",
      "Epoch 9397, Loss: 16.935, Final Batch Loss: 0.479\n",
      "Epoch 9398, Loss: 17.154, Final Batch Loss: 0.480\n",
      "Epoch 9399, Loss: 16.794, Final Batch Loss: 0.538\n",
      "Epoch 9400, Loss: 16.814, Final Batch Loss: 0.479\n",
      "Epoch 9401, Loss: 16.753, Final Batch Loss: 0.438\n",
      "Epoch 9402, Loss: 17.110, Final Batch Loss: 0.570\n",
      "Epoch 9403, Loss: 17.088, Final Batch Loss: 0.543\n",
      "Epoch 9404, Loss: 16.664, Final Batch Loss: 0.364\n",
      "Epoch 9405, Loss: 16.849, Final Batch Loss: 0.445\n",
      "Epoch 9406, Loss: 16.909, Final Batch Loss: 0.444\n",
      "Epoch 9407, Loss: 16.940, Final Batch Loss: 0.385\n",
      "Epoch 9408, Loss: 16.702, Final Batch Loss: 0.405\n",
      "Epoch 9409, Loss: 16.650, Final Batch Loss: 0.447\n",
      "Epoch 9410, Loss: 16.828, Final Batch Loss: 0.486\n",
      "Epoch 9411, Loss: 16.814, Final Batch Loss: 0.506\n",
      "Epoch 9412, Loss: 16.780, Final Batch Loss: 0.535\n",
      "Epoch 9413, Loss: 16.916, Final Batch Loss: 0.496\n",
      "Epoch 9414, Loss: 16.605, Final Batch Loss: 0.496\n",
      "Epoch 9415, Loss: 16.342, Final Batch Loss: 0.409\n",
      "Epoch 9416, Loss: 16.795, Final Batch Loss: 0.550\n",
      "Epoch 9417, Loss: 16.941, Final Batch Loss: 0.401\n",
      "Epoch 9418, Loss: 16.677, Final Batch Loss: 0.463\n",
      "Epoch 9419, Loss: 16.565, Final Batch Loss: 0.450\n",
      "Epoch 9420, Loss: 16.551, Final Batch Loss: 0.392\n",
      "Epoch 9421, Loss: 16.807, Final Batch Loss: 0.410\n",
      "Epoch 9422, Loss: 16.784, Final Batch Loss: 0.406\n",
      "Epoch 9423, Loss: 16.794, Final Batch Loss: 0.459\n",
      "Epoch 9424, Loss: 16.779, Final Batch Loss: 0.509\n",
      "Epoch 9425, Loss: 16.728, Final Batch Loss: 0.563\n",
      "Epoch 9426, Loss: 16.805, Final Batch Loss: 0.466\n",
      "Epoch 9427, Loss: 16.969, Final Batch Loss: 0.369\n",
      "Epoch 9428, Loss: 16.601, Final Batch Loss: 0.422\n",
      "Epoch 9429, Loss: 16.726, Final Batch Loss: 0.448\n",
      "Epoch 9430, Loss: 16.787, Final Batch Loss: 0.439\n",
      "Epoch 9431, Loss: 16.695, Final Batch Loss: 0.419\n",
      "Epoch 9432, Loss: 16.846, Final Batch Loss: 0.392\n",
      "Epoch 9433, Loss: 16.458, Final Batch Loss: 0.480\n",
      "Epoch 9434, Loss: 16.907, Final Batch Loss: 0.498\n",
      "Epoch 9435, Loss: 16.705, Final Batch Loss: 0.495\n",
      "Epoch 9436, Loss: 16.789, Final Batch Loss: 0.441\n",
      "Epoch 9437, Loss: 16.645, Final Batch Loss: 0.382\n",
      "Epoch 9438, Loss: 16.694, Final Batch Loss: 0.421\n",
      "Epoch 9439, Loss: 16.620, Final Batch Loss: 0.448\n",
      "Epoch 9440, Loss: 16.793, Final Batch Loss: 0.543\n",
      "Epoch 9441, Loss: 16.780, Final Batch Loss: 0.403\n",
      "Epoch 9442, Loss: 16.783, Final Batch Loss: 0.466\n",
      "Epoch 9443, Loss: 16.616, Final Batch Loss: 0.412\n",
      "Epoch 9444, Loss: 16.598, Final Batch Loss: 0.482\n",
      "Epoch 9445, Loss: 17.184, Final Batch Loss: 0.559\n",
      "Epoch 9446, Loss: 17.085, Final Batch Loss: 0.587\n",
      "Epoch 9447, Loss: 16.821, Final Batch Loss: 0.513\n",
      "Epoch 9448, Loss: 16.913, Final Batch Loss: 0.404\n",
      "Epoch 9449, Loss: 16.777, Final Batch Loss: 0.452\n",
      "Epoch 9450, Loss: 17.106, Final Batch Loss: 0.539\n",
      "Epoch 9451, Loss: 16.772, Final Batch Loss: 0.427\n",
      "Epoch 9452, Loss: 16.523, Final Batch Loss: 0.401\n",
      "Epoch 9453, Loss: 17.130, Final Batch Loss: 0.435\n",
      "Epoch 9454, Loss: 16.508, Final Batch Loss: 0.407\n",
      "Epoch 9455, Loss: 16.835, Final Batch Loss: 0.551\n",
      "Epoch 9456, Loss: 16.922, Final Batch Loss: 0.449\n",
      "Epoch 9457, Loss: 16.682, Final Batch Loss: 0.555\n",
      "Epoch 9458, Loss: 16.753, Final Batch Loss: 0.441\n",
      "Epoch 9459, Loss: 16.748, Final Batch Loss: 0.514\n",
      "Epoch 9460, Loss: 16.750, Final Batch Loss: 0.484\n",
      "Epoch 9461, Loss: 16.971, Final Batch Loss: 0.477\n",
      "Epoch 9462, Loss: 16.749, Final Batch Loss: 0.433\n",
      "Epoch 9463, Loss: 16.748, Final Batch Loss: 0.395\n",
      "Epoch 9464, Loss: 16.857, Final Batch Loss: 0.556\n",
      "Epoch 9465, Loss: 16.639, Final Batch Loss: 0.449\n",
      "Epoch 9466, Loss: 16.837, Final Batch Loss: 0.473\n",
      "Epoch 9467, Loss: 16.463, Final Batch Loss: 0.395\n",
      "Epoch 9468, Loss: 16.479, Final Batch Loss: 0.466\n",
      "Epoch 9469, Loss: 16.659, Final Batch Loss: 0.450\n",
      "Epoch 9470, Loss: 16.815, Final Batch Loss: 0.439\n",
      "Epoch 9471, Loss: 16.687, Final Batch Loss: 0.505\n",
      "Epoch 9472, Loss: 16.612, Final Batch Loss: 0.442\n",
      "Epoch 9473, Loss: 16.722, Final Batch Loss: 0.574\n",
      "Epoch 9474, Loss: 16.714, Final Batch Loss: 0.370\n",
      "Epoch 9475, Loss: 16.846, Final Batch Loss: 0.537\n",
      "Epoch 9476, Loss: 17.107, Final Batch Loss: 0.536\n",
      "Epoch 9477, Loss: 16.860, Final Batch Loss: 0.649\n",
      "Epoch 9478, Loss: 16.644, Final Batch Loss: 0.424\n",
      "Epoch 9479, Loss: 16.721, Final Batch Loss: 0.499\n",
      "Epoch 9480, Loss: 16.764, Final Batch Loss: 0.443\n",
      "Epoch 9481, Loss: 16.842, Final Batch Loss: 0.491\n",
      "Epoch 9482, Loss: 16.761, Final Batch Loss: 0.453\n",
      "Epoch 9483, Loss: 16.682, Final Batch Loss: 0.485\n",
      "Epoch 9484, Loss: 16.841, Final Batch Loss: 0.464\n",
      "Epoch 9485, Loss: 16.857, Final Batch Loss: 0.538\n",
      "Epoch 9486, Loss: 16.615, Final Batch Loss: 0.414\n",
      "Epoch 9487, Loss: 16.642, Final Batch Loss: 0.492\n",
      "Epoch 9488, Loss: 16.821, Final Batch Loss: 0.469\n",
      "Epoch 9489, Loss: 16.690, Final Batch Loss: 0.485\n",
      "Epoch 9490, Loss: 17.031, Final Batch Loss: 0.568\n",
      "Epoch 9491, Loss: 16.664, Final Batch Loss: 0.497\n",
      "Epoch 9492, Loss: 17.045, Final Batch Loss: 0.536\n",
      "Epoch 9493, Loss: 16.841, Final Batch Loss: 0.530\n",
      "Epoch 9494, Loss: 16.720, Final Batch Loss: 0.544\n",
      "Epoch 9495, Loss: 16.744, Final Batch Loss: 0.393\n",
      "Epoch 9496, Loss: 16.516, Final Batch Loss: 0.412\n",
      "Epoch 9497, Loss: 16.804, Final Batch Loss: 0.454\n",
      "Epoch 9498, Loss: 16.616, Final Batch Loss: 0.389\n",
      "Epoch 9499, Loss: 16.820, Final Batch Loss: 0.623\n",
      "Epoch 9500, Loss: 16.585, Final Batch Loss: 0.553\n",
      "Epoch 9501, Loss: 17.045, Final Batch Loss: 0.535\n",
      "Epoch 9502, Loss: 16.743, Final Batch Loss: 0.413\n",
      "Epoch 9503, Loss: 16.695, Final Batch Loss: 0.504\n",
      "Epoch 9504, Loss: 16.831, Final Batch Loss: 0.499\n",
      "Epoch 9505, Loss: 16.830, Final Batch Loss: 0.590\n",
      "Epoch 9506, Loss: 16.765, Final Batch Loss: 0.409\n",
      "Epoch 9507, Loss: 16.819, Final Batch Loss: 0.561\n",
      "Epoch 9508, Loss: 16.805, Final Batch Loss: 0.460\n",
      "Epoch 9509, Loss: 16.701, Final Batch Loss: 0.396\n",
      "Epoch 9510, Loss: 16.826, Final Batch Loss: 0.423\n",
      "Epoch 9511, Loss: 16.409, Final Batch Loss: 0.367\n",
      "Epoch 9512, Loss: 16.639, Final Batch Loss: 0.487\n",
      "Epoch 9513, Loss: 16.500, Final Batch Loss: 0.382\n",
      "Epoch 9514, Loss: 16.774, Final Batch Loss: 0.616\n",
      "Epoch 9515, Loss: 16.740, Final Batch Loss: 0.533\n",
      "Epoch 9516, Loss: 16.720, Final Batch Loss: 0.465\n",
      "Epoch 9517, Loss: 16.639, Final Batch Loss: 0.524\n",
      "Epoch 9518, Loss: 16.791, Final Batch Loss: 0.308\n",
      "Epoch 9519, Loss: 16.687, Final Batch Loss: 0.532\n",
      "Epoch 9520, Loss: 16.661, Final Batch Loss: 0.537\n",
      "Epoch 9521, Loss: 16.746, Final Batch Loss: 0.489\n",
      "Epoch 9522, Loss: 16.758, Final Batch Loss: 0.494\n",
      "Epoch 9523, Loss: 16.838, Final Batch Loss: 0.375\n",
      "Epoch 9524, Loss: 16.996, Final Batch Loss: 0.572\n",
      "Epoch 9525, Loss: 16.811, Final Batch Loss: 0.457\n",
      "Epoch 9526, Loss: 16.895, Final Batch Loss: 0.437\n",
      "Epoch 9527, Loss: 16.798, Final Batch Loss: 0.453\n",
      "Epoch 9528, Loss: 16.820, Final Batch Loss: 0.562\n",
      "Epoch 9529, Loss: 16.831, Final Batch Loss: 0.545\n",
      "Epoch 9530, Loss: 16.907, Final Batch Loss: 0.431\n",
      "Epoch 9531, Loss: 16.827, Final Batch Loss: 0.446\n",
      "Epoch 9532, Loss: 16.671, Final Batch Loss: 0.459\n",
      "Epoch 9533, Loss: 16.482, Final Batch Loss: 0.463\n",
      "Epoch 9534, Loss: 16.539, Final Batch Loss: 0.532\n",
      "Epoch 9535, Loss: 16.943, Final Batch Loss: 0.463\n",
      "Epoch 9536, Loss: 16.822, Final Batch Loss: 0.472\n",
      "Epoch 9537, Loss: 16.748, Final Batch Loss: 0.459\n",
      "Epoch 9538, Loss: 16.813, Final Batch Loss: 0.619\n",
      "Epoch 9539, Loss: 16.866, Final Batch Loss: 0.444\n",
      "Epoch 9540, Loss: 16.992, Final Batch Loss: 0.526\n",
      "Epoch 9541, Loss: 16.643, Final Batch Loss: 0.483\n",
      "Epoch 9542, Loss: 17.037, Final Batch Loss: 0.658\n",
      "Epoch 9543, Loss: 16.912, Final Batch Loss: 0.372\n",
      "Epoch 9544, Loss: 16.597, Final Batch Loss: 0.474\n",
      "Epoch 9545, Loss: 16.672, Final Batch Loss: 0.480\n",
      "Epoch 9546, Loss: 16.890, Final Batch Loss: 0.504\n",
      "Epoch 9547, Loss: 17.025, Final Batch Loss: 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9548, Loss: 16.769, Final Batch Loss: 0.493\n",
      "Epoch 9549, Loss: 16.579, Final Batch Loss: 0.368\n",
      "Epoch 9550, Loss: 16.691, Final Batch Loss: 0.454\n",
      "Epoch 9551, Loss: 16.595, Final Batch Loss: 0.430\n",
      "Epoch 9552, Loss: 16.622, Final Batch Loss: 0.376\n",
      "Epoch 9553, Loss: 16.667, Final Batch Loss: 0.456\n",
      "Epoch 9554, Loss: 16.821, Final Batch Loss: 0.403\n",
      "Epoch 9555, Loss: 16.785, Final Batch Loss: 0.410\n",
      "Epoch 9556, Loss: 16.806, Final Batch Loss: 0.314\n",
      "Epoch 9557, Loss: 16.702, Final Batch Loss: 0.543\n",
      "Epoch 9558, Loss: 16.774, Final Batch Loss: 0.402\n",
      "Epoch 9559, Loss: 16.817, Final Batch Loss: 0.506\n",
      "Epoch 9560, Loss: 16.584, Final Batch Loss: 0.392\n",
      "Epoch 9561, Loss: 16.605, Final Batch Loss: 0.463\n",
      "Epoch 9562, Loss: 16.532, Final Batch Loss: 0.440\n",
      "Epoch 9563, Loss: 16.739, Final Batch Loss: 0.564\n",
      "Epoch 9564, Loss: 16.652, Final Batch Loss: 0.447\n",
      "Epoch 9565, Loss: 16.757, Final Batch Loss: 0.392\n",
      "Epoch 9566, Loss: 16.806, Final Batch Loss: 0.313\n",
      "Epoch 9567, Loss: 16.669, Final Batch Loss: 0.525\n",
      "Epoch 9568, Loss: 16.828, Final Batch Loss: 0.404\n",
      "Epoch 9569, Loss: 17.069, Final Batch Loss: 0.407\n",
      "Epoch 9570, Loss: 16.570, Final Batch Loss: 0.460\n",
      "Epoch 9571, Loss: 16.500, Final Batch Loss: 0.386\n",
      "Epoch 9572, Loss: 16.922, Final Batch Loss: 0.451\n",
      "Epoch 9573, Loss: 16.695, Final Batch Loss: 0.445\n",
      "Epoch 9574, Loss: 16.700, Final Batch Loss: 0.418\n",
      "Epoch 9575, Loss: 16.738, Final Batch Loss: 0.539\n",
      "Epoch 9576, Loss: 16.530, Final Batch Loss: 0.379\n",
      "Epoch 9577, Loss: 16.785, Final Batch Loss: 0.417\n",
      "Epoch 9578, Loss: 16.906, Final Batch Loss: 0.480\n",
      "Epoch 9579, Loss: 16.664, Final Batch Loss: 0.521\n",
      "Epoch 9580, Loss: 16.932, Final Batch Loss: 0.613\n",
      "Epoch 9581, Loss: 16.751, Final Batch Loss: 0.353\n",
      "Epoch 9582, Loss: 16.654, Final Batch Loss: 0.482\n",
      "Epoch 9583, Loss: 16.781, Final Batch Loss: 0.529\n",
      "Epoch 9584, Loss: 16.918, Final Batch Loss: 0.518\n",
      "Epoch 9585, Loss: 16.685, Final Batch Loss: 0.468\n",
      "Epoch 9586, Loss: 16.550, Final Batch Loss: 0.369\n",
      "Epoch 9587, Loss: 16.768, Final Batch Loss: 0.521\n",
      "Epoch 9588, Loss: 16.742, Final Batch Loss: 0.465\n",
      "Epoch 9589, Loss: 16.743, Final Batch Loss: 0.461\n",
      "Epoch 9590, Loss: 16.746, Final Batch Loss: 0.445\n",
      "Epoch 9591, Loss: 16.699, Final Batch Loss: 0.488\n",
      "Epoch 9592, Loss: 16.572, Final Batch Loss: 0.434\n",
      "Epoch 9593, Loss: 16.585, Final Batch Loss: 0.470\n",
      "Epoch 9594, Loss: 16.662, Final Batch Loss: 0.475\n",
      "Epoch 9595, Loss: 16.701, Final Batch Loss: 0.414\n",
      "Epoch 9596, Loss: 16.538, Final Batch Loss: 0.505\n",
      "Epoch 9597, Loss: 16.507, Final Batch Loss: 0.412\n",
      "Epoch 9598, Loss: 16.733, Final Batch Loss: 0.460\n",
      "Epoch 9599, Loss: 16.609, Final Batch Loss: 0.384\n",
      "Epoch 9600, Loss: 16.579, Final Batch Loss: 0.443\n",
      "Epoch 9601, Loss: 16.847, Final Batch Loss: 0.432\n",
      "Epoch 9602, Loss: 16.669, Final Batch Loss: 0.487\n",
      "Epoch 9603, Loss: 16.638, Final Batch Loss: 0.444\n",
      "Epoch 9604, Loss: 16.672, Final Batch Loss: 0.402\n",
      "Epoch 9605, Loss: 16.632, Final Batch Loss: 0.456\n",
      "Epoch 9606, Loss: 16.838, Final Batch Loss: 0.426\n",
      "Epoch 9607, Loss: 16.769, Final Batch Loss: 0.456\n",
      "Epoch 9608, Loss: 16.676, Final Batch Loss: 0.455\n",
      "Epoch 9609, Loss: 16.675, Final Batch Loss: 0.615\n",
      "Epoch 9610, Loss: 16.862, Final Batch Loss: 0.439\n",
      "Epoch 9611, Loss: 16.826, Final Batch Loss: 0.554\n",
      "Epoch 9612, Loss: 16.881, Final Batch Loss: 0.713\n",
      "Epoch 9613, Loss: 16.780, Final Batch Loss: 0.495\n",
      "Epoch 9614, Loss: 16.961, Final Batch Loss: 0.487\n",
      "Epoch 9615, Loss: 16.560, Final Batch Loss: 0.422\n",
      "Epoch 9616, Loss: 16.862, Final Batch Loss: 0.472\n",
      "Epoch 9617, Loss: 16.318, Final Batch Loss: 0.413\n",
      "Epoch 9618, Loss: 16.980, Final Batch Loss: 0.560\n",
      "Epoch 9619, Loss: 16.758, Final Batch Loss: 0.392\n",
      "Epoch 9620, Loss: 16.702, Final Batch Loss: 0.498\n",
      "Epoch 9621, Loss: 16.679, Final Batch Loss: 0.417\n",
      "Epoch 9622, Loss: 16.542, Final Batch Loss: 0.425\n",
      "Epoch 9623, Loss: 16.919, Final Batch Loss: 0.562\n",
      "Epoch 9624, Loss: 16.745, Final Batch Loss: 0.403\n",
      "Epoch 9625, Loss: 16.708, Final Batch Loss: 0.458\n",
      "Epoch 9626, Loss: 16.902, Final Batch Loss: 0.463\n",
      "Epoch 9627, Loss: 16.919, Final Batch Loss: 0.470\n",
      "Epoch 9628, Loss: 16.896, Final Batch Loss: 0.466\n",
      "Epoch 9629, Loss: 16.961, Final Batch Loss: 0.408\n",
      "Epoch 9630, Loss: 16.805, Final Batch Loss: 0.573\n",
      "Epoch 9631, Loss: 16.618, Final Batch Loss: 0.513\n",
      "Epoch 9632, Loss: 16.639, Final Batch Loss: 0.553\n",
      "Epoch 9633, Loss: 16.746, Final Batch Loss: 0.414\n",
      "Epoch 9634, Loss: 16.863, Final Batch Loss: 0.423\n",
      "Epoch 9635, Loss: 16.880, Final Batch Loss: 0.434\n",
      "Epoch 9636, Loss: 17.120, Final Batch Loss: 0.488\n",
      "Epoch 9637, Loss: 16.803, Final Batch Loss: 0.631\n",
      "Epoch 9638, Loss: 16.595, Final Batch Loss: 0.422\n",
      "Epoch 9639, Loss: 16.440, Final Batch Loss: 0.418\n",
      "Epoch 9640, Loss: 16.917, Final Batch Loss: 0.511\n",
      "Epoch 9641, Loss: 16.716, Final Batch Loss: 0.430\n",
      "Epoch 9642, Loss: 16.816, Final Batch Loss: 0.408\n",
      "Epoch 9643, Loss: 16.872, Final Batch Loss: 0.554\n",
      "Epoch 9644, Loss: 16.805, Final Batch Loss: 0.488\n",
      "Epoch 9645, Loss: 16.727, Final Batch Loss: 0.494\n",
      "Epoch 9646, Loss: 16.827, Final Batch Loss: 0.414\n",
      "Epoch 9647, Loss: 16.808, Final Batch Loss: 0.485\n",
      "Epoch 9648, Loss: 16.833, Final Batch Loss: 0.450\n",
      "Epoch 9649, Loss: 16.767, Final Batch Loss: 0.547\n",
      "Epoch 9650, Loss: 16.629, Final Batch Loss: 0.551\n",
      "Epoch 9651, Loss: 16.516, Final Batch Loss: 0.591\n",
      "Epoch 9652, Loss: 16.775, Final Batch Loss: 0.565\n",
      "Epoch 9653, Loss: 16.640, Final Batch Loss: 0.451\n",
      "Epoch 9654, Loss: 16.578, Final Batch Loss: 0.450\n",
      "Epoch 9655, Loss: 17.022, Final Batch Loss: 0.459\n",
      "Epoch 9656, Loss: 16.731, Final Batch Loss: 0.405\n",
      "Epoch 9657, Loss: 16.817, Final Batch Loss: 0.404\n",
      "Epoch 9658, Loss: 16.683, Final Batch Loss: 0.437\n",
      "Epoch 9659, Loss: 16.772, Final Batch Loss: 0.407\n",
      "Epoch 9660, Loss: 16.862, Final Batch Loss: 0.475\n",
      "Epoch 9661, Loss: 16.503, Final Batch Loss: 0.382\n",
      "Epoch 9662, Loss: 16.686, Final Batch Loss: 0.284\n",
      "Epoch 9663, Loss: 16.968, Final Batch Loss: 0.453\n",
      "Epoch 9664, Loss: 16.679, Final Batch Loss: 0.446\n",
      "Epoch 9665, Loss: 16.911, Final Batch Loss: 0.452\n",
      "Epoch 9666, Loss: 16.734, Final Batch Loss: 0.586\n",
      "Epoch 9667, Loss: 16.774, Final Batch Loss: 0.448\n",
      "Epoch 9668, Loss: 16.851, Final Batch Loss: 0.470\n",
      "Epoch 9669, Loss: 16.547, Final Batch Loss: 0.438\n",
      "Epoch 9670, Loss: 17.051, Final Batch Loss: 0.476\n",
      "Epoch 9671, Loss: 16.834, Final Batch Loss: 0.419\n",
      "Epoch 9672, Loss: 16.584, Final Batch Loss: 0.531\n",
      "Epoch 9673, Loss: 16.765, Final Batch Loss: 0.646\n",
      "Epoch 9674, Loss: 16.795, Final Batch Loss: 0.563\n",
      "Epoch 9675, Loss: 16.530, Final Batch Loss: 0.598\n",
      "Epoch 9676, Loss: 16.984, Final Batch Loss: 0.439\n",
      "Epoch 9677, Loss: 16.760, Final Batch Loss: 0.444\n",
      "Epoch 9678, Loss: 16.831, Final Batch Loss: 0.542\n",
      "Epoch 9679, Loss: 17.026, Final Batch Loss: 0.555\n",
      "Epoch 9680, Loss: 16.707, Final Batch Loss: 0.485\n",
      "Epoch 9681, Loss: 16.559, Final Batch Loss: 0.434\n",
      "Epoch 9682, Loss: 16.640, Final Batch Loss: 0.353\n",
      "Epoch 9683, Loss: 16.705, Final Batch Loss: 0.422\n",
      "Epoch 9684, Loss: 16.830, Final Batch Loss: 0.533\n",
      "Epoch 9685, Loss: 16.742, Final Batch Loss: 0.470\n",
      "Epoch 9686, Loss: 16.990, Final Batch Loss: 0.478\n",
      "Epoch 9687, Loss: 16.606, Final Batch Loss: 0.443\n",
      "Epoch 9688, Loss: 16.683, Final Batch Loss: 0.487\n",
      "Epoch 9689, Loss: 16.803, Final Batch Loss: 0.564\n",
      "Epoch 9690, Loss: 16.735, Final Batch Loss: 0.415\n",
      "Epoch 9691, Loss: 16.776, Final Batch Loss: 0.566\n",
      "Epoch 9692, Loss: 16.467, Final Batch Loss: 0.366\n",
      "Epoch 9693, Loss: 17.017, Final Batch Loss: 0.423\n",
      "Epoch 9694, Loss: 16.812, Final Batch Loss: 0.529\n",
      "Epoch 9695, Loss: 16.698, Final Batch Loss: 0.525\n",
      "Epoch 9696, Loss: 16.673, Final Batch Loss: 0.483\n",
      "Epoch 9697, Loss: 16.569, Final Batch Loss: 0.359\n",
      "Epoch 9698, Loss: 16.745, Final Batch Loss: 0.385\n",
      "Epoch 9699, Loss: 16.786, Final Batch Loss: 0.534\n",
      "Epoch 9700, Loss: 16.832, Final Batch Loss: 0.574\n",
      "Epoch 9701, Loss: 16.722, Final Batch Loss: 0.489\n",
      "Epoch 9702, Loss: 16.642, Final Batch Loss: 0.420\n",
      "Epoch 9703, Loss: 16.538, Final Batch Loss: 0.517\n",
      "Epoch 9704, Loss: 16.575, Final Batch Loss: 0.528\n",
      "Epoch 9705, Loss: 16.639, Final Batch Loss: 0.532\n",
      "Epoch 9706, Loss: 16.720, Final Batch Loss: 0.480\n",
      "Epoch 9707, Loss: 16.659, Final Batch Loss: 0.482\n",
      "Epoch 9708, Loss: 16.788, Final Batch Loss: 0.402\n",
      "Epoch 9709, Loss: 16.494, Final Batch Loss: 0.427\n",
      "Epoch 9710, Loss: 16.623, Final Batch Loss: 0.377\n",
      "Epoch 9711, Loss: 16.672, Final Batch Loss: 0.398\n",
      "Epoch 9712, Loss: 16.517, Final Batch Loss: 0.422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9713, Loss: 16.547, Final Batch Loss: 0.434\n",
      "Epoch 9714, Loss: 16.724, Final Batch Loss: 0.609\n",
      "Epoch 9715, Loss: 16.520, Final Batch Loss: 0.411\n",
      "Epoch 9716, Loss: 16.841, Final Batch Loss: 0.480\n",
      "Epoch 9717, Loss: 16.364, Final Batch Loss: 0.482\n",
      "Epoch 9718, Loss: 16.790, Final Batch Loss: 0.481\n",
      "Epoch 9719, Loss: 16.524, Final Batch Loss: 0.372\n",
      "Epoch 9720, Loss: 16.645, Final Batch Loss: 0.413\n",
      "Epoch 9721, Loss: 16.820, Final Batch Loss: 0.500\n",
      "Epoch 9722, Loss: 16.995, Final Batch Loss: 0.558\n",
      "Epoch 9723, Loss: 16.710, Final Batch Loss: 0.460\n",
      "Epoch 9724, Loss: 16.979, Final Batch Loss: 0.463\n",
      "Epoch 9725, Loss: 16.844, Final Batch Loss: 0.507\n",
      "Epoch 9726, Loss: 16.618, Final Batch Loss: 0.486\n",
      "Epoch 9727, Loss: 16.714, Final Batch Loss: 0.512\n",
      "Epoch 9728, Loss: 16.877, Final Batch Loss: 0.569\n",
      "Epoch 9729, Loss: 16.520, Final Batch Loss: 0.592\n",
      "Epoch 9730, Loss: 16.795, Final Batch Loss: 0.404\n",
      "Epoch 9731, Loss: 16.829, Final Batch Loss: 0.414\n",
      "Epoch 9732, Loss: 16.647, Final Batch Loss: 0.527\n",
      "Epoch 9733, Loss: 16.702, Final Batch Loss: 0.412\n",
      "Epoch 9734, Loss: 16.949, Final Batch Loss: 0.495\n",
      "Epoch 9735, Loss: 17.069, Final Batch Loss: 0.512\n",
      "Epoch 9736, Loss: 16.633, Final Batch Loss: 0.388\n",
      "Epoch 9737, Loss: 16.646, Final Batch Loss: 0.509\n",
      "Epoch 9738, Loss: 16.796, Final Batch Loss: 0.433\n",
      "Epoch 9739, Loss: 16.593, Final Batch Loss: 0.481\n",
      "Epoch 9740, Loss: 16.610, Final Batch Loss: 0.505\n",
      "Epoch 9741, Loss: 16.803, Final Batch Loss: 0.487\n",
      "Epoch 9742, Loss: 16.800, Final Batch Loss: 0.471\n",
      "Epoch 9743, Loss: 16.680, Final Batch Loss: 0.400\n",
      "Epoch 9744, Loss: 16.979, Final Batch Loss: 0.370\n",
      "Epoch 9745, Loss: 16.704, Final Batch Loss: 0.555\n",
      "Epoch 9746, Loss: 16.776, Final Batch Loss: 0.405\n",
      "Epoch 9747, Loss: 16.843, Final Batch Loss: 0.460\n",
      "Epoch 9748, Loss: 16.711, Final Batch Loss: 0.385\n",
      "Epoch 9749, Loss: 16.843, Final Batch Loss: 0.463\n",
      "Epoch 9750, Loss: 17.000, Final Batch Loss: 0.463\n",
      "Epoch 9751, Loss: 16.728, Final Batch Loss: 0.370\n",
      "Epoch 9752, Loss: 16.771, Final Batch Loss: 0.393\n",
      "Epoch 9753, Loss: 16.812, Final Batch Loss: 0.550\n",
      "Epoch 9754, Loss: 16.778, Final Batch Loss: 0.424\n",
      "Epoch 9755, Loss: 16.726, Final Batch Loss: 0.517\n",
      "Epoch 9756, Loss: 16.739, Final Batch Loss: 0.485\n",
      "Epoch 9757, Loss: 16.687, Final Batch Loss: 0.414\n",
      "Epoch 9758, Loss: 16.556, Final Batch Loss: 0.415\n",
      "Epoch 9759, Loss: 16.578, Final Batch Loss: 0.375\n",
      "Epoch 9760, Loss: 16.950, Final Batch Loss: 0.430\n",
      "Epoch 9761, Loss: 16.799, Final Batch Loss: 0.513\n",
      "Epoch 9762, Loss: 16.565, Final Batch Loss: 0.470\n",
      "Epoch 9763, Loss: 16.645, Final Batch Loss: 0.447\n",
      "Epoch 9764, Loss: 16.653, Final Batch Loss: 0.557\n",
      "Epoch 9765, Loss: 16.528, Final Batch Loss: 0.454\n",
      "Epoch 9766, Loss: 16.962, Final Batch Loss: 0.501\n",
      "Epoch 9767, Loss: 16.637, Final Batch Loss: 0.366\n",
      "Epoch 9768, Loss: 16.751, Final Batch Loss: 0.509\n",
      "Epoch 9769, Loss: 16.621, Final Batch Loss: 0.517\n",
      "Epoch 9770, Loss: 16.613, Final Batch Loss: 0.528\n",
      "Epoch 9771, Loss: 16.476, Final Batch Loss: 0.467\n",
      "Epoch 9772, Loss: 16.770, Final Batch Loss: 0.476\n",
      "Epoch 9773, Loss: 16.572, Final Batch Loss: 0.481\n",
      "Epoch 9774, Loss: 16.676, Final Batch Loss: 0.521\n",
      "Epoch 9775, Loss: 16.678, Final Batch Loss: 0.378\n",
      "Epoch 9776, Loss: 17.005, Final Batch Loss: 0.465\n",
      "Epoch 9777, Loss: 16.968, Final Batch Loss: 0.521\n",
      "Epoch 9778, Loss: 16.854, Final Batch Loss: 0.557\n",
      "Epoch 9779, Loss: 16.694, Final Batch Loss: 0.500\n",
      "Epoch 9780, Loss: 16.647, Final Batch Loss: 0.419\n",
      "Epoch 9781, Loss: 16.968, Final Batch Loss: 0.422\n",
      "Epoch 9782, Loss: 16.735, Final Batch Loss: 0.387\n",
      "Epoch 9783, Loss: 16.742, Final Batch Loss: 0.483\n",
      "Epoch 9784, Loss: 16.863, Final Batch Loss: 0.518\n",
      "Epoch 9785, Loss: 16.589, Final Batch Loss: 0.477\n",
      "Epoch 9786, Loss: 16.834, Final Batch Loss: 0.449\n",
      "Epoch 9787, Loss: 16.992, Final Batch Loss: 0.462\n",
      "Epoch 9788, Loss: 16.778, Final Batch Loss: 0.382\n",
      "Epoch 9789, Loss: 16.638, Final Batch Loss: 0.497\n",
      "Epoch 9790, Loss: 16.812, Final Batch Loss: 0.498\n",
      "Epoch 9791, Loss: 16.511, Final Batch Loss: 0.402\n",
      "Epoch 9792, Loss: 16.766, Final Batch Loss: 0.477\n",
      "Epoch 9793, Loss: 16.622, Final Batch Loss: 0.539\n",
      "Epoch 9794, Loss: 16.893, Final Batch Loss: 0.487\n",
      "Epoch 9795, Loss: 16.642, Final Batch Loss: 0.570\n",
      "Epoch 9796, Loss: 16.503, Final Batch Loss: 0.498\n",
      "Epoch 9797, Loss: 16.586, Final Batch Loss: 0.439\n",
      "Epoch 9798, Loss: 16.687, Final Batch Loss: 0.460\n",
      "Epoch 9799, Loss: 16.873, Final Batch Loss: 0.452\n",
      "Epoch 9800, Loss: 16.708, Final Batch Loss: 0.452\n",
      "Epoch 9801, Loss: 16.674, Final Batch Loss: 0.390\n",
      "Epoch 9802, Loss: 16.452, Final Batch Loss: 0.490\n",
      "Epoch 9803, Loss: 16.856, Final Batch Loss: 0.560\n",
      "Epoch 9804, Loss: 16.577, Final Batch Loss: 0.394\n",
      "Epoch 9805, Loss: 16.781, Final Batch Loss: 0.356\n",
      "Epoch 9806, Loss: 16.842, Final Batch Loss: 0.466\n",
      "Epoch 9807, Loss: 16.777, Final Batch Loss: 0.409\n",
      "Epoch 9808, Loss: 16.894, Final Batch Loss: 0.360\n",
      "Epoch 9809, Loss: 16.710, Final Batch Loss: 0.455\n",
      "Epoch 9810, Loss: 16.876, Final Batch Loss: 0.464\n",
      "Epoch 9811, Loss: 16.952, Final Batch Loss: 0.446\n",
      "Epoch 9812, Loss: 16.538, Final Batch Loss: 0.464\n",
      "Epoch 9813, Loss: 16.622, Final Batch Loss: 0.434\n",
      "Epoch 9814, Loss: 16.594, Final Batch Loss: 0.463\n",
      "Epoch 9815, Loss: 16.598, Final Batch Loss: 0.373\n",
      "Epoch 9816, Loss: 16.473, Final Batch Loss: 0.384\n",
      "Epoch 9817, Loss: 16.988, Final Batch Loss: 0.483\n",
      "Epoch 9818, Loss: 16.656, Final Batch Loss: 0.428\n",
      "Epoch 9819, Loss: 16.676, Final Batch Loss: 0.415\n",
      "Epoch 9820, Loss: 16.770, Final Batch Loss: 0.511\n",
      "Epoch 9821, Loss: 16.703, Final Batch Loss: 0.455\n",
      "Epoch 9822, Loss: 16.879, Final Batch Loss: 0.495\n",
      "Epoch 9823, Loss: 16.692, Final Batch Loss: 0.450\n",
      "Epoch 9824, Loss: 16.633, Final Batch Loss: 0.609\n",
      "Epoch 9825, Loss: 16.754, Final Batch Loss: 0.419\n",
      "Epoch 9826, Loss: 16.655, Final Batch Loss: 0.455\n",
      "Epoch 9827, Loss: 16.638, Final Batch Loss: 0.451\n",
      "Epoch 9828, Loss: 16.604, Final Batch Loss: 0.473\n",
      "Epoch 9829, Loss: 16.616, Final Batch Loss: 0.496\n",
      "Epoch 9830, Loss: 16.818, Final Batch Loss: 0.539\n",
      "Epoch 9831, Loss: 16.640, Final Batch Loss: 0.475\n",
      "Epoch 9832, Loss: 16.600, Final Batch Loss: 0.431\n",
      "Epoch 9833, Loss: 16.517, Final Batch Loss: 0.516\n",
      "Epoch 9834, Loss: 17.059, Final Batch Loss: 0.472\n",
      "Epoch 9835, Loss: 16.653, Final Batch Loss: 0.521\n",
      "Epoch 9836, Loss: 16.555, Final Batch Loss: 0.440\n",
      "Epoch 9837, Loss: 16.738, Final Batch Loss: 0.450\n",
      "Epoch 9838, Loss: 16.879, Final Batch Loss: 0.342\n",
      "Epoch 9839, Loss: 16.670, Final Batch Loss: 0.510\n",
      "Epoch 9840, Loss: 16.795, Final Batch Loss: 0.431\n",
      "Epoch 9841, Loss: 16.795, Final Batch Loss: 0.481\n",
      "Epoch 9842, Loss: 16.544, Final Batch Loss: 0.424\n",
      "Epoch 9843, Loss: 16.612, Final Batch Loss: 0.435\n",
      "Epoch 9844, Loss: 16.497, Final Batch Loss: 0.336\n",
      "Epoch 9845, Loss: 16.963, Final Batch Loss: 0.497\n",
      "Epoch 9846, Loss: 16.727, Final Batch Loss: 0.509\n",
      "Epoch 9847, Loss: 16.651, Final Batch Loss: 0.574\n",
      "Epoch 9848, Loss: 16.861, Final Batch Loss: 0.538\n",
      "Epoch 9849, Loss: 16.618, Final Batch Loss: 0.423\n",
      "Epoch 9850, Loss: 16.879, Final Batch Loss: 0.574\n",
      "Epoch 9851, Loss: 16.931, Final Batch Loss: 0.378\n",
      "Epoch 9852, Loss: 16.749, Final Batch Loss: 0.485\n",
      "Epoch 9853, Loss: 16.528, Final Batch Loss: 0.451\n",
      "Epoch 9854, Loss: 16.864, Final Batch Loss: 0.466\n",
      "Epoch 9855, Loss: 16.548, Final Batch Loss: 0.457\n",
      "Epoch 9856, Loss: 17.142, Final Batch Loss: 0.464\n",
      "Epoch 9857, Loss: 16.895, Final Batch Loss: 0.374\n",
      "Epoch 9858, Loss: 16.615, Final Batch Loss: 0.485\n",
      "Epoch 9859, Loss: 16.734, Final Batch Loss: 0.452\n",
      "Epoch 9860, Loss: 16.866, Final Batch Loss: 0.545\n",
      "Epoch 9861, Loss: 16.673, Final Batch Loss: 0.485\n",
      "Epoch 9862, Loss: 16.767, Final Batch Loss: 0.363\n",
      "Epoch 9863, Loss: 16.798, Final Batch Loss: 0.660\n",
      "Epoch 9864, Loss: 16.618, Final Batch Loss: 0.460\n",
      "Epoch 9865, Loss: 16.819, Final Batch Loss: 0.523\n",
      "Epoch 9866, Loss: 16.523, Final Batch Loss: 0.370\n",
      "Epoch 9867, Loss: 16.531, Final Batch Loss: 0.440\n",
      "Epoch 9868, Loss: 16.845, Final Batch Loss: 0.407\n",
      "Epoch 9869, Loss: 17.049, Final Batch Loss: 0.548\n",
      "Epoch 9870, Loss: 16.645, Final Batch Loss: 0.531\n",
      "Epoch 9871, Loss: 16.660, Final Batch Loss: 0.413\n",
      "Epoch 9872, Loss: 16.603, Final Batch Loss: 0.414\n",
      "Epoch 9873, Loss: 16.649, Final Batch Loss: 0.447\n",
      "Epoch 9874, Loss: 16.718, Final Batch Loss: 0.388\n",
      "Epoch 9875, Loss: 16.645, Final Batch Loss: 0.416\n",
      "Epoch 9876, Loss: 16.662, Final Batch Loss: 0.519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9877, Loss: 16.704, Final Batch Loss: 0.468\n",
      "Epoch 9878, Loss: 16.766, Final Batch Loss: 0.452\n",
      "Epoch 9879, Loss: 16.720, Final Batch Loss: 0.450\n",
      "Epoch 9880, Loss: 16.775, Final Batch Loss: 0.355\n",
      "Epoch 9881, Loss: 16.572, Final Batch Loss: 0.402\n",
      "Epoch 9882, Loss: 16.810, Final Batch Loss: 0.448\n",
      "Epoch 9883, Loss: 16.683, Final Batch Loss: 0.411\n",
      "Epoch 9884, Loss: 16.623, Final Batch Loss: 0.426\n",
      "Epoch 9885, Loss: 16.903, Final Batch Loss: 0.430\n",
      "Epoch 9886, Loss: 16.831, Final Batch Loss: 0.444\n",
      "Epoch 9887, Loss: 16.489, Final Batch Loss: 0.455\n",
      "Epoch 9888, Loss: 17.010, Final Batch Loss: 0.504\n",
      "Epoch 9889, Loss: 16.843, Final Batch Loss: 0.505\n",
      "Epoch 9890, Loss: 16.657, Final Batch Loss: 0.397\n",
      "Epoch 9891, Loss: 16.568, Final Batch Loss: 0.443\n",
      "Epoch 9892, Loss: 16.534, Final Batch Loss: 0.335\n",
      "Epoch 9893, Loss: 16.635, Final Batch Loss: 0.433\n",
      "Epoch 9894, Loss: 16.926, Final Batch Loss: 0.623\n",
      "Epoch 9895, Loss: 16.921, Final Batch Loss: 0.588\n",
      "Epoch 9896, Loss: 16.821, Final Batch Loss: 0.477\n",
      "Epoch 9897, Loss: 16.543, Final Batch Loss: 0.494\n",
      "Epoch 9898, Loss: 16.467, Final Batch Loss: 0.403\n",
      "Epoch 9899, Loss: 16.479, Final Batch Loss: 0.433\n",
      "Epoch 9900, Loss: 16.583, Final Batch Loss: 0.496\n",
      "Epoch 9901, Loss: 16.876, Final Batch Loss: 0.467\n",
      "Epoch 9902, Loss: 16.815, Final Batch Loss: 0.501\n",
      "Epoch 9903, Loss: 16.553, Final Batch Loss: 0.466\n",
      "Epoch 9904, Loss: 16.721, Final Batch Loss: 0.545\n",
      "Epoch 9905, Loss: 16.897, Final Batch Loss: 0.542\n",
      "Epoch 9906, Loss: 16.930, Final Batch Loss: 0.517\n",
      "Epoch 9907, Loss: 16.771, Final Batch Loss: 0.518\n",
      "Epoch 9908, Loss: 16.747, Final Batch Loss: 0.599\n",
      "Epoch 9909, Loss: 16.744, Final Batch Loss: 0.454\n",
      "Epoch 9910, Loss: 16.275, Final Batch Loss: 0.426\n",
      "Epoch 9911, Loss: 16.488, Final Batch Loss: 0.402\n",
      "Epoch 9912, Loss: 16.677, Final Batch Loss: 0.551\n",
      "Epoch 9913, Loss: 16.948, Final Batch Loss: 0.498\n",
      "Epoch 9914, Loss: 16.779, Final Batch Loss: 0.656\n",
      "Epoch 9915, Loss: 17.002, Final Batch Loss: 0.457\n",
      "Epoch 9916, Loss: 16.604, Final Batch Loss: 0.514\n",
      "Epoch 9917, Loss: 16.970, Final Batch Loss: 0.511\n",
      "Epoch 9918, Loss: 16.560, Final Batch Loss: 0.378\n",
      "Epoch 9919, Loss: 16.905, Final Batch Loss: 0.551\n",
      "Epoch 9920, Loss: 16.891, Final Batch Loss: 0.438\n",
      "Epoch 9921, Loss: 16.806, Final Batch Loss: 0.423\n",
      "Epoch 9922, Loss: 16.763, Final Batch Loss: 0.502\n",
      "Epoch 9923, Loss: 16.580, Final Batch Loss: 0.563\n",
      "Epoch 9924, Loss: 16.485, Final Batch Loss: 0.594\n",
      "Epoch 9925, Loss: 16.863, Final Batch Loss: 0.522\n",
      "Epoch 9926, Loss: 16.613, Final Batch Loss: 0.578\n",
      "Epoch 9927, Loss: 16.926, Final Batch Loss: 0.381\n",
      "Epoch 9928, Loss: 16.656, Final Batch Loss: 0.479\n",
      "Epoch 9929, Loss: 16.534, Final Batch Loss: 0.397\n",
      "Epoch 9930, Loss: 16.669, Final Batch Loss: 0.505\n",
      "Epoch 9931, Loss: 16.725, Final Batch Loss: 0.454\n",
      "Epoch 9932, Loss: 16.773, Final Batch Loss: 0.440\n",
      "Epoch 9933, Loss: 16.619, Final Batch Loss: 0.407\n",
      "Epoch 9934, Loss: 16.772, Final Batch Loss: 0.385\n",
      "Epoch 9935, Loss: 16.633, Final Batch Loss: 0.408\n",
      "Epoch 9936, Loss: 16.737, Final Batch Loss: 0.366\n",
      "Epoch 9937, Loss: 16.680, Final Batch Loss: 0.488\n",
      "Epoch 9938, Loss: 16.768, Final Batch Loss: 0.566\n",
      "Epoch 9939, Loss: 16.509, Final Batch Loss: 0.477\n",
      "Epoch 9940, Loss: 16.748, Final Batch Loss: 0.523\n",
      "Epoch 9941, Loss: 16.622, Final Batch Loss: 0.388\n",
      "Epoch 9942, Loss: 16.631, Final Batch Loss: 0.502\n",
      "Epoch 9943, Loss: 17.137, Final Batch Loss: 0.560\n",
      "Epoch 9944, Loss: 16.900, Final Batch Loss: 0.455\n",
      "Epoch 9945, Loss: 16.870, Final Batch Loss: 0.405\n",
      "Epoch 9946, Loss: 16.604, Final Batch Loss: 0.498\n",
      "Epoch 9947, Loss: 16.678, Final Batch Loss: 0.422\n",
      "Epoch 9948, Loss: 16.798, Final Batch Loss: 0.436\n",
      "Epoch 9949, Loss: 16.840, Final Batch Loss: 0.474\n",
      "Epoch 9950, Loss: 16.590, Final Batch Loss: 0.489\n",
      "Epoch 9951, Loss: 16.689, Final Batch Loss: 0.389\n",
      "Epoch 9952, Loss: 16.728, Final Batch Loss: 0.404\n",
      "Epoch 9953, Loss: 16.473, Final Batch Loss: 0.401\n",
      "Epoch 9954, Loss: 16.434, Final Batch Loss: 0.514\n",
      "Epoch 9955, Loss: 16.556, Final Batch Loss: 0.442\n",
      "Epoch 9956, Loss: 16.859, Final Batch Loss: 0.578\n",
      "Epoch 9957, Loss: 16.720, Final Batch Loss: 0.489\n",
      "Epoch 9958, Loss: 16.671, Final Batch Loss: 0.547\n",
      "Epoch 9959, Loss: 16.759, Final Batch Loss: 0.342\n",
      "Epoch 9960, Loss: 16.847, Final Batch Loss: 0.425\n",
      "Epoch 9961, Loss: 16.549, Final Batch Loss: 0.501\n",
      "Epoch 9962, Loss: 16.880, Final Batch Loss: 0.437\n",
      "Epoch 9963, Loss: 16.707, Final Batch Loss: 0.555\n",
      "Epoch 9964, Loss: 16.842, Final Batch Loss: 0.469\n",
      "Epoch 9965, Loss: 16.930, Final Batch Loss: 0.443\n",
      "Epoch 9966, Loss: 16.675, Final Batch Loss: 0.481\n",
      "Epoch 9967, Loss: 17.052, Final Batch Loss: 0.550\n",
      "Epoch 9968, Loss: 16.446, Final Batch Loss: 0.354\n",
      "Epoch 9969, Loss: 16.801, Final Batch Loss: 0.525\n",
      "Epoch 9970, Loss: 16.728, Final Batch Loss: 0.482\n",
      "Epoch 9971, Loss: 16.754, Final Batch Loss: 0.434\n",
      "Epoch 9972, Loss: 16.985, Final Batch Loss: 0.482\n",
      "Epoch 9973, Loss: 16.757, Final Batch Loss: 0.521\n",
      "Epoch 9974, Loss: 16.822, Final Batch Loss: 0.450\n",
      "Epoch 9975, Loss: 16.462, Final Batch Loss: 0.533\n",
      "Epoch 9976, Loss: 16.864, Final Batch Loss: 0.516\n",
      "Epoch 9977, Loss: 16.548, Final Batch Loss: 0.426\n",
      "Epoch 9978, Loss: 16.647, Final Batch Loss: 0.518\n",
      "Epoch 9979, Loss: 16.552, Final Batch Loss: 0.368\n",
      "Epoch 9980, Loss: 16.869, Final Batch Loss: 0.407\n",
      "Epoch 9981, Loss: 16.482, Final Batch Loss: 0.420\n",
      "Epoch 9982, Loss: 16.859, Final Batch Loss: 0.457\n",
      "Epoch 9983, Loss: 16.703, Final Batch Loss: 0.452\n",
      "Epoch 9984, Loss: 16.650, Final Batch Loss: 0.382\n",
      "Epoch 9985, Loss: 16.624, Final Batch Loss: 0.520\n",
      "Epoch 9986, Loss: 16.764, Final Batch Loss: 0.462\n",
      "Epoch 9987, Loss: 16.840, Final Batch Loss: 0.455\n",
      "Epoch 9988, Loss: 16.886, Final Batch Loss: 0.565\n",
      "Epoch 9989, Loss: 16.647, Final Batch Loss: 0.503\n",
      "Epoch 9990, Loss: 16.811, Final Batch Loss: 0.461\n",
      "Epoch 9991, Loss: 16.980, Final Batch Loss: 0.537\n",
      "Epoch 9992, Loss: 16.708, Final Batch Loss: 0.506\n",
      "Epoch 9993, Loss: 16.886, Final Batch Loss: 0.546\n",
      "Epoch 9994, Loss: 16.710, Final Batch Loss: 0.465\n",
      "Epoch 9995, Loss: 16.639, Final Batch Loss: 0.522\n",
      "Epoch 9996, Loss: 16.728, Final Batch Loss: 0.447\n",
      "Epoch 9997, Loss: 16.433, Final Batch Loss: 0.346\n",
      "Epoch 9998, Loss: 16.934, Final Batch Loss: 0.435\n",
      "Epoch 9999, Loss: 16.878, Final Batch Loss: 0.386\n",
      "Epoch 10000, Loss: 16.577, Final Batch Loss: 0.452\n",
      "Epoch 10001, Loss: 16.945, Final Batch Loss: 0.408\n",
      "Epoch 10002, Loss: 16.704, Final Batch Loss: 0.402\n",
      "Epoch 10003, Loss: 16.581, Final Batch Loss: 0.386\n",
      "Epoch 10004, Loss: 16.808, Final Batch Loss: 0.383\n",
      "Epoch 10005, Loss: 16.790, Final Batch Loss: 0.463\n",
      "Epoch 10006, Loss: 16.808, Final Batch Loss: 0.467\n",
      "Epoch 10007, Loss: 16.746, Final Batch Loss: 0.530\n",
      "Epoch 10008, Loss: 16.678, Final Batch Loss: 0.376\n",
      "Epoch 10009, Loss: 17.130, Final Batch Loss: 0.571\n",
      "Epoch 10010, Loss: 16.713, Final Batch Loss: 0.502\n",
      "Epoch 10011, Loss: 16.479, Final Batch Loss: 0.425\n",
      "Epoch 10012, Loss: 16.592, Final Batch Loss: 0.353\n",
      "Epoch 10013, Loss: 16.644, Final Batch Loss: 0.527\n",
      "Epoch 10014, Loss: 16.670, Final Batch Loss: 0.455\n",
      "Epoch 10015, Loss: 16.607, Final Batch Loss: 0.356\n",
      "Epoch 10016, Loss: 16.845, Final Batch Loss: 0.510\n",
      "Epoch 10017, Loss: 17.009, Final Batch Loss: 0.527\n",
      "Epoch 10018, Loss: 16.816, Final Batch Loss: 0.501\n",
      "Epoch 10019, Loss: 16.695, Final Batch Loss: 0.441\n",
      "Epoch 10020, Loss: 16.633, Final Batch Loss: 0.452\n",
      "Epoch 10021, Loss: 16.742, Final Batch Loss: 0.485\n",
      "Epoch 10022, Loss: 16.686, Final Batch Loss: 0.496\n",
      "Epoch 10023, Loss: 16.610, Final Batch Loss: 0.586\n",
      "Epoch 10024, Loss: 16.724, Final Batch Loss: 0.522\n",
      "Epoch 10025, Loss: 16.707, Final Batch Loss: 0.485\n",
      "Epoch 10026, Loss: 16.759, Final Batch Loss: 0.435\n",
      "Epoch 10027, Loss: 16.750, Final Batch Loss: 0.460\n",
      "Epoch 10028, Loss: 16.502, Final Batch Loss: 0.489\n",
      "Epoch 10029, Loss: 16.689, Final Batch Loss: 0.428\n",
      "Epoch 10030, Loss: 16.516, Final Batch Loss: 0.429\n",
      "Epoch 10031, Loss: 16.983, Final Batch Loss: 0.430\n",
      "Epoch 10032, Loss: 16.576, Final Batch Loss: 0.443\n",
      "Epoch 10033, Loss: 16.446, Final Batch Loss: 0.537\n",
      "Epoch 10034, Loss: 16.594, Final Batch Loss: 0.334\n",
      "Epoch 10035, Loss: 16.914, Final Batch Loss: 0.444\n",
      "Epoch 10036, Loss: 16.513, Final Batch Loss: 0.421\n",
      "Epoch 10037, Loss: 16.686, Final Batch Loss: 0.402\n",
      "Epoch 10038, Loss: 16.752, Final Batch Loss: 0.481\n",
      "Epoch 10039, Loss: 16.470, Final Batch Loss: 0.465\n",
      "Epoch 10040, Loss: 16.538, Final Batch Loss: 0.414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10041, Loss: 16.610, Final Batch Loss: 0.518\n",
      "Epoch 10042, Loss: 16.648, Final Batch Loss: 0.469\n",
      "Epoch 10043, Loss: 17.064, Final Batch Loss: 0.581\n",
      "Epoch 10044, Loss: 16.743, Final Batch Loss: 0.497\n",
      "Epoch 10045, Loss: 16.845, Final Batch Loss: 0.514\n",
      "Epoch 10046, Loss: 16.689, Final Batch Loss: 0.404\n",
      "Epoch 10047, Loss: 16.800, Final Batch Loss: 0.414\n",
      "Epoch 10048, Loss: 16.537, Final Batch Loss: 0.468\n",
      "Epoch 10049, Loss: 16.657, Final Batch Loss: 0.475\n",
      "Epoch 10050, Loss: 16.823, Final Batch Loss: 0.394\n",
      "Epoch 10051, Loss: 16.588, Final Batch Loss: 0.443\n",
      "Epoch 10052, Loss: 16.625, Final Batch Loss: 0.399\n",
      "Epoch 10053, Loss: 16.688, Final Batch Loss: 0.535\n",
      "Epoch 10054, Loss: 16.938, Final Batch Loss: 0.513\n",
      "Epoch 10055, Loss: 16.998, Final Batch Loss: 0.439\n",
      "Epoch 10056, Loss: 16.691, Final Batch Loss: 0.500\n",
      "Epoch 10057, Loss: 16.717, Final Batch Loss: 0.590\n",
      "Epoch 10058, Loss: 16.483, Final Batch Loss: 0.463\n",
      "Epoch 10059, Loss: 16.756, Final Batch Loss: 0.431\n",
      "Epoch 10060, Loss: 16.648, Final Batch Loss: 0.466\n",
      "Epoch 10061, Loss: 16.527, Final Batch Loss: 0.487\n",
      "Epoch 10062, Loss: 16.773, Final Batch Loss: 0.518\n",
      "Epoch 10063, Loss: 16.902, Final Batch Loss: 0.520\n",
      "Epoch 10064, Loss: 16.802, Final Batch Loss: 0.425\n",
      "Epoch 10065, Loss: 16.607, Final Batch Loss: 0.469\n",
      "Epoch 10066, Loss: 16.698, Final Batch Loss: 0.508\n",
      "Epoch 10067, Loss: 16.722, Final Batch Loss: 0.543\n",
      "Epoch 10068, Loss: 16.741, Final Batch Loss: 0.403\n",
      "Epoch 10069, Loss: 17.121, Final Batch Loss: 0.575\n",
      "Epoch 10070, Loss: 16.727, Final Batch Loss: 0.477\n",
      "Epoch 10071, Loss: 16.689, Final Batch Loss: 0.413\n",
      "Epoch 10072, Loss: 16.545, Final Batch Loss: 0.418\n",
      "Epoch 10073, Loss: 16.359, Final Batch Loss: 0.526\n",
      "Epoch 10074, Loss: 16.804, Final Batch Loss: 0.533\n",
      "Epoch 10075, Loss: 16.382, Final Batch Loss: 0.388\n",
      "Epoch 10076, Loss: 16.950, Final Batch Loss: 0.481\n",
      "Epoch 10077, Loss: 16.763, Final Batch Loss: 0.445\n",
      "Epoch 10078, Loss: 16.779, Final Batch Loss: 0.415\n",
      "Epoch 10079, Loss: 16.668, Final Batch Loss: 0.619\n",
      "Epoch 10080, Loss: 16.877, Final Batch Loss: 0.432\n",
      "Epoch 10081, Loss: 16.837, Final Batch Loss: 0.551\n",
      "Epoch 10082, Loss: 16.878, Final Batch Loss: 0.560\n",
      "Epoch 10083, Loss: 16.642, Final Batch Loss: 0.479\n",
      "Epoch 10084, Loss: 16.676, Final Batch Loss: 0.388\n",
      "Epoch 10085, Loss: 16.791, Final Batch Loss: 0.425\n",
      "Epoch 10086, Loss: 16.665, Final Batch Loss: 0.485\n",
      "Epoch 10087, Loss: 16.767, Final Batch Loss: 0.366\n",
      "Epoch 10088, Loss: 16.826, Final Batch Loss: 0.571\n",
      "Epoch 10089, Loss: 16.817, Final Batch Loss: 0.436\n",
      "Epoch 10090, Loss: 16.461, Final Batch Loss: 0.442\n",
      "Epoch 10091, Loss: 16.867, Final Batch Loss: 0.480\n",
      "Epoch 10092, Loss: 16.930, Final Batch Loss: 0.530\n",
      "Epoch 10093, Loss: 16.545, Final Batch Loss: 0.515\n",
      "Epoch 10094, Loss: 16.910, Final Batch Loss: 0.528\n",
      "Epoch 10095, Loss: 16.483, Final Batch Loss: 0.410\n",
      "Epoch 10096, Loss: 16.836, Final Batch Loss: 0.548\n",
      "Epoch 10097, Loss: 16.739, Final Batch Loss: 0.507\n",
      "Epoch 10098, Loss: 16.551, Final Batch Loss: 0.435\n",
      "Epoch 10099, Loss: 16.741, Final Batch Loss: 0.643\n",
      "Epoch 10100, Loss: 16.756, Final Batch Loss: 0.650\n",
      "Epoch 10101, Loss: 16.719, Final Batch Loss: 0.523\n",
      "Epoch 10102, Loss: 16.920, Final Batch Loss: 0.513\n",
      "Epoch 10103, Loss: 16.768, Final Batch Loss: 0.594\n",
      "Epoch 10104, Loss: 16.653, Final Batch Loss: 0.392\n",
      "Epoch 10105, Loss: 16.906, Final Batch Loss: 0.428\n",
      "Epoch 10106, Loss: 16.663, Final Batch Loss: 0.464\n",
      "Epoch 10107, Loss: 16.326, Final Batch Loss: 0.468\n",
      "Epoch 10108, Loss: 16.684, Final Batch Loss: 0.415\n",
      "Epoch 10109, Loss: 16.689, Final Batch Loss: 0.420\n",
      "Epoch 10110, Loss: 16.465, Final Batch Loss: 0.392\n",
      "Epoch 10111, Loss: 16.540, Final Batch Loss: 0.501\n",
      "Epoch 10112, Loss: 16.633, Final Batch Loss: 0.431\n",
      "Epoch 10113, Loss: 16.971, Final Batch Loss: 0.577\n",
      "Epoch 10114, Loss: 16.876, Final Batch Loss: 0.488\n",
      "Epoch 10115, Loss: 16.700, Final Batch Loss: 0.436\n",
      "Epoch 10116, Loss: 16.753, Final Batch Loss: 0.501\n",
      "Epoch 10117, Loss: 16.494, Final Batch Loss: 0.372\n",
      "Epoch 10118, Loss: 16.798, Final Batch Loss: 0.530\n",
      "Epoch 10119, Loss: 16.796, Final Batch Loss: 0.430\n",
      "Epoch 10120, Loss: 16.497, Final Batch Loss: 0.422\n",
      "Epoch 10121, Loss: 16.501, Final Batch Loss: 0.439\n",
      "Epoch 10122, Loss: 16.650, Final Batch Loss: 0.408\n",
      "Epoch 10123, Loss: 16.795, Final Batch Loss: 0.481\n",
      "Epoch 10124, Loss: 16.828, Final Batch Loss: 0.419\n",
      "Epoch 10125, Loss: 16.812, Final Batch Loss: 0.447\n",
      "Epoch 10126, Loss: 16.573, Final Batch Loss: 0.446\n",
      "Epoch 10127, Loss: 16.728, Final Batch Loss: 0.419\n",
      "Epoch 10128, Loss: 16.675, Final Batch Loss: 0.483\n",
      "Epoch 10129, Loss: 16.670, Final Batch Loss: 0.451\n",
      "Epoch 10130, Loss: 16.786, Final Batch Loss: 0.413\n",
      "Epoch 10131, Loss: 16.959, Final Batch Loss: 0.522\n",
      "Epoch 10132, Loss: 16.539, Final Batch Loss: 0.463\n",
      "Epoch 10133, Loss: 16.671, Final Batch Loss: 0.464\n",
      "Epoch 10134, Loss: 16.723, Final Batch Loss: 0.496\n",
      "Epoch 10135, Loss: 16.708, Final Batch Loss: 0.418\n",
      "Epoch 10136, Loss: 16.910, Final Batch Loss: 0.449\n",
      "Epoch 10137, Loss: 16.698, Final Batch Loss: 0.470\n",
      "Epoch 10138, Loss: 16.583, Final Batch Loss: 0.431\n",
      "Epoch 10139, Loss: 16.639, Final Batch Loss: 0.327\n",
      "Epoch 10140, Loss: 16.874, Final Batch Loss: 0.649\n",
      "Epoch 10141, Loss: 16.627, Final Batch Loss: 0.464\n",
      "Epoch 10142, Loss: 16.871, Final Batch Loss: 0.522\n",
      "Epoch 10143, Loss: 16.839, Final Batch Loss: 0.416\n",
      "Epoch 10144, Loss: 16.999, Final Batch Loss: 0.472\n",
      "Epoch 10145, Loss: 16.771, Final Batch Loss: 0.409\n",
      "Epoch 10146, Loss: 16.555, Final Batch Loss: 0.377\n",
      "Epoch 10147, Loss: 16.345, Final Batch Loss: 0.394\n",
      "Epoch 10148, Loss: 16.590, Final Batch Loss: 0.548\n",
      "Epoch 10149, Loss: 16.967, Final Batch Loss: 0.519\n",
      "Epoch 10150, Loss: 16.591, Final Batch Loss: 0.362\n",
      "Epoch 10151, Loss: 16.443, Final Batch Loss: 0.442\n",
      "Epoch 10152, Loss: 16.732, Final Batch Loss: 0.505\n",
      "Epoch 10153, Loss: 16.770, Final Batch Loss: 0.529\n",
      "Epoch 10154, Loss: 16.736, Final Batch Loss: 0.417\n",
      "Epoch 10155, Loss: 16.559, Final Batch Loss: 0.459\n",
      "Epoch 10156, Loss: 16.604, Final Batch Loss: 0.416\n",
      "Epoch 10157, Loss: 16.758, Final Batch Loss: 0.480\n",
      "Epoch 10158, Loss: 16.682, Final Batch Loss: 0.499\n",
      "Epoch 10159, Loss: 16.750, Final Batch Loss: 0.461\n",
      "Epoch 10160, Loss: 16.864, Final Batch Loss: 0.515\n",
      "Epoch 10161, Loss: 16.810, Final Batch Loss: 0.649\n",
      "Epoch 10162, Loss: 16.412, Final Batch Loss: 0.471\n",
      "Epoch 10163, Loss: 16.810, Final Batch Loss: 0.486\n",
      "Epoch 10164, Loss: 16.731, Final Batch Loss: 0.409\n",
      "Epoch 10165, Loss: 16.905, Final Batch Loss: 0.508\n",
      "Epoch 10166, Loss: 16.836, Final Batch Loss: 0.523\n",
      "Epoch 10167, Loss: 16.483, Final Batch Loss: 0.400\n",
      "Epoch 10168, Loss: 16.913, Final Batch Loss: 0.541\n",
      "Epoch 10169, Loss: 16.481, Final Batch Loss: 0.388\n",
      "Epoch 10170, Loss: 16.802, Final Batch Loss: 0.580\n",
      "Epoch 10171, Loss: 17.075, Final Batch Loss: 0.452\n",
      "Epoch 10172, Loss: 16.404, Final Batch Loss: 0.467\n",
      "Epoch 10173, Loss: 16.828, Final Batch Loss: 0.578\n",
      "Epoch 10174, Loss: 16.712, Final Batch Loss: 0.469\n",
      "Epoch 10175, Loss: 16.624, Final Batch Loss: 0.494\n",
      "Epoch 10176, Loss: 16.644, Final Batch Loss: 0.463\n",
      "Epoch 10177, Loss: 16.785, Final Batch Loss: 0.412\n",
      "Epoch 10178, Loss: 16.900, Final Batch Loss: 0.544\n",
      "Epoch 10179, Loss: 16.924, Final Batch Loss: 0.474\n",
      "Epoch 10180, Loss: 16.749, Final Batch Loss: 0.469\n",
      "Epoch 10181, Loss: 16.634, Final Batch Loss: 0.474\n",
      "Epoch 10182, Loss: 16.989, Final Batch Loss: 0.446\n",
      "Epoch 10183, Loss: 16.734, Final Batch Loss: 0.534\n",
      "Epoch 10184, Loss: 16.657, Final Batch Loss: 0.512\n",
      "Epoch 10185, Loss: 16.818, Final Batch Loss: 0.408\n",
      "Epoch 10186, Loss: 16.707, Final Batch Loss: 0.509\n",
      "Epoch 10187, Loss: 16.569, Final Batch Loss: 0.434\n",
      "Epoch 10188, Loss: 16.674, Final Batch Loss: 0.499\n",
      "Epoch 10189, Loss: 16.866, Final Batch Loss: 0.450\n",
      "Epoch 10190, Loss: 16.545, Final Batch Loss: 0.399\n",
      "Epoch 10191, Loss: 16.775, Final Batch Loss: 0.522\n",
      "Epoch 10192, Loss: 16.541, Final Batch Loss: 0.447\n",
      "Epoch 10193, Loss: 16.539, Final Batch Loss: 0.404\n",
      "Epoch 10194, Loss: 16.806, Final Batch Loss: 0.526\n",
      "Epoch 10195, Loss: 16.463, Final Batch Loss: 0.401\n",
      "Epoch 10196, Loss: 16.796, Final Batch Loss: 0.453\n",
      "Epoch 10197, Loss: 16.692, Final Batch Loss: 0.475\n",
      "Epoch 10198, Loss: 16.524, Final Batch Loss: 0.477\n",
      "Epoch 10199, Loss: 16.646, Final Batch Loss: 0.438\n",
      "Epoch 10200, Loss: 16.692, Final Batch Loss: 0.520\n",
      "Epoch 10201, Loss: 16.808, Final Batch Loss: 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10202, Loss: 16.505, Final Batch Loss: 0.406\n",
      "Epoch 10203, Loss: 16.476, Final Batch Loss: 0.555\n",
      "Epoch 10204, Loss: 16.635, Final Batch Loss: 0.369\n",
      "Epoch 10205, Loss: 16.857, Final Batch Loss: 0.568\n",
      "Epoch 10206, Loss: 16.484, Final Batch Loss: 0.441\n",
      "Epoch 10207, Loss: 16.741, Final Batch Loss: 0.487\n",
      "Epoch 10208, Loss: 16.686, Final Batch Loss: 0.475\n",
      "Epoch 10209, Loss: 16.661, Final Batch Loss: 0.309\n",
      "Epoch 10210, Loss: 16.695, Final Batch Loss: 0.349\n",
      "Epoch 10211, Loss: 16.602, Final Batch Loss: 0.463\n",
      "Epoch 10212, Loss: 16.619, Final Batch Loss: 0.521\n",
      "Epoch 10213, Loss: 16.556, Final Batch Loss: 0.397\n",
      "Epoch 10214, Loss: 17.010, Final Batch Loss: 0.622\n",
      "Epoch 10215, Loss: 16.721, Final Batch Loss: 0.571\n",
      "Epoch 10216, Loss: 16.560, Final Batch Loss: 0.476\n",
      "Epoch 10217, Loss: 16.683, Final Batch Loss: 0.418\n",
      "Epoch 10218, Loss: 16.403, Final Batch Loss: 0.370\n",
      "Epoch 10219, Loss: 16.559, Final Batch Loss: 0.416\n",
      "Epoch 10220, Loss: 16.702, Final Batch Loss: 0.437\n",
      "Epoch 10221, Loss: 16.559, Final Batch Loss: 0.476\n",
      "Epoch 10222, Loss: 16.810, Final Batch Loss: 0.380\n",
      "Epoch 10223, Loss: 16.584, Final Batch Loss: 0.517\n",
      "Epoch 10224, Loss: 16.626, Final Batch Loss: 0.529\n",
      "Epoch 10225, Loss: 16.630, Final Batch Loss: 0.440\n",
      "Epoch 10226, Loss: 16.798, Final Batch Loss: 0.592\n",
      "Epoch 10227, Loss: 16.760, Final Batch Loss: 0.421\n",
      "Epoch 10228, Loss: 16.733, Final Batch Loss: 0.501\n",
      "Epoch 10229, Loss: 16.856, Final Batch Loss: 0.489\n",
      "Epoch 10230, Loss: 16.457, Final Batch Loss: 0.518\n",
      "Epoch 10231, Loss: 16.763, Final Batch Loss: 0.536\n",
      "Epoch 10232, Loss: 16.852, Final Batch Loss: 0.390\n",
      "Epoch 10233, Loss: 16.787, Final Batch Loss: 0.572\n",
      "Epoch 10234, Loss: 16.660, Final Batch Loss: 0.475\n",
      "Epoch 10235, Loss: 16.596, Final Batch Loss: 0.453\n",
      "Epoch 10236, Loss: 16.568, Final Batch Loss: 0.370\n",
      "Epoch 10237, Loss: 16.723, Final Batch Loss: 0.532\n",
      "Epoch 10238, Loss: 16.895, Final Batch Loss: 0.561\n",
      "Epoch 10239, Loss: 16.684, Final Batch Loss: 0.460\n",
      "Epoch 10240, Loss: 16.735, Final Batch Loss: 0.455\n",
      "Epoch 10241, Loss: 16.894, Final Batch Loss: 0.599\n",
      "Epoch 10242, Loss: 16.466, Final Batch Loss: 0.367\n",
      "Epoch 10243, Loss: 16.681, Final Batch Loss: 0.467\n",
      "Epoch 10244, Loss: 16.688, Final Batch Loss: 0.432\n",
      "Epoch 10245, Loss: 16.684, Final Batch Loss: 0.439\n",
      "Epoch 10246, Loss: 16.810, Final Batch Loss: 0.412\n",
      "Epoch 10247, Loss: 16.804, Final Batch Loss: 0.484\n",
      "Epoch 10248, Loss: 16.837, Final Batch Loss: 0.578\n",
      "Epoch 10249, Loss: 16.948, Final Batch Loss: 0.366\n",
      "Epoch 10250, Loss: 16.634, Final Batch Loss: 0.438\n",
      "Epoch 10251, Loss: 16.818, Final Batch Loss: 0.604\n",
      "Epoch 10252, Loss: 16.357, Final Batch Loss: 0.480\n",
      "Epoch 10253, Loss: 16.235, Final Batch Loss: 0.341\n",
      "Epoch 10254, Loss: 16.646, Final Batch Loss: 0.527\n",
      "Epoch 10255, Loss: 16.666, Final Batch Loss: 0.535\n",
      "Epoch 10256, Loss: 16.603, Final Batch Loss: 0.415\n",
      "Epoch 10257, Loss: 16.630, Final Batch Loss: 0.387\n",
      "Epoch 10258, Loss: 16.917, Final Batch Loss: 0.506\n",
      "Epoch 10259, Loss: 16.613, Final Batch Loss: 0.485\n",
      "Epoch 10260, Loss: 16.729, Final Batch Loss: 0.475\n",
      "Epoch 10261, Loss: 16.415, Final Batch Loss: 0.477\n",
      "Epoch 10262, Loss: 16.493, Final Batch Loss: 0.496\n",
      "Epoch 10263, Loss: 16.550, Final Batch Loss: 0.458\n",
      "Epoch 10264, Loss: 16.573, Final Batch Loss: 0.491\n",
      "Epoch 10265, Loss: 16.775, Final Batch Loss: 0.452\n",
      "Epoch 10266, Loss: 16.397, Final Batch Loss: 0.438\n",
      "Epoch 10267, Loss: 16.763, Final Batch Loss: 0.500\n",
      "Epoch 10268, Loss: 16.540, Final Batch Loss: 0.447\n",
      "Epoch 10269, Loss: 16.800, Final Batch Loss: 0.385\n",
      "Epoch 10270, Loss: 16.457, Final Batch Loss: 0.506\n",
      "Epoch 10271, Loss: 16.682, Final Batch Loss: 0.420\n",
      "Epoch 10272, Loss: 16.718, Final Batch Loss: 0.426\n",
      "Epoch 10273, Loss: 16.586, Final Batch Loss: 0.486\n",
      "Epoch 10274, Loss: 16.954, Final Batch Loss: 0.611\n",
      "Epoch 10275, Loss: 16.379, Final Batch Loss: 0.426\n",
      "Epoch 10276, Loss: 16.822, Final Batch Loss: 0.379\n",
      "Epoch 10277, Loss: 16.545, Final Batch Loss: 0.466\n",
      "Epoch 10278, Loss: 16.825, Final Batch Loss: 0.359\n",
      "Epoch 10279, Loss: 17.113, Final Batch Loss: 0.647\n",
      "Epoch 10280, Loss: 16.844, Final Batch Loss: 0.529\n",
      "Epoch 10281, Loss: 16.711, Final Batch Loss: 0.562\n",
      "Epoch 10282, Loss: 16.675, Final Batch Loss: 0.411\n",
      "Epoch 10283, Loss: 16.603, Final Batch Loss: 0.483\n",
      "Epoch 10284, Loss: 16.565, Final Batch Loss: 0.474\n",
      "Epoch 10285, Loss: 16.509, Final Batch Loss: 0.388\n",
      "Epoch 10286, Loss: 16.546, Final Batch Loss: 0.532\n",
      "Epoch 10287, Loss: 16.815, Final Batch Loss: 0.458\n",
      "Epoch 10288, Loss: 16.692, Final Batch Loss: 0.502\n",
      "Epoch 10289, Loss: 16.522, Final Batch Loss: 0.387\n",
      "Epoch 10290, Loss: 16.768, Final Batch Loss: 0.573\n",
      "Epoch 10291, Loss: 16.735, Final Batch Loss: 0.450\n",
      "Epoch 10292, Loss: 16.690, Final Batch Loss: 0.381\n",
      "Epoch 10293, Loss: 16.565, Final Batch Loss: 0.462\n",
      "Epoch 10294, Loss: 16.729, Final Batch Loss: 0.475\n",
      "Epoch 10295, Loss: 16.729, Final Batch Loss: 0.459\n",
      "Epoch 10296, Loss: 16.735, Final Batch Loss: 0.374\n",
      "Epoch 10297, Loss: 16.799, Final Batch Loss: 0.426\n",
      "Epoch 10298, Loss: 16.896, Final Batch Loss: 0.565\n",
      "Epoch 10299, Loss: 16.864, Final Batch Loss: 0.457\n",
      "Epoch 10300, Loss: 16.545, Final Batch Loss: 0.414\n",
      "Epoch 10301, Loss: 16.863, Final Batch Loss: 0.436\n",
      "Epoch 10302, Loss: 16.988, Final Batch Loss: 0.449\n",
      "Epoch 10303, Loss: 16.470, Final Batch Loss: 0.362\n",
      "Epoch 10304, Loss: 16.665, Final Batch Loss: 0.454\n",
      "Epoch 10305, Loss: 16.660, Final Batch Loss: 0.484\n",
      "Epoch 10306, Loss: 16.676, Final Batch Loss: 0.505\n",
      "Epoch 10307, Loss: 16.594, Final Batch Loss: 0.476\n",
      "Epoch 10308, Loss: 16.700, Final Batch Loss: 0.415\n",
      "Epoch 10309, Loss: 16.494, Final Batch Loss: 0.557\n",
      "Epoch 10310, Loss: 16.501, Final Batch Loss: 0.417\n",
      "Epoch 10311, Loss: 16.814, Final Batch Loss: 0.474\n",
      "Epoch 10312, Loss: 16.473, Final Batch Loss: 0.402\n",
      "Epoch 10313, Loss: 16.916, Final Batch Loss: 0.483\n",
      "Epoch 10314, Loss: 16.555, Final Batch Loss: 0.376\n",
      "Epoch 10315, Loss: 16.789, Final Batch Loss: 0.503\n",
      "Epoch 10316, Loss: 16.501, Final Batch Loss: 0.410\n",
      "Epoch 10317, Loss: 16.792, Final Batch Loss: 0.412\n",
      "Epoch 10318, Loss: 16.603, Final Batch Loss: 0.432\n",
      "Epoch 10319, Loss: 16.665, Final Batch Loss: 0.576\n",
      "Epoch 10320, Loss: 16.562, Final Batch Loss: 0.463\n",
      "Epoch 10321, Loss: 16.586, Final Batch Loss: 0.543\n",
      "Epoch 10322, Loss: 16.756, Final Batch Loss: 0.523\n",
      "Epoch 10323, Loss: 16.596, Final Batch Loss: 0.501\n",
      "Epoch 10324, Loss: 16.582, Final Batch Loss: 0.520\n",
      "Epoch 10325, Loss: 16.631, Final Batch Loss: 0.420\n",
      "Epoch 10326, Loss: 16.772, Final Batch Loss: 0.488\n",
      "Epoch 10327, Loss: 16.694, Final Batch Loss: 0.502\n",
      "Epoch 10328, Loss: 16.657, Final Batch Loss: 0.325\n",
      "Epoch 10329, Loss: 16.745, Final Batch Loss: 0.526\n",
      "Epoch 10330, Loss: 16.612, Final Batch Loss: 0.409\n",
      "Epoch 10331, Loss: 16.631, Final Batch Loss: 0.471\n",
      "Epoch 10332, Loss: 16.607, Final Batch Loss: 0.476\n",
      "Epoch 10333, Loss: 16.632, Final Batch Loss: 0.586\n",
      "Epoch 10334, Loss: 16.460, Final Batch Loss: 0.406\n",
      "Epoch 10335, Loss: 16.699, Final Batch Loss: 0.482\n",
      "Epoch 10336, Loss: 16.750, Final Batch Loss: 0.429\n",
      "Epoch 10337, Loss: 16.761, Final Batch Loss: 0.507\n",
      "Epoch 10338, Loss: 16.693, Final Batch Loss: 0.410\n",
      "Epoch 10339, Loss: 16.606, Final Batch Loss: 0.491\n",
      "Epoch 10340, Loss: 16.566, Final Batch Loss: 0.520\n",
      "Epoch 10341, Loss: 16.683, Final Batch Loss: 0.461\n",
      "Epoch 10342, Loss: 16.675, Final Batch Loss: 0.408\n",
      "Epoch 10343, Loss: 16.832, Final Batch Loss: 0.496\n",
      "Epoch 10344, Loss: 16.716, Final Batch Loss: 0.447\n",
      "Epoch 10345, Loss: 16.854, Final Batch Loss: 0.539\n",
      "Epoch 10346, Loss: 16.519, Final Batch Loss: 0.391\n",
      "Epoch 10347, Loss: 16.712, Final Batch Loss: 0.535\n",
      "Epoch 10348, Loss: 16.860, Final Batch Loss: 0.399\n",
      "Epoch 10349, Loss: 16.640, Final Batch Loss: 0.569\n",
      "Epoch 10350, Loss: 16.925, Final Batch Loss: 0.563\n",
      "Epoch 10351, Loss: 16.714, Final Batch Loss: 0.510\n",
      "Epoch 10352, Loss: 16.525, Final Batch Loss: 0.394\n",
      "Epoch 10353, Loss: 16.624, Final Batch Loss: 0.423\n",
      "Epoch 10354, Loss: 16.631, Final Batch Loss: 0.386\n",
      "Epoch 10355, Loss: 17.117, Final Batch Loss: 0.518\n",
      "Epoch 10356, Loss: 16.903, Final Batch Loss: 0.581\n",
      "Epoch 10357, Loss: 17.023, Final Batch Loss: 0.595\n",
      "Epoch 10358, Loss: 16.489, Final Batch Loss: 0.381\n",
      "Epoch 10359, Loss: 16.771, Final Batch Loss: 0.489\n",
      "Epoch 10360, Loss: 16.940, Final Batch Loss: 0.537\n",
      "Epoch 10361, Loss: 16.424, Final Batch Loss: 0.377\n",
      "Epoch 10362, Loss: 16.424, Final Batch Loss: 0.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10363, Loss: 16.380, Final Batch Loss: 0.406\n",
      "Epoch 10364, Loss: 16.517, Final Batch Loss: 0.385\n",
      "Epoch 10365, Loss: 16.533, Final Batch Loss: 0.483\n",
      "Epoch 10366, Loss: 16.533, Final Batch Loss: 0.411\n",
      "Epoch 10367, Loss: 16.826, Final Batch Loss: 0.416\n",
      "Epoch 10368, Loss: 16.920, Final Batch Loss: 0.478\n",
      "Epoch 10369, Loss: 16.819, Final Batch Loss: 0.449\n",
      "Epoch 10370, Loss: 16.804, Final Batch Loss: 0.492\n",
      "Epoch 10371, Loss: 16.532, Final Batch Loss: 0.393\n",
      "Epoch 10372, Loss: 16.636, Final Batch Loss: 0.483\n",
      "Epoch 10373, Loss: 16.599, Final Batch Loss: 0.378\n",
      "Epoch 10374, Loss: 16.475, Final Batch Loss: 0.465\n",
      "Epoch 10375, Loss: 16.190, Final Batch Loss: 0.490\n",
      "Epoch 10376, Loss: 16.496, Final Batch Loss: 0.463\n",
      "Epoch 10377, Loss: 16.793, Final Batch Loss: 0.505\n",
      "Epoch 10378, Loss: 16.516, Final Batch Loss: 0.391\n",
      "Epoch 10379, Loss: 16.765, Final Batch Loss: 0.469\n",
      "Epoch 10380, Loss: 17.081, Final Batch Loss: 0.478\n",
      "Epoch 10381, Loss: 16.632, Final Batch Loss: 0.482\n",
      "Epoch 10382, Loss: 16.668, Final Batch Loss: 0.508\n",
      "Epoch 10383, Loss: 16.705, Final Batch Loss: 0.413\n",
      "Epoch 10384, Loss: 16.631, Final Batch Loss: 0.548\n",
      "Epoch 10385, Loss: 16.682, Final Batch Loss: 0.510\n",
      "Epoch 10386, Loss: 16.721, Final Batch Loss: 0.417\n",
      "Epoch 10387, Loss: 16.960, Final Batch Loss: 0.394\n",
      "Epoch 10388, Loss: 16.749, Final Batch Loss: 0.488\n",
      "Epoch 10389, Loss: 16.780, Final Batch Loss: 0.415\n",
      "Epoch 10390, Loss: 16.923, Final Batch Loss: 0.447\n",
      "Epoch 10391, Loss: 16.675, Final Batch Loss: 0.571\n",
      "Epoch 10392, Loss: 16.643, Final Batch Loss: 0.400\n",
      "Epoch 10393, Loss: 16.742, Final Batch Loss: 0.500\n",
      "Epoch 10394, Loss: 16.604, Final Batch Loss: 0.398\n",
      "Epoch 10395, Loss: 16.621, Final Batch Loss: 0.410\n",
      "Epoch 10396, Loss: 16.479, Final Batch Loss: 0.470\n",
      "Epoch 10397, Loss: 16.716, Final Batch Loss: 0.473\n",
      "Epoch 10398, Loss: 16.656, Final Batch Loss: 0.482\n",
      "Epoch 10399, Loss: 16.917, Final Batch Loss: 0.396\n",
      "Epoch 10400, Loss: 16.549, Final Batch Loss: 0.398\n",
      "Epoch 10401, Loss: 16.778, Final Batch Loss: 0.551\n",
      "Epoch 10402, Loss: 16.442, Final Batch Loss: 0.515\n",
      "Epoch 10403, Loss: 16.566, Final Batch Loss: 0.456\n",
      "Epoch 10404, Loss: 16.552, Final Batch Loss: 0.457\n",
      "Epoch 10405, Loss: 16.511, Final Batch Loss: 0.399\n",
      "Epoch 10406, Loss: 16.888, Final Batch Loss: 0.401\n",
      "Epoch 10407, Loss: 16.473, Final Batch Loss: 0.425\n",
      "Epoch 10408, Loss: 16.704, Final Batch Loss: 0.532\n",
      "Epoch 10409, Loss: 16.721, Final Batch Loss: 0.518\n",
      "Epoch 10410, Loss: 16.863, Final Batch Loss: 0.548\n",
      "Epoch 10411, Loss: 16.595, Final Batch Loss: 0.471\n",
      "Epoch 10412, Loss: 16.849, Final Batch Loss: 0.458\n",
      "Epoch 10413, Loss: 16.814, Final Batch Loss: 0.509\n",
      "Epoch 10414, Loss: 16.800, Final Batch Loss: 0.490\n",
      "Epoch 10415, Loss: 16.883, Final Batch Loss: 0.359\n",
      "Epoch 10416, Loss: 16.630, Final Batch Loss: 0.465\n",
      "Epoch 10417, Loss: 16.715, Final Batch Loss: 0.467\n",
      "Epoch 10418, Loss: 16.841, Final Batch Loss: 0.497\n",
      "Epoch 10419, Loss: 16.960, Final Batch Loss: 0.451\n",
      "Epoch 10420, Loss: 16.559, Final Batch Loss: 0.455\n",
      "Epoch 10421, Loss: 16.420, Final Batch Loss: 0.413\n",
      "Epoch 10422, Loss: 16.704, Final Batch Loss: 0.409\n",
      "Epoch 10423, Loss: 17.018, Final Batch Loss: 0.578\n",
      "Epoch 10424, Loss: 16.562, Final Batch Loss: 0.429\n",
      "Epoch 10425, Loss: 16.590, Final Batch Loss: 0.386\n",
      "Epoch 10426, Loss: 16.676, Final Batch Loss: 0.530\n",
      "Epoch 10427, Loss: 16.891, Final Batch Loss: 0.556\n",
      "Epoch 10428, Loss: 16.550, Final Batch Loss: 0.504\n",
      "Epoch 10429, Loss: 16.677, Final Batch Loss: 0.489\n",
      "Epoch 10430, Loss: 16.608, Final Batch Loss: 0.419\n",
      "Epoch 10431, Loss: 16.936, Final Batch Loss: 0.404\n",
      "Epoch 10432, Loss: 16.360, Final Batch Loss: 0.495\n",
      "Epoch 10433, Loss: 16.650, Final Batch Loss: 0.570\n",
      "Epoch 10434, Loss: 16.555, Final Batch Loss: 0.409\n",
      "Epoch 10435, Loss: 16.734, Final Batch Loss: 0.509\n",
      "Epoch 10436, Loss: 16.666, Final Batch Loss: 0.446\n",
      "Epoch 10437, Loss: 16.572, Final Batch Loss: 0.391\n",
      "Epoch 10438, Loss: 16.597, Final Batch Loss: 0.503\n",
      "Epoch 10439, Loss: 16.960, Final Batch Loss: 0.514\n",
      "Epoch 10440, Loss: 16.966, Final Batch Loss: 0.387\n",
      "Epoch 10441, Loss: 16.566, Final Batch Loss: 0.487\n",
      "Epoch 10442, Loss: 16.492, Final Batch Loss: 0.396\n",
      "Epoch 10443, Loss: 16.588, Final Batch Loss: 0.495\n",
      "Epoch 10444, Loss: 16.979, Final Batch Loss: 0.558\n",
      "Epoch 10445, Loss: 16.808, Final Batch Loss: 0.424\n",
      "Epoch 10446, Loss: 16.749, Final Batch Loss: 0.381\n",
      "Epoch 10447, Loss: 16.719, Final Batch Loss: 0.519\n",
      "Epoch 10448, Loss: 16.738, Final Batch Loss: 0.493\n",
      "Epoch 10449, Loss: 16.746, Final Batch Loss: 0.413\n",
      "Epoch 10450, Loss: 16.847, Final Batch Loss: 0.431\n",
      "Epoch 10451, Loss: 16.545, Final Batch Loss: 0.389\n",
      "Epoch 10452, Loss: 16.705, Final Batch Loss: 0.609\n",
      "Epoch 10453, Loss: 16.507, Final Batch Loss: 0.468\n",
      "Epoch 10454, Loss: 16.594, Final Batch Loss: 0.487\n",
      "Epoch 10455, Loss: 16.696, Final Batch Loss: 0.439\n",
      "Epoch 10456, Loss: 16.999, Final Batch Loss: 0.571\n",
      "Epoch 10457, Loss: 16.822, Final Batch Loss: 0.410\n",
      "Epoch 10458, Loss: 16.831, Final Batch Loss: 0.486\n",
      "Epoch 10459, Loss: 16.723, Final Batch Loss: 0.449\n",
      "Epoch 10460, Loss: 16.759, Final Batch Loss: 0.542\n",
      "Epoch 10461, Loss: 16.723, Final Batch Loss: 0.469\n",
      "Epoch 10462, Loss: 16.664, Final Batch Loss: 0.446\n",
      "Epoch 10463, Loss: 16.496, Final Batch Loss: 0.337\n",
      "Epoch 10464, Loss: 16.518, Final Batch Loss: 0.480\n",
      "Epoch 10465, Loss: 16.631, Final Batch Loss: 0.415\n",
      "Epoch 10466, Loss: 16.441, Final Batch Loss: 0.499\n",
      "Epoch 10467, Loss: 16.841, Final Batch Loss: 0.349\n",
      "Epoch 10468, Loss: 16.737, Final Batch Loss: 0.423\n",
      "Epoch 10469, Loss: 16.941, Final Batch Loss: 0.482\n",
      "Epoch 10470, Loss: 16.794, Final Batch Loss: 0.418\n",
      "Epoch 10471, Loss: 16.783, Final Batch Loss: 0.465\n",
      "Epoch 10472, Loss: 16.560, Final Batch Loss: 0.405\n",
      "Epoch 10473, Loss: 16.814, Final Batch Loss: 0.452\n",
      "Epoch 10474, Loss: 16.716, Final Batch Loss: 0.549\n",
      "Epoch 10475, Loss: 16.752, Final Batch Loss: 0.514\n",
      "Epoch 10476, Loss: 16.668, Final Batch Loss: 0.377\n",
      "Epoch 10477, Loss: 16.537, Final Batch Loss: 0.505\n",
      "Epoch 10478, Loss: 16.548, Final Batch Loss: 0.398\n",
      "Epoch 10479, Loss: 16.518, Final Batch Loss: 0.416\n",
      "Epoch 10480, Loss: 16.890, Final Batch Loss: 0.557\n",
      "Epoch 10481, Loss: 16.650, Final Batch Loss: 0.422\n",
      "Epoch 10482, Loss: 16.752, Final Batch Loss: 0.518\n",
      "Epoch 10483, Loss: 16.632, Final Batch Loss: 0.466\n",
      "Epoch 10484, Loss: 16.539, Final Batch Loss: 0.412\n",
      "Epoch 10485, Loss: 16.792, Final Batch Loss: 0.340\n",
      "Epoch 10486, Loss: 16.709, Final Batch Loss: 0.425\n",
      "Epoch 10487, Loss: 16.900, Final Batch Loss: 0.439\n",
      "Epoch 10488, Loss: 16.877, Final Batch Loss: 0.638\n",
      "Epoch 10489, Loss: 16.407, Final Batch Loss: 0.417\n",
      "Epoch 10490, Loss: 16.581, Final Batch Loss: 0.501\n",
      "Epoch 10491, Loss: 16.687, Final Batch Loss: 0.443\n",
      "Epoch 10492, Loss: 16.513, Final Batch Loss: 0.405\n",
      "Epoch 10493, Loss: 16.583, Final Batch Loss: 0.567\n",
      "Epoch 10494, Loss: 16.781, Final Batch Loss: 0.379\n",
      "Epoch 10495, Loss: 16.702, Final Batch Loss: 0.463\n",
      "Epoch 10496, Loss: 16.528, Final Batch Loss: 0.344\n",
      "Epoch 10497, Loss: 16.368, Final Batch Loss: 0.465\n",
      "Epoch 10498, Loss: 16.775, Final Batch Loss: 0.633\n",
      "Epoch 10499, Loss: 16.804, Final Batch Loss: 0.475\n",
      "Epoch 10500, Loss: 16.952, Final Batch Loss: 0.414\n",
      "Epoch 10501, Loss: 16.637, Final Batch Loss: 0.397\n",
      "Epoch 10502, Loss: 16.768, Final Batch Loss: 0.464\n",
      "Epoch 10503, Loss: 16.683, Final Batch Loss: 0.388\n",
      "Epoch 10504, Loss: 16.503, Final Batch Loss: 0.361\n",
      "Epoch 10505, Loss: 16.606, Final Batch Loss: 0.501\n",
      "Epoch 10506, Loss: 16.655, Final Batch Loss: 0.388\n",
      "Epoch 10507, Loss: 16.684, Final Batch Loss: 0.541\n",
      "Epoch 10508, Loss: 16.740, Final Batch Loss: 0.415\n",
      "Epoch 10509, Loss: 16.518, Final Batch Loss: 0.367\n",
      "Epoch 10510, Loss: 16.722, Final Batch Loss: 0.440\n",
      "Epoch 10511, Loss: 16.723, Final Batch Loss: 0.519\n",
      "Epoch 10512, Loss: 16.335, Final Batch Loss: 0.418\n",
      "Epoch 10513, Loss: 16.481, Final Batch Loss: 0.462\n",
      "Epoch 10514, Loss: 16.799, Final Batch Loss: 0.485\n",
      "Epoch 10515, Loss: 16.789, Final Batch Loss: 0.485\n",
      "Epoch 10516, Loss: 16.657, Final Batch Loss: 0.428\n",
      "Epoch 10517, Loss: 16.578, Final Batch Loss: 0.470\n",
      "Epoch 10518, Loss: 16.720, Final Batch Loss: 0.493\n",
      "Epoch 10519, Loss: 16.695, Final Batch Loss: 0.487\n",
      "Epoch 10520, Loss: 16.602, Final Batch Loss: 0.420\n",
      "Epoch 10521, Loss: 16.703, Final Batch Loss: 0.441\n",
      "Epoch 10522, Loss: 16.682, Final Batch Loss: 0.538\n",
      "Epoch 10523, Loss: 16.701, Final Batch Loss: 0.432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10524, Loss: 16.689, Final Batch Loss: 0.510\n",
      "Epoch 10525, Loss: 16.659, Final Batch Loss: 0.376\n",
      "Epoch 10526, Loss: 16.514, Final Batch Loss: 0.471\n",
      "Epoch 10527, Loss: 16.822, Final Batch Loss: 0.484\n",
      "Epoch 10528, Loss: 16.858, Final Batch Loss: 0.503\n",
      "Epoch 10529, Loss: 16.798, Final Batch Loss: 0.431\n",
      "Epoch 10530, Loss: 16.631, Final Batch Loss: 0.503\n",
      "Epoch 10531, Loss: 16.533, Final Batch Loss: 0.571\n",
      "Epoch 10532, Loss: 16.859, Final Batch Loss: 0.469\n",
      "Epoch 10533, Loss: 16.542, Final Batch Loss: 0.456\n",
      "Epoch 10534, Loss: 16.691, Final Batch Loss: 0.347\n",
      "Epoch 10535, Loss: 16.656, Final Batch Loss: 0.469\n",
      "Epoch 10536, Loss: 16.447, Final Batch Loss: 0.457\n",
      "Epoch 10537, Loss: 16.674, Final Batch Loss: 0.538\n",
      "Epoch 10538, Loss: 16.675, Final Batch Loss: 0.460\n",
      "Epoch 10539, Loss: 16.742, Final Batch Loss: 0.487\n",
      "Epoch 10540, Loss: 16.570, Final Batch Loss: 0.482\n",
      "Epoch 10541, Loss: 16.985, Final Batch Loss: 0.439\n",
      "Epoch 10542, Loss: 16.895, Final Batch Loss: 0.541\n",
      "Epoch 10543, Loss: 16.536, Final Batch Loss: 0.433\n",
      "Epoch 10544, Loss: 16.706, Final Batch Loss: 0.470\n",
      "Epoch 10545, Loss: 16.479, Final Batch Loss: 0.390\n",
      "Epoch 10546, Loss: 16.430, Final Batch Loss: 0.428\n",
      "Epoch 10547, Loss: 16.741, Final Batch Loss: 0.460\n",
      "Epoch 10548, Loss: 16.449, Final Batch Loss: 0.306\n",
      "Epoch 10549, Loss: 16.535, Final Batch Loss: 0.457\n",
      "Epoch 10550, Loss: 16.972, Final Batch Loss: 0.457\n",
      "Epoch 10551, Loss: 16.497, Final Batch Loss: 0.408\n",
      "Epoch 10552, Loss: 16.573, Final Batch Loss: 0.451\n",
      "Epoch 10553, Loss: 16.481, Final Batch Loss: 0.469\n",
      "Epoch 10554, Loss: 16.829, Final Batch Loss: 0.478\n",
      "Epoch 10555, Loss: 16.544, Final Batch Loss: 0.389\n",
      "Epoch 10556, Loss: 16.694, Final Batch Loss: 0.514\n",
      "Epoch 10557, Loss: 16.547, Final Batch Loss: 0.441\n",
      "Epoch 10558, Loss: 16.535, Final Batch Loss: 0.464\n",
      "Epoch 10559, Loss: 16.278, Final Batch Loss: 0.395\n",
      "Epoch 10560, Loss: 16.502, Final Batch Loss: 0.393\n",
      "Epoch 10561, Loss: 16.707, Final Batch Loss: 0.450\n",
      "Epoch 10562, Loss: 16.448, Final Batch Loss: 0.436\n",
      "Epoch 10563, Loss: 16.737, Final Batch Loss: 0.504\n",
      "Epoch 10564, Loss: 16.785, Final Batch Loss: 0.508\n",
      "Epoch 10565, Loss: 16.439, Final Batch Loss: 0.385\n",
      "Epoch 10566, Loss: 16.726, Final Batch Loss: 0.523\n",
      "Epoch 10567, Loss: 16.721, Final Batch Loss: 0.422\n",
      "Epoch 10568, Loss: 16.717, Final Batch Loss: 0.435\n",
      "Epoch 10569, Loss: 16.652, Final Batch Loss: 0.452\n",
      "Epoch 10570, Loss: 16.430, Final Batch Loss: 0.421\n",
      "Epoch 10571, Loss: 16.649, Final Batch Loss: 0.515\n",
      "Epoch 10572, Loss: 16.309, Final Batch Loss: 0.376\n",
      "Epoch 10573, Loss: 16.680, Final Batch Loss: 0.368\n",
      "Epoch 10574, Loss: 16.767, Final Batch Loss: 0.477\n",
      "Epoch 10575, Loss: 16.608, Final Batch Loss: 0.486\n",
      "Epoch 10576, Loss: 16.603, Final Batch Loss: 0.471\n",
      "Epoch 10577, Loss: 16.401, Final Batch Loss: 0.449\n",
      "Epoch 10578, Loss: 16.763, Final Batch Loss: 0.510\n",
      "Epoch 10579, Loss: 16.405, Final Batch Loss: 0.415\n",
      "Epoch 10580, Loss: 16.535, Final Batch Loss: 0.468\n",
      "Epoch 10581, Loss: 16.669, Final Batch Loss: 0.428\n",
      "Epoch 10582, Loss: 16.633, Final Batch Loss: 0.382\n",
      "Epoch 10583, Loss: 16.617, Final Batch Loss: 0.491\n",
      "Epoch 10584, Loss: 16.691, Final Batch Loss: 0.546\n",
      "Epoch 10585, Loss: 16.688, Final Batch Loss: 0.504\n",
      "Epoch 10586, Loss: 16.775, Final Batch Loss: 0.423\n",
      "Epoch 10587, Loss: 16.556, Final Batch Loss: 0.438\n",
      "Epoch 10588, Loss: 16.686, Final Batch Loss: 0.423\n",
      "Epoch 10589, Loss: 16.791, Final Batch Loss: 0.501\n",
      "Epoch 10590, Loss: 16.743, Final Batch Loss: 0.448\n",
      "Epoch 10591, Loss: 16.501, Final Batch Loss: 0.453\n",
      "Epoch 10592, Loss: 17.039, Final Batch Loss: 0.436\n",
      "Epoch 10593, Loss: 16.489, Final Batch Loss: 0.466\n",
      "Epoch 10594, Loss: 16.661, Final Batch Loss: 0.465\n",
      "Epoch 10595, Loss: 16.390, Final Batch Loss: 0.404\n",
      "Epoch 10596, Loss: 16.866, Final Batch Loss: 0.457\n",
      "Epoch 10597, Loss: 16.706, Final Batch Loss: 0.425\n",
      "Epoch 10598, Loss: 16.687, Final Batch Loss: 0.391\n",
      "Epoch 10599, Loss: 16.606, Final Batch Loss: 0.449\n",
      "Epoch 10600, Loss: 16.838, Final Batch Loss: 0.512\n",
      "Epoch 10601, Loss: 16.762, Final Batch Loss: 0.591\n",
      "Epoch 10602, Loss: 16.563, Final Batch Loss: 0.535\n",
      "Epoch 10603, Loss: 16.507, Final Batch Loss: 0.469\n",
      "Epoch 10604, Loss: 16.872, Final Batch Loss: 0.478\n",
      "Epoch 10605, Loss: 16.562, Final Batch Loss: 0.483\n",
      "Epoch 10606, Loss: 16.405, Final Batch Loss: 0.388\n",
      "Epoch 10607, Loss: 16.541, Final Batch Loss: 0.429\n",
      "Epoch 10608, Loss: 16.614, Final Batch Loss: 0.512\n",
      "Epoch 10609, Loss: 16.599, Final Batch Loss: 0.503\n",
      "Epoch 10610, Loss: 16.705, Final Batch Loss: 0.432\n",
      "Epoch 10611, Loss: 16.853, Final Batch Loss: 0.473\n",
      "Epoch 10612, Loss: 16.554, Final Batch Loss: 0.480\n",
      "Epoch 10613, Loss: 16.482, Final Batch Loss: 0.444\n",
      "Epoch 10614, Loss: 16.579, Final Batch Loss: 0.540\n",
      "Epoch 10615, Loss: 16.844, Final Batch Loss: 0.554\n",
      "Epoch 10616, Loss: 16.615, Final Batch Loss: 0.418\n",
      "Epoch 10617, Loss: 16.617, Final Batch Loss: 0.509\n",
      "Epoch 10618, Loss: 16.509, Final Batch Loss: 0.506\n",
      "Epoch 10619, Loss: 16.639, Final Batch Loss: 0.332\n",
      "Epoch 10620, Loss: 16.449, Final Batch Loss: 0.423\n",
      "Epoch 10621, Loss: 16.822, Final Batch Loss: 0.494\n",
      "Epoch 10622, Loss: 16.866, Final Batch Loss: 0.518\n",
      "Epoch 10623, Loss: 16.543, Final Batch Loss: 0.384\n",
      "Epoch 10624, Loss: 16.356, Final Batch Loss: 0.489\n",
      "Epoch 10625, Loss: 16.607, Final Batch Loss: 0.466\n",
      "Epoch 10626, Loss: 16.552, Final Batch Loss: 0.455\n",
      "Epoch 10627, Loss: 16.729, Final Batch Loss: 0.398\n",
      "Epoch 10628, Loss: 16.467, Final Batch Loss: 0.455\n",
      "Epoch 10629, Loss: 16.440, Final Batch Loss: 0.433\n",
      "Epoch 10630, Loss: 16.781, Final Batch Loss: 0.599\n",
      "Epoch 10631, Loss: 16.584, Final Batch Loss: 0.456\n",
      "Epoch 10632, Loss: 16.715, Final Batch Loss: 0.302\n",
      "Epoch 10633, Loss: 16.690, Final Batch Loss: 0.517\n",
      "Epoch 10634, Loss: 16.647, Final Batch Loss: 0.445\n",
      "Epoch 10635, Loss: 16.708, Final Batch Loss: 0.515\n",
      "Epoch 10636, Loss: 16.524, Final Batch Loss: 0.418\n",
      "Epoch 10637, Loss: 16.477, Final Batch Loss: 0.522\n",
      "Epoch 10638, Loss: 16.850, Final Batch Loss: 0.514\n",
      "Epoch 10639, Loss: 16.738, Final Batch Loss: 0.427\n",
      "Epoch 10640, Loss: 16.529, Final Batch Loss: 0.462\n",
      "Epoch 10641, Loss: 16.775, Final Batch Loss: 0.402\n",
      "Epoch 10642, Loss: 16.618, Final Batch Loss: 0.439\n",
      "Epoch 10643, Loss: 16.739, Final Batch Loss: 0.413\n",
      "Epoch 10644, Loss: 16.271, Final Batch Loss: 0.350\n",
      "Epoch 10645, Loss: 16.593, Final Batch Loss: 0.485\n",
      "Epoch 10646, Loss: 16.637, Final Batch Loss: 0.538\n",
      "Epoch 10647, Loss: 16.725, Final Batch Loss: 0.385\n",
      "Epoch 10648, Loss: 16.577, Final Batch Loss: 0.452\n",
      "Epoch 10649, Loss: 16.736, Final Batch Loss: 0.488\n",
      "Epoch 10650, Loss: 16.438, Final Batch Loss: 0.424\n",
      "Epoch 10651, Loss: 16.926, Final Batch Loss: 0.568\n",
      "Epoch 10652, Loss: 16.470, Final Batch Loss: 0.330\n",
      "Epoch 10653, Loss: 16.268, Final Batch Loss: 0.285\n",
      "Epoch 10654, Loss: 16.436, Final Batch Loss: 0.381\n",
      "Epoch 10655, Loss: 16.622, Final Batch Loss: 0.494\n",
      "Epoch 10656, Loss: 16.677, Final Batch Loss: 0.508\n",
      "Epoch 10657, Loss: 16.729, Final Batch Loss: 0.419\n",
      "Epoch 10658, Loss: 16.572, Final Batch Loss: 0.522\n",
      "Epoch 10659, Loss: 16.791, Final Batch Loss: 0.469\n",
      "Epoch 10660, Loss: 16.570, Final Batch Loss: 0.516\n",
      "Epoch 10661, Loss: 16.675, Final Batch Loss: 0.442\n",
      "Epoch 10662, Loss: 16.526, Final Batch Loss: 0.471\n",
      "Epoch 10663, Loss: 16.734, Final Batch Loss: 0.482\n",
      "Epoch 10664, Loss: 16.575, Final Batch Loss: 0.440\n",
      "Epoch 10665, Loss: 16.630, Final Batch Loss: 0.347\n",
      "Epoch 10666, Loss: 16.718, Final Batch Loss: 0.416\n",
      "Epoch 10667, Loss: 17.053, Final Batch Loss: 0.494\n",
      "Epoch 10668, Loss: 16.792, Final Batch Loss: 0.573\n",
      "Epoch 10669, Loss: 16.658, Final Batch Loss: 0.465\n",
      "Epoch 10670, Loss: 16.654, Final Batch Loss: 0.439\n",
      "Epoch 10671, Loss: 16.564, Final Batch Loss: 0.524\n",
      "Epoch 10672, Loss: 16.524, Final Batch Loss: 0.478\n",
      "Epoch 10673, Loss: 16.572, Final Batch Loss: 0.507\n",
      "Epoch 10674, Loss: 16.526, Final Batch Loss: 0.492\n",
      "Epoch 10675, Loss: 16.609, Final Batch Loss: 0.455\n",
      "Epoch 10676, Loss: 16.874, Final Batch Loss: 0.401\n",
      "Epoch 10677, Loss: 16.430, Final Batch Loss: 0.475\n",
      "Epoch 10678, Loss: 16.650, Final Batch Loss: 0.619\n",
      "Epoch 10679, Loss: 16.691, Final Batch Loss: 0.496\n",
      "Epoch 10680, Loss: 16.535, Final Batch Loss: 0.386\n",
      "Epoch 10681, Loss: 16.550, Final Batch Loss: 0.515\n",
      "Epoch 10682, Loss: 16.716, Final Batch Loss: 0.448\n",
      "Epoch 10683, Loss: 16.531, Final Batch Loss: 0.492\n",
      "Epoch 10684, Loss: 16.710, Final Batch Loss: 0.454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10685, Loss: 16.593, Final Batch Loss: 0.364\n",
      "Epoch 10686, Loss: 16.848, Final Batch Loss: 0.424\n",
      "Epoch 10687, Loss: 16.868, Final Batch Loss: 0.522\n",
      "Epoch 10688, Loss: 16.802, Final Batch Loss: 0.503\n",
      "Epoch 10689, Loss: 16.470, Final Batch Loss: 0.400\n",
      "Epoch 10690, Loss: 16.923, Final Batch Loss: 0.545\n",
      "Epoch 10691, Loss: 16.893, Final Batch Loss: 0.324\n",
      "Epoch 10692, Loss: 16.733, Final Batch Loss: 0.642\n",
      "Epoch 10693, Loss: 16.515, Final Batch Loss: 0.493\n",
      "Epoch 10694, Loss: 16.656, Final Batch Loss: 0.562\n",
      "Epoch 10695, Loss: 16.491, Final Batch Loss: 0.416\n",
      "Epoch 10696, Loss: 16.678, Final Batch Loss: 0.437\n",
      "Epoch 10697, Loss: 16.652, Final Batch Loss: 0.413\n",
      "Epoch 10698, Loss: 16.547, Final Batch Loss: 0.508\n",
      "Epoch 10699, Loss: 16.730, Final Batch Loss: 0.451\n",
      "Epoch 10700, Loss: 16.678, Final Batch Loss: 0.395\n",
      "Epoch 10701, Loss: 16.548, Final Batch Loss: 0.587\n",
      "Epoch 10702, Loss: 16.449, Final Batch Loss: 0.493\n",
      "Epoch 10703, Loss: 16.574, Final Batch Loss: 0.448\n",
      "Epoch 10704, Loss: 16.634, Final Batch Loss: 0.550\n",
      "Epoch 10705, Loss: 16.881, Final Batch Loss: 0.491\n",
      "Epoch 10706, Loss: 16.446, Final Batch Loss: 0.398\n",
      "Epoch 10707, Loss: 16.399, Final Batch Loss: 0.502\n",
      "Epoch 10708, Loss: 16.589, Final Batch Loss: 0.421\n",
      "Epoch 10709, Loss: 16.697, Final Batch Loss: 0.553\n",
      "Epoch 10710, Loss: 16.505, Final Batch Loss: 0.518\n",
      "Epoch 10711, Loss: 16.746, Final Batch Loss: 0.472\n",
      "Epoch 10712, Loss: 16.455, Final Batch Loss: 0.491\n",
      "Epoch 10713, Loss: 16.860, Final Batch Loss: 0.450\n",
      "Epoch 10714, Loss: 16.568, Final Batch Loss: 0.449\n",
      "Epoch 10715, Loss: 16.657, Final Batch Loss: 0.501\n",
      "Epoch 10716, Loss: 16.663, Final Batch Loss: 0.561\n",
      "Epoch 10717, Loss: 16.531, Final Batch Loss: 0.445\n",
      "Epoch 10718, Loss: 16.514, Final Batch Loss: 0.425\n",
      "Epoch 10719, Loss: 16.667, Final Batch Loss: 0.487\n",
      "Epoch 10720, Loss: 16.554, Final Batch Loss: 0.469\n",
      "Epoch 10721, Loss: 16.780, Final Batch Loss: 0.497\n",
      "Epoch 10722, Loss: 16.795, Final Batch Loss: 0.463\n",
      "Epoch 10723, Loss: 16.452, Final Batch Loss: 0.401\n",
      "Epoch 10724, Loss: 16.482, Final Batch Loss: 0.416\n",
      "Epoch 10725, Loss: 16.877, Final Batch Loss: 0.487\n",
      "Epoch 10726, Loss: 16.604, Final Batch Loss: 0.459\n",
      "Epoch 10727, Loss: 16.430, Final Batch Loss: 0.488\n",
      "Epoch 10728, Loss: 16.889, Final Batch Loss: 0.484\n",
      "Epoch 10729, Loss: 16.671, Final Batch Loss: 0.477\n",
      "Epoch 10730, Loss: 16.693, Final Batch Loss: 0.648\n",
      "Epoch 10731, Loss: 16.707, Final Batch Loss: 0.548\n",
      "Epoch 10732, Loss: 16.673, Final Batch Loss: 0.340\n",
      "Epoch 10733, Loss: 16.799, Final Batch Loss: 0.533\n",
      "Epoch 10734, Loss: 16.842, Final Batch Loss: 0.457\n",
      "Epoch 10735, Loss: 16.833, Final Batch Loss: 0.517\n",
      "Epoch 10736, Loss: 16.704, Final Batch Loss: 0.529\n",
      "Epoch 10737, Loss: 16.779, Final Batch Loss: 0.481\n",
      "Epoch 10738, Loss: 16.785, Final Batch Loss: 0.427\n",
      "Epoch 10739, Loss: 16.648, Final Batch Loss: 0.394\n",
      "Epoch 10740, Loss: 16.581, Final Batch Loss: 0.350\n",
      "Epoch 10741, Loss: 16.896, Final Batch Loss: 0.554\n",
      "Epoch 10742, Loss: 16.942, Final Batch Loss: 0.569\n",
      "Epoch 10743, Loss: 16.675, Final Batch Loss: 0.486\n",
      "Epoch 10744, Loss: 16.446, Final Batch Loss: 0.369\n",
      "Epoch 10745, Loss: 16.262, Final Batch Loss: 0.367\n",
      "Epoch 10746, Loss: 16.721, Final Batch Loss: 0.517\n",
      "Epoch 10747, Loss: 16.724, Final Batch Loss: 0.508\n",
      "Epoch 10748, Loss: 16.877, Final Batch Loss: 0.512\n",
      "Epoch 10749, Loss: 16.666, Final Batch Loss: 0.414\n",
      "Epoch 10750, Loss: 16.858, Final Batch Loss: 0.377\n",
      "Epoch 10751, Loss: 16.544, Final Batch Loss: 0.523\n",
      "Epoch 10752, Loss: 16.411, Final Batch Loss: 0.458\n",
      "Epoch 10753, Loss: 16.600, Final Batch Loss: 0.517\n",
      "Epoch 10754, Loss: 16.677, Final Batch Loss: 0.603\n",
      "Epoch 10755, Loss: 16.675, Final Batch Loss: 0.433\n",
      "Epoch 10756, Loss: 16.544, Final Batch Loss: 0.452\n",
      "Epoch 10757, Loss: 16.567, Final Batch Loss: 0.616\n",
      "Epoch 10758, Loss: 16.786, Final Batch Loss: 0.409\n",
      "Epoch 10759, Loss: 16.776, Final Batch Loss: 0.496\n",
      "Epoch 10760, Loss: 16.734, Final Batch Loss: 0.326\n",
      "Epoch 10761, Loss: 16.567, Final Batch Loss: 0.531\n",
      "Epoch 10762, Loss: 16.601, Final Batch Loss: 0.502\n",
      "Epoch 10763, Loss: 16.465, Final Batch Loss: 0.396\n",
      "Epoch 10764, Loss: 16.519, Final Batch Loss: 0.414\n",
      "Epoch 10765, Loss: 16.426, Final Batch Loss: 0.450\n",
      "Epoch 10766, Loss: 16.543, Final Batch Loss: 0.463\n",
      "Epoch 10767, Loss: 16.490, Final Batch Loss: 0.442\n",
      "Epoch 10768, Loss: 16.732, Final Batch Loss: 0.478\n",
      "Epoch 10769, Loss: 16.591, Final Batch Loss: 0.509\n",
      "Epoch 10770, Loss: 16.482, Final Batch Loss: 0.473\n",
      "Epoch 10771, Loss: 16.793, Final Batch Loss: 0.474\n",
      "Epoch 10772, Loss: 16.608, Final Batch Loss: 0.444\n",
      "Epoch 10773, Loss: 16.735, Final Batch Loss: 0.403\n",
      "Epoch 10774, Loss: 16.787, Final Batch Loss: 0.482\n",
      "Epoch 10775, Loss: 16.681, Final Batch Loss: 0.474\n",
      "Epoch 10776, Loss: 16.788, Final Batch Loss: 0.477\n",
      "Epoch 10777, Loss: 16.659, Final Batch Loss: 0.424\n",
      "Epoch 10778, Loss: 16.706, Final Batch Loss: 0.483\n",
      "Epoch 10779, Loss: 16.596, Final Batch Loss: 0.513\n",
      "Epoch 10780, Loss: 16.471, Final Batch Loss: 0.353\n",
      "Epoch 10781, Loss: 16.761, Final Batch Loss: 0.575\n",
      "Epoch 10782, Loss: 16.679, Final Batch Loss: 0.462\n",
      "Epoch 10783, Loss: 16.530, Final Batch Loss: 0.605\n",
      "Epoch 10784, Loss: 16.779, Final Batch Loss: 0.443\n",
      "Epoch 10785, Loss: 16.637, Final Batch Loss: 0.471\n",
      "Epoch 10786, Loss: 16.594, Final Batch Loss: 0.431\n",
      "Epoch 10787, Loss: 16.791, Final Batch Loss: 0.442\n",
      "Epoch 10788, Loss: 16.610, Final Batch Loss: 0.437\n",
      "Epoch 10789, Loss: 16.509, Final Batch Loss: 0.432\n",
      "Epoch 10790, Loss: 16.442, Final Batch Loss: 0.459\n",
      "Epoch 10791, Loss: 16.694, Final Batch Loss: 0.551\n",
      "Epoch 10792, Loss: 16.475, Final Batch Loss: 0.455\n",
      "Epoch 10793, Loss: 16.548, Final Batch Loss: 0.465\n",
      "Epoch 10794, Loss: 16.709, Final Batch Loss: 0.486\n",
      "Epoch 10795, Loss: 16.815, Final Batch Loss: 0.427\n",
      "Epoch 10796, Loss: 16.574, Final Batch Loss: 0.518\n",
      "Epoch 10797, Loss: 16.690, Final Batch Loss: 0.504\n",
      "Epoch 10798, Loss: 16.585, Final Batch Loss: 0.447\n",
      "Epoch 10799, Loss: 16.604, Final Batch Loss: 0.405\n",
      "Epoch 10800, Loss: 16.712, Final Batch Loss: 0.518\n",
      "Epoch 10801, Loss: 16.732, Final Batch Loss: 0.465\n",
      "Epoch 10802, Loss: 16.887, Final Batch Loss: 0.515\n",
      "Epoch 10803, Loss: 16.625, Final Batch Loss: 0.516\n",
      "Epoch 10804, Loss: 16.851, Final Batch Loss: 0.466\n",
      "Epoch 10805, Loss: 16.576, Final Batch Loss: 0.376\n",
      "Epoch 10806, Loss: 16.456, Final Batch Loss: 0.493\n",
      "Epoch 10807, Loss: 16.718, Final Batch Loss: 0.351\n",
      "Epoch 10808, Loss: 16.676, Final Batch Loss: 0.523\n",
      "Epoch 10809, Loss: 16.695, Final Batch Loss: 0.494\n",
      "Epoch 10810, Loss: 16.556, Final Batch Loss: 0.409\n",
      "Epoch 10811, Loss: 16.721, Final Batch Loss: 0.468\n",
      "Epoch 10812, Loss: 16.838, Final Batch Loss: 0.424\n",
      "Epoch 10813, Loss: 16.622, Final Batch Loss: 0.367\n",
      "Epoch 10814, Loss: 16.924, Final Batch Loss: 0.398\n",
      "Epoch 10815, Loss: 16.707, Final Batch Loss: 0.556\n",
      "Epoch 10816, Loss: 16.621, Final Batch Loss: 0.502\n",
      "Epoch 10817, Loss: 16.449, Final Batch Loss: 0.377\n",
      "Epoch 10818, Loss: 16.744, Final Batch Loss: 0.436\n",
      "Epoch 10819, Loss: 16.672, Final Batch Loss: 0.496\n",
      "Epoch 10820, Loss: 16.588, Final Batch Loss: 0.442\n",
      "Epoch 10821, Loss: 16.837, Final Batch Loss: 0.391\n",
      "Epoch 10822, Loss: 16.908, Final Batch Loss: 0.466\n",
      "Epoch 10823, Loss: 16.611, Final Batch Loss: 0.404\n",
      "Epoch 10824, Loss: 16.802, Final Batch Loss: 0.520\n",
      "Epoch 10825, Loss: 16.694, Final Batch Loss: 0.499\n",
      "Epoch 10826, Loss: 16.507, Final Batch Loss: 0.375\n",
      "Epoch 10827, Loss: 16.529, Final Batch Loss: 0.509\n",
      "Epoch 10828, Loss: 16.385, Final Batch Loss: 0.417\n",
      "Epoch 10829, Loss: 16.485, Final Batch Loss: 0.388\n",
      "Epoch 10830, Loss: 16.758, Final Batch Loss: 0.532\n",
      "Epoch 10831, Loss: 16.637, Final Batch Loss: 0.488\n",
      "Epoch 10832, Loss: 16.944, Final Batch Loss: 0.525\n",
      "Epoch 10833, Loss: 16.449, Final Batch Loss: 0.400\n",
      "Epoch 10834, Loss: 16.516, Final Batch Loss: 0.486\n",
      "Epoch 10835, Loss: 16.487, Final Batch Loss: 0.490\n",
      "Epoch 10836, Loss: 16.945, Final Batch Loss: 0.435\n",
      "Epoch 10837, Loss: 16.564, Final Batch Loss: 0.490\n",
      "Epoch 10838, Loss: 16.648, Final Batch Loss: 0.328\n",
      "Epoch 10839, Loss: 16.860, Final Batch Loss: 0.528\n",
      "Epoch 10840, Loss: 16.887, Final Batch Loss: 0.417\n",
      "Epoch 10841, Loss: 17.010, Final Batch Loss: 0.561\n",
      "Epoch 10842, Loss: 16.518, Final Batch Loss: 0.521\n",
      "Epoch 10843, Loss: 16.545, Final Batch Loss: 0.476\n",
      "Epoch 10844, Loss: 16.628, Final Batch Loss: 0.394\n",
      "Epoch 10845, Loss: 16.576, Final Batch Loss: 0.528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10846, Loss: 16.587, Final Batch Loss: 0.525\n",
      "Epoch 10847, Loss: 16.455, Final Batch Loss: 0.402\n",
      "Epoch 10848, Loss: 16.826, Final Batch Loss: 0.493\n",
      "Epoch 10849, Loss: 16.536, Final Batch Loss: 0.468\n",
      "Epoch 10850, Loss: 16.643, Final Batch Loss: 0.457\n",
      "Epoch 10851, Loss: 16.717, Final Batch Loss: 0.570\n",
      "Epoch 10852, Loss: 16.782, Final Batch Loss: 0.413\n",
      "Epoch 10853, Loss: 16.646, Final Batch Loss: 0.416\n",
      "Epoch 10854, Loss: 16.728, Final Batch Loss: 0.548\n",
      "Epoch 10855, Loss: 16.786, Final Batch Loss: 0.371\n",
      "Epoch 10856, Loss: 16.699, Final Batch Loss: 0.427\n",
      "Epoch 10857, Loss: 16.867, Final Batch Loss: 0.572\n",
      "Epoch 10858, Loss: 16.727, Final Batch Loss: 0.546\n",
      "Epoch 10859, Loss: 16.967, Final Batch Loss: 0.481\n",
      "Epoch 10860, Loss: 16.774, Final Batch Loss: 0.459\n",
      "Epoch 10861, Loss: 16.597, Final Batch Loss: 0.448\n",
      "Epoch 10862, Loss: 16.773, Final Batch Loss: 0.502\n",
      "Epoch 10863, Loss: 16.755, Final Batch Loss: 0.598\n",
      "Epoch 10864, Loss: 16.648, Final Batch Loss: 0.409\n",
      "Epoch 10865, Loss: 16.561, Final Batch Loss: 0.494\n",
      "Epoch 10866, Loss: 16.716, Final Batch Loss: 0.348\n",
      "Epoch 10867, Loss: 16.653, Final Batch Loss: 0.418\n",
      "Epoch 10868, Loss: 16.875, Final Batch Loss: 0.536\n",
      "Epoch 10869, Loss: 16.825, Final Batch Loss: 0.467\n",
      "Epoch 10870, Loss: 16.438, Final Batch Loss: 0.439\n",
      "Epoch 10871, Loss: 16.753, Final Batch Loss: 0.483\n",
      "Epoch 10872, Loss: 16.638, Final Batch Loss: 0.480\n",
      "Epoch 10873, Loss: 16.707, Final Batch Loss: 0.474\n",
      "Epoch 10874, Loss: 16.550, Final Batch Loss: 0.463\n",
      "Epoch 10875, Loss: 16.890, Final Batch Loss: 0.506\n",
      "Epoch 10876, Loss: 16.804, Final Batch Loss: 0.518\n",
      "Epoch 10877, Loss: 16.477, Final Batch Loss: 0.550\n",
      "Epoch 10878, Loss: 16.629, Final Batch Loss: 0.464\n",
      "Epoch 10879, Loss: 16.578, Final Batch Loss: 0.414\n",
      "Epoch 10880, Loss: 16.592, Final Batch Loss: 0.457\n",
      "Epoch 10881, Loss: 16.551, Final Batch Loss: 0.383\n",
      "Epoch 10882, Loss: 16.713, Final Batch Loss: 0.482\n",
      "Epoch 10883, Loss: 16.769, Final Batch Loss: 0.500\n",
      "Epoch 10884, Loss: 16.739, Final Batch Loss: 0.511\n",
      "Epoch 10885, Loss: 16.571, Final Batch Loss: 0.501\n",
      "Epoch 10886, Loss: 16.471, Final Batch Loss: 0.486\n",
      "Epoch 10887, Loss: 16.531, Final Batch Loss: 0.542\n",
      "Epoch 10888, Loss: 16.527, Final Batch Loss: 0.607\n",
      "Epoch 10889, Loss: 16.675, Final Batch Loss: 0.457\n",
      "Epoch 10890, Loss: 16.406, Final Batch Loss: 0.388\n",
      "Epoch 10891, Loss: 16.822, Final Batch Loss: 0.444\n",
      "Epoch 10892, Loss: 16.819, Final Batch Loss: 0.535\n",
      "Epoch 10893, Loss: 16.647, Final Batch Loss: 0.448\n",
      "Epoch 10894, Loss: 16.429, Final Batch Loss: 0.336\n",
      "Epoch 10895, Loss: 16.366, Final Batch Loss: 0.345\n",
      "Epoch 10896, Loss: 16.575, Final Batch Loss: 0.422\n",
      "Epoch 10897, Loss: 16.565, Final Batch Loss: 0.523\n",
      "Epoch 10898, Loss: 16.427, Final Batch Loss: 0.404\n",
      "Epoch 10899, Loss: 16.825, Final Batch Loss: 0.397\n",
      "Epoch 10900, Loss: 16.679, Final Batch Loss: 0.396\n",
      "Epoch 10901, Loss: 16.405, Final Batch Loss: 0.536\n",
      "Epoch 10902, Loss: 16.645, Final Batch Loss: 0.499\n",
      "Epoch 10903, Loss: 16.705, Final Batch Loss: 0.423\n",
      "Epoch 10904, Loss: 16.459, Final Batch Loss: 0.573\n",
      "Epoch 10905, Loss: 16.844, Final Batch Loss: 0.677\n",
      "Epoch 10906, Loss: 16.775, Final Batch Loss: 0.429\n",
      "Epoch 10907, Loss: 16.669, Final Batch Loss: 0.452\n",
      "Epoch 10908, Loss: 16.670, Final Batch Loss: 0.447\n",
      "Epoch 10909, Loss: 16.550, Final Batch Loss: 0.536\n",
      "Epoch 10910, Loss: 16.621, Final Batch Loss: 0.519\n",
      "Epoch 10911, Loss: 16.885, Final Batch Loss: 0.466\n",
      "Epoch 10912, Loss: 16.557, Final Batch Loss: 0.403\n",
      "Epoch 10913, Loss: 16.660, Final Batch Loss: 0.516\n",
      "Epoch 10914, Loss: 16.619, Final Batch Loss: 0.399\n",
      "Epoch 10915, Loss: 16.608, Final Batch Loss: 0.493\n",
      "Epoch 10916, Loss: 16.670, Final Batch Loss: 0.463\n",
      "Epoch 10917, Loss: 16.564, Final Batch Loss: 0.409\n",
      "Epoch 10918, Loss: 16.734, Final Batch Loss: 0.592\n",
      "Epoch 10919, Loss: 16.688, Final Batch Loss: 0.477\n",
      "Epoch 10920, Loss: 16.482, Final Batch Loss: 0.438\n",
      "Epoch 10921, Loss: 16.984, Final Batch Loss: 0.409\n",
      "Epoch 10922, Loss: 16.596, Final Batch Loss: 0.489\n",
      "Epoch 10923, Loss: 16.663, Final Batch Loss: 0.383\n",
      "Epoch 10924, Loss: 16.437, Final Batch Loss: 0.369\n",
      "Epoch 10925, Loss: 16.555, Final Batch Loss: 0.464\n",
      "Epoch 10926, Loss: 16.428, Final Batch Loss: 0.416\n",
      "Epoch 10927, Loss: 16.508, Final Batch Loss: 0.591\n",
      "Epoch 10928, Loss: 16.583, Final Batch Loss: 0.429\n",
      "Epoch 10929, Loss: 16.573, Final Batch Loss: 0.450\n",
      "Epoch 10930, Loss: 16.587, Final Batch Loss: 0.452\n",
      "Epoch 10931, Loss: 16.762, Final Batch Loss: 0.466\n",
      "Epoch 10932, Loss: 16.684, Final Batch Loss: 0.476\n",
      "Epoch 10933, Loss: 16.614, Final Batch Loss: 0.553\n",
      "Epoch 10934, Loss: 16.498, Final Batch Loss: 0.452\n",
      "Epoch 10935, Loss: 16.516, Final Batch Loss: 0.560\n",
      "Epoch 10936, Loss: 16.783, Final Batch Loss: 0.487\n",
      "Epoch 10937, Loss: 16.591, Final Batch Loss: 0.418\n",
      "Epoch 10938, Loss: 16.754, Final Batch Loss: 0.444\n",
      "Epoch 10939, Loss: 16.505, Final Batch Loss: 0.361\n",
      "Epoch 10940, Loss: 16.391, Final Batch Loss: 0.433\n",
      "Epoch 10941, Loss: 16.630, Final Batch Loss: 0.392\n",
      "Epoch 10942, Loss: 16.673, Final Batch Loss: 0.434\n",
      "Epoch 10943, Loss: 16.459, Final Batch Loss: 0.412\n",
      "Epoch 10944, Loss: 16.449, Final Batch Loss: 0.320\n",
      "Epoch 10945, Loss: 16.334, Final Batch Loss: 0.501\n",
      "Epoch 10946, Loss: 16.689, Final Batch Loss: 0.405\n",
      "Epoch 10947, Loss: 16.797, Final Batch Loss: 0.492\n",
      "Epoch 10948, Loss: 16.807, Final Batch Loss: 0.464\n",
      "Epoch 10949, Loss: 16.609, Final Batch Loss: 0.511\n",
      "Epoch 10950, Loss: 16.733, Final Batch Loss: 0.533\n",
      "Epoch 10951, Loss: 16.635, Final Batch Loss: 0.486\n",
      "Epoch 10952, Loss: 16.535, Final Batch Loss: 0.484\n",
      "Epoch 10953, Loss: 16.539, Final Batch Loss: 0.364\n",
      "Epoch 10954, Loss: 16.531, Final Batch Loss: 0.398\n",
      "Epoch 10955, Loss: 16.849, Final Batch Loss: 0.504\n",
      "Epoch 10956, Loss: 16.803, Final Batch Loss: 0.570\n",
      "Epoch 10957, Loss: 16.506, Final Batch Loss: 0.520\n",
      "Epoch 10958, Loss: 16.715, Final Batch Loss: 0.398\n",
      "Epoch 10959, Loss: 16.553, Final Batch Loss: 0.475\n",
      "Epoch 10960, Loss: 16.516, Final Batch Loss: 0.429\n",
      "Epoch 10961, Loss: 16.545, Final Batch Loss: 0.529\n",
      "Epoch 10962, Loss: 16.761, Final Batch Loss: 0.490\n",
      "Epoch 10963, Loss: 16.766, Final Batch Loss: 0.420\n",
      "Epoch 10964, Loss: 16.814, Final Batch Loss: 0.451\n",
      "Epoch 10965, Loss: 16.446, Final Batch Loss: 0.490\n",
      "Epoch 10966, Loss: 16.464, Final Batch Loss: 0.458\n",
      "Epoch 10967, Loss: 16.729, Final Batch Loss: 0.449\n",
      "Epoch 10968, Loss: 16.554, Final Batch Loss: 0.436\n",
      "Epoch 10969, Loss: 16.804, Final Batch Loss: 0.517\n",
      "Epoch 10970, Loss: 16.460, Final Batch Loss: 0.367\n",
      "Epoch 10971, Loss: 16.484, Final Batch Loss: 0.484\n",
      "Epoch 10972, Loss: 16.673, Final Batch Loss: 0.463\n",
      "Epoch 10973, Loss: 16.654, Final Batch Loss: 0.507\n",
      "Epoch 10974, Loss: 16.627, Final Batch Loss: 0.373\n",
      "Epoch 10975, Loss: 16.640, Final Batch Loss: 0.428\n",
      "Epoch 10976, Loss: 16.937, Final Batch Loss: 0.503\n",
      "Epoch 10977, Loss: 16.756, Final Batch Loss: 0.441\n",
      "Epoch 10978, Loss: 16.667, Final Batch Loss: 0.501\n",
      "Epoch 10979, Loss: 16.695, Final Batch Loss: 0.433\n",
      "Epoch 10980, Loss: 16.675, Final Batch Loss: 0.552\n",
      "Epoch 10981, Loss: 16.455, Final Batch Loss: 0.560\n",
      "Epoch 10982, Loss: 16.513, Final Batch Loss: 0.452\n",
      "Epoch 10983, Loss: 16.642, Final Batch Loss: 0.389\n",
      "Epoch 10984, Loss: 16.659, Final Batch Loss: 0.574\n",
      "Epoch 10985, Loss: 16.817, Final Batch Loss: 0.495\n",
      "Epoch 10986, Loss: 16.665, Final Batch Loss: 0.459\n",
      "Epoch 10987, Loss: 16.570, Final Batch Loss: 0.429\n",
      "Epoch 10988, Loss: 16.688, Final Batch Loss: 0.443\n",
      "Epoch 10989, Loss: 16.735, Final Batch Loss: 0.453\n",
      "Epoch 10990, Loss: 16.557, Final Batch Loss: 0.439\n",
      "Epoch 10991, Loss: 16.480, Final Batch Loss: 0.486\n",
      "Epoch 10992, Loss: 16.719, Final Batch Loss: 0.418\n",
      "Epoch 10993, Loss: 16.256, Final Batch Loss: 0.491\n",
      "Epoch 10994, Loss: 16.660, Final Batch Loss: 0.428\n",
      "Epoch 10995, Loss: 16.690, Final Batch Loss: 0.491\n",
      "Epoch 10996, Loss: 16.486, Final Batch Loss: 0.392\n",
      "Epoch 10997, Loss: 16.426, Final Batch Loss: 0.392\n",
      "Epoch 10998, Loss: 16.665, Final Batch Loss: 0.453\n",
      "Epoch 10999, Loss: 16.807, Final Batch Loss: 0.592\n",
      "Epoch 11000, Loss: 16.584, Final Batch Loss: 0.512\n",
      "Epoch 11001, Loss: 16.608, Final Batch Loss: 0.506\n",
      "Epoch 11002, Loss: 16.424, Final Batch Loss: 0.375\n",
      "Epoch 11003, Loss: 16.494, Final Batch Loss: 0.490\n",
      "Epoch 11004, Loss: 16.593, Final Batch Loss: 0.486\n",
      "Epoch 11005, Loss: 16.662, Final Batch Loss: 0.403\n",
      "Epoch 11006, Loss: 16.376, Final Batch Loss: 0.483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11007, Loss: 16.622, Final Batch Loss: 0.471\n",
      "Epoch 11008, Loss: 16.779, Final Batch Loss: 0.609\n",
      "Epoch 11009, Loss: 16.857, Final Batch Loss: 0.504\n",
      "Epoch 11010, Loss: 16.611, Final Batch Loss: 0.441\n",
      "Epoch 11011, Loss: 16.824, Final Batch Loss: 0.566\n",
      "Epoch 11012, Loss: 16.699, Final Batch Loss: 0.471\n",
      "Epoch 11013, Loss: 16.468, Final Batch Loss: 0.388\n",
      "Epoch 11014, Loss: 16.318, Final Batch Loss: 0.446\n",
      "Epoch 11015, Loss: 16.727, Final Batch Loss: 0.492\n",
      "Epoch 11016, Loss: 16.765, Final Batch Loss: 0.416\n",
      "Epoch 11017, Loss: 16.731, Final Batch Loss: 0.403\n",
      "Epoch 11018, Loss: 16.955, Final Batch Loss: 0.461\n",
      "Epoch 11019, Loss: 16.702, Final Batch Loss: 0.488\n",
      "Epoch 11020, Loss: 16.544, Final Batch Loss: 0.420\n",
      "Epoch 11021, Loss: 16.495, Final Batch Loss: 0.299\n",
      "Epoch 11022, Loss: 16.767, Final Batch Loss: 0.489\n",
      "Epoch 11023, Loss: 16.818, Final Batch Loss: 0.465\n",
      "Epoch 11024, Loss: 16.496, Final Batch Loss: 0.486\n",
      "Epoch 11025, Loss: 16.905, Final Batch Loss: 0.486\n",
      "Epoch 11026, Loss: 16.753, Final Batch Loss: 0.512\n",
      "Epoch 11027, Loss: 16.544, Final Batch Loss: 0.451\n",
      "Epoch 11028, Loss: 16.456, Final Batch Loss: 0.430\n",
      "Epoch 11029, Loss: 16.364, Final Batch Loss: 0.376\n",
      "Epoch 11030, Loss: 16.495, Final Batch Loss: 0.591\n",
      "Epoch 11031, Loss: 16.745, Final Batch Loss: 0.530\n",
      "Epoch 11032, Loss: 16.525, Final Batch Loss: 0.524\n",
      "Epoch 11033, Loss: 16.523, Final Batch Loss: 0.416\n",
      "Epoch 11034, Loss: 16.789, Final Batch Loss: 0.415\n",
      "Epoch 11035, Loss: 16.351, Final Batch Loss: 0.382\n",
      "Epoch 11036, Loss: 16.678, Final Batch Loss: 0.403\n",
      "Epoch 11037, Loss: 16.455, Final Batch Loss: 0.311\n",
      "Epoch 11038, Loss: 16.331, Final Batch Loss: 0.423\n",
      "Epoch 11039, Loss: 16.633, Final Batch Loss: 0.525\n",
      "Epoch 11040, Loss: 16.578, Final Batch Loss: 0.445\n",
      "Epoch 11041, Loss: 16.410, Final Batch Loss: 0.423\n",
      "Epoch 11042, Loss: 16.787, Final Batch Loss: 0.353\n",
      "Epoch 11043, Loss: 16.578, Final Batch Loss: 0.445\n",
      "Epoch 11044, Loss: 16.406, Final Batch Loss: 0.443\n",
      "Epoch 11045, Loss: 16.695, Final Batch Loss: 0.452\n",
      "Epoch 11046, Loss: 16.631, Final Batch Loss: 0.502\n",
      "Epoch 11047, Loss: 16.186, Final Batch Loss: 0.372\n",
      "Epoch 11048, Loss: 16.810, Final Batch Loss: 0.491\n",
      "Epoch 11049, Loss: 16.422, Final Batch Loss: 0.453\n",
      "Epoch 11050, Loss: 16.746, Final Batch Loss: 0.547\n",
      "Epoch 11051, Loss: 16.591, Final Batch Loss: 0.493\n",
      "Epoch 11052, Loss: 16.440, Final Batch Loss: 0.476\n",
      "Epoch 11053, Loss: 16.502, Final Batch Loss: 0.468\n",
      "Epoch 11054, Loss: 16.651, Final Batch Loss: 0.492\n",
      "Epoch 11055, Loss: 16.472, Final Batch Loss: 0.395\n",
      "Epoch 11056, Loss: 16.383, Final Batch Loss: 0.600\n",
      "Epoch 11057, Loss: 16.716, Final Batch Loss: 0.464\n",
      "Epoch 11058, Loss: 16.806, Final Batch Loss: 0.564\n",
      "Epoch 11059, Loss: 16.517, Final Batch Loss: 0.523\n",
      "Epoch 11060, Loss: 16.794, Final Batch Loss: 0.499\n",
      "Epoch 11061, Loss: 16.707, Final Batch Loss: 0.429\n",
      "Epoch 11062, Loss: 16.581, Final Batch Loss: 0.568\n",
      "Epoch 11063, Loss: 16.415, Final Batch Loss: 0.412\n",
      "Epoch 11064, Loss: 16.265, Final Batch Loss: 0.417\n",
      "Epoch 11065, Loss: 16.847, Final Batch Loss: 0.441\n",
      "Epoch 11066, Loss: 16.321, Final Batch Loss: 0.443\n",
      "Epoch 11067, Loss: 16.467, Final Batch Loss: 0.504\n",
      "Epoch 11068, Loss: 16.522, Final Batch Loss: 0.471\n",
      "Epoch 11069, Loss: 16.448, Final Batch Loss: 0.474\n",
      "Epoch 11070, Loss: 16.486, Final Batch Loss: 0.587\n",
      "Epoch 11071, Loss: 16.459, Final Batch Loss: 0.422\n",
      "Epoch 11072, Loss: 16.753, Final Batch Loss: 0.476\n",
      "Epoch 11073, Loss: 16.701, Final Batch Loss: 0.389\n",
      "Epoch 11074, Loss: 16.676, Final Batch Loss: 0.488\n",
      "Epoch 11075, Loss: 16.733, Final Batch Loss: 0.427\n",
      "Epoch 11076, Loss: 16.393, Final Batch Loss: 0.430\n",
      "Epoch 11077, Loss: 16.505, Final Batch Loss: 0.510\n",
      "Epoch 11078, Loss: 16.620, Final Batch Loss: 0.391\n",
      "Epoch 11079, Loss: 16.218, Final Batch Loss: 0.513\n",
      "Epoch 11080, Loss: 16.659, Final Batch Loss: 0.490\n",
      "Epoch 11081, Loss: 16.731, Final Batch Loss: 0.419\n",
      "Epoch 11082, Loss: 16.273, Final Batch Loss: 0.445\n",
      "Epoch 11083, Loss: 16.557, Final Batch Loss: 0.447\n",
      "Epoch 11084, Loss: 16.473, Final Batch Loss: 0.356\n",
      "Epoch 11085, Loss: 16.800, Final Batch Loss: 0.343\n",
      "Epoch 11086, Loss: 16.820, Final Batch Loss: 0.479\n",
      "Epoch 11087, Loss: 16.690, Final Batch Loss: 0.362\n",
      "Epoch 11088, Loss: 16.714, Final Batch Loss: 0.526\n",
      "Epoch 11089, Loss: 17.004, Final Batch Loss: 0.483\n",
      "Epoch 11090, Loss: 16.510, Final Batch Loss: 0.404\n",
      "Epoch 11091, Loss: 16.824, Final Batch Loss: 0.477\n",
      "Epoch 11092, Loss: 16.710, Final Batch Loss: 0.497\n",
      "Epoch 11093, Loss: 16.501, Final Batch Loss: 0.361\n",
      "Epoch 11094, Loss: 16.576, Final Batch Loss: 0.504\n",
      "Epoch 11095, Loss: 16.850, Final Batch Loss: 0.529\n",
      "Epoch 11096, Loss: 16.547, Final Batch Loss: 0.445\n",
      "Epoch 11097, Loss: 16.432, Final Batch Loss: 0.433\n",
      "Epoch 11098, Loss: 16.618, Final Batch Loss: 0.475\n",
      "Epoch 11099, Loss: 16.686, Final Batch Loss: 0.505\n",
      "Epoch 11100, Loss: 16.534, Final Batch Loss: 0.410\n",
      "Epoch 11101, Loss: 16.363, Final Batch Loss: 0.449\n",
      "Epoch 11102, Loss: 16.849, Final Batch Loss: 0.374\n",
      "Epoch 11103, Loss: 16.601, Final Batch Loss: 0.571\n",
      "Epoch 11104, Loss: 16.721, Final Batch Loss: 0.400\n",
      "Epoch 11105, Loss: 16.665, Final Batch Loss: 0.548\n",
      "Epoch 11106, Loss: 16.842, Final Batch Loss: 0.501\n",
      "Epoch 11107, Loss: 16.271, Final Batch Loss: 0.496\n",
      "Epoch 11108, Loss: 16.791, Final Batch Loss: 0.417\n",
      "Epoch 11109, Loss: 16.902, Final Batch Loss: 0.592\n",
      "Epoch 11110, Loss: 16.492, Final Batch Loss: 0.464\n",
      "Epoch 11111, Loss: 16.740, Final Batch Loss: 0.506\n",
      "Epoch 11112, Loss: 16.689, Final Batch Loss: 0.438\n",
      "Epoch 11113, Loss: 16.493, Final Batch Loss: 0.410\n",
      "Epoch 11114, Loss: 16.449, Final Batch Loss: 0.399\n",
      "Epoch 11115, Loss: 16.621, Final Batch Loss: 0.480\n",
      "Epoch 11116, Loss: 16.575, Final Batch Loss: 0.397\n",
      "Epoch 11117, Loss: 16.538, Final Batch Loss: 0.442\n",
      "Epoch 11118, Loss: 16.321, Final Batch Loss: 0.442\n",
      "Epoch 11119, Loss: 16.970, Final Batch Loss: 0.632\n",
      "Epoch 11120, Loss: 16.598, Final Batch Loss: 0.414\n",
      "Epoch 11121, Loss: 16.560, Final Batch Loss: 0.470\n",
      "Epoch 11122, Loss: 16.530, Final Batch Loss: 0.438\n",
      "Epoch 11123, Loss: 16.801, Final Batch Loss: 0.413\n",
      "Epoch 11124, Loss: 16.553, Final Batch Loss: 0.350\n",
      "Epoch 11125, Loss: 16.528, Final Batch Loss: 0.502\n",
      "Epoch 11126, Loss: 16.478, Final Batch Loss: 0.488\n",
      "Epoch 11127, Loss: 16.577, Final Batch Loss: 0.465\n",
      "Epoch 11128, Loss: 16.708, Final Batch Loss: 0.479\n",
      "Epoch 11129, Loss: 16.357, Final Batch Loss: 0.431\n",
      "Epoch 11130, Loss: 16.661, Final Batch Loss: 0.423\n",
      "Epoch 11131, Loss: 16.590, Final Batch Loss: 0.402\n",
      "Epoch 11132, Loss: 16.522, Final Batch Loss: 0.509\n",
      "Epoch 11133, Loss: 16.474, Final Batch Loss: 0.457\n",
      "Epoch 11134, Loss: 16.735, Final Batch Loss: 0.603\n",
      "Epoch 11135, Loss: 16.751, Final Batch Loss: 0.533\n",
      "Epoch 11136, Loss: 16.661, Final Batch Loss: 0.476\n",
      "Epoch 11137, Loss: 16.489, Final Batch Loss: 0.458\n",
      "Epoch 11138, Loss: 16.526, Final Batch Loss: 0.376\n",
      "Epoch 11139, Loss: 16.721, Final Batch Loss: 0.444\n",
      "Epoch 11140, Loss: 16.454, Final Batch Loss: 0.456\n",
      "Epoch 11141, Loss: 16.940, Final Batch Loss: 0.555\n",
      "Epoch 11142, Loss: 16.743, Final Batch Loss: 0.380\n",
      "Epoch 11143, Loss: 16.379, Final Batch Loss: 0.409\n",
      "Epoch 11144, Loss: 16.622, Final Batch Loss: 0.504\n",
      "Epoch 11145, Loss: 16.630, Final Batch Loss: 0.408\n",
      "Epoch 11146, Loss: 16.776, Final Batch Loss: 0.542\n",
      "Epoch 11147, Loss: 16.561, Final Batch Loss: 0.498\n",
      "Epoch 11148, Loss: 16.494, Final Batch Loss: 0.411\n",
      "Epoch 11149, Loss: 16.574, Final Batch Loss: 0.534\n",
      "Epoch 11150, Loss: 16.705, Final Batch Loss: 0.395\n",
      "Epoch 11151, Loss: 16.415, Final Batch Loss: 0.431\n",
      "Epoch 11152, Loss: 16.410, Final Batch Loss: 0.397\n",
      "Epoch 11153, Loss: 16.609, Final Batch Loss: 0.467\n",
      "Epoch 11154, Loss: 16.844, Final Batch Loss: 0.423\n",
      "Epoch 11155, Loss: 16.413, Final Batch Loss: 0.434\n",
      "Epoch 11156, Loss: 16.563, Final Batch Loss: 0.494\n",
      "Epoch 11157, Loss: 16.700, Final Batch Loss: 0.470\n",
      "Epoch 11158, Loss: 16.530, Final Batch Loss: 0.483\n",
      "Epoch 11159, Loss: 16.758, Final Batch Loss: 0.603\n",
      "Epoch 11160, Loss: 16.541, Final Batch Loss: 0.417\n",
      "Epoch 11161, Loss: 16.475, Final Batch Loss: 0.441\n",
      "Epoch 11162, Loss: 16.460, Final Batch Loss: 0.384\n",
      "Epoch 11163, Loss: 16.401, Final Batch Loss: 0.398\n",
      "Epoch 11164, Loss: 16.700, Final Batch Loss: 0.526\n",
      "Epoch 11165, Loss: 16.482, Final Batch Loss: 0.417\n",
      "Epoch 11166, Loss: 16.655, Final Batch Loss: 0.499\n",
      "Epoch 11167, Loss: 16.684, Final Batch Loss: 0.474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11168, Loss: 16.307, Final Batch Loss: 0.515\n",
      "Epoch 11169, Loss: 16.470, Final Batch Loss: 0.428\n",
      "Epoch 11170, Loss: 16.793, Final Batch Loss: 0.480\n",
      "Epoch 11171, Loss: 16.805, Final Batch Loss: 0.378\n",
      "Epoch 11172, Loss: 16.418, Final Batch Loss: 0.465\n",
      "Epoch 11173, Loss: 16.195, Final Batch Loss: 0.412\n",
      "Epoch 11174, Loss: 16.463, Final Batch Loss: 0.415\n",
      "Epoch 11175, Loss: 16.513, Final Batch Loss: 0.438\n",
      "Epoch 11176, Loss: 16.584, Final Batch Loss: 0.456\n",
      "Epoch 11177, Loss: 16.592, Final Batch Loss: 0.472\n",
      "Epoch 11178, Loss: 16.376, Final Batch Loss: 0.442\n",
      "Epoch 11179, Loss: 16.410, Final Batch Loss: 0.445\n",
      "Epoch 11180, Loss: 16.484, Final Batch Loss: 0.448\n",
      "Epoch 11181, Loss: 16.495, Final Batch Loss: 0.390\n",
      "Epoch 11182, Loss: 16.556, Final Batch Loss: 0.425\n",
      "Epoch 11183, Loss: 16.432, Final Batch Loss: 0.320\n",
      "Epoch 11184, Loss: 16.653, Final Batch Loss: 0.420\n",
      "Epoch 11185, Loss: 16.540, Final Batch Loss: 0.421\n",
      "Epoch 11186, Loss: 16.465, Final Batch Loss: 0.428\n",
      "Epoch 11187, Loss: 16.586, Final Batch Loss: 0.426\n",
      "Epoch 11188, Loss: 16.256, Final Batch Loss: 0.442\n",
      "Epoch 11189, Loss: 16.758, Final Batch Loss: 0.413\n",
      "Epoch 11190, Loss: 16.595, Final Batch Loss: 0.338\n",
      "Epoch 11191, Loss: 16.603, Final Batch Loss: 0.429\n",
      "Epoch 11192, Loss: 16.577, Final Batch Loss: 0.458\n",
      "Epoch 11193, Loss: 16.489, Final Batch Loss: 0.403\n",
      "Epoch 11194, Loss: 16.465, Final Batch Loss: 0.414\n",
      "Epoch 11195, Loss: 16.719, Final Batch Loss: 0.469\n",
      "Epoch 11196, Loss: 16.359, Final Batch Loss: 0.524\n",
      "Epoch 11197, Loss: 16.613, Final Batch Loss: 0.347\n",
      "Epoch 11198, Loss: 16.596, Final Batch Loss: 0.405\n",
      "Epoch 11199, Loss: 16.844, Final Batch Loss: 0.484\n",
      "Epoch 11200, Loss: 16.515, Final Batch Loss: 0.507\n",
      "Epoch 11201, Loss: 16.618, Final Batch Loss: 0.476\n",
      "Epoch 11202, Loss: 16.704, Final Batch Loss: 0.553\n",
      "Epoch 11203, Loss: 16.618, Final Batch Loss: 0.438\n",
      "Epoch 11204, Loss: 16.210, Final Batch Loss: 0.449\n",
      "Epoch 11205, Loss: 16.668, Final Batch Loss: 0.360\n",
      "Epoch 11206, Loss: 16.299, Final Batch Loss: 0.483\n",
      "Epoch 11207, Loss: 16.491, Final Batch Loss: 0.426\n",
      "Epoch 11208, Loss: 16.612, Final Batch Loss: 0.500\n",
      "Epoch 11209, Loss: 16.767, Final Batch Loss: 0.480\n",
      "Epoch 11210, Loss: 16.627, Final Batch Loss: 0.406\n",
      "Epoch 11211, Loss: 16.354, Final Batch Loss: 0.435\n",
      "Epoch 11212, Loss: 16.611, Final Batch Loss: 0.463\n",
      "Epoch 11213, Loss: 16.997, Final Batch Loss: 0.477\n",
      "Epoch 11214, Loss: 16.644, Final Batch Loss: 0.424\n",
      "Epoch 11215, Loss: 16.524, Final Batch Loss: 0.480\n",
      "Epoch 11216, Loss: 16.827, Final Batch Loss: 0.515\n",
      "Epoch 11217, Loss: 16.460, Final Batch Loss: 0.497\n",
      "Epoch 11218, Loss: 16.711, Final Batch Loss: 0.495\n",
      "Epoch 11219, Loss: 16.566, Final Batch Loss: 0.430\n",
      "Epoch 11220, Loss: 16.521, Final Batch Loss: 0.488\n",
      "Epoch 11221, Loss: 16.470, Final Batch Loss: 0.476\n",
      "Epoch 11222, Loss: 16.486, Final Batch Loss: 0.490\n",
      "Epoch 11223, Loss: 16.559, Final Batch Loss: 0.572\n",
      "Epoch 11224, Loss: 16.739, Final Batch Loss: 0.428\n",
      "Epoch 11225, Loss: 16.679, Final Batch Loss: 0.540\n",
      "Epoch 11226, Loss: 16.389, Final Batch Loss: 0.445\n",
      "Epoch 11227, Loss: 16.630, Final Batch Loss: 0.500\n",
      "Epoch 11228, Loss: 16.753, Final Batch Loss: 0.592\n",
      "Epoch 11229, Loss: 16.496, Final Batch Loss: 0.539\n",
      "Epoch 11230, Loss: 16.572, Final Batch Loss: 0.588\n",
      "Epoch 11231, Loss: 16.541, Final Batch Loss: 0.566\n",
      "Epoch 11232, Loss: 16.774, Final Batch Loss: 0.378\n",
      "Epoch 11233, Loss: 16.644, Final Batch Loss: 0.420\n",
      "Epoch 11234, Loss: 16.727, Final Batch Loss: 0.433\n",
      "Epoch 11235, Loss: 16.569, Final Batch Loss: 0.498\n",
      "Epoch 11236, Loss: 16.729, Final Batch Loss: 0.475\n",
      "Epoch 11237, Loss: 16.550, Final Batch Loss: 0.404\n",
      "Epoch 11238, Loss: 16.537, Final Batch Loss: 0.421\n",
      "Epoch 11239, Loss: 16.740, Final Batch Loss: 0.435\n",
      "Epoch 11240, Loss: 16.669, Final Batch Loss: 0.428\n",
      "Epoch 11241, Loss: 16.618, Final Batch Loss: 0.392\n",
      "Epoch 11242, Loss: 16.594, Final Batch Loss: 0.475\n",
      "Epoch 11243, Loss: 16.758, Final Batch Loss: 0.544\n",
      "Epoch 11244, Loss: 16.461, Final Batch Loss: 0.432\n",
      "Epoch 11245, Loss: 16.430, Final Batch Loss: 0.410\n",
      "Epoch 11246, Loss: 16.596, Final Batch Loss: 0.460\n",
      "Epoch 11247, Loss: 16.959, Final Batch Loss: 0.519\n",
      "Epoch 11248, Loss: 16.504, Final Batch Loss: 0.453\n",
      "Epoch 11249, Loss: 16.507, Final Batch Loss: 0.470\n",
      "Epoch 11250, Loss: 16.519, Final Batch Loss: 0.494\n",
      "Epoch 11251, Loss: 16.808, Final Batch Loss: 0.480\n",
      "Epoch 11252, Loss: 16.429, Final Batch Loss: 0.416\n",
      "Epoch 11253, Loss: 16.584, Final Batch Loss: 0.380\n",
      "Epoch 11254, Loss: 16.431, Final Batch Loss: 0.411\n",
      "Epoch 11255, Loss: 16.788, Final Batch Loss: 0.422\n",
      "Epoch 11256, Loss: 16.574, Final Batch Loss: 0.502\n",
      "Epoch 11257, Loss: 16.464, Final Batch Loss: 0.476\n",
      "Epoch 11258, Loss: 16.379, Final Batch Loss: 0.536\n",
      "Epoch 11259, Loss: 16.706, Final Batch Loss: 0.453\n",
      "Epoch 11260, Loss: 16.691, Final Batch Loss: 0.605\n",
      "Epoch 11261, Loss: 16.328, Final Batch Loss: 0.434\n",
      "Epoch 11262, Loss: 16.432, Final Batch Loss: 0.501\n",
      "Epoch 11263, Loss: 16.675, Final Batch Loss: 0.452\n",
      "Epoch 11264, Loss: 16.530, Final Batch Loss: 0.418\n",
      "Epoch 11265, Loss: 16.760, Final Batch Loss: 0.485\n",
      "Epoch 11266, Loss: 16.795, Final Batch Loss: 0.446\n",
      "Epoch 11267, Loss: 16.612, Final Batch Loss: 0.521\n",
      "Epoch 11268, Loss: 16.373, Final Batch Loss: 0.364\n",
      "Epoch 11269, Loss: 16.620, Final Batch Loss: 0.476\n",
      "Epoch 11270, Loss: 16.686, Final Batch Loss: 0.510\n",
      "Epoch 11271, Loss: 16.579, Final Batch Loss: 0.402\n",
      "Epoch 11272, Loss: 16.535, Final Batch Loss: 0.411\n",
      "Epoch 11273, Loss: 16.498, Final Batch Loss: 0.520\n",
      "Epoch 11274, Loss: 16.522, Final Batch Loss: 0.402\n",
      "Epoch 11275, Loss: 16.747, Final Batch Loss: 0.620\n",
      "Epoch 11276, Loss: 16.500, Final Batch Loss: 0.546\n",
      "Epoch 11277, Loss: 16.435, Final Batch Loss: 0.471\n",
      "Epoch 11278, Loss: 16.743, Final Batch Loss: 0.399\n",
      "Epoch 11279, Loss: 16.518, Final Batch Loss: 0.555\n",
      "Epoch 11280, Loss: 16.479, Final Batch Loss: 0.507\n",
      "Epoch 11281, Loss: 16.462, Final Batch Loss: 0.418\n",
      "Epoch 11282, Loss: 16.302, Final Batch Loss: 0.453\n",
      "Epoch 11283, Loss: 16.481, Final Batch Loss: 0.460\n",
      "Epoch 11284, Loss: 16.694, Final Batch Loss: 0.461\n",
      "Epoch 11285, Loss: 16.584, Final Batch Loss: 0.396\n",
      "Epoch 11286, Loss: 16.669, Final Batch Loss: 0.543\n",
      "Epoch 11287, Loss: 16.473, Final Batch Loss: 0.546\n",
      "Epoch 11288, Loss: 16.317, Final Batch Loss: 0.363\n",
      "Epoch 11289, Loss: 16.416, Final Batch Loss: 0.479\n",
      "Epoch 11290, Loss: 16.701, Final Batch Loss: 0.422\n",
      "Epoch 11291, Loss: 16.582, Final Batch Loss: 0.523\n",
      "Epoch 11292, Loss: 16.839, Final Batch Loss: 0.539\n",
      "Epoch 11293, Loss: 16.393, Final Batch Loss: 0.491\n",
      "Epoch 11294, Loss: 16.681, Final Batch Loss: 0.458\n",
      "Epoch 11295, Loss: 16.750, Final Batch Loss: 0.559\n",
      "Epoch 11296, Loss: 16.539, Final Batch Loss: 0.446\n",
      "Epoch 11297, Loss: 16.758, Final Batch Loss: 0.422\n",
      "Epoch 11298, Loss: 16.588, Final Batch Loss: 0.410\n",
      "Epoch 11299, Loss: 16.747, Final Batch Loss: 0.487\n",
      "Epoch 11300, Loss: 16.730, Final Batch Loss: 0.520\n",
      "Epoch 11301, Loss: 16.604, Final Batch Loss: 0.420\n",
      "Epoch 11302, Loss: 16.669, Final Batch Loss: 0.462\n",
      "Epoch 11303, Loss: 16.415, Final Batch Loss: 0.455\n",
      "Epoch 11304, Loss: 16.411, Final Batch Loss: 0.430\n",
      "Epoch 11305, Loss: 16.793, Final Batch Loss: 0.546\n",
      "Epoch 11306, Loss: 16.403, Final Batch Loss: 0.368\n",
      "Epoch 11307, Loss: 16.615, Final Batch Loss: 0.439\n",
      "Epoch 11308, Loss: 16.619, Final Batch Loss: 0.418\n",
      "Epoch 11309, Loss: 16.552, Final Batch Loss: 0.353\n",
      "Epoch 11310, Loss: 16.543, Final Batch Loss: 0.483\n",
      "Epoch 11311, Loss: 16.453, Final Batch Loss: 0.427\n",
      "Epoch 11312, Loss: 16.809, Final Batch Loss: 0.513\n",
      "Epoch 11313, Loss: 16.502, Final Batch Loss: 0.396\n",
      "Epoch 11314, Loss: 16.404, Final Batch Loss: 0.434\n",
      "Epoch 11315, Loss: 16.673, Final Batch Loss: 0.439\n",
      "Epoch 11316, Loss: 16.452, Final Batch Loss: 0.494\n",
      "Epoch 11317, Loss: 16.396, Final Batch Loss: 0.442\n",
      "Epoch 11318, Loss: 16.587, Final Batch Loss: 0.380\n",
      "Epoch 11319, Loss: 16.782, Final Batch Loss: 0.497\n",
      "Epoch 11320, Loss: 16.667, Final Batch Loss: 0.488\n",
      "Epoch 11321, Loss: 16.350, Final Batch Loss: 0.354\n",
      "Epoch 11322, Loss: 16.741, Final Batch Loss: 0.599\n",
      "Epoch 11323, Loss: 16.598, Final Batch Loss: 0.477\n",
      "Epoch 11324, Loss: 16.795, Final Batch Loss: 0.536\n",
      "Epoch 11325, Loss: 16.680, Final Batch Loss: 0.529\n",
      "Epoch 11326, Loss: 16.570, Final Batch Loss: 0.514\n",
      "Epoch 11327, Loss: 16.502, Final Batch Loss: 0.480\n",
      "Epoch 11328, Loss: 16.753, Final Batch Loss: 0.579\n",
      "Epoch 11329, Loss: 16.428, Final Batch Loss: 0.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11330, Loss: 16.612, Final Batch Loss: 0.511\n",
      "Epoch 11331, Loss: 16.362, Final Batch Loss: 0.427\n",
      "Epoch 11332, Loss: 16.372, Final Batch Loss: 0.311\n",
      "Epoch 11333, Loss: 16.630, Final Batch Loss: 0.433\n",
      "Epoch 11334, Loss: 16.599, Final Batch Loss: 0.430\n",
      "Epoch 11335, Loss: 16.589, Final Batch Loss: 0.528\n",
      "Epoch 11336, Loss: 16.667, Final Batch Loss: 0.587\n",
      "Epoch 11337, Loss: 16.672, Final Batch Loss: 0.500\n",
      "Epoch 11338, Loss: 16.513, Final Batch Loss: 0.416\n",
      "Epoch 11339, Loss: 16.814, Final Batch Loss: 0.495\n",
      "Epoch 11340, Loss: 16.462, Final Batch Loss: 0.395\n",
      "Epoch 11341, Loss: 16.662, Final Batch Loss: 0.564\n",
      "Epoch 11342, Loss: 16.733, Final Batch Loss: 0.413\n",
      "Epoch 11343, Loss: 16.519, Final Batch Loss: 0.537\n",
      "Epoch 11344, Loss: 16.503, Final Batch Loss: 0.441\n",
      "Epoch 11345, Loss: 16.330, Final Batch Loss: 0.446\n",
      "Epoch 11346, Loss: 16.661, Final Batch Loss: 0.503\n",
      "Epoch 11347, Loss: 16.606, Final Batch Loss: 0.373\n",
      "Epoch 11348, Loss: 16.527, Final Batch Loss: 0.506\n",
      "Epoch 11349, Loss: 16.774, Final Batch Loss: 0.659\n",
      "Epoch 11350, Loss: 16.559, Final Batch Loss: 0.516\n",
      "Epoch 11351, Loss: 16.631, Final Batch Loss: 0.489\n",
      "Epoch 11352, Loss: 16.412, Final Batch Loss: 0.423\n",
      "Epoch 11353, Loss: 16.347, Final Batch Loss: 0.427\n",
      "Epoch 11354, Loss: 16.663, Final Batch Loss: 0.562\n",
      "Epoch 11355, Loss: 16.344, Final Batch Loss: 0.346\n",
      "Epoch 11356, Loss: 16.418, Final Batch Loss: 0.498\n",
      "Epoch 11357, Loss: 16.852, Final Batch Loss: 0.412\n",
      "Epoch 11358, Loss: 16.342, Final Batch Loss: 0.393\n",
      "Epoch 11359, Loss: 16.638, Final Batch Loss: 0.414\n",
      "Epoch 11360, Loss: 16.528, Final Batch Loss: 0.489\n",
      "Epoch 11361, Loss: 16.626, Final Batch Loss: 0.412\n",
      "Epoch 11362, Loss: 16.730, Final Batch Loss: 0.502\n",
      "Epoch 11363, Loss: 16.558, Final Batch Loss: 0.427\n",
      "Epoch 11364, Loss: 16.424, Final Batch Loss: 0.449\n",
      "Epoch 11365, Loss: 16.615, Final Batch Loss: 0.435\n",
      "Epoch 11366, Loss: 16.698, Final Batch Loss: 0.573\n",
      "Epoch 11367, Loss: 16.557, Final Batch Loss: 0.413\n",
      "Epoch 11368, Loss: 16.480, Final Batch Loss: 0.381\n",
      "Epoch 11369, Loss: 16.523, Final Batch Loss: 0.422\n",
      "Epoch 11370, Loss: 16.850, Final Batch Loss: 0.529\n",
      "Epoch 11371, Loss: 16.431, Final Batch Loss: 0.456\n",
      "Epoch 11372, Loss: 16.614, Final Batch Loss: 0.388\n",
      "Epoch 11373, Loss: 16.475, Final Batch Loss: 0.450\n",
      "Epoch 11374, Loss: 16.601, Final Batch Loss: 0.442\n",
      "Epoch 11375, Loss: 16.655, Final Batch Loss: 0.401\n",
      "Epoch 11376, Loss: 16.539, Final Batch Loss: 0.426\n",
      "Epoch 11377, Loss: 16.458, Final Batch Loss: 0.459\n",
      "Epoch 11378, Loss: 16.522, Final Batch Loss: 0.557\n",
      "Epoch 11379, Loss: 16.765, Final Batch Loss: 0.522\n",
      "Epoch 11380, Loss: 16.592, Final Batch Loss: 0.474\n",
      "Epoch 11381, Loss: 16.693, Final Batch Loss: 0.434\n",
      "Epoch 11382, Loss: 16.720, Final Batch Loss: 0.445\n",
      "Epoch 11383, Loss: 16.492, Final Batch Loss: 0.453\n",
      "Epoch 11384, Loss: 16.525, Final Batch Loss: 0.429\n",
      "Epoch 11385, Loss: 16.751, Final Batch Loss: 0.556\n",
      "Epoch 11386, Loss: 16.705, Final Batch Loss: 0.533\n",
      "Epoch 11387, Loss: 16.291, Final Batch Loss: 0.476\n",
      "Epoch 11388, Loss: 16.442, Final Batch Loss: 0.441\n",
      "Epoch 11389, Loss: 16.675, Final Batch Loss: 0.449\n",
      "Epoch 11390, Loss: 16.603, Final Batch Loss: 0.439\n",
      "Epoch 11391, Loss: 16.235, Final Batch Loss: 0.298\n",
      "Epoch 11392, Loss: 16.532, Final Batch Loss: 0.540\n",
      "Epoch 11393, Loss: 16.608, Final Batch Loss: 0.466\n",
      "Epoch 11394, Loss: 16.718, Final Batch Loss: 0.493\n",
      "Epoch 11395, Loss: 16.516, Final Batch Loss: 0.461\n",
      "Epoch 11396, Loss: 16.478, Final Batch Loss: 0.424\n",
      "Epoch 11397, Loss: 16.485, Final Batch Loss: 0.440\n",
      "Epoch 11398, Loss: 16.679, Final Batch Loss: 0.495\n",
      "Epoch 11399, Loss: 16.628, Final Batch Loss: 0.548\n",
      "Epoch 11400, Loss: 16.579, Final Batch Loss: 0.425\n",
      "Epoch 11401, Loss: 16.771, Final Batch Loss: 0.448\n",
      "Epoch 11402, Loss: 16.879, Final Batch Loss: 0.457\n",
      "Epoch 11403, Loss: 16.738, Final Batch Loss: 0.509\n",
      "Epoch 11404, Loss: 16.611, Final Batch Loss: 0.412\n",
      "Epoch 11405, Loss: 16.626, Final Batch Loss: 0.522\n",
      "Epoch 11406, Loss: 16.675, Final Batch Loss: 0.611\n",
      "Epoch 11407, Loss: 16.561, Final Batch Loss: 0.436\n",
      "Epoch 11408, Loss: 16.532, Final Batch Loss: 0.457\n",
      "Epoch 11409, Loss: 16.771, Final Batch Loss: 0.527\n",
      "Epoch 11410, Loss: 16.629, Final Batch Loss: 0.392\n",
      "Epoch 11411, Loss: 16.744, Final Batch Loss: 0.448\n",
      "Epoch 11412, Loss: 16.399, Final Batch Loss: 0.428\n",
      "Epoch 11413, Loss: 16.570, Final Batch Loss: 0.445\n",
      "Epoch 11414, Loss: 16.487, Final Batch Loss: 0.467\n",
      "Epoch 11415, Loss: 16.953, Final Batch Loss: 0.532\n",
      "Epoch 11416, Loss: 16.313, Final Batch Loss: 0.512\n",
      "Epoch 11417, Loss: 16.529, Final Batch Loss: 0.444\n",
      "Epoch 11418, Loss: 16.692, Final Batch Loss: 0.525\n",
      "Epoch 11419, Loss: 16.775, Final Batch Loss: 0.559\n",
      "Epoch 11420, Loss: 16.732, Final Batch Loss: 0.389\n",
      "Epoch 11421, Loss: 16.742, Final Batch Loss: 0.561\n",
      "Epoch 11422, Loss: 16.591, Final Batch Loss: 0.370\n",
      "Epoch 11423, Loss: 16.461, Final Batch Loss: 0.437\n",
      "Epoch 11424, Loss: 16.738, Final Batch Loss: 0.467\n",
      "Epoch 11425, Loss: 16.815, Final Batch Loss: 0.389\n",
      "Epoch 11426, Loss: 16.803, Final Batch Loss: 0.462\n",
      "Epoch 11427, Loss: 16.612, Final Batch Loss: 0.445\n",
      "Epoch 11428, Loss: 16.597, Final Batch Loss: 0.525\n",
      "Epoch 11429, Loss: 16.846, Final Batch Loss: 0.473\n",
      "Epoch 11430, Loss: 16.755, Final Batch Loss: 0.437\n",
      "Epoch 11431, Loss: 16.340, Final Batch Loss: 0.425\n",
      "Epoch 11432, Loss: 16.456, Final Batch Loss: 0.408\n",
      "Epoch 11433, Loss: 16.625, Final Batch Loss: 0.380\n",
      "Epoch 11434, Loss: 16.553, Final Batch Loss: 0.349\n",
      "Epoch 11435, Loss: 16.580, Final Batch Loss: 0.538\n",
      "Epoch 11436, Loss: 16.431, Final Batch Loss: 0.501\n",
      "Epoch 11437, Loss: 16.814, Final Batch Loss: 0.495\n",
      "Epoch 11438, Loss: 16.732, Final Batch Loss: 0.479\n",
      "Epoch 11439, Loss: 16.527, Final Batch Loss: 0.370\n",
      "Epoch 11440, Loss: 16.410, Final Batch Loss: 0.459\n",
      "Epoch 11441, Loss: 16.654, Final Batch Loss: 0.413\n",
      "Epoch 11442, Loss: 16.248, Final Batch Loss: 0.483\n",
      "Epoch 11443, Loss: 16.602, Final Batch Loss: 0.476\n",
      "Epoch 11444, Loss: 16.805, Final Batch Loss: 0.484\n",
      "Epoch 11445, Loss: 16.682, Final Batch Loss: 0.522\n",
      "Epoch 11446, Loss: 16.345, Final Batch Loss: 0.486\n",
      "Epoch 11447, Loss: 16.816, Final Batch Loss: 0.424\n",
      "Epoch 11448, Loss: 16.655, Final Batch Loss: 0.517\n",
      "Epoch 11449, Loss: 16.615, Final Batch Loss: 0.457\n",
      "Epoch 11450, Loss: 16.752, Final Batch Loss: 0.524\n",
      "Epoch 11451, Loss: 16.998, Final Batch Loss: 0.576\n",
      "Epoch 11452, Loss: 16.289, Final Batch Loss: 0.442\n",
      "Epoch 11453, Loss: 16.916, Final Batch Loss: 0.472\n",
      "Epoch 11454, Loss: 16.547, Final Batch Loss: 0.402\n",
      "Epoch 11455, Loss: 16.734, Final Batch Loss: 0.438\n",
      "Epoch 11456, Loss: 16.426, Final Batch Loss: 0.423\n",
      "Epoch 11457, Loss: 16.608, Final Batch Loss: 0.534\n",
      "Epoch 11458, Loss: 16.697, Final Batch Loss: 0.443\n",
      "Epoch 11459, Loss: 16.724, Final Batch Loss: 0.517\n",
      "Epoch 11460, Loss: 16.766, Final Batch Loss: 0.489\n",
      "Epoch 11461, Loss: 16.683, Final Batch Loss: 0.526\n",
      "Epoch 11462, Loss: 16.561, Final Batch Loss: 0.392\n",
      "Epoch 11463, Loss: 16.539, Final Batch Loss: 0.479\n",
      "Epoch 11464, Loss: 16.749, Final Batch Loss: 0.558\n",
      "Epoch 11465, Loss: 16.440, Final Batch Loss: 0.473\n",
      "Epoch 11466, Loss: 16.593, Final Batch Loss: 0.368\n",
      "Epoch 11467, Loss: 16.645, Final Batch Loss: 0.436\n",
      "Epoch 11468, Loss: 16.738, Final Batch Loss: 0.450\n",
      "Epoch 11469, Loss: 16.407, Final Batch Loss: 0.442\n",
      "Epoch 11470, Loss: 16.948, Final Batch Loss: 0.552\n",
      "Epoch 11471, Loss: 16.744, Final Batch Loss: 0.478\n",
      "Epoch 11472, Loss: 16.369, Final Batch Loss: 0.438\n",
      "Epoch 11473, Loss: 16.415, Final Batch Loss: 0.487\n",
      "Epoch 11474, Loss: 16.453, Final Batch Loss: 0.475\n",
      "Epoch 11475, Loss: 16.584, Final Batch Loss: 0.440\n",
      "Epoch 11476, Loss: 16.339, Final Batch Loss: 0.399\n",
      "Epoch 11477, Loss: 16.319, Final Batch Loss: 0.418\n",
      "Epoch 11478, Loss: 16.668, Final Batch Loss: 0.367\n",
      "Epoch 11479, Loss: 16.760, Final Batch Loss: 0.482\n",
      "Epoch 11480, Loss: 16.452, Final Batch Loss: 0.436\n",
      "Epoch 11481, Loss: 16.723, Final Batch Loss: 0.471\n",
      "Epoch 11482, Loss: 16.557, Final Batch Loss: 0.438\n",
      "Epoch 11483, Loss: 16.541, Final Batch Loss: 0.495\n",
      "Epoch 11484, Loss: 16.622, Final Batch Loss: 0.557\n",
      "Epoch 11485, Loss: 16.974, Final Batch Loss: 0.515\n",
      "Epoch 11486, Loss: 16.661, Final Batch Loss: 0.368\n",
      "Epoch 11487, Loss: 16.560, Final Batch Loss: 0.466\n",
      "Epoch 11488, Loss: 16.415, Final Batch Loss: 0.401\n",
      "Epoch 11489, Loss: 16.324, Final Batch Loss: 0.493\n",
      "Epoch 11490, Loss: 16.637, Final Batch Loss: 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11491, Loss: 16.322, Final Batch Loss: 0.449\n",
      "Epoch 11492, Loss: 16.650, Final Batch Loss: 0.630\n",
      "Epoch 11493, Loss: 16.348, Final Batch Loss: 0.417\n",
      "Epoch 11494, Loss: 16.777, Final Batch Loss: 0.481\n",
      "Epoch 11495, Loss: 16.659, Final Batch Loss: 0.543\n",
      "Epoch 11496, Loss: 16.728, Final Batch Loss: 0.480\n",
      "Epoch 11497, Loss: 16.584, Final Batch Loss: 0.582\n",
      "Epoch 11498, Loss: 16.446, Final Batch Loss: 0.473\n",
      "Epoch 11499, Loss: 16.703, Final Batch Loss: 0.456\n",
      "Epoch 11500, Loss: 16.665, Final Batch Loss: 0.469\n",
      "Epoch 11501, Loss: 16.637, Final Batch Loss: 0.403\n",
      "Epoch 11502, Loss: 16.293, Final Batch Loss: 0.436\n",
      "Epoch 11503, Loss: 16.583, Final Batch Loss: 0.412\n",
      "Epoch 11504, Loss: 16.433, Final Batch Loss: 0.394\n",
      "Epoch 11505, Loss: 16.864, Final Batch Loss: 0.459\n",
      "Epoch 11506, Loss: 16.849, Final Batch Loss: 0.490\n",
      "Epoch 11507, Loss: 16.541, Final Batch Loss: 0.403\n",
      "Epoch 11508, Loss: 16.368, Final Batch Loss: 0.368\n",
      "Epoch 11509, Loss: 16.400, Final Batch Loss: 0.475\n",
      "Epoch 11510, Loss: 16.389, Final Batch Loss: 0.467\n",
      "Epoch 11511, Loss: 16.462, Final Batch Loss: 0.513\n",
      "Epoch 11512, Loss: 16.559, Final Batch Loss: 0.498\n",
      "Epoch 11513, Loss: 16.698, Final Batch Loss: 0.457\n",
      "Epoch 11514, Loss: 16.725, Final Batch Loss: 0.530\n",
      "Epoch 11515, Loss: 16.557, Final Batch Loss: 0.411\n",
      "Epoch 11516, Loss: 16.430, Final Batch Loss: 0.562\n",
      "Epoch 11517, Loss: 16.292, Final Batch Loss: 0.445\n",
      "Epoch 11518, Loss: 16.553, Final Batch Loss: 0.485\n",
      "Epoch 11519, Loss: 16.476, Final Batch Loss: 0.433\n",
      "Epoch 11520, Loss: 16.575, Final Batch Loss: 0.511\n",
      "Epoch 11521, Loss: 16.609, Final Batch Loss: 0.474\n",
      "Epoch 11522, Loss: 16.622, Final Batch Loss: 0.404\n",
      "Epoch 11523, Loss: 16.621, Final Batch Loss: 0.377\n",
      "Epoch 11524, Loss: 16.575, Final Batch Loss: 0.431\n",
      "Epoch 11525, Loss: 16.947, Final Batch Loss: 0.442\n",
      "Epoch 11526, Loss: 16.466, Final Batch Loss: 0.464\n",
      "Epoch 11527, Loss: 16.551, Final Batch Loss: 0.591\n",
      "Epoch 11528, Loss: 16.521, Final Batch Loss: 0.329\n",
      "Epoch 11529, Loss: 16.280, Final Batch Loss: 0.358\n",
      "Epoch 11530, Loss: 16.745, Final Batch Loss: 0.518\n",
      "Epoch 11531, Loss: 16.531, Final Batch Loss: 0.502\n",
      "Epoch 11532, Loss: 16.401, Final Batch Loss: 0.376\n",
      "Epoch 11533, Loss: 16.614, Final Batch Loss: 0.531\n",
      "Epoch 11534, Loss: 16.540, Final Batch Loss: 0.439\n",
      "Epoch 11535, Loss: 16.772, Final Batch Loss: 0.426\n",
      "Epoch 11536, Loss: 16.756, Final Batch Loss: 0.326\n",
      "Epoch 11537, Loss: 16.680, Final Batch Loss: 0.566\n",
      "Epoch 11538, Loss: 16.840, Final Batch Loss: 0.538\n",
      "Epoch 11539, Loss: 16.613, Final Batch Loss: 0.481\n",
      "Epoch 11540, Loss: 16.542, Final Batch Loss: 0.404\n",
      "Epoch 11541, Loss: 16.474, Final Batch Loss: 0.529\n",
      "Epoch 11542, Loss: 16.445, Final Batch Loss: 0.403\n",
      "Epoch 11543, Loss: 16.266, Final Batch Loss: 0.418\n",
      "Epoch 11544, Loss: 16.562, Final Batch Loss: 0.455\n",
      "Epoch 11545, Loss: 16.769, Final Batch Loss: 0.526\n",
      "Epoch 11546, Loss: 16.527, Final Batch Loss: 0.360\n",
      "Epoch 11547, Loss: 16.652, Final Batch Loss: 0.564\n",
      "Epoch 11548, Loss: 16.530, Final Batch Loss: 0.406\n",
      "Epoch 11549, Loss: 16.617, Final Batch Loss: 0.412\n",
      "Epoch 11550, Loss: 16.734, Final Batch Loss: 0.558\n",
      "Epoch 11551, Loss: 16.615, Final Batch Loss: 0.519\n",
      "Epoch 11552, Loss: 16.493, Final Batch Loss: 0.514\n",
      "Epoch 11553, Loss: 16.553, Final Batch Loss: 0.496\n",
      "Epoch 11554, Loss: 16.506, Final Batch Loss: 0.424\n",
      "Epoch 11555, Loss: 16.455, Final Batch Loss: 0.420\n",
      "Epoch 11556, Loss: 16.444, Final Batch Loss: 0.511\n",
      "Epoch 11557, Loss: 16.501, Final Batch Loss: 0.518\n",
      "Epoch 11558, Loss: 16.612, Final Batch Loss: 0.576\n",
      "Epoch 11559, Loss: 16.602, Final Batch Loss: 0.472\n",
      "Epoch 11560, Loss: 16.585, Final Batch Loss: 0.534\n",
      "Epoch 11561, Loss: 16.340, Final Batch Loss: 0.523\n",
      "Epoch 11562, Loss: 16.838, Final Batch Loss: 0.470\n",
      "Epoch 11563, Loss: 16.431, Final Batch Loss: 0.554\n",
      "Epoch 11564, Loss: 16.666, Final Batch Loss: 0.410\n",
      "Epoch 11565, Loss: 16.922, Final Batch Loss: 0.481\n",
      "Epoch 11566, Loss: 16.479, Final Batch Loss: 0.493\n",
      "Epoch 11567, Loss: 16.512, Final Batch Loss: 0.433\n",
      "Epoch 11568, Loss: 16.509, Final Batch Loss: 0.449\n",
      "Epoch 11569, Loss: 16.739, Final Batch Loss: 0.486\n",
      "Epoch 11570, Loss: 16.490, Final Batch Loss: 0.521\n",
      "Epoch 11571, Loss: 16.484, Final Batch Loss: 0.426\n",
      "Epoch 11572, Loss: 16.722, Final Batch Loss: 0.466\n",
      "Epoch 11573, Loss: 16.558, Final Batch Loss: 0.500\n",
      "Epoch 11574, Loss: 16.404, Final Batch Loss: 0.414\n",
      "Epoch 11575, Loss: 16.649, Final Batch Loss: 0.467\n",
      "Epoch 11576, Loss: 16.884, Final Batch Loss: 0.341\n",
      "Epoch 11577, Loss: 16.579, Final Batch Loss: 0.492\n",
      "Epoch 11578, Loss: 16.511, Final Batch Loss: 0.394\n",
      "Epoch 11579, Loss: 16.503, Final Batch Loss: 0.393\n",
      "Epoch 11580, Loss: 16.610, Final Batch Loss: 0.501\n",
      "Epoch 11581, Loss: 16.529, Final Batch Loss: 0.418\n",
      "Epoch 11582, Loss: 16.713, Final Batch Loss: 0.448\n",
      "Epoch 11583, Loss: 16.544, Final Batch Loss: 0.542\n",
      "Epoch 11584, Loss: 16.541, Final Batch Loss: 0.466\n",
      "Epoch 11585, Loss: 16.208, Final Batch Loss: 0.450\n",
      "Epoch 11586, Loss: 16.434, Final Batch Loss: 0.408\n",
      "Epoch 11587, Loss: 16.590, Final Batch Loss: 0.346\n",
      "Epoch 11588, Loss: 16.577, Final Batch Loss: 0.484\n",
      "Epoch 11589, Loss: 16.138, Final Batch Loss: 0.391\n",
      "Epoch 11590, Loss: 16.703, Final Batch Loss: 0.538\n",
      "Epoch 11591, Loss: 16.433, Final Batch Loss: 0.512\n",
      "Epoch 11592, Loss: 16.567, Final Batch Loss: 0.517\n",
      "Epoch 11593, Loss: 16.437, Final Batch Loss: 0.459\n",
      "Epoch 11594, Loss: 16.767, Final Batch Loss: 0.541\n",
      "Epoch 11595, Loss: 16.608, Final Batch Loss: 0.471\n",
      "Epoch 11596, Loss: 16.735, Final Batch Loss: 0.407\n",
      "Epoch 11597, Loss: 16.372, Final Batch Loss: 0.433\n",
      "Epoch 11598, Loss: 16.474, Final Batch Loss: 0.384\n",
      "Epoch 11599, Loss: 16.688, Final Batch Loss: 0.407\n",
      "Epoch 11600, Loss: 16.423, Final Batch Loss: 0.437\n",
      "Epoch 11601, Loss: 16.646, Final Batch Loss: 0.554\n",
      "Epoch 11602, Loss: 16.615, Final Batch Loss: 0.505\n",
      "Epoch 11603, Loss: 16.625, Final Batch Loss: 0.353\n",
      "Epoch 11604, Loss: 16.766, Final Batch Loss: 0.504\n",
      "Epoch 11605, Loss: 16.424, Final Batch Loss: 0.408\n",
      "Epoch 11606, Loss: 16.769, Final Batch Loss: 0.447\n",
      "Epoch 11607, Loss: 16.482, Final Batch Loss: 0.444\n",
      "Epoch 11608, Loss: 16.470, Final Batch Loss: 0.455\n",
      "Epoch 11609, Loss: 16.481, Final Batch Loss: 0.404\n",
      "Epoch 11610, Loss: 16.488, Final Batch Loss: 0.540\n",
      "Epoch 11611, Loss: 16.374, Final Batch Loss: 0.461\n",
      "Epoch 11612, Loss: 16.357, Final Batch Loss: 0.474\n",
      "Epoch 11613, Loss: 16.779, Final Batch Loss: 0.467\n",
      "Epoch 11614, Loss: 16.549, Final Batch Loss: 0.433\n",
      "Epoch 11615, Loss: 16.690, Final Batch Loss: 0.448\n",
      "Epoch 11616, Loss: 16.568, Final Batch Loss: 0.535\n",
      "Epoch 11617, Loss: 16.558, Final Batch Loss: 0.442\n",
      "Epoch 11618, Loss: 16.934, Final Batch Loss: 0.515\n",
      "Epoch 11619, Loss: 16.613, Final Batch Loss: 0.425\n",
      "Epoch 11620, Loss: 16.656, Final Batch Loss: 0.415\n",
      "Epoch 11621, Loss: 16.719, Final Batch Loss: 0.582\n",
      "Epoch 11622, Loss: 16.512, Final Batch Loss: 0.456\n",
      "Epoch 11623, Loss: 16.524, Final Batch Loss: 0.409\n",
      "Epoch 11624, Loss: 16.770, Final Batch Loss: 0.432\n",
      "Epoch 11625, Loss: 16.412, Final Batch Loss: 0.466\n",
      "Epoch 11626, Loss: 16.164, Final Batch Loss: 0.334\n",
      "Epoch 11627, Loss: 16.431, Final Batch Loss: 0.442\n",
      "Epoch 11628, Loss: 16.612, Final Batch Loss: 0.486\n",
      "Epoch 11629, Loss: 16.466, Final Batch Loss: 0.402\n",
      "Epoch 11630, Loss: 16.503, Final Batch Loss: 0.489\n",
      "Epoch 11631, Loss: 16.590, Final Batch Loss: 0.405\n",
      "Epoch 11632, Loss: 16.561, Final Batch Loss: 0.500\n",
      "Epoch 11633, Loss: 16.373, Final Batch Loss: 0.429\n",
      "Epoch 11634, Loss: 16.446, Final Batch Loss: 0.407\n",
      "Epoch 11635, Loss: 16.648, Final Batch Loss: 0.453\n",
      "Epoch 11636, Loss: 16.574, Final Batch Loss: 0.381\n",
      "Epoch 11637, Loss: 16.735, Final Batch Loss: 0.460\n",
      "Epoch 11638, Loss: 16.335, Final Batch Loss: 0.450\n",
      "Epoch 11639, Loss: 16.680, Final Batch Loss: 0.391\n",
      "Epoch 11640, Loss: 16.614, Final Batch Loss: 0.419\n",
      "Epoch 11641, Loss: 16.263, Final Batch Loss: 0.451\n",
      "Epoch 11642, Loss: 16.374, Final Batch Loss: 0.497\n",
      "Epoch 11643, Loss: 16.562, Final Batch Loss: 0.608\n",
      "Epoch 11644, Loss: 16.716, Final Batch Loss: 0.404\n",
      "Epoch 11645, Loss: 16.500, Final Batch Loss: 0.482\n",
      "Epoch 11646, Loss: 16.702, Final Batch Loss: 0.444\n",
      "Epoch 11647, Loss: 16.636, Final Batch Loss: 0.429\n",
      "Epoch 11648, Loss: 16.712, Final Batch Loss: 0.489\n",
      "Epoch 11649, Loss: 16.310, Final Batch Loss: 0.406\n",
      "Epoch 11650, Loss: 16.469, Final Batch Loss: 0.392\n",
      "Epoch 11651, Loss: 16.537, Final Batch Loss: 0.543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11652, Loss: 16.378, Final Batch Loss: 0.583\n",
      "Epoch 11653, Loss: 16.417, Final Batch Loss: 0.436\n",
      "Epoch 11654, Loss: 16.361, Final Batch Loss: 0.490\n",
      "Epoch 11655, Loss: 16.727, Final Batch Loss: 0.436\n",
      "Epoch 11656, Loss: 16.562, Final Batch Loss: 0.328\n",
      "Epoch 11657, Loss: 16.611, Final Batch Loss: 0.512\n",
      "Epoch 11658, Loss: 16.261, Final Batch Loss: 0.423\n",
      "Epoch 11659, Loss: 16.510, Final Batch Loss: 0.484\n",
      "Epoch 11660, Loss: 16.397, Final Batch Loss: 0.425\n",
      "Epoch 11661, Loss: 16.571, Final Batch Loss: 0.552\n",
      "Epoch 11662, Loss: 16.834, Final Batch Loss: 0.500\n",
      "Epoch 11663, Loss: 16.778, Final Batch Loss: 0.481\n",
      "Epoch 11664, Loss: 16.535, Final Batch Loss: 0.327\n",
      "Epoch 11665, Loss: 16.585, Final Batch Loss: 0.424\n",
      "Epoch 11666, Loss: 16.600, Final Batch Loss: 0.382\n",
      "Epoch 11667, Loss: 16.592, Final Batch Loss: 0.388\n",
      "Epoch 11668, Loss: 16.772, Final Batch Loss: 0.479\n",
      "Epoch 11669, Loss: 16.732, Final Batch Loss: 0.445\n",
      "Epoch 11670, Loss: 16.579, Final Batch Loss: 0.587\n",
      "Epoch 11671, Loss: 16.393, Final Batch Loss: 0.435\n",
      "Epoch 11672, Loss: 16.822, Final Batch Loss: 0.591\n",
      "Epoch 11673, Loss: 16.651, Final Batch Loss: 0.459\n",
      "Epoch 11674, Loss: 16.433, Final Batch Loss: 0.434\n",
      "Epoch 11675, Loss: 16.493, Final Batch Loss: 0.551\n",
      "Epoch 11676, Loss: 16.478, Final Batch Loss: 0.441\n",
      "Epoch 11677, Loss: 16.365, Final Batch Loss: 0.498\n",
      "Epoch 11678, Loss: 16.563, Final Batch Loss: 0.442\n",
      "Epoch 11679, Loss: 16.637, Final Batch Loss: 0.508\n",
      "Epoch 11680, Loss: 16.678, Final Batch Loss: 0.510\n",
      "Epoch 11681, Loss: 16.307, Final Batch Loss: 0.518\n",
      "Epoch 11682, Loss: 16.749, Final Batch Loss: 0.453\n",
      "Epoch 11683, Loss: 16.679, Final Batch Loss: 0.429\n",
      "Epoch 11684, Loss: 16.761, Final Batch Loss: 0.474\n",
      "Epoch 11685, Loss: 16.406, Final Batch Loss: 0.490\n",
      "Epoch 11686, Loss: 16.812, Final Batch Loss: 0.438\n",
      "Epoch 11687, Loss: 16.691, Final Batch Loss: 0.551\n",
      "Epoch 11688, Loss: 16.853, Final Batch Loss: 0.515\n",
      "Epoch 11689, Loss: 16.673, Final Batch Loss: 0.458\n",
      "Epoch 11690, Loss: 16.588, Final Batch Loss: 0.429\n",
      "Epoch 11691, Loss: 16.577, Final Batch Loss: 0.360\n",
      "Epoch 11692, Loss: 16.335, Final Batch Loss: 0.461\n",
      "Epoch 11693, Loss: 16.563, Final Batch Loss: 0.418\n",
      "Epoch 11694, Loss: 16.656, Final Batch Loss: 0.546\n",
      "Epoch 11695, Loss: 16.358, Final Batch Loss: 0.366\n",
      "Epoch 11696, Loss: 16.651, Final Batch Loss: 0.365\n",
      "Epoch 11697, Loss: 16.734, Final Batch Loss: 0.488\n",
      "Epoch 11698, Loss: 16.651, Final Batch Loss: 0.583\n",
      "Epoch 11699, Loss: 16.547, Final Batch Loss: 0.404\n",
      "Epoch 11700, Loss: 16.346, Final Batch Loss: 0.385\n",
      "Epoch 11701, Loss: 16.741, Final Batch Loss: 0.353\n",
      "Epoch 11702, Loss: 16.691, Final Batch Loss: 0.374\n",
      "Epoch 11703, Loss: 16.625, Final Batch Loss: 0.525\n",
      "Epoch 11704, Loss: 16.691, Final Batch Loss: 0.527\n",
      "Epoch 11705, Loss: 16.572, Final Batch Loss: 0.513\n",
      "Epoch 11706, Loss: 16.351, Final Batch Loss: 0.428\n",
      "Epoch 11707, Loss: 16.812, Final Batch Loss: 0.612\n",
      "Epoch 11708, Loss: 16.316, Final Batch Loss: 0.444\n",
      "Epoch 11709, Loss: 16.562, Final Batch Loss: 0.413\n",
      "Epoch 11710, Loss: 16.503, Final Batch Loss: 0.658\n",
      "Epoch 11711, Loss: 16.402, Final Batch Loss: 0.465\n",
      "Epoch 11712, Loss: 16.675, Final Batch Loss: 0.320\n",
      "Epoch 11713, Loss: 16.715, Final Batch Loss: 0.346\n",
      "Epoch 11714, Loss: 16.604, Final Batch Loss: 0.449\n",
      "Epoch 11715, Loss: 16.643, Final Batch Loss: 0.373\n",
      "Epoch 11716, Loss: 16.795, Final Batch Loss: 0.461\n",
      "Epoch 11717, Loss: 16.369, Final Batch Loss: 0.449\n",
      "Epoch 11718, Loss: 16.458, Final Batch Loss: 0.463\n",
      "Epoch 11719, Loss: 16.672, Final Batch Loss: 0.457\n",
      "Epoch 11720, Loss: 16.607, Final Batch Loss: 0.442\n",
      "Epoch 11721, Loss: 17.027, Final Batch Loss: 0.496\n",
      "Epoch 11722, Loss: 16.284, Final Batch Loss: 0.419\n",
      "Epoch 11723, Loss: 16.601, Final Batch Loss: 0.516\n",
      "Epoch 11724, Loss: 16.549, Final Batch Loss: 0.501\n",
      "Epoch 11725, Loss: 16.291, Final Batch Loss: 0.407\n",
      "Epoch 11726, Loss: 16.404, Final Batch Loss: 0.426\n",
      "Epoch 11727, Loss: 16.425, Final Batch Loss: 0.419\n",
      "Epoch 11728, Loss: 16.683, Final Batch Loss: 0.464\n",
      "Epoch 11729, Loss: 16.714, Final Batch Loss: 0.360\n",
      "Epoch 11730, Loss: 16.611, Final Batch Loss: 0.488\n",
      "Epoch 11731, Loss: 16.250, Final Batch Loss: 0.501\n",
      "Epoch 11732, Loss: 16.613, Final Batch Loss: 0.380\n",
      "Epoch 11733, Loss: 16.406, Final Batch Loss: 0.561\n",
      "Epoch 11734, Loss: 16.510, Final Batch Loss: 0.448\n",
      "Epoch 11735, Loss: 16.640, Final Batch Loss: 0.368\n",
      "Epoch 11736, Loss: 16.435, Final Batch Loss: 0.444\n",
      "Epoch 11737, Loss: 16.482, Final Batch Loss: 0.449\n",
      "Epoch 11738, Loss: 16.462, Final Batch Loss: 0.408\n",
      "Epoch 11739, Loss: 16.644, Final Batch Loss: 0.489\n",
      "Epoch 11740, Loss: 16.544, Final Batch Loss: 0.644\n",
      "Epoch 11741, Loss: 16.413, Final Batch Loss: 0.500\n",
      "Epoch 11742, Loss: 16.718, Final Batch Loss: 0.439\n",
      "Epoch 11743, Loss: 16.570, Final Batch Loss: 0.454\n",
      "Epoch 11744, Loss: 16.665, Final Batch Loss: 0.564\n",
      "Epoch 11745, Loss: 16.687, Final Batch Loss: 0.483\n",
      "Epoch 11746, Loss: 16.848, Final Batch Loss: 0.456\n",
      "Epoch 11747, Loss: 16.511, Final Batch Loss: 0.531\n",
      "Epoch 11748, Loss: 16.777, Final Batch Loss: 0.514\n",
      "Epoch 11749, Loss: 16.502, Final Batch Loss: 0.572\n",
      "Epoch 11750, Loss: 16.396, Final Batch Loss: 0.518\n",
      "Epoch 11751, Loss: 16.919, Final Batch Loss: 0.585\n",
      "Epoch 11752, Loss: 16.659, Final Batch Loss: 0.551\n",
      "Epoch 11753, Loss: 16.500, Final Batch Loss: 0.462\n",
      "Epoch 11754, Loss: 16.237, Final Batch Loss: 0.494\n",
      "Epoch 11755, Loss: 16.723, Final Batch Loss: 0.416\n",
      "Epoch 11756, Loss: 16.337, Final Batch Loss: 0.546\n",
      "Epoch 11757, Loss: 16.408, Final Batch Loss: 0.497\n",
      "Epoch 11758, Loss: 16.397, Final Batch Loss: 0.465\n",
      "Epoch 11759, Loss: 16.634, Final Batch Loss: 0.459\n",
      "Epoch 11760, Loss: 16.561, Final Batch Loss: 0.472\n",
      "Epoch 11761, Loss: 16.355, Final Batch Loss: 0.339\n",
      "Epoch 11762, Loss: 16.350, Final Batch Loss: 0.399\n",
      "Epoch 11763, Loss: 16.364, Final Batch Loss: 0.411\n",
      "Epoch 11764, Loss: 16.491, Final Batch Loss: 0.518\n",
      "Epoch 11765, Loss: 16.590, Final Batch Loss: 0.454\n",
      "Epoch 11766, Loss: 16.582, Final Batch Loss: 0.349\n",
      "Epoch 11767, Loss: 16.431, Final Batch Loss: 0.439\n",
      "Epoch 11768, Loss: 16.277, Final Batch Loss: 0.418\n",
      "Epoch 11769, Loss: 16.419, Final Batch Loss: 0.339\n",
      "Epoch 11770, Loss: 16.525, Final Batch Loss: 0.583\n",
      "Epoch 11771, Loss: 16.301, Final Batch Loss: 0.359\n",
      "Epoch 11772, Loss: 16.359, Final Batch Loss: 0.453\n",
      "Epoch 11773, Loss: 16.774, Final Batch Loss: 0.442\n",
      "Epoch 11774, Loss: 16.266, Final Batch Loss: 0.554\n",
      "Epoch 11775, Loss: 16.690, Final Batch Loss: 0.497\n",
      "Epoch 11776, Loss: 16.681, Final Batch Loss: 0.428\n",
      "Epoch 11777, Loss: 16.501, Final Batch Loss: 0.484\n",
      "Epoch 11778, Loss: 16.550, Final Batch Loss: 0.408\n",
      "Epoch 11779, Loss: 16.565, Final Batch Loss: 0.503\n",
      "Epoch 11780, Loss: 16.369, Final Batch Loss: 0.426\n",
      "Epoch 11781, Loss: 16.464, Final Batch Loss: 0.285\n",
      "Epoch 11782, Loss: 16.674, Final Batch Loss: 0.511\n",
      "Epoch 11783, Loss: 16.747, Final Batch Loss: 0.425\n",
      "Epoch 11784, Loss: 16.614, Final Batch Loss: 0.555\n",
      "Epoch 11785, Loss: 16.447, Final Batch Loss: 0.364\n",
      "Epoch 11786, Loss: 16.635, Final Batch Loss: 0.460\n",
      "Epoch 11787, Loss: 16.692, Final Batch Loss: 0.390\n",
      "Epoch 11788, Loss: 16.418, Final Batch Loss: 0.470\n",
      "Epoch 11789, Loss: 16.681, Final Batch Loss: 0.445\n",
      "Epoch 11790, Loss: 16.347, Final Batch Loss: 0.434\n",
      "Epoch 11791, Loss: 16.530, Final Batch Loss: 0.498\n",
      "Epoch 11792, Loss: 16.749, Final Batch Loss: 0.401\n",
      "Epoch 11793, Loss: 16.593, Final Batch Loss: 0.537\n",
      "Epoch 11794, Loss: 16.664, Final Batch Loss: 0.452\n",
      "Epoch 11795, Loss: 16.579, Final Batch Loss: 0.521\n",
      "Epoch 11796, Loss: 16.428, Final Batch Loss: 0.474\n",
      "Epoch 11797, Loss: 16.566, Final Batch Loss: 0.455\n",
      "Epoch 11798, Loss: 16.660, Final Batch Loss: 0.458\n",
      "Epoch 11799, Loss: 16.697, Final Batch Loss: 0.477\n",
      "Epoch 11800, Loss: 16.723, Final Batch Loss: 0.415\n",
      "Epoch 11801, Loss: 16.897, Final Batch Loss: 0.438\n",
      "Epoch 11802, Loss: 16.692, Final Batch Loss: 0.528\n",
      "Epoch 11803, Loss: 16.926, Final Batch Loss: 0.507\n",
      "Epoch 11804, Loss: 16.517, Final Batch Loss: 0.401\n",
      "Epoch 11805, Loss: 16.663, Final Batch Loss: 0.510\n",
      "Epoch 11806, Loss: 16.466, Final Batch Loss: 0.421\n",
      "Epoch 11807, Loss: 16.677, Final Batch Loss: 0.507\n",
      "Epoch 11808, Loss: 16.348, Final Batch Loss: 0.471\n",
      "Epoch 11809, Loss: 16.558, Final Batch Loss: 0.497\n",
      "Epoch 11810, Loss: 16.561, Final Batch Loss: 0.519\n",
      "Epoch 11811, Loss: 16.571, Final Batch Loss: 0.548\n",
      "Epoch 11812, Loss: 16.767, Final Batch Loss: 0.435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11813, Loss: 16.419, Final Batch Loss: 0.434\n",
      "Epoch 11814, Loss: 16.410, Final Batch Loss: 0.416\n",
      "Epoch 11815, Loss: 16.688, Final Batch Loss: 0.545\n",
      "Epoch 11816, Loss: 16.628, Final Batch Loss: 0.428\n",
      "Epoch 11817, Loss: 16.473, Final Batch Loss: 0.504\n",
      "Epoch 11818, Loss: 16.272, Final Batch Loss: 0.409\n",
      "Epoch 11819, Loss: 16.729, Final Batch Loss: 0.478\n",
      "Epoch 11820, Loss: 16.592, Final Batch Loss: 0.443\n",
      "Epoch 11821, Loss: 16.447, Final Batch Loss: 0.484\n",
      "Epoch 11822, Loss: 16.661, Final Batch Loss: 0.503\n",
      "Epoch 11823, Loss: 16.445, Final Batch Loss: 0.448\n",
      "Epoch 11824, Loss: 16.503, Final Batch Loss: 0.436\n",
      "Epoch 11825, Loss: 16.605, Final Batch Loss: 0.517\n",
      "Epoch 11826, Loss: 16.632, Final Batch Loss: 0.558\n",
      "Epoch 11827, Loss: 16.487, Final Batch Loss: 0.491\n",
      "Epoch 11828, Loss: 16.738, Final Batch Loss: 0.542\n",
      "Epoch 11829, Loss: 16.560, Final Batch Loss: 0.481\n",
      "Epoch 11830, Loss: 16.545, Final Batch Loss: 0.307\n",
      "Epoch 11831, Loss: 16.672, Final Batch Loss: 0.449\n",
      "Epoch 11832, Loss: 16.897, Final Batch Loss: 0.447\n",
      "Epoch 11833, Loss: 16.528, Final Batch Loss: 0.381\n",
      "Epoch 11834, Loss: 16.414, Final Batch Loss: 0.340\n",
      "Epoch 11835, Loss: 16.741, Final Batch Loss: 0.528\n",
      "Epoch 11836, Loss: 16.380, Final Batch Loss: 0.370\n",
      "Epoch 11837, Loss: 16.448, Final Batch Loss: 0.395\n",
      "Epoch 11838, Loss: 16.801, Final Batch Loss: 0.349\n",
      "Epoch 11839, Loss: 16.449, Final Batch Loss: 0.432\n",
      "Epoch 11840, Loss: 16.595, Final Batch Loss: 0.472\n",
      "Epoch 11841, Loss: 16.854, Final Batch Loss: 0.523\n",
      "Epoch 11842, Loss: 16.416, Final Batch Loss: 0.435\n",
      "Epoch 11843, Loss: 16.571, Final Batch Loss: 0.527\n",
      "Epoch 11844, Loss: 16.709, Final Batch Loss: 0.439\n",
      "Epoch 11845, Loss: 16.532, Final Batch Loss: 0.449\n",
      "Epoch 11846, Loss: 16.377, Final Batch Loss: 0.520\n",
      "Epoch 11847, Loss: 16.298, Final Batch Loss: 0.361\n",
      "Epoch 11848, Loss: 16.774, Final Batch Loss: 0.517\n",
      "Epoch 11849, Loss: 16.642, Final Batch Loss: 0.486\n",
      "Epoch 11850, Loss: 16.804, Final Batch Loss: 0.532\n",
      "Epoch 11851, Loss: 16.410, Final Batch Loss: 0.380\n",
      "Epoch 11852, Loss: 16.511, Final Batch Loss: 0.450\n",
      "Epoch 11853, Loss: 16.613, Final Batch Loss: 0.448\n",
      "Epoch 11854, Loss: 16.355, Final Batch Loss: 0.389\n",
      "Epoch 11855, Loss: 16.393, Final Batch Loss: 0.382\n",
      "Epoch 11856, Loss: 16.616, Final Batch Loss: 0.471\n",
      "Epoch 11857, Loss: 16.546, Final Batch Loss: 0.445\n",
      "Epoch 11858, Loss: 16.576, Final Batch Loss: 0.473\n",
      "Epoch 11859, Loss: 16.834, Final Batch Loss: 0.441\n",
      "Epoch 11860, Loss: 16.414, Final Batch Loss: 0.454\n",
      "Epoch 11861, Loss: 16.415, Final Batch Loss: 0.448\n",
      "Epoch 11862, Loss: 16.468, Final Batch Loss: 0.402\n",
      "Epoch 11863, Loss: 16.664, Final Batch Loss: 0.473\n",
      "Epoch 11864, Loss: 16.821, Final Batch Loss: 0.446\n",
      "Epoch 11865, Loss: 16.512, Final Batch Loss: 0.492\n",
      "Epoch 11866, Loss: 16.417, Final Batch Loss: 0.616\n",
      "Epoch 11867, Loss: 16.515, Final Batch Loss: 0.472\n",
      "Epoch 11868, Loss: 16.423, Final Batch Loss: 0.431\n",
      "Epoch 11869, Loss: 16.703, Final Batch Loss: 0.429\n",
      "Epoch 11870, Loss: 16.400, Final Batch Loss: 0.369\n",
      "Epoch 11871, Loss: 16.601, Final Batch Loss: 0.479\n",
      "Epoch 11872, Loss: 16.656, Final Batch Loss: 0.544\n",
      "Epoch 11873, Loss: 16.513, Final Batch Loss: 0.472\n",
      "Epoch 11874, Loss: 16.883, Final Batch Loss: 0.501\n",
      "Epoch 11875, Loss: 16.303, Final Batch Loss: 0.380\n",
      "Epoch 11876, Loss: 16.502, Final Batch Loss: 0.464\n",
      "Epoch 11877, Loss: 16.275, Final Batch Loss: 0.366\n",
      "Epoch 11878, Loss: 16.523, Final Batch Loss: 0.484\n",
      "Epoch 11879, Loss: 16.416, Final Batch Loss: 0.343\n",
      "Epoch 11880, Loss: 16.405, Final Batch Loss: 0.388\n",
      "Epoch 11881, Loss: 16.771, Final Batch Loss: 0.492\n",
      "Epoch 11882, Loss: 16.694, Final Batch Loss: 0.468\n",
      "Epoch 11883, Loss: 16.774, Final Batch Loss: 0.533\n",
      "Epoch 11884, Loss: 16.682, Final Batch Loss: 0.458\n",
      "Epoch 11885, Loss: 16.754, Final Batch Loss: 0.419\n",
      "Epoch 11886, Loss: 16.591, Final Batch Loss: 0.501\n",
      "Epoch 11887, Loss: 16.658, Final Batch Loss: 0.431\n",
      "Epoch 11888, Loss: 16.432, Final Batch Loss: 0.546\n",
      "Epoch 11889, Loss: 16.622, Final Batch Loss: 0.493\n",
      "Epoch 11890, Loss: 16.760, Final Batch Loss: 0.528\n",
      "Epoch 11891, Loss: 16.750, Final Batch Loss: 0.542\n",
      "Epoch 11892, Loss: 16.567, Final Batch Loss: 0.506\n",
      "Epoch 11893, Loss: 16.562, Final Batch Loss: 0.362\n",
      "Epoch 11894, Loss: 16.711, Final Batch Loss: 0.420\n",
      "Epoch 11895, Loss: 16.519, Final Batch Loss: 0.509\n",
      "Epoch 11896, Loss: 16.593, Final Batch Loss: 0.439\n",
      "Epoch 11897, Loss: 16.646, Final Batch Loss: 0.530\n",
      "Epoch 11898, Loss: 16.692, Final Batch Loss: 0.426\n",
      "Epoch 11899, Loss: 16.354, Final Batch Loss: 0.281\n",
      "Epoch 11900, Loss: 16.683, Final Batch Loss: 0.404\n",
      "Epoch 11901, Loss: 16.582, Final Batch Loss: 0.437\n",
      "Epoch 11902, Loss: 16.678, Final Batch Loss: 0.423\n",
      "Epoch 11903, Loss: 16.513, Final Batch Loss: 0.536\n",
      "Epoch 11904, Loss: 16.893, Final Batch Loss: 0.438\n",
      "Epoch 11905, Loss: 16.688, Final Batch Loss: 0.466\n",
      "Epoch 11906, Loss: 16.644, Final Batch Loss: 0.519\n",
      "Epoch 11907, Loss: 16.477, Final Batch Loss: 0.487\n",
      "Epoch 11908, Loss: 16.508, Final Batch Loss: 0.513\n",
      "Epoch 11909, Loss: 16.254, Final Batch Loss: 0.482\n",
      "Epoch 11910, Loss: 16.464, Final Batch Loss: 0.490\n",
      "Epoch 11911, Loss: 16.353, Final Batch Loss: 0.412\n",
      "Epoch 11912, Loss: 16.577, Final Batch Loss: 0.425\n",
      "Epoch 11913, Loss: 16.741, Final Batch Loss: 0.445\n",
      "Epoch 11914, Loss: 16.485, Final Batch Loss: 0.457\n",
      "Epoch 11915, Loss: 16.644, Final Batch Loss: 0.505\n",
      "Epoch 11916, Loss: 16.840, Final Batch Loss: 0.445\n",
      "Epoch 11917, Loss: 16.533, Final Batch Loss: 0.516\n",
      "Epoch 11918, Loss: 16.206, Final Batch Loss: 0.383\n",
      "Epoch 11919, Loss: 16.680, Final Batch Loss: 0.442\n",
      "Epoch 11920, Loss: 16.478, Final Batch Loss: 0.460\n",
      "Epoch 11921, Loss: 16.614, Final Batch Loss: 0.374\n",
      "Epoch 11922, Loss: 16.743, Final Batch Loss: 0.524\n",
      "Epoch 11923, Loss: 16.684, Final Batch Loss: 0.561\n",
      "Epoch 11924, Loss: 16.683, Final Batch Loss: 0.443\n",
      "Epoch 11925, Loss: 16.549, Final Batch Loss: 0.569\n",
      "Epoch 11926, Loss: 16.774, Final Batch Loss: 0.522\n",
      "Epoch 11927, Loss: 16.479, Final Batch Loss: 0.400\n",
      "Epoch 11928, Loss: 16.550, Final Batch Loss: 0.356\n",
      "Epoch 11929, Loss: 16.200, Final Batch Loss: 0.426\n",
      "Epoch 11930, Loss: 16.593, Final Batch Loss: 0.387\n",
      "Epoch 11931, Loss: 16.329, Final Batch Loss: 0.449\n",
      "Epoch 11932, Loss: 16.726, Final Batch Loss: 0.519\n",
      "Epoch 11933, Loss: 16.381, Final Batch Loss: 0.482\n",
      "Epoch 11934, Loss: 16.594, Final Batch Loss: 0.596\n",
      "Epoch 11935, Loss: 16.252, Final Batch Loss: 0.385\n",
      "Epoch 11936, Loss: 16.419, Final Batch Loss: 0.515\n",
      "Epoch 11937, Loss: 16.875, Final Batch Loss: 0.456\n",
      "Epoch 11938, Loss: 16.682, Final Batch Loss: 0.391\n",
      "Epoch 11939, Loss: 16.376, Final Batch Loss: 0.427\n",
      "Epoch 11940, Loss: 16.585, Final Batch Loss: 0.395\n",
      "Epoch 11941, Loss: 16.580, Final Batch Loss: 0.414\n",
      "Epoch 11942, Loss: 16.460, Final Batch Loss: 0.363\n",
      "Epoch 11943, Loss: 16.422, Final Batch Loss: 0.370\n",
      "Epoch 11944, Loss: 16.373, Final Batch Loss: 0.438\n",
      "Epoch 11945, Loss: 16.395, Final Batch Loss: 0.537\n",
      "Epoch 11946, Loss: 16.489, Final Batch Loss: 0.477\n",
      "Epoch 11947, Loss: 16.656, Final Batch Loss: 0.488\n",
      "Epoch 11948, Loss: 16.493, Final Batch Loss: 0.354\n",
      "Epoch 11949, Loss: 16.388, Final Batch Loss: 0.522\n",
      "Epoch 11950, Loss: 16.920, Final Batch Loss: 0.440\n",
      "Epoch 11951, Loss: 16.711, Final Batch Loss: 0.455\n",
      "Epoch 11952, Loss: 16.664, Final Batch Loss: 0.404\n",
      "Epoch 11953, Loss: 16.612, Final Batch Loss: 0.425\n",
      "Epoch 11954, Loss: 16.383, Final Batch Loss: 0.576\n",
      "Epoch 11955, Loss: 16.315, Final Batch Loss: 0.366\n",
      "Epoch 11956, Loss: 16.468, Final Batch Loss: 0.454\n",
      "Epoch 11957, Loss: 16.381, Final Batch Loss: 0.388\n",
      "Epoch 11958, Loss: 16.591, Final Batch Loss: 0.584\n",
      "Epoch 11959, Loss: 16.805, Final Batch Loss: 0.527\n",
      "Epoch 11960, Loss: 16.350, Final Batch Loss: 0.440\n",
      "Epoch 11961, Loss: 16.563, Final Batch Loss: 0.568\n",
      "Epoch 11962, Loss: 16.692, Final Batch Loss: 0.495\n",
      "Epoch 11963, Loss: 16.587, Final Batch Loss: 0.550\n",
      "Epoch 11964, Loss: 16.421, Final Batch Loss: 0.442\n",
      "Epoch 11965, Loss: 16.337, Final Batch Loss: 0.446\n",
      "Epoch 11966, Loss: 16.728, Final Batch Loss: 0.451\n",
      "Epoch 11967, Loss: 16.495, Final Batch Loss: 0.484\n",
      "Epoch 11968, Loss: 16.319, Final Batch Loss: 0.461\n",
      "Epoch 11969, Loss: 16.646, Final Batch Loss: 0.511\n",
      "Epoch 11970, Loss: 16.125, Final Batch Loss: 0.439\n",
      "Epoch 11971, Loss: 16.574, Final Batch Loss: 0.483\n",
      "Epoch 11972, Loss: 16.463, Final Batch Loss: 0.383\n",
      "Epoch 11973, Loss: 16.266, Final Batch Loss: 0.440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11974, Loss: 16.633, Final Batch Loss: 0.474\n",
      "Epoch 11975, Loss: 16.480, Final Batch Loss: 0.403\n",
      "Epoch 11976, Loss: 16.241, Final Batch Loss: 0.457\n",
      "Epoch 11977, Loss: 16.461, Final Batch Loss: 0.386\n",
      "Epoch 11978, Loss: 16.330, Final Batch Loss: 0.426\n",
      "Epoch 11979, Loss: 16.556, Final Batch Loss: 0.485\n",
      "Epoch 11980, Loss: 16.560, Final Batch Loss: 0.468\n",
      "Epoch 11981, Loss: 16.492, Final Batch Loss: 0.442\n",
      "Epoch 11982, Loss: 16.615, Final Batch Loss: 0.450\n",
      "Epoch 11983, Loss: 16.676, Final Batch Loss: 0.548\n",
      "Epoch 11984, Loss: 16.754, Final Batch Loss: 0.435\n",
      "Epoch 11985, Loss: 16.478, Final Batch Loss: 0.415\n",
      "Epoch 11986, Loss: 16.528, Final Batch Loss: 0.480\n",
      "Epoch 11987, Loss: 16.497, Final Batch Loss: 0.420\n",
      "Epoch 11988, Loss: 16.243, Final Batch Loss: 0.397\n",
      "Epoch 11989, Loss: 16.520, Final Batch Loss: 0.483\n",
      "Epoch 11990, Loss: 16.560, Final Batch Loss: 0.448\n",
      "Epoch 11991, Loss: 16.493, Final Batch Loss: 0.352\n",
      "Epoch 11992, Loss: 16.598, Final Batch Loss: 0.430\n",
      "Epoch 11993, Loss: 16.635, Final Batch Loss: 0.478\n",
      "Epoch 11994, Loss: 16.566, Final Batch Loss: 0.488\n",
      "Epoch 11995, Loss: 16.404, Final Batch Loss: 0.366\n",
      "Epoch 11996, Loss: 16.741, Final Batch Loss: 0.486\n",
      "Epoch 11997, Loss: 16.315, Final Batch Loss: 0.497\n",
      "Epoch 11998, Loss: 16.793, Final Batch Loss: 0.484\n",
      "Epoch 11999, Loss: 16.569, Final Batch Loss: 0.531\n",
      "Epoch 12000, Loss: 16.735, Final Batch Loss: 0.412\n",
      "Epoch 12001, Loss: 16.634, Final Batch Loss: 0.516\n",
      "Epoch 12002, Loss: 16.587, Final Batch Loss: 0.491\n",
      "Epoch 12003, Loss: 16.352, Final Batch Loss: 0.556\n",
      "Epoch 12004, Loss: 16.540, Final Batch Loss: 0.491\n",
      "Epoch 12005, Loss: 16.453, Final Batch Loss: 0.429\n",
      "Epoch 12006, Loss: 16.506, Final Batch Loss: 0.466\n",
      "Epoch 12007, Loss: 16.773, Final Batch Loss: 0.538\n",
      "Epoch 12008, Loss: 16.331, Final Batch Loss: 0.345\n",
      "Epoch 12009, Loss: 16.520, Final Batch Loss: 0.368\n",
      "Epoch 12010, Loss: 16.665, Final Batch Loss: 0.531\n",
      "Epoch 12011, Loss: 16.378, Final Batch Loss: 0.543\n",
      "Epoch 12012, Loss: 16.335, Final Batch Loss: 0.477\n",
      "Epoch 12013, Loss: 16.247, Final Batch Loss: 0.401\n",
      "Epoch 12014, Loss: 16.627, Final Batch Loss: 0.384\n",
      "Epoch 12015, Loss: 16.856, Final Batch Loss: 0.501\n",
      "Epoch 12016, Loss: 16.620, Final Batch Loss: 0.526\n",
      "Epoch 12017, Loss: 16.723, Final Batch Loss: 0.469\n",
      "Epoch 12018, Loss: 16.628, Final Batch Loss: 0.525\n",
      "Epoch 12019, Loss: 16.866, Final Batch Loss: 0.571\n",
      "Epoch 12020, Loss: 16.473, Final Batch Loss: 0.443\n",
      "Epoch 12021, Loss: 16.448, Final Batch Loss: 0.385\n",
      "Epoch 12022, Loss: 16.530, Final Batch Loss: 0.388\n",
      "Epoch 12023, Loss: 16.660, Final Batch Loss: 0.400\n",
      "Epoch 12024, Loss: 16.637, Final Batch Loss: 0.547\n",
      "Epoch 12025, Loss: 16.462, Final Batch Loss: 0.409\n",
      "Epoch 12026, Loss: 16.360, Final Batch Loss: 0.416\n",
      "Epoch 12027, Loss: 16.219, Final Batch Loss: 0.447\n",
      "Epoch 12028, Loss: 16.460, Final Batch Loss: 0.451\n",
      "Epoch 12029, Loss: 16.680, Final Batch Loss: 0.511\n",
      "Epoch 12030, Loss: 16.236, Final Batch Loss: 0.486\n",
      "Epoch 12031, Loss: 16.674, Final Batch Loss: 0.625\n",
      "Epoch 12032, Loss: 16.518, Final Batch Loss: 0.368\n",
      "Epoch 12033, Loss: 16.409, Final Batch Loss: 0.374\n",
      "Epoch 12034, Loss: 16.592, Final Batch Loss: 0.474\n",
      "Epoch 12035, Loss: 16.613, Final Batch Loss: 0.497\n",
      "Epoch 12036, Loss: 16.721, Final Batch Loss: 0.452\n",
      "Epoch 12037, Loss: 16.398, Final Batch Loss: 0.503\n",
      "Epoch 12038, Loss: 16.544, Final Batch Loss: 0.472\n",
      "Epoch 12039, Loss: 16.587, Final Batch Loss: 0.406\n",
      "Epoch 12040, Loss: 16.497, Final Batch Loss: 0.416\n",
      "Epoch 12041, Loss: 16.489, Final Batch Loss: 0.501\n",
      "Epoch 12042, Loss: 16.496, Final Batch Loss: 0.405\n",
      "Epoch 12043, Loss: 16.278, Final Batch Loss: 0.457\n",
      "Epoch 12044, Loss: 16.263, Final Batch Loss: 0.424\n",
      "Epoch 12045, Loss: 16.375, Final Batch Loss: 0.408\n",
      "Epoch 12046, Loss: 16.275, Final Batch Loss: 0.605\n",
      "Epoch 12047, Loss: 16.706, Final Batch Loss: 0.455\n",
      "Epoch 12048, Loss: 16.424, Final Batch Loss: 0.496\n",
      "Epoch 12049, Loss: 16.542, Final Batch Loss: 0.534\n",
      "Epoch 12050, Loss: 16.844, Final Batch Loss: 0.479\n",
      "Epoch 12051, Loss: 16.697, Final Batch Loss: 0.572\n",
      "Epoch 12052, Loss: 16.588, Final Batch Loss: 0.383\n",
      "Epoch 12053, Loss: 16.558, Final Batch Loss: 0.514\n",
      "Epoch 12054, Loss: 16.373, Final Batch Loss: 0.453\n",
      "Epoch 12055, Loss: 16.436, Final Batch Loss: 0.443\n",
      "Epoch 12056, Loss: 16.664, Final Batch Loss: 0.475\n",
      "Epoch 12057, Loss: 16.421, Final Batch Loss: 0.396\n",
      "Epoch 12058, Loss: 16.359, Final Batch Loss: 0.515\n",
      "Epoch 12059, Loss: 16.440, Final Batch Loss: 0.386\n",
      "Epoch 12060, Loss: 16.362, Final Batch Loss: 0.359\n",
      "Epoch 12061, Loss: 16.662, Final Batch Loss: 0.459\n",
      "Epoch 12062, Loss: 16.712, Final Batch Loss: 0.529\n",
      "Epoch 12063, Loss: 16.393, Final Batch Loss: 0.438\n",
      "Epoch 12064, Loss: 16.240, Final Batch Loss: 0.397\n",
      "Epoch 12065, Loss: 16.679, Final Batch Loss: 0.447\n",
      "Epoch 12066, Loss: 16.510, Final Batch Loss: 0.398\n",
      "Epoch 12067, Loss: 16.568, Final Batch Loss: 0.522\n",
      "Epoch 12068, Loss: 16.463, Final Batch Loss: 0.542\n",
      "Epoch 12069, Loss: 16.547, Final Batch Loss: 0.515\n",
      "Epoch 12070, Loss: 16.634, Final Batch Loss: 0.521\n",
      "Epoch 12071, Loss: 16.217, Final Batch Loss: 0.395\n",
      "Epoch 12072, Loss: 16.325, Final Batch Loss: 0.448\n",
      "Epoch 12073, Loss: 16.537, Final Batch Loss: 0.354\n",
      "Epoch 12074, Loss: 16.710, Final Batch Loss: 0.420\n",
      "Epoch 12075, Loss: 16.529, Final Batch Loss: 0.400\n",
      "Epoch 12076, Loss: 16.813, Final Batch Loss: 0.372\n",
      "Epoch 12077, Loss: 16.313, Final Batch Loss: 0.443\n",
      "Epoch 12078, Loss: 16.724, Final Batch Loss: 0.502\n",
      "Epoch 12079, Loss: 16.329, Final Batch Loss: 0.610\n",
      "Epoch 12080, Loss: 16.659, Final Batch Loss: 0.483\n",
      "Epoch 12081, Loss: 16.449, Final Batch Loss: 0.506\n",
      "Epoch 12082, Loss: 16.874, Final Batch Loss: 0.441\n",
      "Epoch 12083, Loss: 16.373, Final Batch Loss: 0.518\n",
      "Epoch 12084, Loss: 16.870, Final Batch Loss: 0.445\n",
      "Epoch 12085, Loss: 16.604, Final Batch Loss: 0.534\n",
      "Epoch 12086, Loss: 16.584, Final Batch Loss: 0.464\n",
      "Epoch 12087, Loss: 16.421, Final Batch Loss: 0.448\n",
      "Epoch 12088, Loss: 16.410, Final Batch Loss: 0.470\n",
      "Epoch 12089, Loss: 16.422, Final Batch Loss: 0.396\n",
      "Epoch 12090, Loss: 16.611, Final Batch Loss: 0.551\n",
      "Epoch 12091, Loss: 16.575, Final Batch Loss: 0.484\n",
      "Epoch 12092, Loss: 16.465, Final Batch Loss: 0.465\n",
      "Epoch 12093, Loss: 16.464, Final Batch Loss: 0.385\n",
      "Epoch 12094, Loss: 16.401, Final Batch Loss: 0.480\n",
      "Epoch 12095, Loss: 16.854, Final Batch Loss: 0.456\n",
      "Epoch 12096, Loss: 16.705, Final Batch Loss: 0.476\n",
      "Epoch 12097, Loss: 16.688, Final Batch Loss: 0.451\n",
      "Epoch 12098, Loss: 16.286, Final Batch Loss: 0.342\n",
      "Epoch 12099, Loss: 16.647, Final Batch Loss: 0.481\n",
      "Epoch 12100, Loss: 16.605, Final Batch Loss: 0.428\n",
      "Epoch 12101, Loss: 16.376, Final Batch Loss: 0.428\n",
      "Epoch 12102, Loss: 16.664, Final Batch Loss: 0.458\n",
      "Epoch 12103, Loss: 16.461, Final Batch Loss: 0.422\n",
      "Epoch 12104, Loss: 16.532, Final Batch Loss: 0.383\n",
      "Epoch 12105, Loss: 16.721, Final Batch Loss: 0.561\n",
      "Epoch 12106, Loss: 16.855, Final Batch Loss: 0.424\n",
      "Epoch 12107, Loss: 16.704, Final Batch Loss: 0.443\n",
      "Epoch 12108, Loss: 16.369, Final Batch Loss: 0.387\n",
      "Epoch 12109, Loss: 16.600, Final Batch Loss: 0.447\n",
      "Epoch 12110, Loss: 16.479, Final Batch Loss: 0.380\n",
      "Epoch 12111, Loss: 16.575, Final Batch Loss: 0.551\n",
      "Epoch 12112, Loss: 16.786, Final Batch Loss: 0.518\n",
      "Epoch 12113, Loss: 16.257, Final Batch Loss: 0.364\n",
      "Epoch 12114, Loss: 16.359, Final Batch Loss: 0.345\n",
      "Epoch 12115, Loss: 16.409, Final Batch Loss: 0.398\n",
      "Epoch 12116, Loss: 16.404, Final Batch Loss: 0.478\n",
      "Epoch 12117, Loss: 16.253, Final Batch Loss: 0.318\n",
      "Epoch 12118, Loss: 16.571, Final Batch Loss: 0.450\n",
      "Epoch 12119, Loss: 16.870, Final Batch Loss: 0.538\n",
      "Epoch 12120, Loss: 16.748, Final Batch Loss: 0.383\n",
      "Epoch 12121, Loss: 16.465, Final Batch Loss: 0.507\n",
      "Epoch 12122, Loss: 16.402, Final Batch Loss: 0.390\n",
      "Epoch 12123, Loss: 16.376, Final Batch Loss: 0.384\n",
      "Epoch 12124, Loss: 16.483, Final Batch Loss: 0.455\n",
      "Epoch 12125, Loss: 16.579, Final Batch Loss: 0.464\n",
      "Epoch 12126, Loss: 16.614, Final Batch Loss: 0.507\n",
      "Epoch 12127, Loss: 16.567, Final Batch Loss: 0.590\n",
      "Epoch 12128, Loss: 16.597, Final Batch Loss: 0.415\n",
      "Epoch 12129, Loss: 16.646, Final Batch Loss: 0.451\n",
      "Epoch 12130, Loss: 16.653, Final Batch Loss: 0.625\n",
      "Epoch 12131, Loss: 16.290, Final Batch Loss: 0.440\n",
      "Epoch 12132, Loss: 16.383, Final Batch Loss: 0.431\n",
      "Epoch 12133, Loss: 16.780, Final Batch Loss: 0.657\n",
      "Epoch 12134, Loss: 16.787, Final Batch Loss: 0.488\n",
      "Epoch 12135, Loss: 16.350, Final Batch Loss: 0.346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12136, Loss: 16.544, Final Batch Loss: 0.456\n",
      "Epoch 12137, Loss: 16.931, Final Batch Loss: 0.461\n",
      "Epoch 12138, Loss: 16.675, Final Batch Loss: 0.462\n",
      "Epoch 12139, Loss: 16.477, Final Batch Loss: 0.447\n",
      "Epoch 12140, Loss: 16.410, Final Batch Loss: 0.466\n",
      "Epoch 12141, Loss: 16.330, Final Batch Loss: 0.559\n",
      "Epoch 12142, Loss: 16.472, Final Batch Loss: 0.505\n",
      "Epoch 12143, Loss: 16.640, Final Batch Loss: 0.508\n",
      "Epoch 12144, Loss: 16.459, Final Batch Loss: 0.390\n",
      "Epoch 12145, Loss: 16.610, Final Batch Loss: 0.452\n",
      "Epoch 12146, Loss: 16.381, Final Batch Loss: 0.431\n",
      "Epoch 12147, Loss: 16.430, Final Batch Loss: 0.402\n",
      "Epoch 12148, Loss: 16.504, Final Batch Loss: 0.344\n",
      "Epoch 12149, Loss: 16.726, Final Batch Loss: 0.445\n",
      "Epoch 12150, Loss: 16.413, Final Batch Loss: 0.441\n",
      "Epoch 12151, Loss: 16.371, Final Batch Loss: 0.456\n",
      "Epoch 12152, Loss: 16.488, Final Batch Loss: 0.459\n",
      "Epoch 12153, Loss: 16.453, Final Batch Loss: 0.420\n",
      "Epoch 12154, Loss: 16.644, Final Batch Loss: 0.470\n",
      "Epoch 12155, Loss: 16.575, Final Batch Loss: 0.443\n",
      "Epoch 12156, Loss: 16.535, Final Batch Loss: 0.422\n",
      "Epoch 12157, Loss: 16.501, Final Batch Loss: 0.442\n",
      "Epoch 12158, Loss: 16.461, Final Batch Loss: 0.407\n",
      "Epoch 12159, Loss: 16.629, Final Batch Loss: 0.488\n",
      "Epoch 12160, Loss: 16.514, Final Batch Loss: 0.483\n",
      "Epoch 12161, Loss: 16.409, Final Batch Loss: 0.492\n",
      "Epoch 12162, Loss: 16.415, Final Batch Loss: 0.435\n",
      "Epoch 12163, Loss: 16.586, Final Batch Loss: 0.496\n",
      "Epoch 12164, Loss: 16.674, Final Batch Loss: 0.402\n",
      "Epoch 12165, Loss: 16.538, Final Batch Loss: 0.609\n",
      "Epoch 12166, Loss: 16.458, Final Batch Loss: 0.439\n",
      "Epoch 12167, Loss: 16.359, Final Batch Loss: 0.436\n",
      "Epoch 12168, Loss: 16.449, Final Batch Loss: 0.443\n",
      "Epoch 12169, Loss: 16.529, Final Batch Loss: 0.381\n",
      "Epoch 12170, Loss: 16.491, Final Batch Loss: 0.470\n",
      "Epoch 12171, Loss: 16.718, Final Batch Loss: 0.465\n",
      "Epoch 12172, Loss: 16.324, Final Batch Loss: 0.450\n",
      "Epoch 12173, Loss: 16.482, Final Batch Loss: 0.404\n",
      "Epoch 12174, Loss: 16.756, Final Batch Loss: 0.528\n",
      "Epoch 12175, Loss: 16.360, Final Batch Loss: 0.528\n",
      "Epoch 12176, Loss: 16.640, Final Batch Loss: 0.521\n",
      "Epoch 12177, Loss: 16.643, Final Batch Loss: 0.455\n",
      "Epoch 12178, Loss: 16.571, Final Batch Loss: 0.494\n",
      "Epoch 12179, Loss: 16.649, Final Batch Loss: 0.420\n",
      "Epoch 12180, Loss: 16.388, Final Batch Loss: 0.356\n",
      "Epoch 12181, Loss: 16.480, Final Batch Loss: 0.555\n",
      "Epoch 12182, Loss: 16.584, Final Batch Loss: 0.431\n",
      "Epoch 12183, Loss: 16.283, Final Batch Loss: 0.428\n",
      "Epoch 12184, Loss: 16.409, Final Batch Loss: 0.373\n",
      "Epoch 12185, Loss: 16.632, Final Batch Loss: 0.485\n",
      "Epoch 12186, Loss: 16.431, Final Batch Loss: 0.395\n",
      "Epoch 12187, Loss: 16.590, Final Batch Loss: 0.474\n",
      "Epoch 12188, Loss: 16.573, Final Batch Loss: 0.489\n",
      "Epoch 12189, Loss: 16.628, Final Batch Loss: 0.544\n",
      "Epoch 12190, Loss: 16.626, Final Batch Loss: 0.489\n",
      "Epoch 12191, Loss: 16.451, Final Batch Loss: 0.408\n",
      "Epoch 12192, Loss: 16.598, Final Batch Loss: 0.462\n",
      "Epoch 12193, Loss: 16.719, Final Batch Loss: 0.535\n",
      "Epoch 12194, Loss: 16.580, Final Batch Loss: 0.430\n",
      "Epoch 12195, Loss: 16.571, Final Batch Loss: 0.416\n",
      "Epoch 12196, Loss: 16.541, Final Batch Loss: 0.521\n",
      "Epoch 12197, Loss: 16.658, Final Batch Loss: 0.523\n",
      "Epoch 12198, Loss: 16.453, Final Batch Loss: 0.496\n",
      "Epoch 12199, Loss: 16.631, Final Batch Loss: 0.569\n",
      "Epoch 12200, Loss: 16.800, Final Batch Loss: 0.462\n",
      "Epoch 12201, Loss: 16.698, Final Batch Loss: 0.480\n",
      "Epoch 12202, Loss: 16.522, Final Batch Loss: 0.415\n",
      "Epoch 12203, Loss: 16.600, Final Batch Loss: 0.441\n",
      "Epoch 12204, Loss: 16.381, Final Batch Loss: 0.530\n",
      "Epoch 12205, Loss: 16.499, Final Batch Loss: 0.458\n",
      "Epoch 12206, Loss: 16.429, Final Batch Loss: 0.390\n",
      "Epoch 12207, Loss: 16.390, Final Batch Loss: 0.458\n",
      "Epoch 12208, Loss: 16.385, Final Batch Loss: 0.534\n",
      "Epoch 12209, Loss: 16.574, Final Batch Loss: 0.442\n",
      "Epoch 12210, Loss: 16.664, Final Batch Loss: 0.615\n",
      "Epoch 12211, Loss: 16.568, Final Batch Loss: 0.373\n",
      "Epoch 12212, Loss: 16.379, Final Batch Loss: 0.561\n",
      "Epoch 12213, Loss: 16.239, Final Batch Loss: 0.415\n",
      "Epoch 12214, Loss: 16.198, Final Batch Loss: 0.509\n",
      "Epoch 12215, Loss: 16.515, Final Batch Loss: 0.474\n",
      "Epoch 12216, Loss: 16.426, Final Batch Loss: 0.354\n",
      "Epoch 12217, Loss: 16.512, Final Batch Loss: 0.406\n",
      "Epoch 12218, Loss: 16.599, Final Batch Loss: 0.468\n",
      "Epoch 12219, Loss: 16.320, Final Batch Loss: 0.382\n",
      "Epoch 12220, Loss: 16.310, Final Batch Loss: 0.522\n",
      "Epoch 12221, Loss: 16.766, Final Batch Loss: 0.393\n",
      "Epoch 12222, Loss: 16.406, Final Batch Loss: 0.408\n",
      "Epoch 12223, Loss: 16.411, Final Batch Loss: 0.432\n",
      "Epoch 12224, Loss: 16.363, Final Batch Loss: 0.550\n",
      "Epoch 12225, Loss: 16.499, Final Batch Loss: 0.413\n",
      "Epoch 12226, Loss: 16.671, Final Batch Loss: 0.537\n",
      "Epoch 12227, Loss: 16.770, Final Batch Loss: 0.498\n",
      "Epoch 12228, Loss: 16.411, Final Batch Loss: 0.528\n",
      "Epoch 12229, Loss: 16.750, Final Batch Loss: 0.376\n",
      "Epoch 12230, Loss: 16.597, Final Batch Loss: 0.428\n",
      "Epoch 12231, Loss: 16.496, Final Batch Loss: 0.446\n",
      "Epoch 12232, Loss: 16.849, Final Batch Loss: 0.406\n",
      "Epoch 12233, Loss: 16.763, Final Batch Loss: 0.453\n",
      "Epoch 12234, Loss: 16.554, Final Batch Loss: 0.415\n",
      "Epoch 12235, Loss: 16.384, Final Batch Loss: 0.435\n",
      "Epoch 12236, Loss: 16.410, Final Batch Loss: 0.441\n",
      "Epoch 12237, Loss: 16.582, Final Batch Loss: 0.456\n",
      "Epoch 12238, Loss: 16.484, Final Batch Loss: 0.506\n",
      "Epoch 12239, Loss: 16.417, Final Batch Loss: 0.419\n",
      "Epoch 12240, Loss: 16.505, Final Batch Loss: 0.617\n",
      "Epoch 12241, Loss: 16.331, Final Batch Loss: 0.410\n",
      "Epoch 12242, Loss: 16.656, Final Batch Loss: 0.398\n",
      "Epoch 12243, Loss: 16.429, Final Batch Loss: 0.398\n",
      "Epoch 12244, Loss: 16.467, Final Batch Loss: 0.415\n",
      "Epoch 12245, Loss: 16.499, Final Batch Loss: 0.448\n",
      "Epoch 12246, Loss: 16.363, Final Batch Loss: 0.333\n",
      "Epoch 12247, Loss: 16.544, Final Batch Loss: 0.402\n",
      "Epoch 12248, Loss: 16.684, Final Batch Loss: 0.475\n",
      "Epoch 12249, Loss: 16.941, Final Batch Loss: 0.451\n",
      "Epoch 12250, Loss: 16.432, Final Batch Loss: 0.401\n",
      "Epoch 12251, Loss: 16.441, Final Batch Loss: 0.525\n",
      "Epoch 12252, Loss: 16.520, Final Batch Loss: 0.464\n",
      "Epoch 12253, Loss: 16.416, Final Batch Loss: 0.413\n",
      "Epoch 12254, Loss: 16.592, Final Batch Loss: 0.549\n",
      "Epoch 12255, Loss: 16.270, Final Batch Loss: 0.417\n",
      "Epoch 12256, Loss: 16.711, Final Batch Loss: 0.532\n",
      "Epoch 12257, Loss: 16.620, Final Batch Loss: 0.391\n",
      "Epoch 12258, Loss: 16.399, Final Batch Loss: 0.499\n",
      "Epoch 12259, Loss: 16.339, Final Batch Loss: 0.323\n",
      "Epoch 12260, Loss: 16.726, Final Batch Loss: 0.668\n",
      "Epoch 12261, Loss: 16.513, Final Batch Loss: 0.435\n",
      "Epoch 12262, Loss: 16.496, Final Batch Loss: 0.420\n",
      "Epoch 12263, Loss: 16.721, Final Batch Loss: 0.464\n",
      "Epoch 12264, Loss: 16.389, Final Batch Loss: 0.465\n",
      "Epoch 12265, Loss: 16.480, Final Batch Loss: 0.387\n",
      "Epoch 12266, Loss: 16.350, Final Batch Loss: 0.396\n",
      "Epoch 12267, Loss: 16.512, Final Batch Loss: 0.497\n",
      "Epoch 12268, Loss: 16.480, Final Batch Loss: 0.475\n",
      "Epoch 12269, Loss: 16.544, Final Batch Loss: 0.515\n",
      "Epoch 12270, Loss: 16.477, Final Batch Loss: 0.525\n",
      "Epoch 12271, Loss: 16.450, Final Batch Loss: 0.347\n",
      "Epoch 12272, Loss: 16.423, Final Batch Loss: 0.349\n",
      "Epoch 12273, Loss: 16.294, Final Batch Loss: 0.450\n",
      "Epoch 12274, Loss: 16.305, Final Batch Loss: 0.401\n",
      "Epoch 12275, Loss: 16.357, Final Batch Loss: 0.489\n",
      "Epoch 12276, Loss: 16.676, Final Batch Loss: 0.426\n",
      "Epoch 12277, Loss: 16.555, Final Batch Loss: 0.394\n",
      "Epoch 12278, Loss: 16.381, Final Batch Loss: 0.425\n",
      "Epoch 12279, Loss: 16.503, Final Batch Loss: 0.408\n",
      "Epoch 12280, Loss: 16.510, Final Batch Loss: 0.438\n",
      "Epoch 12281, Loss: 16.578, Final Batch Loss: 0.563\n",
      "Epoch 12282, Loss: 16.495, Final Batch Loss: 0.487\n",
      "Epoch 12283, Loss: 16.704, Final Batch Loss: 0.545\n",
      "Epoch 12284, Loss: 16.713, Final Batch Loss: 0.524\n",
      "Epoch 12285, Loss: 16.339, Final Batch Loss: 0.371\n",
      "Epoch 12286, Loss: 16.469, Final Batch Loss: 0.525\n",
      "Epoch 12287, Loss: 16.342, Final Batch Loss: 0.504\n",
      "Epoch 12288, Loss: 16.293, Final Batch Loss: 0.445\n",
      "Epoch 12289, Loss: 16.794, Final Batch Loss: 0.406\n",
      "Epoch 12290, Loss: 16.441, Final Batch Loss: 0.472\n",
      "Epoch 12291, Loss: 16.697, Final Batch Loss: 0.479\n",
      "Epoch 12292, Loss: 16.537, Final Batch Loss: 0.431\n",
      "Epoch 12293, Loss: 16.249, Final Batch Loss: 0.485\n",
      "Epoch 12294, Loss: 16.347, Final Batch Loss: 0.428\n",
      "Epoch 12295, Loss: 16.324, Final Batch Loss: 0.470\n",
      "Epoch 12296, Loss: 16.360, Final Batch Loss: 0.373\n",
      "Epoch 12297, Loss: 16.618, Final Batch Loss: 0.406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12298, Loss: 16.437, Final Batch Loss: 0.430\n",
      "Epoch 12299, Loss: 16.562, Final Batch Loss: 0.568\n",
      "Epoch 12300, Loss: 16.320, Final Batch Loss: 0.620\n",
      "Epoch 12301, Loss: 16.626, Final Batch Loss: 0.416\n",
      "Epoch 12302, Loss: 16.534, Final Batch Loss: 0.399\n",
      "Epoch 12303, Loss: 16.413, Final Batch Loss: 0.370\n",
      "Epoch 12304, Loss: 16.508, Final Batch Loss: 0.502\n",
      "Epoch 12305, Loss: 16.595, Final Batch Loss: 0.520\n",
      "Epoch 12306, Loss: 16.445, Final Batch Loss: 0.388\n",
      "Epoch 12307, Loss: 16.242, Final Batch Loss: 0.461\n",
      "Epoch 12308, Loss: 16.646, Final Batch Loss: 0.551\n",
      "Epoch 12309, Loss: 16.353, Final Batch Loss: 0.464\n",
      "Epoch 12310, Loss: 16.578, Final Batch Loss: 0.490\n",
      "Epoch 12311, Loss: 16.679, Final Batch Loss: 0.569\n",
      "Epoch 12312, Loss: 16.525, Final Batch Loss: 0.555\n",
      "Epoch 12313, Loss: 16.446, Final Batch Loss: 0.342\n",
      "Epoch 12314, Loss: 16.642, Final Batch Loss: 0.542\n",
      "Epoch 12315, Loss: 16.672, Final Batch Loss: 0.378\n",
      "Epoch 12316, Loss: 16.496, Final Batch Loss: 0.472\n",
      "Epoch 12317, Loss: 16.679, Final Batch Loss: 0.473\n",
      "Epoch 12318, Loss: 16.491, Final Batch Loss: 0.501\n",
      "Epoch 12319, Loss: 16.162, Final Batch Loss: 0.379\n",
      "Epoch 12320, Loss: 16.660, Final Batch Loss: 0.383\n",
      "Epoch 12321, Loss: 16.449, Final Batch Loss: 0.449\n",
      "Epoch 12322, Loss: 16.552, Final Batch Loss: 0.506\n",
      "Epoch 12323, Loss: 16.587, Final Batch Loss: 0.483\n",
      "Epoch 12324, Loss: 16.498, Final Batch Loss: 0.486\n",
      "Epoch 12325, Loss: 16.585, Final Batch Loss: 0.525\n",
      "Epoch 12326, Loss: 16.482, Final Batch Loss: 0.455\n",
      "Epoch 12327, Loss: 16.592, Final Batch Loss: 0.554\n",
      "Epoch 12328, Loss: 16.506, Final Batch Loss: 0.584\n",
      "Epoch 12329, Loss: 16.284, Final Batch Loss: 0.493\n",
      "Epoch 12330, Loss: 16.680, Final Batch Loss: 0.373\n",
      "Epoch 12331, Loss: 16.580, Final Batch Loss: 0.536\n",
      "Epoch 12332, Loss: 16.488, Final Batch Loss: 0.382\n",
      "Epoch 12333, Loss: 16.602, Final Batch Loss: 0.432\n",
      "Epoch 12334, Loss: 16.445, Final Batch Loss: 0.572\n",
      "Epoch 12335, Loss: 16.644, Final Batch Loss: 0.481\n",
      "Epoch 12336, Loss: 16.525, Final Batch Loss: 0.463\n",
      "Epoch 12337, Loss: 16.334, Final Batch Loss: 0.458\n",
      "Epoch 12338, Loss: 16.326, Final Batch Loss: 0.430\n",
      "Epoch 12339, Loss: 16.460, Final Batch Loss: 0.380\n",
      "Epoch 12340, Loss: 16.514, Final Batch Loss: 0.505\n",
      "Epoch 12341, Loss: 16.509, Final Batch Loss: 0.504\n",
      "Epoch 12342, Loss: 16.318, Final Batch Loss: 0.441\n",
      "Epoch 12343, Loss: 16.510, Final Batch Loss: 0.496\n",
      "Epoch 12344, Loss: 16.590, Final Batch Loss: 0.430\n",
      "Epoch 12345, Loss: 16.402, Final Batch Loss: 0.428\n",
      "Epoch 12346, Loss: 16.914, Final Batch Loss: 0.534\n",
      "Epoch 12347, Loss: 16.288, Final Batch Loss: 0.477\n",
      "Epoch 12348, Loss: 16.446, Final Batch Loss: 0.444\n",
      "Epoch 12349, Loss: 16.433, Final Batch Loss: 0.408\n",
      "Epoch 12350, Loss: 16.383, Final Batch Loss: 0.426\n",
      "Epoch 12351, Loss: 16.562, Final Batch Loss: 0.568\n",
      "Epoch 12352, Loss: 16.471, Final Batch Loss: 0.419\n",
      "Epoch 12353, Loss: 16.642, Final Batch Loss: 0.401\n",
      "Epoch 12354, Loss: 16.357, Final Batch Loss: 0.373\n",
      "Epoch 12355, Loss: 16.304, Final Batch Loss: 0.488\n",
      "Epoch 12356, Loss: 16.309, Final Batch Loss: 0.407\n",
      "Epoch 12357, Loss: 16.621, Final Batch Loss: 0.407\n",
      "Epoch 12358, Loss: 16.318, Final Batch Loss: 0.405\n",
      "Epoch 12359, Loss: 16.566, Final Batch Loss: 0.403\n",
      "Epoch 12360, Loss: 16.505, Final Batch Loss: 0.434\n",
      "Epoch 12361, Loss: 16.559, Final Batch Loss: 0.460\n",
      "Epoch 12362, Loss: 16.432, Final Batch Loss: 0.554\n",
      "Epoch 12363, Loss: 16.385, Final Batch Loss: 0.458\n",
      "Epoch 12364, Loss: 16.621, Final Batch Loss: 0.444\n",
      "Epoch 12365, Loss: 16.749, Final Batch Loss: 0.529\n",
      "Epoch 12366, Loss: 16.653, Final Batch Loss: 0.513\n",
      "Epoch 12367, Loss: 16.738, Final Batch Loss: 0.567\n",
      "Epoch 12368, Loss: 16.695, Final Batch Loss: 0.489\n",
      "Epoch 12369, Loss: 16.338, Final Batch Loss: 0.393\n",
      "Epoch 12370, Loss: 16.263, Final Batch Loss: 0.456\n",
      "Epoch 12371, Loss: 16.378, Final Batch Loss: 0.474\n",
      "Epoch 12372, Loss: 16.213, Final Batch Loss: 0.418\n",
      "Epoch 12373, Loss: 16.462, Final Batch Loss: 0.389\n",
      "Epoch 12374, Loss: 16.544, Final Batch Loss: 0.511\n",
      "Epoch 12375, Loss: 16.663, Final Batch Loss: 0.424\n",
      "Epoch 12376, Loss: 16.369, Final Batch Loss: 0.365\n",
      "Epoch 12377, Loss: 16.255, Final Batch Loss: 0.409\n",
      "Epoch 12378, Loss: 16.336, Final Batch Loss: 0.505\n",
      "Epoch 12379, Loss: 16.639, Final Batch Loss: 0.474\n",
      "Epoch 12380, Loss: 16.601, Final Batch Loss: 0.451\n",
      "Epoch 12381, Loss: 16.420, Final Batch Loss: 0.447\n",
      "Epoch 12382, Loss: 16.589, Final Batch Loss: 0.540\n",
      "Epoch 12383, Loss: 16.624, Final Batch Loss: 0.375\n",
      "Epoch 12384, Loss: 16.476, Final Batch Loss: 0.417\n",
      "Epoch 12385, Loss: 16.329, Final Batch Loss: 0.496\n",
      "Epoch 12386, Loss: 16.417, Final Batch Loss: 0.523\n",
      "Epoch 12387, Loss: 16.363, Final Batch Loss: 0.521\n",
      "Epoch 12388, Loss: 16.306, Final Batch Loss: 0.426\n",
      "Epoch 12389, Loss: 16.637, Final Batch Loss: 0.483\n",
      "Epoch 12390, Loss: 16.254, Final Batch Loss: 0.418\n",
      "Epoch 12391, Loss: 16.446, Final Batch Loss: 0.562\n",
      "Epoch 12392, Loss: 16.348, Final Batch Loss: 0.412\n",
      "Epoch 12393, Loss: 16.551, Final Batch Loss: 0.491\n",
      "Epoch 12394, Loss: 16.622, Final Batch Loss: 0.384\n",
      "Epoch 12395, Loss: 16.460, Final Batch Loss: 0.338\n",
      "Epoch 12396, Loss: 16.512, Final Batch Loss: 0.426\n",
      "Epoch 12397, Loss: 16.671, Final Batch Loss: 0.530\n",
      "Epoch 12398, Loss: 16.745, Final Batch Loss: 0.503\n",
      "Epoch 12399, Loss: 16.443, Final Batch Loss: 0.481\n",
      "Epoch 12400, Loss: 16.300, Final Batch Loss: 0.530\n",
      "Epoch 12401, Loss: 16.398, Final Batch Loss: 0.486\n",
      "Epoch 12402, Loss: 16.449, Final Batch Loss: 0.477\n",
      "Epoch 12403, Loss: 16.349, Final Batch Loss: 0.494\n",
      "Epoch 12404, Loss: 16.537, Final Batch Loss: 0.488\n",
      "Epoch 12405, Loss: 16.510, Final Batch Loss: 0.422\n",
      "Epoch 12406, Loss: 16.530, Final Batch Loss: 0.558\n",
      "Epoch 12407, Loss: 16.729, Final Batch Loss: 0.493\n",
      "Epoch 12408, Loss: 16.902, Final Batch Loss: 0.420\n",
      "Epoch 12409, Loss: 16.555, Final Batch Loss: 0.516\n",
      "Epoch 12410, Loss: 16.365, Final Batch Loss: 0.466\n",
      "Epoch 12411, Loss: 16.406, Final Batch Loss: 0.408\n",
      "Epoch 12412, Loss: 16.559, Final Batch Loss: 0.460\n",
      "Epoch 12413, Loss: 16.587, Final Batch Loss: 0.574\n",
      "Epoch 12414, Loss: 16.513, Final Batch Loss: 0.495\n",
      "Epoch 12415, Loss: 16.709, Final Batch Loss: 0.488\n",
      "Epoch 12416, Loss: 16.793, Final Batch Loss: 0.518\n",
      "Epoch 12417, Loss: 16.505, Final Batch Loss: 0.504\n",
      "Epoch 12418, Loss: 16.361, Final Batch Loss: 0.377\n",
      "Epoch 12419, Loss: 16.863, Final Batch Loss: 0.483\n",
      "Epoch 12420, Loss: 16.467, Final Batch Loss: 0.457\n",
      "Epoch 12421, Loss: 16.567, Final Batch Loss: 0.482\n",
      "Epoch 12422, Loss: 16.459, Final Batch Loss: 0.391\n",
      "Epoch 12423, Loss: 16.658, Final Batch Loss: 0.505\n",
      "Epoch 12424, Loss: 16.813, Final Batch Loss: 0.550\n",
      "Epoch 12425, Loss: 16.545, Final Batch Loss: 0.469\n",
      "Epoch 12426, Loss: 16.563, Final Batch Loss: 0.509\n",
      "Epoch 12427, Loss: 16.748, Final Batch Loss: 0.455\n",
      "Epoch 12428, Loss: 16.507, Final Batch Loss: 0.423\n",
      "Epoch 12429, Loss: 16.442, Final Batch Loss: 0.589\n",
      "Epoch 12430, Loss: 16.375, Final Batch Loss: 0.444\n",
      "Epoch 12431, Loss: 16.333, Final Batch Loss: 0.395\n",
      "Epoch 12432, Loss: 16.391, Final Batch Loss: 0.424\n",
      "Epoch 12433, Loss: 16.556, Final Batch Loss: 0.558\n",
      "Epoch 12434, Loss: 16.594, Final Batch Loss: 0.429\n",
      "Epoch 12435, Loss: 16.267, Final Batch Loss: 0.479\n",
      "Epoch 12436, Loss: 16.662, Final Batch Loss: 0.488\n",
      "Epoch 12437, Loss: 16.450, Final Batch Loss: 0.473\n",
      "Epoch 12438, Loss: 16.528, Final Batch Loss: 0.498\n",
      "Epoch 12439, Loss: 16.620, Final Batch Loss: 0.555\n",
      "Epoch 12440, Loss: 16.322, Final Batch Loss: 0.440\n",
      "Epoch 12441, Loss: 16.775, Final Batch Loss: 0.468\n",
      "Epoch 12442, Loss: 16.372, Final Batch Loss: 0.507\n",
      "Epoch 12443, Loss: 16.332, Final Batch Loss: 0.410\n",
      "Epoch 12444, Loss: 16.594, Final Batch Loss: 0.542\n",
      "Epoch 12445, Loss: 16.591, Final Batch Loss: 0.425\n",
      "Epoch 12446, Loss: 16.764, Final Batch Loss: 0.407\n",
      "Epoch 12447, Loss: 16.454, Final Batch Loss: 0.484\n",
      "Epoch 12448, Loss: 16.596, Final Batch Loss: 0.427\n",
      "Epoch 12449, Loss: 16.360, Final Batch Loss: 0.540\n",
      "Epoch 12450, Loss: 16.330, Final Batch Loss: 0.494\n",
      "Epoch 12451, Loss: 16.449, Final Batch Loss: 0.467\n",
      "Epoch 12452, Loss: 16.069, Final Batch Loss: 0.480\n",
      "Epoch 12453, Loss: 16.535, Final Batch Loss: 0.478\n",
      "Epoch 12454, Loss: 16.364, Final Batch Loss: 0.484\n",
      "Epoch 12455, Loss: 16.386, Final Batch Loss: 0.385\n",
      "Epoch 12456, Loss: 16.343, Final Batch Loss: 0.359\n",
      "Epoch 12457, Loss: 16.522, Final Batch Loss: 0.513\n",
      "Epoch 12458, Loss: 16.576, Final Batch Loss: 0.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12459, Loss: 16.389, Final Batch Loss: 0.452\n",
      "Epoch 12460, Loss: 16.649, Final Batch Loss: 0.468\n",
      "Epoch 12461, Loss: 16.155, Final Batch Loss: 0.338\n",
      "Epoch 12462, Loss: 16.451, Final Batch Loss: 0.432\n",
      "Epoch 12463, Loss: 16.415, Final Batch Loss: 0.350\n",
      "Epoch 12464, Loss: 16.267, Final Batch Loss: 0.399\n",
      "Epoch 12465, Loss: 16.211, Final Batch Loss: 0.470\n",
      "Epoch 12466, Loss: 16.515, Final Batch Loss: 0.534\n",
      "Epoch 12467, Loss: 16.634, Final Batch Loss: 0.487\n",
      "Epoch 12468, Loss: 16.354, Final Batch Loss: 0.512\n",
      "Epoch 12469, Loss: 16.326, Final Batch Loss: 0.461\n",
      "Epoch 12470, Loss: 16.661, Final Batch Loss: 0.625\n",
      "Epoch 12471, Loss: 16.302, Final Batch Loss: 0.528\n",
      "Epoch 12472, Loss: 16.424, Final Batch Loss: 0.399\n",
      "Epoch 12473, Loss: 16.225, Final Batch Loss: 0.503\n",
      "Epoch 12474, Loss: 16.439, Final Batch Loss: 0.553\n",
      "Epoch 12475, Loss: 16.352, Final Batch Loss: 0.456\n",
      "Epoch 12476, Loss: 16.331, Final Batch Loss: 0.461\n",
      "Epoch 12477, Loss: 16.353, Final Batch Loss: 0.416\n",
      "Epoch 12478, Loss: 16.244, Final Batch Loss: 0.417\n",
      "Epoch 12479, Loss: 16.434, Final Batch Loss: 0.492\n",
      "Epoch 12480, Loss: 16.495, Final Batch Loss: 0.409\n",
      "Epoch 12481, Loss: 16.408, Final Batch Loss: 0.471\n",
      "Epoch 12482, Loss: 16.740, Final Batch Loss: 0.455\n",
      "Epoch 12483, Loss: 16.294, Final Batch Loss: 0.474\n",
      "Epoch 12484, Loss: 16.395, Final Batch Loss: 0.479\n",
      "Epoch 12485, Loss: 16.394, Final Batch Loss: 0.401\n",
      "Epoch 12486, Loss: 16.737, Final Batch Loss: 0.444\n",
      "Epoch 12487, Loss: 16.638, Final Batch Loss: 0.384\n",
      "Epoch 12488, Loss: 16.462, Final Batch Loss: 0.382\n",
      "Epoch 12489, Loss: 16.615, Final Batch Loss: 0.495\n",
      "Epoch 12490, Loss: 16.580, Final Batch Loss: 0.423\n",
      "Epoch 12491, Loss: 16.534, Final Batch Loss: 0.483\n",
      "Epoch 12492, Loss: 16.373, Final Batch Loss: 0.431\n",
      "Epoch 12493, Loss: 16.460, Final Batch Loss: 0.447\n",
      "Epoch 12494, Loss: 16.472, Final Batch Loss: 0.453\n",
      "Epoch 12495, Loss: 16.421, Final Batch Loss: 0.477\n",
      "Epoch 12496, Loss: 16.672, Final Batch Loss: 0.494\n",
      "Epoch 12497, Loss: 16.495, Final Batch Loss: 0.438\n",
      "Epoch 12498, Loss: 16.610, Final Batch Loss: 0.475\n",
      "Epoch 12499, Loss: 16.589, Final Batch Loss: 0.446\n",
      "Epoch 12500, Loss: 16.319, Final Batch Loss: 0.455\n",
      "Epoch 12501, Loss: 16.702, Final Batch Loss: 0.447\n",
      "Epoch 12502, Loss: 16.407, Final Batch Loss: 0.362\n",
      "Epoch 12503, Loss: 16.651, Final Batch Loss: 0.504\n",
      "Epoch 12504, Loss: 16.535, Final Batch Loss: 0.378\n",
      "Epoch 12505, Loss: 16.272, Final Batch Loss: 0.434\n",
      "Epoch 12506, Loss: 16.485, Final Batch Loss: 0.416\n",
      "Epoch 12507, Loss: 16.692, Final Batch Loss: 0.457\n",
      "Epoch 12508, Loss: 16.452, Final Batch Loss: 0.405\n",
      "Epoch 12509, Loss: 16.365, Final Batch Loss: 0.367\n",
      "Epoch 12510, Loss: 16.687, Final Batch Loss: 0.528\n",
      "Epoch 12511, Loss: 16.654, Final Batch Loss: 0.435\n",
      "Epoch 12512, Loss: 16.392, Final Batch Loss: 0.390\n",
      "Epoch 12513, Loss: 16.480, Final Batch Loss: 0.437\n",
      "Epoch 12514, Loss: 16.480, Final Batch Loss: 0.375\n",
      "Epoch 12515, Loss: 16.360, Final Batch Loss: 0.459\n",
      "Epoch 12516, Loss: 16.715, Final Batch Loss: 0.578\n",
      "Epoch 12517, Loss: 16.600, Final Batch Loss: 0.424\n",
      "Epoch 12518, Loss: 16.231, Final Batch Loss: 0.462\n",
      "Epoch 12519, Loss: 16.516, Final Batch Loss: 0.545\n",
      "Epoch 12520, Loss: 16.610, Final Batch Loss: 0.410\n",
      "Epoch 12521, Loss: 16.280, Final Batch Loss: 0.560\n",
      "Epoch 12522, Loss: 16.335, Final Batch Loss: 0.379\n",
      "Epoch 12523, Loss: 16.324, Final Batch Loss: 0.436\n",
      "Epoch 12524, Loss: 16.484, Final Batch Loss: 0.380\n",
      "Epoch 12525, Loss: 16.405, Final Batch Loss: 0.534\n",
      "Epoch 12526, Loss: 16.611, Final Batch Loss: 0.486\n",
      "Epoch 12527, Loss: 16.203, Final Batch Loss: 0.429\n",
      "Epoch 12528, Loss: 16.446, Final Batch Loss: 0.527\n",
      "Epoch 12529, Loss: 16.529, Final Batch Loss: 0.481\n",
      "Epoch 12530, Loss: 16.424, Final Batch Loss: 0.370\n",
      "Epoch 12531, Loss: 16.461, Final Batch Loss: 0.559\n",
      "Epoch 12532, Loss: 16.459, Final Batch Loss: 0.445\n",
      "Epoch 12533, Loss: 16.499, Final Batch Loss: 0.543\n",
      "Epoch 12534, Loss: 16.362, Final Batch Loss: 0.562\n",
      "Epoch 12535, Loss: 16.419, Final Batch Loss: 0.475\n",
      "Epoch 12536, Loss: 16.701, Final Batch Loss: 0.487\n",
      "Epoch 12537, Loss: 16.483, Final Batch Loss: 0.386\n",
      "Epoch 12538, Loss: 16.670, Final Batch Loss: 0.439\n",
      "Epoch 12539, Loss: 16.781, Final Batch Loss: 0.465\n",
      "Epoch 12540, Loss: 16.472, Final Batch Loss: 0.358\n",
      "Epoch 12541, Loss: 16.507, Final Batch Loss: 0.439\n",
      "Epoch 12542, Loss: 16.361, Final Batch Loss: 0.431\n",
      "Epoch 12543, Loss: 16.401, Final Batch Loss: 0.421\n",
      "Epoch 12544, Loss: 16.469, Final Batch Loss: 0.465\n",
      "Epoch 12545, Loss: 16.387, Final Batch Loss: 0.485\n",
      "Epoch 12546, Loss: 16.375, Final Batch Loss: 0.444\n",
      "Epoch 12547, Loss: 15.957, Final Batch Loss: 0.374\n",
      "Epoch 12548, Loss: 16.502, Final Batch Loss: 0.392\n",
      "Epoch 12549, Loss: 16.379, Final Batch Loss: 0.577\n",
      "Epoch 12550, Loss: 16.901, Final Batch Loss: 0.452\n",
      "Epoch 12551, Loss: 16.470, Final Batch Loss: 0.422\n",
      "Epoch 12552, Loss: 16.401, Final Batch Loss: 0.434\n",
      "Epoch 12553, Loss: 16.340, Final Batch Loss: 0.387\n",
      "Epoch 12554, Loss: 16.504, Final Batch Loss: 0.453\n",
      "Epoch 12555, Loss: 16.433, Final Batch Loss: 0.361\n",
      "Epoch 12556, Loss: 16.657, Final Batch Loss: 0.516\n",
      "Epoch 12557, Loss: 16.480, Final Batch Loss: 0.419\n",
      "Epoch 12558, Loss: 16.484, Final Batch Loss: 0.573\n",
      "Epoch 12559, Loss: 16.426, Final Batch Loss: 0.477\n",
      "Epoch 12560, Loss: 16.449, Final Batch Loss: 0.481\n",
      "Epoch 12561, Loss: 16.597, Final Batch Loss: 0.505\n",
      "Epoch 12562, Loss: 16.582, Final Batch Loss: 0.412\n",
      "Epoch 12563, Loss: 16.393, Final Batch Loss: 0.435\n",
      "Epoch 12564, Loss: 16.384, Final Batch Loss: 0.455\n",
      "Epoch 12565, Loss: 16.303, Final Batch Loss: 0.534\n",
      "Epoch 12566, Loss: 16.739, Final Batch Loss: 0.489\n",
      "Epoch 12567, Loss: 16.523, Final Batch Loss: 0.474\n",
      "Epoch 12568, Loss: 16.491, Final Batch Loss: 0.604\n",
      "Epoch 12569, Loss: 16.536, Final Batch Loss: 0.430\n",
      "Epoch 12570, Loss: 16.588, Final Batch Loss: 0.481\n",
      "Epoch 12571, Loss: 16.577, Final Batch Loss: 0.491\n",
      "Epoch 12572, Loss: 16.501, Final Batch Loss: 0.403\n",
      "Epoch 12573, Loss: 16.290, Final Batch Loss: 0.438\n",
      "Epoch 12574, Loss: 16.499, Final Batch Loss: 0.368\n",
      "Epoch 12575, Loss: 16.304, Final Batch Loss: 0.398\n",
      "Epoch 12576, Loss: 16.596, Final Batch Loss: 0.547\n",
      "Epoch 12577, Loss: 16.409, Final Batch Loss: 0.452\n",
      "Epoch 12578, Loss: 16.510, Final Batch Loss: 0.405\n",
      "Epoch 12579, Loss: 16.356, Final Batch Loss: 0.451\n",
      "Epoch 12580, Loss: 16.478, Final Batch Loss: 0.465\n",
      "Epoch 12581, Loss: 16.452, Final Batch Loss: 0.361\n",
      "Epoch 12582, Loss: 16.514, Final Batch Loss: 0.427\n",
      "Epoch 12583, Loss: 16.397, Final Batch Loss: 0.434\n",
      "Epoch 12584, Loss: 16.169, Final Batch Loss: 0.362\n",
      "Epoch 12585, Loss: 16.610, Final Batch Loss: 0.460\n",
      "Epoch 12586, Loss: 16.622, Final Batch Loss: 0.430\n",
      "Epoch 12587, Loss: 16.400, Final Batch Loss: 0.368\n",
      "Epoch 12588, Loss: 16.461, Final Batch Loss: 0.431\n",
      "Epoch 12589, Loss: 16.664, Final Batch Loss: 0.445\n",
      "Epoch 12590, Loss: 16.646, Final Batch Loss: 0.449\n",
      "Epoch 12591, Loss: 16.292, Final Batch Loss: 0.406\n",
      "Epoch 12592, Loss: 16.547, Final Batch Loss: 0.508\n",
      "Epoch 12593, Loss: 16.206, Final Batch Loss: 0.297\n",
      "Epoch 12594, Loss: 16.504, Final Batch Loss: 0.461\n",
      "Epoch 12595, Loss: 16.512, Final Batch Loss: 0.459\n",
      "Epoch 12596, Loss: 16.631, Final Batch Loss: 0.498\n",
      "Epoch 12597, Loss: 16.636, Final Batch Loss: 0.505\n",
      "Epoch 12598, Loss: 16.704, Final Batch Loss: 0.454\n",
      "Epoch 12599, Loss: 16.588, Final Batch Loss: 0.554\n",
      "Epoch 12600, Loss: 16.677, Final Batch Loss: 0.444\n",
      "Epoch 12601, Loss: 16.271, Final Batch Loss: 0.460\n",
      "Epoch 12602, Loss: 16.514, Final Batch Loss: 0.484\n",
      "Epoch 12603, Loss: 16.343, Final Batch Loss: 0.348\n",
      "Epoch 12604, Loss: 16.432, Final Batch Loss: 0.351\n",
      "Epoch 12605, Loss: 16.817, Final Batch Loss: 0.611\n",
      "Epoch 12606, Loss: 16.496, Final Batch Loss: 0.312\n",
      "Epoch 12607, Loss: 16.368, Final Batch Loss: 0.392\n",
      "Epoch 12608, Loss: 16.566, Final Batch Loss: 0.459\n",
      "Epoch 12609, Loss: 16.279, Final Batch Loss: 0.449\n",
      "Epoch 12610, Loss: 16.608, Final Batch Loss: 0.478\n",
      "Epoch 12611, Loss: 16.470, Final Batch Loss: 0.423\n",
      "Epoch 12612, Loss: 16.576, Final Batch Loss: 0.456\n",
      "Epoch 12613, Loss: 16.569, Final Batch Loss: 0.405\n",
      "Epoch 12614, Loss: 16.461, Final Batch Loss: 0.490\n",
      "Epoch 12615, Loss: 16.319, Final Batch Loss: 0.490\n",
      "Epoch 12616, Loss: 16.489, Final Batch Loss: 0.452\n",
      "Epoch 12617, Loss: 16.298, Final Batch Loss: 0.514\n",
      "Epoch 12618, Loss: 16.427, Final Batch Loss: 0.477\n",
      "Epoch 12619, Loss: 16.622, Final Batch Loss: 0.549\n",
      "Epoch 12620, Loss: 16.509, Final Batch Loss: 0.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12621, Loss: 16.089, Final Batch Loss: 0.497\n",
      "Epoch 12622, Loss: 16.230, Final Batch Loss: 0.467\n",
      "Epoch 12623, Loss: 16.321, Final Batch Loss: 0.525\n",
      "Epoch 12624, Loss: 16.549, Final Batch Loss: 0.356\n",
      "Epoch 12625, Loss: 16.398, Final Batch Loss: 0.396\n",
      "Epoch 12626, Loss: 16.265, Final Batch Loss: 0.489\n",
      "Epoch 12627, Loss: 16.481, Final Batch Loss: 0.627\n",
      "Epoch 12628, Loss: 16.444, Final Batch Loss: 0.467\n",
      "Epoch 12629, Loss: 16.498, Final Batch Loss: 0.451\n",
      "Epoch 12630, Loss: 16.340, Final Batch Loss: 0.517\n",
      "Epoch 12631, Loss: 16.368, Final Batch Loss: 0.463\n",
      "Epoch 12632, Loss: 16.424, Final Batch Loss: 0.386\n",
      "Epoch 12633, Loss: 16.403, Final Batch Loss: 0.457\n",
      "Epoch 12634, Loss: 16.680, Final Batch Loss: 0.426\n",
      "Epoch 12635, Loss: 16.424, Final Batch Loss: 0.487\n",
      "Epoch 12636, Loss: 16.450, Final Batch Loss: 0.388\n",
      "Epoch 12637, Loss: 16.452, Final Batch Loss: 0.407\n",
      "Epoch 12638, Loss: 16.606, Final Batch Loss: 0.367\n",
      "Epoch 12639, Loss: 16.554, Final Batch Loss: 0.486\n",
      "Epoch 12640, Loss: 16.362, Final Batch Loss: 0.460\n",
      "Epoch 12641, Loss: 16.525, Final Batch Loss: 0.374\n",
      "Epoch 12642, Loss: 16.557, Final Batch Loss: 0.426\n",
      "Epoch 12643, Loss: 16.279, Final Batch Loss: 0.430\n",
      "Epoch 12644, Loss: 16.498, Final Batch Loss: 0.531\n",
      "Epoch 12645, Loss: 16.263, Final Batch Loss: 0.390\n",
      "Epoch 12646, Loss: 16.300, Final Batch Loss: 0.557\n",
      "Epoch 12647, Loss: 16.476, Final Batch Loss: 0.570\n",
      "Epoch 12648, Loss: 16.378, Final Batch Loss: 0.438\n",
      "Epoch 12649, Loss: 16.667, Final Batch Loss: 0.571\n",
      "Epoch 12650, Loss: 16.394, Final Batch Loss: 0.361\n",
      "Epoch 12651, Loss: 16.498, Final Batch Loss: 0.541\n",
      "Epoch 12652, Loss: 16.185, Final Batch Loss: 0.369\n",
      "Epoch 12653, Loss: 16.612, Final Batch Loss: 0.497\n",
      "Epoch 12654, Loss: 16.193, Final Batch Loss: 0.382\n",
      "Epoch 12655, Loss: 16.749, Final Batch Loss: 0.458\n",
      "Epoch 12656, Loss: 16.419, Final Batch Loss: 0.453\n",
      "Epoch 12657, Loss: 16.253, Final Batch Loss: 0.438\n",
      "Epoch 12658, Loss: 16.354, Final Batch Loss: 0.421\n",
      "Epoch 12659, Loss: 16.557, Final Batch Loss: 0.494\n",
      "Epoch 12660, Loss: 16.529, Final Batch Loss: 0.430\n",
      "Epoch 12661, Loss: 16.471, Final Batch Loss: 0.374\n",
      "Epoch 12662, Loss: 16.390, Final Batch Loss: 0.471\n",
      "Epoch 12663, Loss: 16.499, Final Batch Loss: 0.423\n",
      "Epoch 12664, Loss: 16.409, Final Batch Loss: 0.435\n",
      "Epoch 12665, Loss: 16.323, Final Batch Loss: 0.447\n",
      "Epoch 12666, Loss: 16.513, Final Batch Loss: 0.420\n",
      "Epoch 12667, Loss: 16.506, Final Batch Loss: 0.439\n",
      "Epoch 12668, Loss: 16.264, Final Batch Loss: 0.479\n",
      "Epoch 12669, Loss: 16.276, Final Batch Loss: 0.543\n",
      "Epoch 12670, Loss: 16.640, Final Batch Loss: 0.482\n",
      "Epoch 12671, Loss: 16.293, Final Batch Loss: 0.415\n",
      "Epoch 12672, Loss: 16.617, Final Batch Loss: 0.497\n",
      "Epoch 12673, Loss: 16.498, Final Batch Loss: 0.416\n",
      "Epoch 12674, Loss: 16.525, Final Batch Loss: 0.519\n",
      "Epoch 12675, Loss: 16.436, Final Batch Loss: 0.392\n",
      "Epoch 12676, Loss: 16.476, Final Batch Loss: 0.507\n",
      "Epoch 12677, Loss: 16.437, Final Batch Loss: 0.448\n",
      "Epoch 12678, Loss: 16.471, Final Batch Loss: 0.547\n",
      "Epoch 12679, Loss: 16.506, Final Batch Loss: 0.482\n",
      "Epoch 12680, Loss: 16.564, Final Batch Loss: 0.695\n",
      "Epoch 12681, Loss: 16.530, Final Batch Loss: 0.432\n",
      "Epoch 12682, Loss: 16.452, Final Batch Loss: 0.480\n",
      "Epoch 12683, Loss: 16.662, Final Batch Loss: 0.579\n",
      "Epoch 12684, Loss: 16.233, Final Batch Loss: 0.410\n",
      "Epoch 12685, Loss: 16.738, Final Batch Loss: 0.488\n",
      "Epoch 12686, Loss: 16.277, Final Batch Loss: 0.500\n",
      "Epoch 12687, Loss: 16.702, Final Batch Loss: 0.427\n",
      "Epoch 12688, Loss: 16.384, Final Batch Loss: 0.469\n",
      "Epoch 12689, Loss: 16.317, Final Batch Loss: 0.475\n",
      "Epoch 12690, Loss: 16.452, Final Batch Loss: 0.381\n",
      "Epoch 12691, Loss: 16.340, Final Batch Loss: 0.463\n",
      "Epoch 12692, Loss: 16.259, Final Batch Loss: 0.446\n",
      "Epoch 12693, Loss: 16.544, Final Batch Loss: 0.527\n",
      "Epoch 12694, Loss: 16.271, Final Batch Loss: 0.499\n",
      "Epoch 12695, Loss: 16.355, Final Batch Loss: 0.360\n",
      "Epoch 12696, Loss: 16.422, Final Batch Loss: 0.429\n",
      "Epoch 12697, Loss: 16.564, Final Batch Loss: 0.489\n",
      "Epoch 12698, Loss: 16.483, Final Batch Loss: 0.459\n",
      "Epoch 12699, Loss: 16.530, Final Batch Loss: 0.377\n",
      "Epoch 12700, Loss: 16.335, Final Batch Loss: 0.417\n",
      "Epoch 12701, Loss: 16.583, Final Batch Loss: 0.473\n",
      "Epoch 12702, Loss: 16.554, Final Batch Loss: 0.528\n",
      "Epoch 12703, Loss: 16.777, Final Batch Loss: 0.481\n",
      "Epoch 12704, Loss: 16.522, Final Batch Loss: 0.517\n",
      "Epoch 12705, Loss: 16.306, Final Batch Loss: 0.429\n",
      "Epoch 12706, Loss: 16.557, Final Batch Loss: 0.378\n",
      "Epoch 12707, Loss: 16.652, Final Batch Loss: 0.549\n",
      "Epoch 12708, Loss: 16.670, Final Batch Loss: 0.517\n",
      "Epoch 12709, Loss: 16.515, Final Batch Loss: 0.495\n",
      "Epoch 12710, Loss: 16.806, Final Batch Loss: 0.367\n",
      "Epoch 12711, Loss: 16.370, Final Batch Loss: 0.464\n",
      "Epoch 12712, Loss: 16.512, Final Batch Loss: 0.358\n",
      "Epoch 12713, Loss: 16.278, Final Batch Loss: 0.587\n",
      "Epoch 12714, Loss: 16.471, Final Batch Loss: 0.453\n",
      "Epoch 12715, Loss: 16.611, Final Batch Loss: 0.494\n",
      "Epoch 12716, Loss: 16.578, Final Batch Loss: 0.418\n",
      "Epoch 12717, Loss: 16.375, Final Batch Loss: 0.550\n",
      "Epoch 12718, Loss: 16.390, Final Batch Loss: 0.445\n",
      "Epoch 12719, Loss: 16.675, Final Batch Loss: 0.556\n",
      "Epoch 12720, Loss: 16.655, Final Batch Loss: 0.514\n",
      "Epoch 12721, Loss: 16.262, Final Batch Loss: 0.506\n",
      "Epoch 12722, Loss: 16.337, Final Batch Loss: 0.531\n",
      "Epoch 12723, Loss: 16.696, Final Batch Loss: 0.433\n",
      "Epoch 12724, Loss: 16.817, Final Batch Loss: 0.666\n",
      "Epoch 12725, Loss: 16.380, Final Batch Loss: 0.497\n",
      "Epoch 12726, Loss: 16.320, Final Batch Loss: 0.483\n",
      "Epoch 12727, Loss: 16.508, Final Batch Loss: 0.473\n",
      "Epoch 12728, Loss: 16.325, Final Batch Loss: 0.463\n",
      "Epoch 12729, Loss: 16.497, Final Batch Loss: 0.509\n",
      "Epoch 12730, Loss: 16.625, Final Batch Loss: 0.422\n",
      "Epoch 12731, Loss: 16.382, Final Batch Loss: 0.375\n",
      "Epoch 12732, Loss: 16.231, Final Batch Loss: 0.421\n",
      "Epoch 12733, Loss: 16.535, Final Batch Loss: 0.449\n",
      "Epoch 12734, Loss: 16.402, Final Batch Loss: 0.513\n",
      "Epoch 12735, Loss: 16.586, Final Batch Loss: 0.452\n",
      "Epoch 12736, Loss: 16.263, Final Batch Loss: 0.497\n",
      "Epoch 12737, Loss: 16.094, Final Batch Loss: 0.455\n",
      "Epoch 12738, Loss: 16.201, Final Batch Loss: 0.455\n",
      "Epoch 12739, Loss: 16.609, Final Batch Loss: 0.388\n",
      "Epoch 12740, Loss: 16.373, Final Batch Loss: 0.451\n",
      "Epoch 12741, Loss: 16.394, Final Batch Loss: 0.508\n",
      "Epoch 12742, Loss: 16.485, Final Batch Loss: 0.386\n",
      "Epoch 12743, Loss: 16.123, Final Batch Loss: 0.356\n",
      "Epoch 12744, Loss: 16.386, Final Batch Loss: 0.433\n",
      "Epoch 12745, Loss: 16.481, Final Batch Loss: 0.535\n",
      "Epoch 12746, Loss: 16.788, Final Batch Loss: 0.431\n",
      "Epoch 12747, Loss: 16.027, Final Batch Loss: 0.320\n",
      "Epoch 12748, Loss: 16.256, Final Batch Loss: 0.444\n",
      "Epoch 12749, Loss: 16.491, Final Batch Loss: 0.481\n",
      "Epoch 12750, Loss: 16.562, Final Batch Loss: 0.627\n",
      "Epoch 12751, Loss: 16.509, Final Batch Loss: 0.553\n",
      "Epoch 12752, Loss: 16.486, Final Batch Loss: 0.520\n",
      "Epoch 12753, Loss: 16.597, Final Batch Loss: 0.497\n",
      "Epoch 12754, Loss: 16.455, Final Batch Loss: 0.551\n",
      "Epoch 12755, Loss: 16.575, Final Batch Loss: 0.475\n",
      "Epoch 12756, Loss: 16.472, Final Batch Loss: 0.456\n",
      "Epoch 12757, Loss: 16.605, Final Batch Loss: 0.499\n",
      "Epoch 12758, Loss: 16.335, Final Batch Loss: 0.427\n",
      "Epoch 12759, Loss: 16.756, Final Batch Loss: 0.441\n",
      "Epoch 12760, Loss: 16.525, Final Batch Loss: 0.434\n",
      "Epoch 12761, Loss: 16.416, Final Batch Loss: 0.452\n",
      "Epoch 12762, Loss: 16.515, Final Batch Loss: 0.440\n",
      "Epoch 12763, Loss: 16.599, Final Batch Loss: 0.474\n",
      "Epoch 12764, Loss: 16.156, Final Batch Loss: 0.437\n",
      "Epoch 12765, Loss: 16.121, Final Batch Loss: 0.403\n",
      "Epoch 12766, Loss: 16.199, Final Batch Loss: 0.504\n",
      "Epoch 12767, Loss: 16.546, Final Batch Loss: 0.455\n",
      "Epoch 12768, Loss: 16.567, Final Batch Loss: 0.514\n",
      "Epoch 12769, Loss: 16.330, Final Batch Loss: 0.518\n",
      "Epoch 12770, Loss: 16.512, Final Batch Loss: 0.434\n",
      "Epoch 12771, Loss: 16.597, Final Batch Loss: 0.574\n",
      "Epoch 12772, Loss: 16.375, Final Batch Loss: 0.500\n",
      "Epoch 12773, Loss: 16.349, Final Batch Loss: 0.450\n",
      "Epoch 12774, Loss: 16.679, Final Batch Loss: 0.450\n",
      "Epoch 12775, Loss: 16.214, Final Batch Loss: 0.475\n",
      "Epoch 12776, Loss: 16.337, Final Batch Loss: 0.464\n",
      "Epoch 12777, Loss: 16.632, Final Batch Loss: 0.460\n",
      "Epoch 12778, Loss: 16.355, Final Batch Loss: 0.450\n",
      "Epoch 12779, Loss: 16.445, Final Batch Loss: 0.459\n",
      "Epoch 12780, Loss: 16.267, Final Batch Loss: 0.392\n",
      "Epoch 12781, Loss: 16.109, Final Batch Loss: 0.388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12782, Loss: 16.500, Final Batch Loss: 0.427\n",
      "Epoch 12783, Loss: 16.488, Final Batch Loss: 0.400\n",
      "Epoch 12784, Loss: 16.304, Final Batch Loss: 0.373\n",
      "Epoch 12785, Loss: 16.516, Final Batch Loss: 0.451\n",
      "Epoch 12786, Loss: 16.348, Final Batch Loss: 0.447\n",
      "Epoch 12787, Loss: 16.298, Final Batch Loss: 0.465\n",
      "Epoch 12788, Loss: 16.407, Final Batch Loss: 0.490\n",
      "Epoch 12789, Loss: 16.666, Final Batch Loss: 0.460\n",
      "Epoch 12790, Loss: 16.297, Final Batch Loss: 0.431\n",
      "Epoch 12791, Loss: 16.456, Final Batch Loss: 0.415\n",
      "Epoch 12792, Loss: 16.427, Final Batch Loss: 0.426\n",
      "Epoch 12793, Loss: 16.418, Final Batch Loss: 0.479\n",
      "Epoch 12794, Loss: 16.223, Final Batch Loss: 0.337\n",
      "Epoch 12795, Loss: 16.417, Final Batch Loss: 0.405\n",
      "Epoch 12796, Loss: 16.491, Final Batch Loss: 0.612\n",
      "Epoch 12797, Loss: 16.257, Final Batch Loss: 0.446\n",
      "Epoch 12798, Loss: 16.498, Final Batch Loss: 0.477\n",
      "Epoch 12799, Loss: 16.234, Final Batch Loss: 0.342\n",
      "Epoch 12800, Loss: 16.476, Final Batch Loss: 0.456\n",
      "Epoch 12801, Loss: 16.752, Final Batch Loss: 0.634\n",
      "Epoch 12802, Loss: 16.479, Final Batch Loss: 0.523\n",
      "Epoch 12803, Loss: 16.319, Final Batch Loss: 0.482\n",
      "Epoch 12804, Loss: 16.369, Final Batch Loss: 0.483\n",
      "Epoch 12805, Loss: 16.529, Final Batch Loss: 0.473\n",
      "Epoch 12806, Loss: 16.492, Final Batch Loss: 0.485\n",
      "Epoch 12807, Loss: 16.431, Final Batch Loss: 0.432\n",
      "Epoch 12808, Loss: 16.224, Final Batch Loss: 0.356\n",
      "Epoch 12809, Loss: 16.263, Final Batch Loss: 0.526\n",
      "Epoch 12810, Loss: 16.578, Final Batch Loss: 0.477\n",
      "Epoch 12811, Loss: 16.634, Final Batch Loss: 0.417\n",
      "Epoch 12812, Loss: 16.435, Final Batch Loss: 0.532\n",
      "Epoch 12813, Loss: 16.410, Final Batch Loss: 0.433\n",
      "Epoch 12814, Loss: 16.619, Final Batch Loss: 0.363\n",
      "Epoch 12815, Loss: 16.343, Final Batch Loss: 0.484\n",
      "Epoch 12816, Loss: 16.408, Final Batch Loss: 0.509\n",
      "Epoch 12817, Loss: 16.250, Final Batch Loss: 0.463\n",
      "Epoch 12818, Loss: 16.716, Final Batch Loss: 0.521\n",
      "Epoch 12819, Loss: 16.342, Final Batch Loss: 0.461\n",
      "Epoch 12820, Loss: 16.184, Final Batch Loss: 0.399\n",
      "Epoch 12821, Loss: 16.596, Final Batch Loss: 0.405\n",
      "Epoch 12822, Loss: 16.449, Final Batch Loss: 0.438\n",
      "Epoch 12823, Loss: 16.505, Final Batch Loss: 0.361\n",
      "Epoch 12824, Loss: 16.249, Final Batch Loss: 0.382\n",
      "Epoch 12825, Loss: 16.501, Final Batch Loss: 0.500\n",
      "Epoch 12826, Loss: 16.499, Final Batch Loss: 0.439\n",
      "Epoch 12827, Loss: 16.429, Final Batch Loss: 0.428\n",
      "Epoch 12828, Loss: 16.580, Final Batch Loss: 0.603\n",
      "Epoch 12829, Loss: 16.479, Final Batch Loss: 0.453\n",
      "Epoch 12830, Loss: 16.122, Final Batch Loss: 0.421\n",
      "Epoch 12831, Loss: 16.516, Final Batch Loss: 0.428\n",
      "Epoch 12832, Loss: 16.336, Final Batch Loss: 0.358\n",
      "Epoch 12833, Loss: 16.590, Final Batch Loss: 0.415\n",
      "Epoch 12834, Loss: 16.306, Final Batch Loss: 0.589\n",
      "Epoch 12835, Loss: 16.325, Final Batch Loss: 0.524\n",
      "Epoch 12836, Loss: 16.348, Final Batch Loss: 0.391\n",
      "Epoch 12837, Loss: 16.339, Final Batch Loss: 0.448\n",
      "Epoch 12838, Loss: 16.186, Final Batch Loss: 0.458\n",
      "Epoch 12839, Loss: 16.250, Final Batch Loss: 0.469\n",
      "Epoch 12840, Loss: 16.487, Final Batch Loss: 0.482\n",
      "Epoch 12841, Loss: 16.568, Final Batch Loss: 0.359\n",
      "Epoch 12842, Loss: 16.421, Final Batch Loss: 0.414\n",
      "Epoch 12843, Loss: 16.772, Final Batch Loss: 0.650\n",
      "Epoch 12844, Loss: 16.280, Final Batch Loss: 0.452\n",
      "Epoch 12845, Loss: 16.386, Final Batch Loss: 0.519\n",
      "Epoch 12846, Loss: 16.317, Final Batch Loss: 0.361\n",
      "Epoch 12847, Loss: 16.308, Final Batch Loss: 0.439\n",
      "Epoch 12848, Loss: 16.389, Final Batch Loss: 0.391\n",
      "Epoch 12849, Loss: 16.343, Final Batch Loss: 0.475\n",
      "Epoch 12850, Loss: 16.077, Final Batch Loss: 0.407\n",
      "Epoch 12851, Loss: 16.484, Final Batch Loss: 0.422\n",
      "Epoch 12852, Loss: 16.453, Final Batch Loss: 0.548\n",
      "Epoch 12853, Loss: 16.233, Final Batch Loss: 0.402\n",
      "Epoch 12854, Loss: 16.393, Final Batch Loss: 0.440\n",
      "Epoch 12855, Loss: 16.548, Final Batch Loss: 0.485\n",
      "Epoch 12856, Loss: 16.737, Final Batch Loss: 0.467\n",
      "Epoch 12857, Loss: 16.262, Final Batch Loss: 0.494\n",
      "Epoch 12858, Loss: 16.299, Final Batch Loss: 0.468\n",
      "Epoch 12859, Loss: 16.285, Final Batch Loss: 0.358\n",
      "Epoch 12860, Loss: 16.583, Final Batch Loss: 0.471\n",
      "Epoch 12861, Loss: 16.135, Final Batch Loss: 0.486\n",
      "Epoch 12862, Loss: 16.502, Final Batch Loss: 0.444\n",
      "Epoch 12863, Loss: 16.217, Final Batch Loss: 0.421\n",
      "Epoch 12864, Loss: 16.491, Final Batch Loss: 0.482\n",
      "Epoch 12865, Loss: 16.544, Final Batch Loss: 0.391\n",
      "Epoch 12866, Loss: 16.663, Final Batch Loss: 0.532\n",
      "Epoch 12867, Loss: 16.277, Final Batch Loss: 0.368\n",
      "Epoch 12868, Loss: 16.710, Final Batch Loss: 0.431\n",
      "Epoch 12869, Loss: 16.400, Final Batch Loss: 0.531\n",
      "Epoch 12870, Loss: 16.711, Final Batch Loss: 0.501\n",
      "Epoch 12871, Loss: 16.359, Final Batch Loss: 0.483\n",
      "Epoch 12872, Loss: 16.139, Final Batch Loss: 0.455\n",
      "Epoch 12873, Loss: 16.288, Final Batch Loss: 0.434\n",
      "Epoch 12874, Loss: 16.112, Final Batch Loss: 0.450\n",
      "Epoch 12875, Loss: 16.660, Final Batch Loss: 0.390\n",
      "Epoch 12876, Loss: 16.412, Final Batch Loss: 0.424\n",
      "Epoch 12877, Loss: 16.749, Final Batch Loss: 0.619\n",
      "Epoch 12878, Loss: 16.212, Final Batch Loss: 0.411\n",
      "Epoch 12879, Loss: 16.383, Final Batch Loss: 0.485\n",
      "Epoch 12880, Loss: 16.323, Final Batch Loss: 0.425\n",
      "Epoch 12881, Loss: 16.104, Final Batch Loss: 0.574\n",
      "Epoch 12882, Loss: 16.515, Final Batch Loss: 0.440\n",
      "Epoch 12883, Loss: 16.458, Final Batch Loss: 0.480\n",
      "Epoch 12884, Loss: 16.510, Final Batch Loss: 0.576\n",
      "Epoch 12885, Loss: 16.385, Final Batch Loss: 0.528\n",
      "Epoch 12886, Loss: 16.225, Final Batch Loss: 0.425\n",
      "Epoch 12887, Loss: 16.353, Final Batch Loss: 0.375\n",
      "Epoch 12888, Loss: 16.300, Final Batch Loss: 0.392\n",
      "Epoch 12889, Loss: 16.401, Final Batch Loss: 0.453\n",
      "Epoch 12890, Loss: 16.363, Final Batch Loss: 0.481\n",
      "Epoch 12891, Loss: 16.553, Final Batch Loss: 0.388\n",
      "Epoch 12892, Loss: 16.651, Final Batch Loss: 0.593\n",
      "Epoch 12893, Loss: 16.419, Final Batch Loss: 0.539\n",
      "Epoch 12894, Loss: 16.587, Final Batch Loss: 0.458\n",
      "Epoch 12895, Loss: 16.569, Final Batch Loss: 0.536\n",
      "Epoch 12896, Loss: 16.480, Final Batch Loss: 0.515\n",
      "Epoch 12897, Loss: 16.443, Final Batch Loss: 0.509\n",
      "Epoch 12898, Loss: 16.296, Final Batch Loss: 0.355\n",
      "Epoch 12899, Loss: 16.364, Final Batch Loss: 0.369\n",
      "Epoch 12900, Loss: 16.336, Final Batch Loss: 0.384\n",
      "Epoch 12901, Loss: 16.295, Final Batch Loss: 0.533\n",
      "Epoch 12902, Loss: 16.560, Final Batch Loss: 0.580\n",
      "Epoch 12903, Loss: 16.557, Final Batch Loss: 0.471\n",
      "Epoch 12904, Loss: 16.358, Final Batch Loss: 0.510\n",
      "Epoch 12905, Loss: 16.289, Final Batch Loss: 0.343\n",
      "Epoch 12906, Loss: 16.421, Final Batch Loss: 0.449\n",
      "Epoch 12907, Loss: 16.494, Final Batch Loss: 0.506\n",
      "Epoch 12908, Loss: 16.511, Final Batch Loss: 0.529\n",
      "Epoch 12909, Loss: 16.365, Final Batch Loss: 0.487\n",
      "Epoch 12910, Loss: 16.307, Final Batch Loss: 0.418\n",
      "Epoch 12911, Loss: 16.462, Final Batch Loss: 0.452\n",
      "Epoch 12912, Loss: 16.599, Final Batch Loss: 0.421\n",
      "Epoch 12913, Loss: 16.293, Final Batch Loss: 0.420\n",
      "Epoch 12914, Loss: 16.370, Final Batch Loss: 0.452\n",
      "Epoch 12915, Loss: 16.538, Final Batch Loss: 0.440\n",
      "Epoch 12916, Loss: 16.490, Final Batch Loss: 0.369\n",
      "Epoch 12917, Loss: 16.048, Final Batch Loss: 0.385\n",
      "Epoch 12918, Loss: 16.303, Final Batch Loss: 0.514\n",
      "Epoch 12919, Loss: 16.437, Final Batch Loss: 0.497\n",
      "Epoch 12920, Loss: 16.452, Final Batch Loss: 0.457\n",
      "Epoch 12921, Loss: 16.330, Final Batch Loss: 0.485\n",
      "Epoch 12922, Loss: 16.599, Final Batch Loss: 0.421\n",
      "Epoch 12923, Loss: 16.584, Final Batch Loss: 0.363\n",
      "Epoch 12924, Loss: 16.372, Final Batch Loss: 0.578\n",
      "Epoch 12925, Loss: 16.504, Final Batch Loss: 0.385\n",
      "Epoch 12926, Loss: 16.384, Final Batch Loss: 0.475\n",
      "Epoch 12927, Loss: 16.542, Final Batch Loss: 0.522\n",
      "Epoch 12928, Loss: 16.374, Final Batch Loss: 0.499\n",
      "Epoch 12929, Loss: 16.266, Final Batch Loss: 0.532\n",
      "Epoch 12930, Loss: 16.615, Final Batch Loss: 0.478\n",
      "Epoch 12931, Loss: 16.320, Final Batch Loss: 0.422\n",
      "Epoch 12932, Loss: 16.620, Final Batch Loss: 0.427\n",
      "Epoch 12933, Loss: 16.508, Final Batch Loss: 0.424\n",
      "Epoch 12934, Loss: 16.877, Final Batch Loss: 0.505\n",
      "Epoch 12935, Loss: 16.288, Final Batch Loss: 0.395\n",
      "Epoch 12936, Loss: 16.399, Final Batch Loss: 0.496\n",
      "Epoch 12937, Loss: 16.508, Final Batch Loss: 0.460\n",
      "Epoch 12938, Loss: 16.117, Final Batch Loss: 0.423\n",
      "Epoch 12939, Loss: 16.335, Final Batch Loss: 0.376\n",
      "Epoch 12940, Loss: 16.354, Final Batch Loss: 0.484\n",
      "Epoch 12941, Loss: 16.377, Final Batch Loss: 0.513\n",
      "Epoch 12942, Loss: 16.401, Final Batch Loss: 0.386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12943, Loss: 16.242, Final Batch Loss: 0.350\n",
      "Epoch 12944, Loss: 16.551, Final Batch Loss: 0.537\n",
      "Epoch 12945, Loss: 16.495, Final Batch Loss: 0.519\n",
      "Epoch 12946, Loss: 16.277, Final Batch Loss: 0.417\n",
      "Epoch 12947, Loss: 16.226, Final Batch Loss: 0.365\n",
      "Epoch 12948, Loss: 16.486, Final Batch Loss: 0.404\n",
      "Epoch 12949, Loss: 16.308, Final Batch Loss: 0.355\n",
      "Epoch 12950, Loss: 16.406, Final Batch Loss: 0.612\n",
      "Epoch 12951, Loss: 16.293, Final Batch Loss: 0.514\n",
      "Epoch 12952, Loss: 16.209, Final Batch Loss: 0.419\n",
      "Epoch 12953, Loss: 16.230, Final Batch Loss: 0.426\n",
      "Epoch 12954, Loss: 16.270, Final Batch Loss: 0.365\n",
      "Epoch 12955, Loss: 16.730, Final Batch Loss: 0.527\n",
      "Epoch 12956, Loss: 16.515, Final Batch Loss: 0.425\n",
      "Epoch 12957, Loss: 16.473, Final Batch Loss: 0.534\n",
      "Epoch 12958, Loss: 16.456, Final Batch Loss: 0.389\n",
      "Epoch 12959, Loss: 16.652, Final Batch Loss: 0.390\n",
      "Epoch 12960, Loss: 16.288, Final Batch Loss: 0.515\n",
      "Epoch 12961, Loss: 16.556, Final Batch Loss: 0.380\n",
      "Epoch 12962, Loss: 16.219, Final Batch Loss: 0.432\n",
      "Epoch 12963, Loss: 16.517, Final Batch Loss: 0.408\n",
      "Epoch 12964, Loss: 16.496, Final Batch Loss: 0.420\n",
      "Epoch 12965, Loss: 16.304, Final Batch Loss: 0.501\n",
      "Epoch 12966, Loss: 16.215, Final Batch Loss: 0.463\n",
      "Epoch 12967, Loss: 16.346, Final Batch Loss: 0.444\n",
      "Epoch 12968, Loss: 16.436, Final Batch Loss: 0.392\n",
      "Epoch 12969, Loss: 16.453, Final Batch Loss: 0.477\n",
      "Epoch 12970, Loss: 16.403, Final Batch Loss: 0.483\n",
      "Epoch 12971, Loss: 16.498, Final Batch Loss: 0.468\n",
      "Epoch 12972, Loss: 16.573, Final Batch Loss: 0.411\n",
      "Epoch 12973, Loss: 16.316, Final Batch Loss: 0.462\n",
      "Epoch 12974, Loss: 16.068, Final Batch Loss: 0.420\n",
      "Epoch 12975, Loss: 16.519, Final Batch Loss: 0.458\n",
      "Epoch 12976, Loss: 16.592, Final Batch Loss: 0.369\n",
      "Epoch 12977, Loss: 16.502, Final Batch Loss: 0.421\n",
      "Epoch 12978, Loss: 16.394, Final Batch Loss: 0.435\n",
      "Epoch 12979, Loss: 16.368, Final Batch Loss: 0.473\n",
      "Epoch 12980, Loss: 16.501, Final Batch Loss: 0.554\n",
      "Epoch 12981, Loss: 16.283, Final Batch Loss: 0.444\n",
      "Epoch 12982, Loss: 16.264, Final Batch Loss: 0.405\n",
      "Epoch 12983, Loss: 16.477, Final Batch Loss: 0.513\n",
      "Epoch 12984, Loss: 16.380, Final Batch Loss: 0.491\n",
      "Epoch 12985, Loss: 16.418, Final Batch Loss: 0.478\n",
      "Epoch 12986, Loss: 16.515, Final Batch Loss: 0.476\n",
      "Epoch 12987, Loss: 16.434, Final Batch Loss: 0.414\n",
      "Epoch 12988, Loss: 16.582, Final Batch Loss: 0.538\n",
      "Epoch 12989, Loss: 16.254, Final Batch Loss: 0.485\n",
      "Epoch 12990, Loss: 16.554, Final Batch Loss: 0.497\n",
      "Epoch 12991, Loss: 16.346, Final Batch Loss: 0.457\n",
      "Epoch 12992, Loss: 16.447, Final Batch Loss: 0.403\n",
      "Epoch 12993, Loss: 16.842, Final Batch Loss: 0.605\n",
      "Epoch 12994, Loss: 16.532, Final Batch Loss: 0.582\n",
      "Epoch 12995, Loss: 16.425, Final Batch Loss: 0.393\n",
      "Epoch 12996, Loss: 16.447, Final Batch Loss: 0.401\n",
      "Epoch 12997, Loss: 16.304, Final Batch Loss: 0.514\n",
      "Epoch 12998, Loss: 16.396, Final Batch Loss: 0.434\n",
      "Epoch 12999, Loss: 16.343, Final Batch Loss: 0.439\n",
      "Epoch 13000, Loss: 16.502, Final Batch Loss: 0.470\n",
      "Epoch 13001, Loss: 16.293, Final Batch Loss: 0.417\n",
      "Epoch 13002, Loss: 16.120, Final Batch Loss: 0.398\n",
      "Epoch 13003, Loss: 16.383, Final Batch Loss: 0.552\n",
      "Epoch 13004, Loss: 16.312, Final Batch Loss: 0.428\n",
      "Epoch 13005, Loss: 16.415, Final Batch Loss: 0.476\n",
      "Epoch 13006, Loss: 16.393, Final Batch Loss: 0.380\n",
      "Epoch 13007, Loss: 16.645, Final Batch Loss: 0.504\n",
      "Epoch 13008, Loss: 16.914, Final Batch Loss: 0.623\n",
      "Epoch 13009, Loss: 16.364, Final Batch Loss: 0.543\n",
      "Epoch 13010, Loss: 16.294, Final Batch Loss: 0.414\n",
      "Epoch 13011, Loss: 16.459, Final Batch Loss: 0.405\n",
      "Epoch 13012, Loss: 16.181, Final Batch Loss: 0.560\n",
      "Epoch 13013, Loss: 16.519, Final Batch Loss: 0.609\n",
      "Epoch 13014, Loss: 16.539, Final Batch Loss: 0.460\n",
      "Epoch 13015, Loss: 16.416, Final Batch Loss: 0.487\n",
      "Epoch 13016, Loss: 16.440, Final Batch Loss: 0.464\n",
      "Epoch 13017, Loss: 16.537, Final Batch Loss: 0.460\n",
      "Epoch 13018, Loss: 16.400, Final Batch Loss: 0.353\n",
      "Epoch 13019, Loss: 16.372, Final Batch Loss: 0.368\n",
      "Epoch 13020, Loss: 16.389, Final Batch Loss: 0.363\n",
      "Epoch 13021, Loss: 16.455, Final Batch Loss: 0.527\n",
      "Epoch 13022, Loss: 16.382, Final Batch Loss: 0.469\n",
      "Epoch 13023, Loss: 16.627, Final Batch Loss: 0.376\n",
      "Epoch 13024, Loss: 16.077, Final Batch Loss: 0.363\n",
      "Epoch 13025, Loss: 16.592, Final Batch Loss: 0.501\n",
      "Epoch 13026, Loss: 16.452, Final Batch Loss: 0.463\n",
      "Epoch 13027, Loss: 16.358, Final Batch Loss: 0.602\n",
      "Epoch 13028, Loss: 16.223, Final Batch Loss: 0.519\n",
      "Epoch 13029, Loss: 16.552, Final Batch Loss: 0.527\n",
      "Epoch 13030, Loss: 16.586, Final Batch Loss: 0.405\n",
      "Epoch 13031, Loss: 16.412, Final Batch Loss: 0.450\n",
      "Epoch 13032, Loss: 16.603, Final Batch Loss: 0.546\n",
      "Epoch 13033, Loss: 16.524, Final Batch Loss: 0.568\n",
      "Epoch 13034, Loss: 16.265, Final Batch Loss: 0.506\n",
      "Epoch 13035, Loss: 16.197, Final Batch Loss: 0.434\n",
      "Epoch 13036, Loss: 16.492, Final Batch Loss: 0.403\n",
      "Epoch 13037, Loss: 16.273, Final Batch Loss: 0.430\n",
      "Epoch 13038, Loss: 16.320, Final Batch Loss: 0.458\n",
      "Epoch 13039, Loss: 16.544, Final Batch Loss: 0.478\n",
      "Epoch 13040, Loss: 16.231, Final Batch Loss: 0.421\n",
      "Epoch 13041, Loss: 16.321, Final Batch Loss: 0.566\n",
      "Epoch 13042, Loss: 16.379, Final Batch Loss: 0.485\n",
      "Epoch 13043, Loss: 16.475, Final Batch Loss: 0.475\n",
      "Epoch 13044, Loss: 16.279, Final Batch Loss: 0.430\n",
      "Epoch 13045, Loss: 16.271, Final Batch Loss: 0.580\n",
      "Epoch 13046, Loss: 16.492, Final Batch Loss: 0.471\n",
      "Epoch 13047, Loss: 16.478, Final Batch Loss: 0.354\n",
      "Epoch 13048, Loss: 16.500, Final Batch Loss: 0.443\n",
      "Epoch 13049, Loss: 16.389, Final Batch Loss: 0.491\n",
      "Epoch 13050, Loss: 16.159, Final Batch Loss: 0.414\n",
      "Epoch 13051, Loss: 16.271, Final Batch Loss: 0.491\n",
      "Epoch 13052, Loss: 16.528, Final Batch Loss: 0.478\n",
      "Epoch 13053, Loss: 16.475, Final Batch Loss: 0.408\n",
      "Epoch 13054, Loss: 16.648, Final Batch Loss: 0.514\n",
      "Epoch 13055, Loss: 16.290, Final Batch Loss: 0.506\n",
      "Epoch 13056, Loss: 16.246, Final Batch Loss: 0.432\n",
      "Epoch 13057, Loss: 16.301, Final Batch Loss: 0.495\n",
      "Epoch 13058, Loss: 16.485, Final Batch Loss: 0.462\n",
      "Epoch 13059, Loss: 16.159, Final Batch Loss: 0.433\n",
      "Epoch 13060, Loss: 16.503, Final Batch Loss: 0.414\n",
      "Epoch 13061, Loss: 16.755, Final Batch Loss: 0.539\n",
      "Epoch 13062, Loss: 16.245, Final Batch Loss: 0.546\n",
      "Epoch 13063, Loss: 16.556, Final Batch Loss: 0.521\n",
      "Epoch 13064, Loss: 16.113, Final Batch Loss: 0.450\n",
      "Epoch 13065, Loss: 16.349, Final Batch Loss: 0.353\n",
      "Epoch 13066, Loss: 16.493, Final Batch Loss: 0.468\n",
      "Epoch 13067, Loss: 16.381, Final Batch Loss: 0.450\n",
      "Epoch 13068, Loss: 16.329, Final Batch Loss: 0.465\n",
      "Epoch 13069, Loss: 16.446, Final Batch Loss: 0.488\n",
      "Epoch 13070, Loss: 16.203, Final Batch Loss: 0.397\n",
      "Epoch 13071, Loss: 16.584, Final Batch Loss: 0.653\n",
      "Epoch 13072, Loss: 16.345, Final Batch Loss: 0.334\n",
      "Epoch 13073, Loss: 16.399, Final Batch Loss: 0.420\n",
      "Epoch 13074, Loss: 16.183, Final Batch Loss: 0.465\n",
      "Epoch 13075, Loss: 16.521, Final Batch Loss: 0.437\n",
      "Epoch 13076, Loss: 16.402, Final Batch Loss: 0.368\n",
      "Epoch 13077, Loss: 16.440, Final Batch Loss: 0.447\n",
      "Epoch 13078, Loss: 16.423, Final Batch Loss: 0.387\n",
      "Epoch 13079, Loss: 16.466, Final Batch Loss: 0.485\n",
      "Epoch 13080, Loss: 16.404, Final Batch Loss: 0.406\n",
      "Epoch 13081, Loss: 16.479, Final Batch Loss: 0.534\n",
      "Epoch 13082, Loss: 16.407, Final Batch Loss: 0.406\n",
      "Epoch 13083, Loss: 16.525, Final Batch Loss: 0.381\n",
      "Epoch 13084, Loss: 16.238, Final Batch Loss: 0.363\n",
      "Epoch 13085, Loss: 16.179, Final Batch Loss: 0.383\n",
      "Epoch 13086, Loss: 16.555, Final Batch Loss: 0.517\n",
      "Epoch 13087, Loss: 16.345, Final Batch Loss: 0.504\n",
      "Epoch 13088, Loss: 16.358, Final Batch Loss: 0.394\n",
      "Epoch 13089, Loss: 16.249, Final Batch Loss: 0.387\n",
      "Epoch 13090, Loss: 16.508, Final Batch Loss: 0.443\n",
      "Epoch 13091, Loss: 16.731, Final Batch Loss: 0.436\n",
      "Epoch 13092, Loss: 16.238, Final Batch Loss: 0.390\n",
      "Epoch 13093, Loss: 16.091, Final Batch Loss: 0.401\n",
      "Epoch 13094, Loss: 16.249, Final Batch Loss: 0.490\n",
      "Epoch 13095, Loss: 16.238, Final Batch Loss: 0.446\n",
      "Epoch 13096, Loss: 16.206, Final Batch Loss: 0.364\n",
      "Epoch 13097, Loss: 16.240, Final Batch Loss: 0.430\n",
      "Epoch 13098, Loss: 16.310, Final Batch Loss: 0.462\n",
      "Epoch 13099, Loss: 16.319, Final Batch Loss: 0.479\n",
      "Epoch 13100, Loss: 16.299, Final Batch Loss: 0.451\n",
      "Epoch 13101, Loss: 16.548, Final Batch Loss: 0.395\n",
      "Epoch 13102, Loss: 16.390, Final Batch Loss: 0.381\n",
      "Epoch 13103, Loss: 16.276, Final Batch Loss: 0.358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13104, Loss: 16.618, Final Batch Loss: 0.429\n",
      "Epoch 13105, Loss: 16.259, Final Batch Loss: 0.475\n",
      "Epoch 13106, Loss: 16.286, Final Batch Loss: 0.359\n",
      "Epoch 13107, Loss: 16.206, Final Batch Loss: 0.500\n",
      "Epoch 13108, Loss: 16.378, Final Batch Loss: 0.467\n",
      "Epoch 13109, Loss: 16.438, Final Batch Loss: 0.482\n",
      "Epoch 13110, Loss: 16.367, Final Batch Loss: 0.445\n",
      "Epoch 13111, Loss: 16.531, Final Batch Loss: 0.447\n",
      "Epoch 13112, Loss: 16.499, Final Batch Loss: 0.367\n",
      "Epoch 13113, Loss: 16.418, Final Batch Loss: 0.470\n",
      "Epoch 13114, Loss: 16.561, Final Batch Loss: 0.454\n",
      "Epoch 13115, Loss: 16.194, Final Batch Loss: 0.480\n",
      "Epoch 13116, Loss: 16.329, Final Batch Loss: 0.425\n",
      "Epoch 13117, Loss: 16.349, Final Batch Loss: 0.489\n",
      "Epoch 13118, Loss: 16.334, Final Batch Loss: 0.450\n",
      "Epoch 13119, Loss: 16.389, Final Batch Loss: 0.487\n",
      "Epoch 13120, Loss: 16.242, Final Batch Loss: 0.460\n",
      "Epoch 13121, Loss: 16.340, Final Batch Loss: 0.521\n",
      "Epoch 13122, Loss: 16.052, Final Batch Loss: 0.349\n",
      "Epoch 13123, Loss: 16.085, Final Batch Loss: 0.458\n",
      "Epoch 13124, Loss: 16.408, Final Batch Loss: 0.406\n",
      "Epoch 13125, Loss: 16.772, Final Batch Loss: 0.508\n",
      "Epoch 13126, Loss: 16.253, Final Batch Loss: 0.389\n",
      "Epoch 13127, Loss: 16.390, Final Batch Loss: 0.486\n",
      "Epoch 13128, Loss: 16.246, Final Batch Loss: 0.415\n",
      "Epoch 13129, Loss: 16.463, Final Batch Loss: 0.473\n",
      "Epoch 13130, Loss: 16.254, Final Batch Loss: 0.457\n",
      "Epoch 13131, Loss: 16.256, Final Batch Loss: 0.455\n",
      "Epoch 13132, Loss: 16.417, Final Batch Loss: 0.502\n",
      "Epoch 13133, Loss: 16.170, Final Batch Loss: 0.407\n",
      "Epoch 13134, Loss: 16.500, Final Batch Loss: 0.489\n",
      "Epoch 13135, Loss: 16.727, Final Batch Loss: 0.458\n",
      "Epoch 13136, Loss: 16.148, Final Batch Loss: 0.422\n",
      "Epoch 13137, Loss: 16.453, Final Batch Loss: 0.474\n",
      "Epoch 13138, Loss: 16.491, Final Batch Loss: 0.512\n",
      "Epoch 13139, Loss: 16.372, Final Batch Loss: 0.452\n",
      "Epoch 13140, Loss: 16.053, Final Batch Loss: 0.403\n",
      "Epoch 13141, Loss: 16.517, Final Batch Loss: 0.530\n",
      "Epoch 13142, Loss: 16.331, Final Batch Loss: 0.441\n",
      "Epoch 13143, Loss: 16.438, Final Batch Loss: 0.452\n",
      "Epoch 13144, Loss: 16.638, Final Batch Loss: 0.502\n",
      "Epoch 13145, Loss: 16.316, Final Batch Loss: 0.483\n",
      "Epoch 13146, Loss: 16.393, Final Batch Loss: 0.531\n",
      "Epoch 13147, Loss: 16.607, Final Batch Loss: 0.490\n",
      "Epoch 13148, Loss: 16.235, Final Batch Loss: 0.421\n",
      "Epoch 13149, Loss: 16.251, Final Batch Loss: 0.449\n",
      "Epoch 13150, Loss: 16.203, Final Batch Loss: 0.462\n",
      "Epoch 13151, Loss: 16.513, Final Batch Loss: 0.545\n",
      "Epoch 13152, Loss: 16.618, Final Batch Loss: 0.653\n",
      "Epoch 13153, Loss: 16.402, Final Batch Loss: 0.547\n",
      "Epoch 13154, Loss: 16.399, Final Batch Loss: 0.473\n",
      "Epoch 13155, Loss: 16.409, Final Batch Loss: 0.471\n",
      "Epoch 13156, Loss: 16.341, Final Batch Loss: 0.457\n",
      "Epoch 13157, Loss: 16.267, Final Batch Loss: 0.362\n",
      "Epoch 13158, Loss: 16.413, Final Batch Loss: 0.506\n",
      "Epoch 13159, Loss: 16.477, Final Batch Loss: 0.459\n",
      "Epoch 13160, Loss: 16.225, Final Batch Loss: 0.413\n",
      "Epoch 13161, Loss: 16.426, Final Batch Loss: 0.437\n",
      "Epoch 13162, Loss: 16.055, Final Batch Loss: 0.502\n",
      "Epoch 13163, Loss: 16.251, Final Batch Loss: 0.510\n",
      "Epoch 13164, Loss: 16.538, Final Batch Loss: 0.461\n",
      "Epoch 13165, Loss: 16.570, Final Batch Loss: 0.518\n",
      "Epoch 13166, Loss: 16.236, Final Batch Loss: 0.402\n",
      "Epoch 13167, Loss: 16.224, Final Batch Loss: 0.361\n",
      "Epoch 13168, Loss: 16.599, Final Batch Loss: 0.548\n",
      "Epoch 13169, Loss: 16.485, Final Batch Loss: 0.480\n",
      "Epoch 13170, Loss: 16.472, Final Batch Loss: 0.497\n",
      "Epoch 13171, Loss: 16.237, Final Batch Loss: 0.387\n",
      "Epoch 13172, Loss: 16.460, Final Batch Loss: 0.465\n",
      "Epoch 13173, Loss: 16.295, Final Batch Loss: 0.452\n",
      "Epoch 13174, Loss: 16.145, Final Batch Loss: 0.443\n",
      "Epoch 13175, Loss: 16.091, Final Batch Loss: 0.455\n",
      "Epoch 13176, Loss: 16.454, Final Batch Loss: 0.464\n",
      "Epoch 13177, Loss: 16.157, Final Batch Loss: 0.415\n",
      "Epoch 13178, Loss: 16.557, Final Batch Loss: 0.399\n",
      "Epoch 13179, Loss: 16.305, Final Batch Loss: 0.400\n",
      "Epoch 13180, Loss: 16.258, Final Batch Loss: 0.422\n",
      "Epoch 13181, Loss: 16.378, Final Batch Loss: 0.574\n",
      "Epoch 13182, Loss: 16.628, Final Batch Loss: 0.551\n",
      "Epoch 13183, Loss: 16.426, Final Batch Loss: 0.521\n",
      "Epoch 13184, Loss: 16.366, Final Batch Loss: 0.441\n",
      "Epoch 13185, Loss: 16.515, Final Batch Loss: 0.427\n",
      "Epoch 13186, Loss: 16.341, Final Batch Loss: 0.499\n",
      "Epoch 13187, Loss: 16.320, Final Batch Loss: 0.522\n",
      "Epoch 13188, Loss: 16.574, Final Batch Loss: 0.411\n",
      "Epoch 13189, Loss: 16.295, Final Batch Loss: 0.413\n",
      "Epoch 13190, Loss: 16.256, Final Batch Loss: 0.387\n",
      "Epoch 13191, Loss: 16.318, Final Batch Loss: 0.467\n",
      "Epoch 13192, Loss: 16.316, Final Batch Loss: 0.419\n",
      "Epoch 13193, Loss: 16.155, Final Batch Loss: 0.450\n",
      "Epoch 13194, Loss: 16.347, Final Batch Loss: 0.483\n",
      "Epoch 13195, Loss: 16.344, Final Batch Loss: 0.465\n",
      "Epoch 13196, Loss: 16.330, Final Batch Loss: 0.463\n",
      "Epoch 13197, Loss: 16.264, Final Batch Loss: 0.423\n",
      "Epoch 13198, Loss: 16.212, Final Batch Loss: 0.363\n",
      "Epoch 13199, Loss: 16.176, Final Batch Loss: 0.406\n",
      "Epoch 13200, Loss: 16.517, Final Batch Loss: 0.482\n",
      "Epoch 13201, Loss: 16.336, Final Batch Loss: 0.357\n",
      "Epoch 13202, Loss: 16.383, Final Batch Loss: 0.487\n",
      "Epoch 13203, Loss: 16.167, Final Batch Loss: 0.392\n",
      "Epoch 13204, Loss: 16.308, Final Batch Loss: 0.496\n",
      "Epoch 13205, Loss: 16.245, Final Batch Loss: 0.429\n",
      "Epoch 13206, Loss: 16.444, Final Batch Loss: 0.370\n",
      "Epoch 13207, Loss: 16.474, Final Batch Loss: 0.550\n",
      "Epoch 13208, Loss: 16.612, Final Batch Loss: 0.522\n",
      "Epoch 13209, Loss: 16.276, Final Batch Loss: 0.485\n",
      "Epoch 13210, Loss: 16.416, Final Batch Loss: 0.435\n",
      "Epoch 13211, Loss: 16.081, Final Batch Loss: 0.463\n",
      "Epoch 13212, Loss: 16.232, Final Batch Loss: 0.531\n",
      "Epoch 13213, Loss: 16.494, Final Batch Loss: 0.447\n",
      "Epoch 13214, Loss: 16.497, Final Batch Loss: 0.421\n",
      "Epoch 13215, Loss: 16.288, Final Batch Loss: 0.494\n",
      "Epoch 13216, Loss: 16.461, Final Batch Loss: 0.532\n",
      "Epoch 13217, Loss: 16.420, Final Batch Loss: 0.534\n",
      "Epoch 13218, Loss: 16.349, Final Batch Loss: 0.551\n",
      "Epoch 13219, Loss: 16.444, Final Batch Loss: 0.529\n",
      "Epoch 13220, Loss: 16.364, Final Batch Loss: 0.453\n",
      "Epoch 13221, Loss: 16.429, Final Batch Loss: 0.446\n",
      "Epoch 13222, Loss: 16.324, Final Batch Loss: 0.475\n",
      "Epoch 13223, Loss: 16.219, Final Batch Loss: 0.484\n",
      "Epoch 13224, Loss: 16.286, Final Batch Loss: 0.452\n",
      "Epoch 13225, Loss: 16.324, Final Batch Loss: 0.507\n",
      "Epoch 13226, Loss: 16.550, Final Batch Loss: 0.401\n",
      "Epoch 13227, Loss: 16.604, Final Batch Loss: 0.429\n",
      "Epoch 13228, Loss: 16.345, Final Batch Loss: 0.481\n",
      "Epoch 13229, Loss: 16.416, Final Batch Loss: 0.447\n",
      "Epoch 13230, Loss: 16.241, Final Batch Loss: 0.429\n",
      "Epoch 13231, Loss: 16.445, Final Batch Loss: 0.489\n",
      "Epoch 13232, Loss: 16.438, Final Batch Loss: 0.386\n",
      "Epoch 13233, Loss: 16.248, Final Batch Loss: 0.445\n",
      "Epoch 13234, Loss: 16.366, Final Batch Loss: 0.458\n",
      "Epoch 13235, Loss: 16.442, Final Batch Loss: 0.497\n",
      "Epoch 13236, Loss: 16.219, Final Batch Loss: 0.525\n",
      "Epoch 13237, Loss: 16.370, Final Batch Loss: 0.487\n",
      "Epoch 13238, Loss: 16.657, Final Batch Loss: 0.527\n",
      "Epoch 13239, Loss: 16.480, Final Batch Loss: 0.376\n",
      "Epoch 13240, Loss: 16.187, Final Batch Loss: 0.462\n",
      "Epoch 13241, Loss: 16.367, Final Batch Loss: 0.505\n",
      "Epoch 13242, Loss: 16.284, Final Batch Loss: 0.424\n",
      "Epoch 13243, Loss: 16.432, Final Batch Loss: 0.529\n",
      "Epoch 13244, Loss: 16.420, Final Batch Loss: 0.443\n",
      "Epoch 13245, Loss: 16.169, Final Batch Loss: 0.442\n",
      "Epoch 13246, Loss: 16.539, Final Batch Loss: 0.529\n",
      "Epoch 13247, Loss: 16.161, Final Batch Loss: 0.461\n",
      "Epoch 13248, Loss: 16.337, Final Batch Loss: 0.408\n",
      "Epoch 13249, Loss: 16.546, Final Batch Loss: 0.472\n",
      "Epoch 13250, Loss: 16.248, Final Batch Loss: 0.363\n",
      "Epoch 13251, Loss: 16.384, Final Batch Loss: 0.437\n",
      "Epoch 13252, Loss: 16.383, Final Batch Loss: 0.409\n",
      "Epoch 13253, Loss: 16.209, Final Batch Loss: 0.391\n",
      "Epoch 13254, Loss: 16.394, Final Batch Loss: 0.537\n",
      "Epoch 13255, Loss: 16.433, Final Batch Loss: 0.471\n",
      "Epoch 13256, Loss: 16.340, Final Batch Loss: 0.288\n",
      "Epoch 13257, Loss: 16.362, Final Batch Loss: 0.401\n",
      "Epoch 13258, Loss: 16.481, Final Batch Loss: 0.531\n",
      "Epoch 13259, Loss: 16.161, Final Batch Loss: 0.415\n",
      "Epoch 13260, Loss: 16.507, Final Batch Loss: 0.515\n",
      "Epoch 13261, Loss: 16.410, Final Batch Loss: 0.453\n",
      "Epoch 13262, Loss: 16.382, Final Batch Loss: 0.471\n",
      "Epoch 13263, Loss: 16.423, Final Batch Loss: 0.510\n",
      "Epoch 13264, Loss: 16.464, Final Batch Loss: 0.562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13265, Loss: 16.420, Final Batch Loss: 0.529\n",
      "Epoch 13266, Loss: 16.375, Final Batch Loss: 0.431\n",
      "Epoch 13267, Loss: 16.514, Final Batch Loss: 0.507\n",
      "Epoch 13268, Loss: 16.223, Final Batch Loss: 0.548\n",
      "Epoch 13269, Loss: 16.160, Final Batch Loss: 0.491\n",
      "Epoch 13270, Loss: 16.536, Final Batch Loss: 0.544\n",
      "Epoch 13271, Loss: 16.124, Final Batch Loss: 0.390\n",
      "Epoch 13272, Loss: 16.369, Final Batch Loss: 0.342\n",
      "Epoch 13273, Loss: 16.098, Final Batch Loss: 0.341\n",
      "Epoch 13274, Loss: 16.366, Final Batch Loss: 0.451\n",
      "Epoch 13275, Loss: 16.027, Final Batch Loss: 0.470\n",
      "Epoch 13276, Loss: 16.435, Final Batch Loss: 0.312\n",
      "Epoch 13277, Loss: 16.300, Final Batch Loss: 0.449\n",
      "Epoch 13278, Loss: 16.247, Final Batch Loss: 0.354\n",
      "Epoch 13279, Loss: 16.470, Final Batch Loss: 0.455\n",
      "Epoch 13280, Loss: 16.494, Final Batch Loss: 0.463\n",
      "Epoch 13281, Loss: 16.405, Final Batch Loss: 0.415\n",
      "Epoch 13282, Loss: 16.323, Final Batch Loss: 0.365\n",
      "Epoch 13283, Loss: 16.285, Final Batch Loss: 0.373\n",
      "Epoch 13284, Loss: 16.140, Final Batch Loss: 0.524\n",
      "Epoch 13285, Loss: 16.320, Final Batch Loss: 0.402\n",
      "Epoch 13286, Loss: 16.624, Final Batch Loss: 0.459\n",
      "Epoch 13287, Loss: 16.619, Final Batch Loss: 0.492\n",
      "Epoch 13288, Loss: 16.403, Final Batch Loss: 0.439\n",
      "Epoch 13289, Loss: 16.321, Final Batch Loss: 0.442\n",
      "Epoch 13290, Loss: 16.209, Final Batch Loss: 0.449\n",
      "Epoch 13291, Loss: 15.976, Final Batch Loss: 0.412\n",
      "Epoch 13292, Loss: 16.291, Final Batch Loss: 0.466\n",
      "Epoch 13293, Loss: 16.475, Final Batch Loss: 0.472\n",
      "Epoch 13294, Loss: 16.264, Final Batch Loss: 0.596\n",
      "Epoch 13295, Loss: 16.304, Final Batch Loss: 0.399\n",
      "Epoch 13296, Loss: 16.480, Final Batch Loss: 0.389\n",
      "Epoch 13297, Loss: 16.793, Final Batch Loss: 0.567\n",
      "Epoch 13298, Loss: 16.282, Final Batch Loss: 0.514\n",
      "Epoch 13299, Loss: 16.273, Final Batch Loss: 0.424\n",
      "Epoch 13300, Loss: 16.165, Final Batch Loss: 0.434\n",
      "Epoch 13301, Loss: 16.204, Final Batch Loss: 0.373\n",
      "Epoch 13302, Loss: 16.406, Final Batch Loss: 0.442\n",
      "Epoch 13303, Loss: 16.515, Final Batch Loss: 0.402\n",
      "Epoch 13304, Loss: 16.312, Final Batch Loss: 0.400\n",
      "Epoch 13305, Loss: 16.534, Final Batch Loss: 0.546\n",
      "Epoch 13306, Loss: 16.387, Final Batch Loss: 0.551\n",
      "Epoch 13307, Loss: 16.436, Final Batch Loss: 0.372\n",
      "Epoch 13308, Loss: 16.350, Final Batch Loss: 0.414\n",
      "Epoch 13309, Loss: 16.545, Final Batch Loss: 0.437\n",
      "Epoch 13310, Loss: 16.524, Final Batch Loss: 0.355\n",
      "Epoch 13311, Loss: 16.102, Final Batch Loss: 0.460\n",
      "Epoch 13312, Loss: 16.042, Final Batch Loss: 0.371\n",
      "Epoch 13313, Loss: 16.520, Final Batch Loss: 0.412\n",
      "Epoch 13314, Loss: 16.382, Final Batch Loss: 0.417\n",
      "Epoch 13315, Loss: 16.600, Final Batch Loss: 0.485\n",
      "Epoch 13316, Loss: 16.314, Final Batch Loss: 0.466\n",
      "Epoch 13317, Loss: 16.269, Final Batch Loss: 0.442\n",
      "Epoch 13318, Loss: 16.439, Final Batch Loss: 0.562\n",
      "Epoch 13319, Loss: 16.178, Final Batch Loss: 0.369\n",
      "Epoch 13320, Loss: 16.557, Final Batch Loss: 0.538\n",
      "Epoch 13321, Loss: 16.378, Final Batch Loss: 0.420\n",
      "Epoch 13322, Loss: 16.706, Final Batch Loss: 0.460\n",
      "Epoch 13323, Loss: 16.382, Final Batch Loss: 0.542\n",
      "Epoch 13324, Loss: 16.326, Final Batch Loss: 0.451\n",
      "Epoch 13325, Loss: 16.692, Final Batch Loss: 0.579\n",
      "Epoch 13326, Loss: 16.298, Final Batch Loss: 0.441\n",
      "Epoch 13327, Loss: 16.126, Final Batch Loss: 0.438\n",
      "Epoch 13328, Loss: 16.515, Final Batch Loss: 0.574\n",
      "Epoch 13329, Loss: 16.193, Final Batch Loss: 0.530\n",
      "Epoch 13330, Loss: 16.375, Final Batch Loss: 0.485\n",
      "Epoch 13331, Loss: 16.277, Final Batch Loss: 0.367\n",
      "Epoch 13332, Loss: 16.146, Final Batch Loss: 0.343\n",
      "Epoch 13333, Loss: 16.340, Final Batch Loss: 0.366\n",
      "Epoch 13334, Loss: 16.505, Final Batch Loss: 0.469\n",
      "Epoch 13335, Loss: 16.318, Final Batch Loss: 0.445\n",
      "Epoch 13336, Loss: 16.093, Final Batch Loss: 0.448\n",
      "Epoch 13337, Loss: 16.520, Final Batch Loss: 0.390\n",
      "Epoch 13338, Loss: 16.385, Final Batch Loss: 0.402\n",
      "Epoch 13339, Loss: 16.466, Final Batch Loss: 0.372\n",
      "Epoch 13340, Loss: 16.638, Final Batch Loss: 0.459\n",
      "Epoch 13341, Loss: 16.210, Final Batch Loss: 0.357\n",
      "Epoch 13342, Loss: 16.408, Final Batch Loss: 0.530\n",
      "Epoch 13343, Loss: 16.030, Final Batch Loss: 0.414\n",
      "Epoch 13344, Loss: 16.309, Final Batch Loss: 0.527\n",
      "Epoch 13345, Loss: 16.199, Final Batch Loss: 0.426\n",
      "Epoch 13346, Loss: 16.256, Final Batch Loss: 0.497\n",
      "Epoch 13347, Loss: 16.162, Final Batch Loss: 0.422\n",
      "Epoch 13348, Loss: 16.213, Final Batch Loss: 0.505\n",
      "Epoch 13349, Loss: 16.530, Final Batch Loss: 0.436\n",
      "Epoch 13350, Loss: 16.478, Final Batch Loss: 0.385\n",
      "Epoch 13351, Loss: 16.366, Final Batch Loss: 0.366\n",
      "Epoch 13352, Loss: 16.603, Final Batch Loss: 0.427\n",
      "Epoch 13353, Loss: 16.283, Final Batch Loss: 0.506\n",
      "Epoch 13354, Loss: 16.385, Final Batch Loss: 0.431\n",
      "Epoch 13355, Loss: 16.536, Final Batch Loss: 0.480\n",
      "Epoch 13356, Loss: 16.305, Final Batch Loss: 0.406\n",
      "Epoch 13357, Loss: 16.428, Final Batch Loss: 0.415\n",
      "Epoch 13358, Loss: 16.176, Final Batch Loss: 0.373\n",
      "Epoch 13359, Loss: 16.502, Final Batch Loss: 0.422\n",
      "Epoch 13360, Loss: 16.256, Final Batch Loss: 0.429\n",
      "Epoch 13361, Loss: 16.313, Final Batch Loss: 0.454\n",
      "Epoch 13362, Loss: 16.298, Final Batch Loss: 0.426\n",
      "Epoch 13363, Loss: 16.476, Final Batch Loss: 0.439\n",
      "Epoch 13364, Loss: 16.458, Final Batch Loss: 0.532\n",
      "Epoch 13365, Loss: 16.344, Final Batch Loss: 0.463\n",
      "Epoch 13366, Loss: 16.339, Final Batch Loss: 0.465\n",
      "Epoch 13367, Loss: 15.917, Final Batch Loss: 0.351\n",
      "Epoch 13368, Loss: 16.324, Final Batch Loss: 0.439\n",
      "Epoch 13369, Loss: 16.393, Final Batch Loss: 0.393\n",
      "Epoch 13370, Loss: 16.519, Final Batch Loss: 0.598\n",
      "Epoch 13371, Loss: 16.492, Final Batch Loss: 0.627\n",
      "Epoch 13372, Loss: 16.592, Final Batch Loss: 0.437\n",
      "Epoch 13373, Loss: 16.274, Final Batch Loss: 0.366\n",
      "Epoch 13374, Loss: 16.402, Final Batch Loss: 0.413\n",
      "Epoch 13375, Loss: 16.679, Final Batch Loss: 0.581\n",
      "Epoch 13376, Loss: 16.436, Final Batch Loss: 0.516\n",
      "Epoch 13377, Loss: 16.512, Final Batch Loss: 0.477\n",
      "Epoch 13378, Loss: 16.459, Final Batch Loss: 0.467\n",
      "Epoch 13379, Loss: 16.567, Final Batch Loss: 0.469\n",
      "Epoch 13380, Loss: 16.255, Final Batch Loss: 0.471\n",
      "Epoch 13381, Loss: 16.634, Final Batch Loss: 0.479\n",
      "Epoch 13382, Loss: 16.314, Final Batch Loss: 0.499\n",
      "Epoch 13383, Loss: 16.233, Final Batch Loss: 0.506\n",
      "Epoch 13384, Loss: 16.075, Final Batch Loss: 0.421\n",
      "Epoch 13385, Loss: 16.618, Final Batch Loss: 0.455\n",
      "Epoch 13386, Loss: 16.261, Final Batch Loss: 0.485\n",
      "Epoch 13387, Loss: 16.448, Final Batch Loss: 0.510\n",
      "Epoch 13388, Loss: 16.297, Final Batch Loss: 0.441\n",
      "Epoch 13389, Loss: 16.418, Final Batch Loss: 0.464\n",
      "Epoch 13390, Loss: 16.502, Final Batch Loss: 0.488\n",
      "Epoch 13391, Loss: 16.418, Final Batch Loss: 0.462\n",
      "Epoch 13392, Loss: 16.246, Final Batch Loss: 0.512\n",
      "Epoch 13393, Loss: 16.152, Final Batch Loss: 0.361\n",
      "Epoch 13394, Loss: 16.450, Final Batch Loss: 0.423\n",
      "Epoch 13395, Loss: 16.086, Final Batch Loss: 0.444\n",
      "Epoch 13396, Loss: 16.354, Final Batch Loss: 0.419\n",
      "Epoch 13397, Loss: 16.539, Final Batch Loss: 0.533\n",
      "Epoch 13398, Loss: 16.156, Final Batch Loss: 0.579\n",
      "Epoch 13399, Loss: 16.111, Final Batch Loss: 0.500\n",
      "Epoch 13400, Loss: 16.332, Final Batch Loss: 0.341\n",
      "Epoch 13401, Loss: 16.031, Final Batch Loss: 0.421\n",
      "Epoch 13402, Loss: 16.421, Final Batch Loss: 0.413\n",
      "Epoch 13403, Loss: 16.506, Final Batch Loss: 0.538\n",
      "Epoch 13404, Loss: 16.187, Final Batch Loss: 0.420\n",
      "Epoch 13405, Loss: 16.264, Final Batch Loss: 0.457\n",
      "Epoch 13406, Loss: 15.995, Final Batch Loss: 0.400\n",
      "Epoch 13407, Loss: 16.275, Final Batch Loss: 0.409\n",
      "Epoch 13408, Loss: 16.103, Final Batch Loss: 0.423\n",
      "Epoch 13409, Loss: 16.076, Final Batch Loss: 0.441\n",
      "Epoch 13410, Loss: 16.577, Final Batch Loss: 0.393\n",
      "Epoch 13411, Loss: 16.239, Final Batch Loss: 0.439\n",
      "Epoch 13412, Loss: 16.718, Final Batch Loss: 0.504\n",
      "Epoch 13413, Loss: 16.268, Final Batch Loss: 0.459\n",
      "Epoch 13414, Loss: 16.248, Final Batch Loss: 0.409\n",
      "Epoch 13415, Loss: 16.175, Final Batch Loss: 0.370\n",
      "Epoch 13416, Loss: 16.465, Final Batch Loss: 0.378\n",
      "Epoch 13417, Loss: 16.176, Final Batch Loss: 0.490\n",
      "Epoch 13418, Loss: 15.962, Final Batch Loss: 0.463\n",
      "Epoch 13419, Loss: 16.291, Final Batch Loss: 0.467\n",
      "Epoch 13420, Loss: 16.344, Final Batch Loss: 0.518\n",
      "Epoch 13421, Loss: 16.335, Final Batch Loss: 0.564\n",
      "Epoch 13422, Loss: 16.196, Final Batch Loss: 0.319\n",
      "Epoch 13423, Loss: 16.529, Final Batch Loss: 0.363\n",
      "Epoch 13424, Loss: 16.361, Final Batch Loss: 0.538\n",
      "Epoch 13425, Loss: 16.211, Final Batch Loss: 0.347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13426, Loss: 16.227, Final Batch Loss: 0.390\n",
      "Epoch 13427, Loss: 15.902, Final Batch Loss: 0.497\n",
      "Epoch 13428, Loss: 16.534, Final Batch Loss: 0.558\n",
      "Epoch 13429, Loss: 16.131, Final Batch Loss: 0.415\n",
      "Epoch 13430, Loss: 16.567, Final Batch Loss: 0.572\n",
      "Epoch 13431, Loss: 16.330, Final Batch Loss: 0.378\n",
      "Epoch 13432, Loss: 16.172, Final Batch Loss: 0.384\n",
      "Epoch 13433, Loss: 16.503, Final Batch Loss: 0.401\n",
      "Epoch 13434, Loss: 16.554, Final Batch Loss: 0.390\n",
      "Epoch 13435, Loss: 16.508, Final Batch Loss: 0.371\n",
      "Epoch 13436, Loss: 16.290, Final Batch Loss: 0.426\n",
      "Epoch 13437, Loss: 16.347, Final Batch Loss: 0.461\n",
      "Epoch 13438, Loss: 16.495, Final Batch Loss: 0.343\n",
      "Epoch 13439, Loss: 16.213, Final Batch Loss: 0.462\n",
      "Epoch 13440, Loss: 16.586, Final Batch Loss: 0.480\n",
      "Epoch 13441, Loss: 16.441, Final Batch Loss: 0.396\n",
      "Epoch 13442, Loss: 16.102, Final Batch Loss: 0.406\n",
      "Epoch 13443, Loss: 16.403, Final Batch Loss: 0.417\n",
      "Epoch 13444, Loss: 16.438, Final Batch Loss: 0.481\n",
      "Epoch 13445, Loss: 16.481, Final Batch Loss: 0.338\n",
      "Epoch 13446, Loss: 16.371, Final Batch Loss: 0.431\n",
      "Epoch 13447, Loss: 16.210, Final Batch Loss: 0.440\n",
      "Epoch 13448, Loss: 16.763, Final Batch Loss: 0.780\n",
      "Epoch 13449, Loss: 16.359, Final Batch Loss: 0.351\n",
      "Epoch 13450, Loss: 16.113, Final Batch Loss: 0.410\n",
      "Epoch 13451, Loss: 16.342, Final Batch Loss: 0.442\n",
      "Epoch 13452, Loss: 16.288, Final Batch Loss: 0.382\n",
      "Epoch 13453, Loss: 16.508, Final Batch Loss: 0.403\n",
      "Epoch 13454, Loss: 16.319, Final Batch Loss: 0.434\n",
      "Epoch 13455, Loss: 16.141, Final Batch Loss: 0.534\n",
      "Epoch 13456, Loss: 16.199, Final Batch Loss: 0.390\n",
      "Epoch 13457, Loss: 16.503, Final Batch Loss: 0.503\n",
      "Epoch 13458, Loss: 16.239, Final Batch Loss: 0.484\n",
      "Epoch 13459, Loss: 16.432, Final Batch Loss: 0.406\n",
      "Epoch 13460, Loss: 16.382, Final Batch Loss: 0.494\n",
      "Epoch 13461, Loss: 16.167, Final Batch Loss: 0.400\n",
      "Epoch 13462, Loss: 16.472, Final Batch Loss: 0.515\n",
      "Epoch 13463, Loss: 16.290, Final Batch Loss: 0.415\n",
      "Epoch 13464, Loss: 16.251, Final Batch Loss: 0.389\n",
      "Epoch 13465, Loss: 16.353, Final Batch Loss: 0.485\n",
      "Epoch 13466, Loss: 16.553, Final Batch Loss: 0.474\n",
      "Epoch 13467, Loss: 16.533, Final Batch Loss: 0.454\n",
      "Epoch 13468, Loss: 16.158, Final Batch Loss: 0.554\n",
      "Epoch 13469, Loss: 16.333, Final Batch Loss: 0.412\n",
      "Epoch 13470, Loss: 16.325, Final Batch Loss: 0.417\n",
      "Epoch 13471, Loss: 16.373, Final Batch Loss: 0.388\n",
      "Epoch 13472, Loss: 16.317, Final Batch Loss: 0.502\n",
      "Epoch 13473, Loss: 16.399, Final Batch Loss: 0.439\n",
      "Epoch 13474, Loss: 16.356, Final Batch Loss: 0.518\n",
      "Epoch 13475, Loss: 16.419, Final Batch Loss: 0.358\n",
      "Epoch 13476, Loss: 16.444, Final Batch Loss: 0.562\n",
      "Epoch 13477, Loss: 16.314, Final Batch Loss: 0.537\n",
      "Epoch 13478, Loss: 16.361, Final Batch Loss: 0.517\n",
      "Epoch 13479, Loss: 16.492, Final Batch Loss: 0.468\n",
      "Epoch 13480, Loss: 16.214, Final Batch Loss: 0.327\n",
      "Epoch 13481, Loss: 16.271, Final Batch Loss: 0.436\n",
      "Epoch 13482, Loss: 16.425, Final Batch Loss: 0.405\n",
      "Epoch 13483, Loss: 16.489, Final Batch Loss: 0.371\n",
      "Epoch 13484, Loss: 16.381, Final Batch Loss: 0.373\n",
      "Epoch 13485, Loss: 16.214, Final Batch Loss: 0.457\n",
      "Epoch 13486, Loss: 16.136, Final Batch Loss: 0.462\n",
      "Epoch 13487, Loss: 16.599, Final Batch Loss: 0.525\n",
      "Epoch 13488, Loss: 16.241, Final Batch Loss: 0.416\n",
      "Epoch 13489, Loss: 16.443, Final Batch Loss: 0.383\n",
      "Epoch 13490, Loss: 16.261, Final Batch Loss: 0.555\n",
      "Epoch 13491, Loss: 16.471, Final Batch Loss: 0.454\n",
      "Epoch 13492, Loss: 16.213, Final Batch Loss: 0.471\n",
      "Epoch 13493, Loss: 16.340, Final Batch Loss: 0.459\n",
      "Epoch 13494, Loss: 16.100, Final Batch Loss: 0.477\n",
      "Epoch 13495, Loss: 16.230, Final Batch Loss: 0.385\n",
      "Epoch 13496, Loss: 16.503, Final Batch Loss: 0.454\n",
      "Epoch 13497, Loss: 16.479, Final Batch Loss: 0.401\n",
      "Epoch 13498, Loss: 16.562, Final Batch Loss: 0.390\n",
      "Epoch 13499, Loss: 16.587, Final Batch Loss: 0.343\n",
      "Epoch 13500, Loss: 16.160, Final Batch Loss: 0.445\n",
      "Epoch 13501, Loss: 16.501, Final Batch Loss: 0.436\n",
      "Epoch 13502, Loss: 16.339, Final Batch Loss: 0.490\n",
      "Epoch 13503, Loss: 16.215, Final Batch Loss: 0.338\n",
      "Epoch 13504, Loss: 16.216, Final Batch Loss: 0.462\n",
      "Epoch 13505, Loss: 16.478, Final Batch Loss: 0.472\n",
      "Epoch 13506, Loss: 16.504, Final Batch Loss: 0.443\n",
      "Epoch 13507, Loss: 16.214, Final Batch Loss: 0.482\n",
      "Epoch 13508, Loss: 15.988, Final Batch Loss: 0.466\n",
      "Epoch 13509, Loss: 16.096, Final Batch Loss: 0.396\n",
      "Epoch 13510, Loss: 16.296, Final Batch Loss: 0.498\n",
      "Epoch 13511, Loss: 16.229, Final Batch Loss: 0.592\n",
      "Epoch 13512, Loss: 16.442, Final Batch Loss: 0.474\n",
      "Epoch 13513, Loss: 16.188, Final Batch Loss: 0.457\n",
      "Epoch 13514, Loss: 16.380, Final Batch Loss: 0.410\n",
      "Epoch 13515, Loss: 16.370, Final Batch Loss: 0.416\n",
      "Epoch 13516, Loss: 16.062, Final Batch Loss: 0.407\n",
      "Epoch 13517, Loss: 16.064, Final Batch Loss: 0.485\n",
      "Epoch 13518, Loss: 16.453, Final Batch Loss: 0.483\n",
      "Epoch 13519, Loss: 16.265, Final Batch Loss: 0.514\n",
      "Epoch 13520, Loss: 16.087, Final Batch Loss: 0.444\n",
      "Epoch 13521, Loss: 16.071, Final Batch Loss: 0.437\n",
      "Epoch 13522, Loss: 16.421, Final Batch Loss: 0.479\n",
      "Epoch 13523, Loss: 16.246, Final Batch Loss: 0.454\n",
      "Epoch 13524, Loss: 16.360, Final Batch Loss: 0.508\n",
      "Epoch 13525, Loss: 16.159, Final Batch Loss: 0.415\n",
      "Epoch 13526, Loss: 16.095, Final Batch Loss: 0.444\n",
      "Epoch 13527, Loss: 16.376, Final Batch Loss: 0.360\n",
      "Epoch 13528, Loss: 16.149, Final Batch Loss: 0.391\n",
      "Epoch 13529, Loss: 16.504, Final Batch Loss: 0.469\n",
      "Epoch 13530, Loss: 16.270, Final Batch Loss: 0.482\n",
      "Epoch 13531, Loss: 16.279, Final Batch Loss: 0.438\n",
      "Epoch 13532, Loss: 16.009, Final Batch Loss: 0.427\n",
      "Epoch 13533, Loss: 16.399, Final Batch Loss: 0.467\n",
      "Epoch 13534, Loss: 16.425, Final Batch Loss: 0.453\n",
      "Epoch 13535, Loss: 16.171, Final Batch Loss: 0.431\n",
      "Epoch 13536, Loss: 16.192, Final Batch Loss: 0.568\n",
      "Epoch 13537, Loss: 16.517, Final Batch Loss: 0.595\n",
      "Epoch 13538, Loss: 16.490, Final Batch Loss: 0.586\n",
      "Epoch 13539, Loss: 16.197, Final Batch Loss: 0.345\n",
      "Epoch 13540, Loss: 16.120, Final Batch Loss: 0.377\n",
      "Epoch 13541, Loss: 16.312, Final Batch Loss: 0.434\n",
      "Epoch 13542, Loss: 16.512, Final Batch Loss: 0.456\n",
      "Epoch 13543, Loss: 16.642, Final Batch Loss: 0.324\n",
      "Epoch 13544, Loss: 16.340, Final Batch Loss: 0.342\n",
      "Epoch 13545, Loss: 16.286, Final Batch Loss: 0.426\n",
      "Epoch 13546, Loss: 16.157, Final Batch Loss: 0.399\n",
      "Epoch 13547, Loss: 16.449, Final Batch Loss: 0.437\n",
      "Epoch 13548, Loss: 16.095, Final Batch Loss: 0.445\n",
      "Epoch 13549, Loss: 16.260, Final Batch Loss: 0.429\n",
      "Epoch 13550, Loss: 16.207, Final Batch Loss: 0.470\n",
      "Epoch 13551, Loss: 16.450, Final Batch Loss: 0.523\n",
      "Epoch 13552, Loss: 16.647, Final Batch Loss: 0.449\n",
      "Epoch 13553, Loss: 16.381, Final Batch Loss: 0.388\n",
      "Epoch 13554, Loss: 16.367, Final Batch Loss: 0.444\n",
      "Epoch 13555, Loss: 16.457, Final Batch Loss: 0.495\n",
      "Epoch 13556, Loss: 16.359, Final Batch Loss: 0.441\n",
      "Epoch 13557, Loss: 16.287, Final Batch Loss: 0.396\n",
      "Epoch 13558, Loss: 16.268, Final Batch Loss: 0.425\n",
      "Epoch 13559, Loss: 16.310, Final Batch Loss: 0.444\n",
      "Epoch 13560, Loss: 16.353, Final Batch Loss: 0.510\n",
      "Epoch 13561, Loss: 16.443, Final Batch Loss: 0.571\n",
      "Epoch 13562, Loss: 16.143, Final Batch Loss: 0.500\n",
      "Epoch 13563, Loss: 16.257, Final Batch Loss: 0.432\n",
      "Epoch 13564, Loss: 16.818, Final Batch Loss: 0.617\n",
      "Epoch 13565, Loss: 16.359, Final Batch Loss: 0.478\n",
      "Epoch 13566, Loss: 16.314, Final Batch Loss: 0.494\n",
      "Epoch 13567, Loss: 16.456, Final Batch Loss: 0.513\n",
      "Epoch 13568, Loss: 16.515, Final Batch Loss: 0.551\n",
      "Epoch 13569, Loss: 16.141, Final Batch Loss: 0.388\n",
      "Epoch 13570, Loss: 16.265, Final Batch Loss: 0.483\n",
      "Epoch 13571, Loss: 16.479, Final Batch Loss: 0.382\n",
      "Epoch 13572, Loss: 16.300, Final Batch Loss: 0.473\n",
      "Epoch 13573, Loss: 16.201, Final Batch Loss: 0.481\n",
      "Epoch 13574, Loss: 16.319, Final Batch Loss: 0.432\n",
      "Epoch 13575, Loss: 16.233, Final Batch Loss: 0.434\n",
      "Epoch 13576, Loss: 16.581, Final Batch Loss: 0.387\n",
      "Epoch 13577, Loss: 15.877, Final Batch Loss: 0.383\n",
      "Epoch 13578, Loss: 16.439, Final Batch Loss: 0.465\n",
      "Epoch 13579, Loss: 16.702, Final Batch Loss: 0.420\n",
      "Epoch 13580, Loss: 16.375, Final Batch Loss: 0.397\n",
      "Epoch 13581, Loss: 16.250, Final Batch Loss: 0.430\n",
      "Epoch 13582, Loss: 16.363, Final Batch Loss: 0.414\n",
      "Epoch 13583, Loss: 16.361, Final Batch Loss: 0.381\n",
      "Epoch 13584, Loss: 16.297, Final Batch Loss: 0.405\n",
      "Epoch 13585, Loss: 16.391, Final Batch Loss: 0.506\n",
      "Epoch 13586, Loss: 16.232, Final Batch Loss: 0.489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13587, Loss: 16.283, Final Batch Loss: 0.540\n",
      "Epoch 13588, Loss: 16.531, Final Batch Loss: 0.441\n",
      "Epoch 13589, Loss: 16.161, Final Batch Loss: 0.490\n",
      "Epoch 13590, Loss: 16.461, Final Batch Loss: 0.469\n",
      "Epoch 13591, Loss: 16.432, Final Batch Loss: 0.552\n",
      "Epoch 13592, Loss: 16.033, Final Batch Loss: 0.402\n",
      "Epoch 13593, Loss: 16.030, Final Batch Loss: 0.489\n",
      "Epoch 13594, Loss: 16.319, Final Batch Loss: 0.469\n",
      "Epoch 13595, Loss: 16.181, Final Batch Loss: 0.366\n",
      "Epoch 13596, Loss: 16.468, Final Batch Loss: 0.485\n",
      "Epoch 13597, Loss: 16.462, Final Batch Loss: 0.472\n",
      "Epoch 13598, Loss: 16.015, Final Batch Loss: 0.312\n",
      "Epoch 13599, Loss: 16.523, Final Batch Loss: 0.375\n",
      "Epoch 13600, Loss: 16.318, Final Batch Loss: 0.437\n",
      "Epoch 13601, Loss: 16.073, Final Batch Loss: 0.302\n",
      "Epoch 13602, Loss: 16.516, Final Batch Loss: 0.463\n",
      "Epoch 13603, Loss: 16.095, Final Batch Loss: 0.364\n",
      "Epoch 13604, Loss: 16.298, Final Batch Loss: 0.423\n",
      "Epoch 13605, Loss: 16.543, Final Batch Loss: 0.518\n",
      "Epoch 13606, Loss: 16.659, Final Batch Loss: 0.502\n",
      "Epoch 13607, Loss: 16.121, Final Batch Loss: 0.404\n",
      "Epoch 13608, Loss: 16.307, Final Batch Loss: 0.415\n",
      "Epoch 13609, Loss: 16.347, Final Batch Loss: 0.390\n",
      "Epoch 13610, Loss: 16.123, Final Batch Loss: 0.384\n",
      "Epoch 13611, Loss: 16.067, Final Batch Loss: 0.480\n",
      "Epoch 13612, Loss: 16.265, Final Batch Loss: 0.575\n",
      "Epoch 13613, Loss: 16.056, Final Batch Loss: 0.504\n",
      "Epoch 13614, Loss: 16.665, Final Batch Loss: 0.550\n",
      "Epoch 13615, Loss: 16.360, Final Batch Loss: 0.454\n",
      "Epoch 13616, Loss: 16.308, Final Batch Loss: 0.452\n",
      "Epoch 13617, Loss: 16.328, Final Batch Loss: 0.516\n",
      "Epoch 13618, Loss: 16.094, Final Batch Loss: 0.360\n",
      "Epoch 13619, Loss: 16.047, Final Batch Loss: 0.348\n",
      "Epoch 13620, Loss: 16.007, Final Batch Loss: 0.367\n",
      "Epoch 13621, Loss: 16.231, Final Batch Loss: 0.381\n",
      "Epoch 13622, Loss: 15.979, Final Batch Loss: 0.479\n",
      "Epoch 13623, Loss: 16.347, Final Batch Loss: 0.454\n",
      "Epoch 13624, Loss: 16.182, Final Batch Loss: 0.334\n",
      "Epoch 13625, Loss: 16.641, Final Batch Loss: 0.370\n",
      "Epoch 13626, Loss: 16.172, Final Batch Loss: 0.506\n",
      "Epoch 13627, Loss: 16.262, Final Batch Loss: 0.428\n",
      "Epoch 13628, Loss: 16.483, Final Batch Loss: 0.389\n",
      "Epoch 13629, Loss: 16.167, Final Batch Loss: 0.417\n",
      "Epoch 13630, Loss: 16.243, Final Batch Loss: 0.434\n",
      "Epoch 13631, Loss: 16.244, Final Batch Loss: 0.487\n",
      "Epoch 13632, Loss: 16.159, Final Batch Loss: 0.345\n",
      "Epoch 13633, Loss: 16.522, Final Batch Loss: 0.515\n",
      "Epoch 13634, Loss: 16.293, Final Batch Loss: 0.429\n",
      "Epoch 13635, Loss: 16.218, Final Batch Loss: 0.478\n",
      "Epoch 13636, Loss: 16.060, Final Batch Loss: 0.398\n",
      "Epoch 13637, Loss: 16.431, Final Batch Loss: 0.620\n",
      "Epoch 13638, Loss: 16.092, Final Batch Loss: 0.469\n",
      "Epoch 13639, Loss: 16.105, Final Batch Loss: 0.349\n",
      "Epoch 13640, Loss: 16.510, Final Batch Loss: 0.459\n",
      "Epoch 13641, Loss: 16.495, Final Batch Loss: 0.473\n",
      "Epoch 13642, Loss: 16.082, Final Batch Loss: 0.485\n",
      "Epoch 13643, Loss: 16.275, Final Batch Loss: 0.459\n",
      "Epoch 13644, Loss: 16.474, Final Batch Loss: 0.573\n",
      "Epoch 13645, Loss: 16.147, Final Batch Loss: 0.432\n",
      "Epoch 13646, Loss: 16.362, Final Batch Loss: 0.514\n",
      "Epoch 13647, Loss: 16.118, Final Batch Loss: 0.507\n",
      "Epoch 13648, Loss: 16.104, Final Batch Loss: 0.390\n",
      "Epoch 13649, Loss: 16.528, Final Batch Loss: 0.547\n",
      "Epoch 13650, Loss: 16.486, Final Batch Loss: 0.448\n",
      "Epoch 13651, Loss: 16.236, Final Batch Loss: 0.490\n",
      "Epoch 13652, Loss: 16.362, Final Batch Loss: 0.452\n",
      "Epoch 13653, Loss: 16.365, Final Batch Loss: 0.426\n",
      "Epoch 13654, Loss: 16.158, Final Batch Loss: 0.399\n",
      "Epoch 13655, Loss: 16.117, Final Batch Loss: 0.453\n",
      "Epoch 13656, Loss: 16.431, Final Batch Loss: 0.521\n",
      "Epoch 13657, Loss: 16.224, Final Batch Loss: 0.468\n",
      "Epoch 13658, Loss: 16.305, Final Batch Loss: 0.421\n",
      "Epoch 13659, Loss: 16.692, Final Batch Loss: 0.434\n",
      "Epoch 13660, Loss: 16.233, Final Batch Loss: 0.422\n",
      "Epoch 13661, Loss: 16.221, Final Batch Loss: 0.399\n",
      "Epoch 13662, Loss: 16.158, Final Batch Loss: 0.465\n",
      "Epoch 13663, Loss: 16.269, Final Batch Loss: 0.562\n",
      "Epoch 13664, Loss: 16.393, Final Batch Loss: 0.442\n",
      "Epoch 13665, Loss: 16.495, Final Batch Loss: 0.531\n",
      "Epoch 13666, Loss: 16.363, Final Batch Loss: 0.432\n",
      "Epoch 13667, Loss: 16.186, Final Batch Loss: 0.377\n",
      "Epoch 13668, Loss: 16.347, Final Batch Loss: 0.469\n",
      "Epoch 13669, Loss: 16.351, Final Batch Loss: 0.527\n",
      "Epoch 13670, Loss: 16.384, Final Batch Loss: 0.513\n",
      "Epoch 13671, Loss: 16.243, Final Batch Loss: 0.363\n",
      "Epoch 13672, Loss: 16.418, Final Batch Loss: 0.400\n",
      "Epoch 13673, Loss: 15.989, Final Batch Loss: 0.436\n",
      "Epoch 13674, Loss: 16.240, Final Batch Loss: 0.362\n",
      "Epoch 13675, Loss: 16.443, Final Batch Loss: 0.433\n",
      "Epoch 13676, Loss: 16.359, Final Batch Loss: 0.464\n",
      "Epoch 13677, Loss: 16.183, Final Batch Loss: 0.475\n",
      "Epoch 13678, Loss: 16.513, Final Batch Loss: 0.473\n",
      "Epoch 13679, Loss: 16.324, Final Batch Loss: 0.527\n",
      "Epoch 13680, Loss: 16.050, Final Batch Loss: 0.457\n",
      "Epoch 13681, Loss: 16.164, Final Batch Loss: 0.449\n",
      "Epoch 13682, Loss: 16.135, Final Batch Loss: 0.502\n",
      "Epoch 13683, Loss: 16.346, Final Batch Loss: 0.507\n",
      "Epoch 13684, Loss: 16.295, Final Batch Loss: 0.375\n",
      "Epoch 13685, Loss: 16.347, Final Batch Loss: 0.435\n",
      "Epoch 13686, Loss: 16.156, Final Batch Loss: 0.465\n",
      "Epoch 13687, Loss: 16.340, Final Batch Loss: 0.446\n",
      "Epoch 13688, Loss: 16.340, Final Batch Loss: 0.480\n",
      "Epoch 13689, Loss: 16.463, Final Batch Loss: 0.387\n",
      "Epoch 13690, Loss: 16.201, Final Batch Loss: 0.412\n",
      "Epoch 13691, Loss: 16.523, Final Batch Loss: 0.434\n",
      "Epoch 13692, Loss: 16.248, Final Batch Loss: 0.531\n",
      "Epoch 13693, Loss: 16.487, Final Batch Loss: 0.477\n",
      "Epoch 13694, Loss: 16.226, Final Batch Loss: 0.460\n",
      "Epoch 13695, Loss: 16.325, Final Batch Loss: 0.361\n",
      "Epoch 13696, Loss: 16.130, Final Batch Loss: 0.510\n",
      "Epoch 13697, Loss: 16.545, Final Batch Loss: 0.442\n",
      "Epoch 13698, Loss: 16.338, Final Batch Loss: 0.476\n",
      "Epoch 13699, Loss: 16.267, Final Batch Loss: 0.505\n",
      "Epoch 13700, Loss: 16.065, Final Batch Loss: 0.458\n",
      "Epoch 13701, Loss: 16.439, Final Batch Loss: 0.546\n",
      "Epoch 13702, Loss: 16.372, Final Batch Loss: 0.459\n",
      "Epoch 13703, Loss: 16.257, Final Batch Loss: 0.388\n",
      "Epoch 13704, Loss: 16.481, Final Batch Loss: 0.533\n",
      "Epoch 13705, Loss: 16.216, Final Batch Loss: 0.424\n",
      "Epoch 13706, Loss: 16.406, Final Batch Loss: 0.489\n",
      "Epoch 13707, Loss: 16.377, Final Batch Loss: 0.513\n",
      "Epoch 13708, Loss: 16.389, Final Batch Loss: 0.494\n",
      "Epoch 13709, Loss: 16.303, Final Batch Loss: 0.380\n",
      "Epoch 13710, Loss: 16.261, Final Batch Loss: 0.370\n",
      "Epoch 13711, Loss: 15.912, Final Batch Loss: 0.472\n",
      "Epoch 13712, Loss: 16.561, Final Batch Loss: 0.509\n",
      "Epoch 13713, Loss: 16.217, Final Batch Loss: 0.317\n",
      "Epoch 13714, Loss: 16.185, Final Batch Loss: 0.561\n",
      "Epoch 13715, Loss: 16.306, Final Batch Loss: 0.526\n",
      "Epoch 13716, Loss: 16.321, Final Batch Loss: 0.451\n",
      "Epoch 13717, Loss: 16.184, Final Batch Loss: 0.418\n",
      "Epoch 13718, Loss: 16.374, Final Batch Loss: 0.603\n",
      "Epoch 13719, Loss: 16.349, Final Batch Loss: 0.426\n",
      "Epoch 13720, Loss: 16.196, Final Batch Loss: 0.451\n",
      "Epoch 13721, Loss: 16.180, Final Batch Loss: 0.449\n",
      "Epoch 13722, Loss: 16.319, Final Batch Loss: 0.414\n",
      "Epoch 13723, Loss: 16.245, Final Batch Loss: 0.493\n",
      "Epoch 13724, Loss: 16.381, Final Batch Loss: 0.447\n",
      "Epoch 13725, Loss: 16.103, Final Batch Loss: 0.400\n",
      "Epoch 13726, Loss: 16.246, Final Batch Loss: 0.527\n",
      "Epoch 13727, Loss: 16.250, Final Batch Loss: 0.361\n",
      "Epoch 13728, Loss: 16.364, Final Batch Loss: 0.408\n",
      "Epoch 13729, Loss: 16.440, Final Batch Loss: 0.580\n",
      "Epoch 13730, Loss: 16.459, Final Batch Loss: 0.386\n",
      "Epoch 13731, Loss: 16.137, Final Batch Loss: 0.397\n",
      "Epoch 13732, Loss: 16.290, Final Batch Loss: 0.422\n",
      "Epoch 13733, Loss: 16.257, Final Batch Loss: 0.443\n",
      "Epoch 13734, Loss: 16.117, Final Batch Loss: 0.503\n",
      "Epoch 13735, Loss: 16.099, Final Batch Loss: 0.314\n",
      "Epoch 13736, Loss: 15.995, Final Batch Loss: 0.401\n",
      "Epoch 13737, Loss: 16.211, Final Batch Loss: 0.350\n",
      "Epoch 13738, Loss: 16.676, Final Batch Loss: 0.485\n",
      "Epoch 13739, Loss: 16.239, Final Batch Loss: 0.422\n",
      "Epoch 13740, Loss: 16.428, Final Batch Loss: 0.514\n",
      "Epoch 13741, Loss: 16.361, Final Batch Loss: 0.599\n",
      "Epoch 13742, Loss: 16.246, Final Batch Loss: 0.413\n",
      "Epoch 13743, Loss: 16.260, Final Batch Loss: 0.322\n",
      "Epoch 13744, Loss: 16.402, Final Batch Loss: 0.540\n",
      "Epoch 13745, Loss: 16.473, Final Batch Loss: 0.475\n",
      "Epoch 13746, Loss: 16.251, Final Batch Loss: 0.425\n",
      "Epoch 13747, Loss: 16.379, Final Batch Loss: 0.640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13748, Loss: 16.079, Final Batch Loss: 0.389\n",
      "Epoch 13749, Loss: 16.026, Final Batch Loss: 0.444\n",
      "Epoch 13750, Loss: 15.896, Final Batch Loss: 0.343\n",
      "Epoch 13751, Loss: 16.429, Final Batch Loss: 0.431\n",
      "Epoch 13752, Loss: 16.092, Final Batch Loss: 0.467\n",
      "Epoch 13753, Loss: 16.181, Final Batch Loss: 0.479\n",
      "Epoch 13754, Loss: 16.370, Final Batch Loss: 0.347\n",
      "Epoch 13755, Loss: 16.096, Final Batch Loss: 0.528\n",
      "Epoch 13756, Loss: 16.041, Final Batch Loss: 0.458\n",
      "Epoch 13757, Loss: 16.349, Final Batch Loss: 0.444\n",
      "Epoch 13758, Loss: 16.299, Final Batch Loss: 0.423\n",
      "Epoch 13759, Loss: 16.472, Final Batch Loss: 0.516\n",
      "Epoch 13760, Loss: 16.087, Final Batch Loss: 0.354\n",
      "Epoch 13761, Loss: 16.343, Final Batch Loss: 0.446\n",
      "Epoch 13762, Loss: 15.689, Final Batch Loss: 0.361\n",
      "Epoch 13763, Loss: 16.333, Final Batch Loss: 0.515\n",
      "Epoch 13764, Loss: 16.054, Final Batch Loss: 0.428\n",
      "Epoch 13765, Loss: 16.395, Final Batch Loss: 0.386\n",
      "Epoch 13766, Loss: 16.360, Final Batch Loss: 0.516\n",
      "Epoch 13767, Loss: 16.197, Final Batch Loss: 0.439\n",
      "Epoch 13768, Loss: 16.284, Final Batch Loss: 0.489\n",
      "Epoch 13769, Loss: 16.359, Final Batch Loss: 0.435\n",
      "Epoch 13770, Loss: 16.247, Final Batch Loss: 0.479\n",
      "Epoch 13771, Loss: 16.401, Final Batch Loss: 0.441\n",
      "Epoch 13772, Loss: 16.258, Final Batch Loss: 0.533\n",
      "Epoch 13773, Loss: 15.929, Final Batch Loss: 0.503\n",
      "Epoch 13774, Loss: 16.334, Final Batch Loss: 0.430\n",
      "Epoch 13775, Loss: 16.195, Final Batch Loss: 0.385\n",
      "Epoch 13776, Loss: 16.576, Final Batch Loss: 0.499\n",
      "Epoch 13777, Loss: 16.214, Final Batch Loss: 0.328\n",
      "Epoch 13778, Loss: 16.214, Final Batch Loss: 0.450\n",
      "Epoch 13779, Loss: 16.110, Final Batch Loss: 0.487\n",
      "Epoch 13780, Loss: 16.500, Final Batch Loss: 0.580\n",
      "Epoch 13781, Loss: 16.267, Final Batch Loss: 0.380\n",
      "Epoch 13782, Loss: 16.372, Final Batch Loss: 0.451\n",
      "Epoch 13783, Loss: 16.447, Final Batch Loss: 0.520\n",
      "Epoch 13784, Loss: 16.395, Final Batch Loss: 0.382\n",
      "Epoch 13785, Loss: 16.374, Final Batch Loss: 0.479\n",
      "Epoch 13786, Loss: 16.245, Final Batch Loss: 0.441\n",
      "Epoch 13787, Loss: 16.330, Final Batch Loss: 0.477\n",
      "Epoch 13788, Loss: 16.170, Final Batch Loss: 0.506\n",
      "Epoch 13789, Loss: 16.820, Final Batch Loss: 0.451\n",
      "Epoch 13790, Loss: 16.179, Final Batch Loss: 0.354\n",
      "Epoch 13791, Loss: 16.383, Final Batch Loss: 0.481\n",
      "Epoch 13792, Loss: 16.444, Final Batch Loss: 0.535\n",
      "Epoch 13793, Loss: 16.386, Final Batch Loss: 0.403\n",
      "Epoch 13794, Loss: 16.323, Final Batch Loss: 0.405\n",
      "Epoch 13795, Loss: 16.246, Final Batch Loss: 0.401\n",
      "Epoch 13796, Loss: 16.136, Final Batch Loss: 0.456\n",
      "Epoch 13797, Loss: 16.419, Final Batch Loss: 0.482\n",
      "Epoch 13798, Loss: 16.192, Final Batch Loss: 0.522\n",
      "Epoch 13799, Loss: 16.706, Final Batch Loss: 0.485\n",
      "Epoch 13800, Loss: 16.125, Final Batch Loss: 0.563\n",
      "Epoch 13801, Loss: 16.506, Final Batch Loss: 0.465\n",
      "Epoch 13802, Loss: 16.418, Final Batch Loss: 0.490\n",
      "Epoch 13803, Loss: 16.057, Final Batch Loss: 0.415\n",
      "Epoch 13804, Loss: 16.019, Final Batch Loss: 0.407\n",
      "Epoch 13805, Loss: 16.358, Final Batch Loss: 0.444\n",
      "Epoch 13806, Loss: 16.107, Final Batch Loss: 0.535\n",
      "Epoch 13807, Loss: 16.260, Final Batch Loss: 0.484\n",
      "Epoch 13808, Loss: 16.229, Final Batch Loss: 0.513\n",
      "Epoch 13809, Loss: 16.390, Final Batch Loss: 0.451\n",
      "Epoch 13810, Loss: 16.285, Final Batch Loss: 0.492\n",
      "Epoch 13811, Loss: 16.241, Final Batch Loss: 0.384\n",
      "Epoch 13812, Loss: 16.457, Final Batch Loss: 0.436\n",
      "Epoch 13813, Loss: 16.150, Final Batch Loss: 0.462\n",
      "Epoch 13814, Loss: 16.410, Final Batch Loss: 0.446\n",
      "Epoch 13815, Loss: 16.178, Final Batch Loss: 0.454\n",
      "Epoch 13816, Loss: 16.454, Final Batch Loss: 0.508\n",
      "Epoch 13817, Loss: 16.412, Final Batch Loss: 0.450\n",
      "Epoch 13818, Loss: 16.326, Final Batch Loss: 0.398\n",
      "Epoch 13819, Loss: 16.303, Final Batch Loss: 0.360\n",
      "Epoch 13820, Loss: 16.074, Final Batch Loss: 0.450\n",
      "Epoch 13821, Loss: 16.445, Final Batch Loss: 0.466\n",
      "Epoch 13822, Loss: 16.184, Final Batch Loss: 0.509\n",
      "Epoch 13823, Loss: 16.400, Final Batch Loss: 0.508\n",
      "Epoch 13824, Loss: 16.129, Final Batch Loss: 0.441\n",
      "Epoch 13825, Loss: 16.061, Final Batch Loss: 0.440\n",
      "Epoch 13826, Loss: 16.384, Final Batch Loss: 0.453\n",
      "Epoch 13827, Loss: 16.171, Final Batch Loss: 0.351\n",
      "Epoch 13828, Loss: 16.389, Final Batch Loss: 0.477\n",
      "Epoch 13829, Loss: 16.361, Final Batch Loss: 0.510\n",
      "Epoch 13830, Loss: 16.339, Final Batch Loss: 0.507\n",
      "Epoch 13831, Loss: 16.308, Final Batch Loss: 0.489\n",
      "Epoch 13832, Loss: 16.134, Final Batch Loss: 0.445\n",
      "Epoch 13833, Loss: 16.205, Final Batch Loss: 0.449\n",
      "Epoch 13834, Loss: 16.397, Final Batch Loss: 0.500\n",
      "Epoch 13835, Loss: 15.943, Final Batch Loss: 0.464\n",
      "Epoch 13836, Loss: 16.234, Final Batch Loss: 0.453\n",
      "Epoch 13837, Loss: 16.045, Final Batch Loss: 0.557\n",
      "Epoch 13838, Loss: 16.520, Final Batch Loss: 0.444\n",
      "Epoch 13839, Loss: 16.106, Final Batch Loss: 0.435\n",
      "Epoch 13840, Loss: 16.224, Final Batch Loss: 0.411\n",
      "Epoch 13841, Loss: 16.153, Final Batch Loss: 0.409\n",
      "Epoch 13842, Loss: 16.366, Final Batch Loss: 0.585\n",
      "Epoch 13843, Loss: 16.327, Final Batch Loss: 0.528\n",
      "Epoch 13844, Loss: 16.544, Final Batch Loss: 0.540\n",
      "Epoch 13845, Loss: 16.198, Final Batch Loss: 0.522\n",
      "Epoch 13846, Loss: 16.152, Final Batch Loss: 0.506\n",
      "Epoch 13847, Loss: 16.312, Final Batch Loss: 0.441\n",
      "Epoch 13848, Loss: 16.081, Final Batch Loss: 0.515\n",
      "Epoch 13849, Loss: 16.419, Final Batch Loss: 0.493\n",
      "Epoch 13850, Loss: 16.461, Final Batch Loss: 0.422\n",
      "Epoch 13851, Loss: 16.526, Final Batch Loss: 0.509\n",
      "Epoch 13852, Loss: 16.261, Final Batch Loss: 0.412\n",
      "Epoch 13853, Loss: 16.141, Final Batch Loss: 0.428\n",
      "Epoch 13854, Loss: 16.307, Final Batch Loss: 0.420\n",
      "Epoch 13855, Loss: 16.311, Final Batch Loss: 0.509\n",
      "Epoch 13856, Loss: 16.446, Final Batch Loss: 0.444\n",
      "Epoch 13857, Loss: 16.134, Final Batch Loss: 0.472\n",
      "Epoch 13858, Loss: 16.303, Final Batch Loss: 0.446\n",
      "Epoch 13859, Loss: 16.361, Final Batch Loss: 0.564\n",
      "Epoch 13860, Loss: 16.192, Final Batch Loss: 0.346\n",
      "Epoch 13861, Loss: 16.275, Final Batch Loss: 0.420\n",
      "Epoch 13862, Loss: 16.513, Final Batch Loss: 0.403\n",
      "Epoch 13863, Loss: 16.287, Final Batch Loss: 0.450\n",
      "Epoch 13864, Loss: 16.173, Final Batch Loss: 0.486\n",
      "Epoch 13865, Loss: 16.211, Final Batch Loss: 0.492\n",
      "Epoch 13866, Loss: 16.098, Final Batch Loss: 0.551\n",
      "Epoch 13867, Loss: 16.382, Final Batch Loss: 0.377\n",
      "Epoch 13868, Loss: 16.432, Final Batch Loss: 0.456\n",
      "Epoch 13869, Loss: 16.452, Final Batch Loss: 0.497\n",
      "Epoch 13870, Loss: 16.334, Final Batch Loss: 0.418\n",
      "Epoch 13871, Loss: 16.232, Final Batch Loss: 0.468\n",
      "Epoch 13872, Loss: 16.181, Final Batch Loss: 0.387\n",
      "Epoch 13873, Loss: 16.204, Final Batch Loss: 0.450\n",
      "Epoch 13874, Loss: 16.093, Final Batch Loss: 0.432\n",
      "Epoch 13875, Loss: 16.152, Final Batch Loss: 0.409\n",
      "Epoch 13876, Loss: 16.486, Final Batch Loss: 0.450\n",
      "Epoch 13877, Loss: 16.197, Final Batch Loss: 0.506\n",
      "Epoch 13878, Loss: 16.169, Final Batch Loss: 0.332\n",
      "Epoch 13879, Loss: 16.268, Final Batch Loss: 0.344\n",
      "Epoch 13880, Loss: 16.085, Final Batch Loss: 0.440\n",
      "Epoch 13881, Loss: 16.284, Final Batch Loss: 0.345\n",
      "Epoch 13882, Loss: 16.297, Final Batch Loss: 0.467\n",
      "Epoch 13883, Loss: 16.425, Final Batch Loss: 0.436\n",
      "Epoch 13884, Loss: 16.203, Final Batch Loss: 0.422\n",
      "Epoch 13885, Loss: 16.604, Final Batch Loss: 0.475\n",
      "Epoch 13886, Loss: 16.273, Final Batch Loss: 0.610\n",
      "Epoch 13887, Loss: 16.290, Final Batch Loss: 0.461\n",
      "Epoch 13888, Loss: 16.307, Final Batch Loss: 0.448\n",
      "Epoch 13889, Loss: 16.352, Final Batch Loss: 0.360\n",
      "Epoch 13890, Loss: 16.416, Final Batch Loss: 0.383\n",
      "Epoch 13891, Loss: 16.393, Final Batch Loss: 0.427\n",
      "Epoch 13892, Loss: 16.040, Final Batch Loss: 0.421\n",
      "Epoch 13893, Loss: 16.201, Final Batch Loss: 0.446\n",
      "Epoch 13894, Loss: 16.150, Final Batch Loss: 0.434\n",
      "Epoch 13895, Loss: 16.242, Final Batch Loss: 0.414\n",
      "Epoch 13896, Loss: 16.342, Final Batch Loss: 0.526\n",
      "Epoch 13897, Loss: 16.018, Final Batch Loss: 0.480\n",
      "Epoch 13898, Loss: 16.052, Final Batch Loss: 0.409\n",
      "Epoch 13899, Loss: 16.124, Final Batch Loss: 0.489\n",
      "Epoch 13900, Loss: 16.066, Final Batch Loss: 0.576\n",
      "Epoch 13901, Loss: 16.354, Final Batch Loss: 0.460\n",
      "Epoch 13902, Loss: 16.226, Final Batch Loss: 0.399\n",
      "Epoch 13903, Loss: 16.288, Final Batch Loss: 0.505\n",
      "Epoch 13904, Loss: 16.159, Final Batch Loss: 0.315\n",
      "Epoch 13905, Loss: 16.143, Final Batch Loss: 0.377\n",
      "Epoch 13906, Loss: 16.362, Final Batch Loss: 0.443\n",
      "Epoch 13907, Loss: 16.347, Final Batch Loss: 0.374\n",
      "Epoch 13908, Loss: 16.267, Final Batch Loss: 0.469\n",
      "Epoch 13909, Loss: 16.296, Final Batch Loss: 0.468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13910, Loss: 16.509, Final Batch Loss: 0.427\n",
      "Epoch 13911, Loss: 16.292, Final Batch Loss: 0.451\n",
      "Epoch 13912, Loss: 16.223, Final Batch Loss: 0.388\n",
      "Epoch 13913, Loss: 16.359, Final Batch Loss: 0.528\n",
      "Epoch 13914, Loss: 16.343, Final Batch Loss: 0.482\n",
      "Epoch 13915, Loss: 15.918, Final Batch Loss: 0.411\n",
      "Epoch 13916, Loss: 16.237, Final Batch Loss: 0.562\n",
      "Epoch 13917, Loss: 16.427, Final Batch Loss: 0.509\n",
      "Epoch 13918, Loss: 16.545, Final Batch Loss: 0.504\n",
      "Epoch 13919, Loss: 16.173, Final Batch Loss: 0.604\n",
      "Epoch 13920, Loss: 16.004, Final Batch Loss: 0.428\n",
      "Epoch 13921, Loss: 16.537, Final Batch Loss: 0.498\n",
      "Epoch 13922, Loss: 16.264, Final Batch Loss: 0.416\n",
      "Epoch 13923, Loss: 16.300, Final Batch Loss: 0.476\n",
      "Epoch 13924, Loss: 16.466, Final Batch Loss: 0.481\n",
      "Epoch 13925, Loss: 16.481, Final Batch Loss: 0.500\n",
      "Epoch 13926, Loss: 16.211, Final Batch Loss: 0.390\n",
      "Epoch 13927, Loss: 16.193, Final Batch Loss: 0.369\n",
      "Epoch 13928, Loss: 16.285, Final Batch Loss: 0.378\n",
      "Epoch 13929, Loss: 16.382, Final Batch Loss: 0.390\n",
      "Epoch 13930, Loss: 16.467, Final Batch Loss: 0.497\n",
      "Epoch 13931, Loss: 16.283, Final Batch Loss: 0.491\n",
      "Epoch 13932, Loss: 16.345, Final Batch Loss: 0.356\n",
      "Epoch 13933, Loss: 16.333, Final Batch Loss: 0.484\n",
      "Epoch 13934, Loss: 16.313, Final Batch Loss: 0.519\n",
      "Epoch 13935, Loss: 16.159, Final Batch Loss: 0.410\n",
      "Epoch 13936, Loss: 16.216, Final Batch Loss: 0.376\n",
      "Epoch 13937, Loss: 16.247, Final Batch Loss: 0.445\n",
      "Epoch 13938, Loss: 16.285, Final Batch Loss: 0.479\n",
      "Epoch 13939, Loss: 16.241, Final Batch Loss: 0.428\n",
      "Epoch 13940, Loss: 16.380, Final Batch Loss: 0.447\n",
      "Epoch 13941, Loss: 16.805, Final Batch Loss: 0.512\n",
      "Epoch 13942, Loss: 16.491, Final Batch Loss: 0.396\n",
      "Epoch 13943, Loss: 16.264, Final Batch Loss: 0.454\n",
      "Epoch 13944, Loss: 16.200, Final Batch Loss: 0.488\n",
      "Epoch 13945, Loss: 16.068, Final Batch Loss: 0.485\n",
      "Epoch 13946, Loss: 16.527, Final Batch Loss: 0.477\n",
      "Epoch 13947, Loss: 16.711, Final Batch Loss: 0.403\n",
      "Epoch 13948, Loss: 16.174, Final Batch Loss: 0.440\n",
      "Epoch 13949, Loss: 16.316, Final Batch Loss: 0.527\n",
      "Epoch 13950, Loss: 16.139, Final Batch Loss: 0.404\n",
      "Epoch 13951, Loss: 16.024, Final Batch Loss: 0.443\n",
      "Epoch 13952, Loss: 16.115, Final Batch Loss: 0.366\n",
      "Epoch 13953, Loss: 16.454, Final Batch Loss: 0.447\n",
      "Epoch 13954, Loss: 16.507, Final Batch Loss: 0.495\n",
      "Epoch 13955, Loss: 16.227, Final Batch Loss: 0.391\n",
      "Epoch 13956, Loss: 16.217, Final Batch Loss: 0.429\n",
      "Epoch 13957, Loss: 16.437, Final Batch Loss: 0.421\n",
      "Epoch 13958, Loss: 16.140, Final Batch Loss: 0.415\n",
      "Epoch 13959, Loss: 16.216, Final Batch Loss: 0.424\n",
      "Epoch 13960, Loss: 16.125, Final Batch Loss: 0.488\n",
      "Epoch 13961, Loss: 16.180, Final Batch Loss: 0.540\n",
      "Epoch 13962, Loss: 16.211, Final Batch Loss: 0.365\n",
      "Epoch 13963, Loss: 16.256, Final Batch Loss: 0.483\n",
      "Epoch 13964, Loss: 16.067, Final Batch Loss: 0.377\n",
      "Epoch 13965, Loss: 16.520, Final Batch Loss: 0.528\n",
      "Epoch 13966, Loss: 16.256, Final Batch Loss: 0.515\n",
      "Epoch 13967, Loss: 16.377, Final Batch Loss: 0.505\n",
      "Epoch 13968, Loss: 16.597, Final Batch Loss: 0.459\n",
      "Epoch 13969, Loss: 16.336, Final Batch Loss: 0.505\n",
      "Epoch 13970, Loss: 16.333, Final Batch Loss: 0.422\n",
      "Epoch 13971, Loss: 16.398, Final Batch Loss: 0.427\n",
      "Epoch 13972, Loss: 16.319, Final Batch Loss: 0.463\n",
      "Epoch 13973, Loss: 16.181, Final Batch Loss: 0.519\n",
      "Epoch 13974, Loss: 16.147, Final Batch Loss: 0.515\n",
      "Epoch 13975, Loss: 16.238, Final Batch Loss: 0.415\n",
      "Epoch 13976, Loss: 16.122, Final Batch Loss: 0.456\n",
      "Epoch 13977, Loss: 16.329, Final Batch Loss: 0.343\n",
      "Epoch 13978, Loss: 16.514, Final Batch Loss: 0.379\n",
      "Epoch 13979, Loss: 16.489, Final Batch Loss: 0.427\n",
      "Epoch 13980, Loss: 16.252, Final Batch Loss: 0.489\n",
      "Epoch 13981, Loss: 16.341, Final Batch Loss: 0.407\n",
      "Epoch 13982, Loss: 16.331, Final Batch Loss: 0.421\n",
      "Epoch 13983, Loss: 16.283, Final Batch Loss: 0.392\n",
      "Epoch 13984, Loss: 16.225, Final Batch Loss: 0.416\n",
      "Epoch 13985, Loss: 16.234, Final Batch Loss: 0.419\n",
      "Epoch 13986, Loss: 16.251, Final Batch Loss: 0.417\n",
      "Epoch 13987, Loss: 16.393, Final Batch Loss: 0.338\n",
      "Epoch 13988, Loss: 16.307, Final Batch Loss: 0.624\n",
      "Epoch 13989, Loss: 16.124, Final Batch Loss: 0.437\n",
      "Epoch 13990, Loss: 16.398, Final Batch Loss: 0.364\n",
      "Epoch 13991, Loss: 16.129, Final Batch Loss: 0.463\n",
      "Epoch 13992, Loss: 16.390, Final Batch Loss: 0.434\n",
      "Epoch 13993, Loss: 16.158, Final Batch Loss: 0.450\n",
      "Epoch 13994, Loss: 16.306, Final Batch Loss: 0.371\n",
      "Epoch 13995, Loss: 16.215, Final Batch Loss: 0.419\n",
      "Epoch 13996, Loss: 16.115, Final Batch Loss: 0.373\n",
      "Epoch 13997, Loss: 16.393, Final Batch Loss: 0.340\n",
      "Epoch 13998, Loss: 16.070, Final Batch Loss: 0.427\n",
      "Epoch 13999, Loss: 16.215, Final Batch Loss: 0.427\n",
      "Epoch 14000, Loss: 15.924, Final Batch Loss: 0.382\n",
      "Epoch 14001, Loss: 16.408, Final Batch Loss: 0.502\n",
      "Epoch 14002, Loss: 15.986, Final Batch Loss: 0.459\n",
      "Epoch 14003, Loss: 16.762, Final Batch Loss: 0.455\n",
      "Epoch 14004, Loss: 16.495, Final Batch Loss: 0.330\n",
      "Epoch 14005, Loss: 16.038, Final Batch Loss: 0.334\n",
      "Epoch 14006, Loss: 16.104, Final Batch Loss: 0.396\n",
      "Epoch 14007, Loss: 16.140, Final Batch Loss: 0.416\n",
      "Epoch 14008, Loss: 16.440, Final Batch Loss: 0.548\n",
      "Epoch 14009, Loss: 16.106, Final Batch Loss: 0.436\n",
      "Epoch 14010, Loss: 16.190, Final Batch Loss: 0.542\n",
      "Epoch 14011, Loss: 16.142, Final Batch Loss: 0.381\n",
      "Epoch 14012, Loss: 16.134, Final Batch Loss: 0.458\n",
      "Epoch 14013, Loss: 16.398, Final Batch Loss: 0.590\n",
      "Epoch 14014, Loss: 16.230, Final Batch Loss: 0.479\n",
      "Epoch 14015, Loss: 16.501, Final Batch Loss: 0.486\n",
      "Epoch 14016, Loss: 16.484, Final Batch Loss: 0.443\n",
      "Epoch 14017, Loss: 16.058, Final Batch Loss: 0.387\n",
      "Epoch 14018, Loss: 16.350, Final Batch Loss: 0.603\n",
      "Epoch 14019, Loss: 16.587, Final Batch Loss: 0.374\n",
      "Epoch 14020, Loss: 16.049, Final Batch Loss: 0.475\n",
      "Epoch 14021, Loss: 15.941, Final Batch Loss: 0.454\n",
      "Epoch 14022, Loss: 16.287, Final Batch Loss: 0.468\n",
      "Epoch 14023, Loss: 16.276, Final Batch Loss: 0.447\n",
      "Epoch 14024, Loss: 16.172, Final Batch Loss: 0.401\n",
      "Epoch 14025, Loss: 16.261, Final Batch Loss: 0.470\n",
      "Epoch 14026, Loss: 16.146, Final Batch Loss: 0.413\n",
      "Epoch 14027, Loss: 16.149, Final Batch Loss: 0.534\n",
      "Epoch 14028, Loss: 16.309, Final Batch Loss: 0.397\n",
      "Epoch 14029, Loss: 16.364, Final Batch Loss: 0.490\n",
      "Epoch 14030, Loss: 16.453, Final Batch Loss: 0.416\n",
      "Epoch 14031, Loss: 16.344, Final Batch Loss: 0.451\n",
      "Epoch 14032, Loss: 16.311, Final Batch Loss: 0.431\n",
      "Epoch 14033, Loss: 16.290, Final Batch Loss: 0.411\n",
      "Epoch 14034, Loss: 15.987, Final Batch Loss: 0.498\n",
      "Epoch 14035, Loss: 16.365, Final Batch Loss: 0.460\n",
      "Epoch 14036, Loss: 16.514, Final Batch Loss: 0.385\n",
      "Epoch 14037, Loss: 16.220, Final Batch Loss: 0.492\n",
      "Epoch 14038, Loss: 16.375, Final Batch Loss: 0.488\n",
      "Epoch 14039, Loss: 16.168, Final Batch Loss: 0.453\n",
      "Epoch 14040, Loss: 16.184, Final Batch Loss: 0.398\n",
      "Epoch 14041, Loss: 16.366, Final Batch Loss: 0.509\n",
      "Epoch 14042, Loss: 16.378, Final Batch Loss: 0.363\n",
      "Epoch 14043, Loss: 16.237, Final Batch Loss: 0.429\n",
      "Epoch 14044, Loss: 16.145, Final Batch Loss: 0.375\n",
      "Epoch 14045, Loss: 16.408, Final Batch Loss: 0.436\n",
      "Epoch 14046, Loss: 16.168, Final Batch Loss: 0.386\n",
      "Epoch 14047, Loss: 16.287, Final Batch Loss: 0.478\n",
      "Epoch 14048, Loss: 16.435, Final Batch Loss: 0.384\n",
      "Epoch 14049, Loss: 16.193, Final Batch Loss: 0.425\n",
      "Epoch 14050, Loss: 16.336, Final Batch Loss: 0.485\n",
      "Epoch 14051, Loss: 15.918, Final Batch Loss: 0.336\n",
      "Epoch 14052, Loss: 16.260, Final Batch Loss: 0.399\n",
      "Epoch 14053, Loss: 16.428, Final Batch Loss: 0.420\n",
      "Epoch 14054, Loss: 16.130, Final Batch Loss: 0.459\n",
      "Epoch 14055, Loss: 16.331, Final Batch Loss: 0.407\n",
      "Epoch 14056, Loss: 16.615, Final Batch Loss: 0.418\n",
      "Epoch 14057, Loss: 16.345, Final Batch Loss: 0.435\n",
      "Epoch 14058, Loss: 16.200, Final Batch Loss: 0.452\n",
      "Epoch 14059, Loss: 16.303, Final Batch Loss: 0.454\n",
      "Epoch 14060, Loss: 16.569, Final Batch Loss: 0.461\n",
      "Epoch 14061, Loss: 16.117, Final Batch Loss: 0.511\n",
      "Epoch 14062, Loss: 16.042, Final Batch Loss: 0.445\n",
      "Epoch 14063, Loss: 16.463, Final Batch Loss: 0.424\n",
      "Epoch 14064, Loss: 16.305, Final Batch Loss: 0.431\n",
      "Epoch 14065, Loss: 16.211, Final Batch Loss: 0.537\n",
      "Epoch 14066, Loss: 16.444, Final Batch Loss: 0.451\n",
      "Epoch 14067, Loss: 16.237, Final Batch Loss: 0.565\n",
      "Epoch 14068, Loss: 16.017, Final Batch Loss: 0.384\n",
      "Epoch 14069, Loss: 16.485, Final Batch Loss: 0.459\n",
      "Epoch 14070, Loss: 16.310, Final Batch Loss: 0.477\n",
      "Epoch 14071, Loss: 16.432, Final Batch Loss: 0.431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14072, Loss: 16.449, Final Batch Loss: 0.413\n",
      "Epoch 14073, Loss: 16.546, Final Batch Loss: 0.443\n",
      "Epoch 14074, Loss: 16.337, Final Batch Loss: 0.418\n",
      "Epoch 14075, Loss: 16.289, Final Batch Loss: 0.519\n",
      "Epoch 14076, Loss: 16.292, Final Batch Loss: 0.392\n",
      "Epoch 14077, Loss: 16.338, Final Batch Loss: 0.419\n",
      "Epoch 14078, Loss: 16.617, Final Batch Loss: 0.434\n",
      "Epoch 14079, Loss: 16.145, Final Batch Loss: 0.363\n",
      "Epoch 14080, Loss: 16.305, Final Batch Loss: 0.432\n",
      "Epoch 14081, Loss: 16.267, Final Batch Loss: 0.364\n",
      "Epoch 14082, Loss: 16.170, Final Batch Loss: 0.433\n",
      "Epoch 14083, Loss: 16.216, Final Batch Loss: 0.466\n",
      "Epoch 14084, Loss: 16.174, Final Batch Loss: 0.491\n",
      "Epoch 14085, Loss: 16.452, Final Batch Loss: 0.362\n",
      "Epoch 14086, Loss: 16.674, Final Batch Loss: 0.555\n",
      "Epoch 14087, Loss: 16.360, Final Batch Loss: 0.507\n",
      "Epoch 14088, Loss: 16.376, Final Batch Loss: 0.460\n",
      "Epoch 14089, Loss: 16.256, Final Batch Loss: 0.455\n",
      "Epoch 14090, Loss: 16.367, Final Batch Loss: 0.502\n",
      "Epoch 14091, Loss: 16.056, Final Batch Loss: 0.438\n",
      "Epoch 14092, Loss: 16.405, Final Batch Loss: 0.472\n",
      "Epoch 14093, Loss: 16.337, Final Batch Loss: 0.415\n",
      "Epoch 14094, Loss: 16.502, Final Batch Loss: 0.403\n",
      "Epoch 14095, Loss: 16.455, Final Batch Loss: 0.535\n",
      "Epoch 14096, Loss: 16.107, Final Batch Loss: 0.502\n",
      "Epoch 14097, Loss: 16.468, Final Batch Loss: 0.525\n",
      "Epoch 14098, Loss: 16.076, Final Batch Loss: 0.540\n",
      "Epoch 14099, Loss: 16.591, Final Batch Loss: 0.566\n",
      "Epoch 14100, Loss: 16.383, Final Batch Loss: 0.503\n",
      "Epoch 14101, Loss: 16.153, Final Batch Loss: 0.386\n",
      "Epoch 14102, Loss: 15.849, Final Batch Loss: 0.372\n",
      "Epoch 14103, Loss: 16.341, Final Batch Loss: 0.490\n",
      "Epoch 14104, Loss: 16.022, Final Batch Loss: 0.368\n",
      "Epoch 14105, Loss: 16.058, Final Batch Loss: 0.480\n",
      "Epoch 14106, Loss: 16.013, Final Batch Loss: 0.407\n",
      "Epoch 14107, Loss: 16.324, Final Batch Loss: 0.364\n",
      "Epoch 14108, Loss: 16.696, Final Batch Loss: 0.440\n",
      "Epoch 14109, Loss: 16.186, Final Batch Loss: 0.609\n",
      "Epoch 14110, Loss: 16.236, Final Batch Loss: 0.362\n",
      "Epoch 14111, Loss: 16.176, Final Batch Loss: 0.434\n",
      "Epoch 14112, Loss: 16.258, Final Batch Loss: 0.454\n",
      "Epoch 14113, Loss: 16.084, Final Batch Loss: 0.467\n",
      "Epoch 14114, Loss: 16.248, Final Batch Loss: 0.579\n",
      "Epoch 14115, Loss: 16.277, Final Batch Loss: 0.435\n",
      "Epoch 14116, Loss: 15.868, Final Batch Loss: 0.440\n",
      "Epoch 14117, Loss: 16.427, Final Batch Loss: 0.363\n",
      "Epoch 14118, Loss: 16.158, Final Batch Loss: 0.483\n",
      "Epoch 14119, Loss: 16.397, Final Batch Loss: 0.514\n",
      "Epoch 14120, Loss: 16.282, Final Batch Loss: 0.478\n",
      "Epoch 14121, Loss: 16.405, Final Batch Loss: 0.435\n",
      "Epoch 14122, Loss: 16.262, Final Batch Loss: 0.438\n",
      "Epoch 14123, Loss: 16.271, Final Batch Loss: 0.419\n",
      "Epoch 14124, Loss: 16.169, Final Batch Loss: 0.440\n",
      "Epoch 14125, Loss: 16.500, Final Batch Loss: 0.441\n",
      "Epoch 14126, Loss: 16.219, Final Batch Loss: 0.430\n",
      "Epoch 14127, Loss: 16.394, Final Batch Loss: 0.452\n",
      "Epoch 14128, Loss: 16.233, Final Batch Loss: 0.454\n",
      "Epoch 14129, Loss: 16.140, Final Batch Loss: 0.404\n",
      "Epoch 14130, Loss: 16.195, Final Batch Loss: 0.440\n",
      "Epoch 14131, Loss: 16.024, Final Batch Loss: 0.429\n",
      "Epoch 14132, Loss: 16.376, Final Batch Loss: 0.483\n",
      "Epoch 14133, Loss: 16.186, Final Batch Loss: 0.384\n",
      "Epoch 14134, Loss: 16.364, Final Batch Loss: 0.475\n",
      "Epoch 14135, Loss: 16.003, Final Batch Loss: 0.561\n",
      "Epoch 14136, Loss: 16.337, Final Batch Loss: 0.436\n",
      "Epoch 14137, Loss: 16.278, Final Batch Loss: 0.512\n",
      "Epoch 14138, Loss: 16.249, Final Batch Loss: 0.463\n",
      "Epoch 14139, Loss: 16.359, Final Batch Loss: 0.393\n",
      "Epoch 14140, Loss: 16.303, Final Batch Loss: 0.278\n",
      "Epoch 14141, Loss: 16.355, Final Batch Loss: 0.556\n",
      "Epoch 14142, Loss: 16.026, Final Batch Loss: 0.369\n",
      "Epoch 14143, Loss: 16.185, Final Batch Loss: 0.418\n",
      "Epoch 14144, Loss: 16.082, Final Batch Loss: 0.410\n",
      "Epoch 14145, Loss: 16.406, Final Batch Loss: 0.507\n",
      "Epoch 14146, Loss: 16.378, Final Batch Loss: 0.428\n",
      "Epoch 14147, Loss: 16.188, Final Batch Loss: 0.396\n",
      "Epoch 14148, Loss: 16.290, Final Batch Loss: 0.531\n",
      "Epoch 14149, Loss: 16.578, Final Batch Loss: 0.442\n",
      "Epoch 14150, Loss: 16.463, Final Batch Loss: 0.503\n",
      "Epoch 14151, Loss: 16.443, Final Batch Loss: 0.434\n",
      "Epoch 14152, Loss: 16.139, Final Batch Loss: 0.601\n",
      "Epoch 14153, Loss: 16.156, Final Batch Loss: 0.360\n",
      "Epoch 14154, Loss: 16.641, Final Batch Loss: 0.462\n",
      "Epoch 14155, Loss: 16.300, Final Batch Loss: 0.473\n",
      "Epoch 14156, Loss: 16.432, Final Batch Loss: 0.544\n",
      "Epoch 14157, Loss: 16.335, Final Batch Loss: 0.482\n",
      "Epoch 14158, Loss: 16.394, Final Batch Loss: 0.457\n",
      "Epoch 14159, Loss: 16.188, Final Batch Loss: 0.451\n",
      "Epoch 14160, Loss: 16.320, Final Batch Loss: 0.370\n",
      "Epoch 14161, Loss: 16.161, Final Batch Loss: 0.452\n",
      "Epoch 14162, Loss: 16.175, Final Batch Loss: 0.448\n",
      "Epoch 14163, Loss: 16.380, Final Batch Loss: 0.414\n",
      "Epoch 14164, Loss: 16.264, Final Batch Loss: 0.460\n",
      "Epoch 14165, Loss: 15.990, Final Batch Loss: 0.420\n",
      "Epoch 14166, Loss: 16.172, Final Batch Loss: 0.448\n",
      "Epoch 14167, Loss: 16.280, Final Batch Loss: 0.455\n",
      "Epoch 14168, Loss: 16.334, Final Batch Loss: 0.593\n",
      "Epoch 14169, Loss: 16.130, Final Batch Loss: 0.489\n",
      "Epoch 14170, Loss: 16.382, Final Batch Loss: 0.487\n",
      "Epoch 14171, Loss: 16.254, Final Batch Loss: 0.424\n",
      "Epoch 14172, Loss: 16.290, Final Batch Loss: 0.487\n",
      "Epoch 14173, Loss: 16.151, Final Batch Loss: 0.450\n",
      "Epoch 14174, Loss: 16.264, Final Batch Loss: 0.557\n",
      "Epoch 14175, Loss: 16.057, Final Batch Loss: 0.368\n",
      "Epoch 14176, Loss: 16.219, Final Batch Loss: 0.461\n",
      "Epoch 14177, Loss: 16.570, Final Batch Loss: 0.414\n",
      "Epoch 14178, Loss: 16.329, Final Batch Loss: 0.452\n",
      "Epoch 14179, Loss: 16.366, Final Batch Loss: 0.431\n",
      "Epoch 14180, Loss: 16.246, Final Batch Loss: 0.515\n",
      "Epoch 14181, Loss: 16.258, Final Batch Loss: 0.474\n",
      "Epoch 14182, Loss: 16.103, Final Batch Loss: 0.468\n",
      "Epoch 14183, Loss: 16.124, Final Batch Loss: 0.456\n",
      "Epoch 14184, Loss: 16.274, Final Batch Loss: 0.533\n",
      "Epoch 14185, Loss: 16.144, Final Batch Loss: 0.522\n",
      "Epoch 14186, Loss: 16.008, Final Batch Loss: 0.510\n",
      "Epoch 14187, Loss: 16.179, Final Batch Loss: 0.384\n",
      "Epoch 14188, Loss: 16.174, Final Batch Loss: 0.537\n",
      "Epoch 14189, Loss: 16.167, Final Batch Loss: 0.426\n",
      "Epoch 14190, Loss: 16.537, Final Batch Loss: 0.491\n",
      "Epoch 14191, Loss: 16.102, Final Batch Loss: 0.367\n",
      "Epoch 14192, Loss: 16.394, Final Batch Loss: 0.476\n",
      "Epoch 14193, Loss: 16.440, Final Batch Loss: 0.559\n",
      "Epoch 14194, Loss: 16.131, Final Batch Loss: 0.470\n",
      "Epoch 14195, Loss: 16.460, Final Batch Loss: 0.442\n",
      "Epoch 14196, Loss: 16.143, Final Batch Loss: 0.387\n",
      "Epoch 14197, Loss: 16.244, Final Batch Loss: 0.417\n",
      "Epoch 14198, Loss: 16.552, Final Batch Loss: 0.543\n",
      "Epoch 14199, Loss: 16.715, Final Batch Loss: 0.585\n",
      "Epoch 14200, Loss: 16.325, Final Batch Loss: 0.422\n",
      "Epoch 14201, Loss: 16.360, Final Batch Loss: 0.488\n",
      "Epoch 14202, Loss: 16.165, Final Batch Loss: 0.416\n",
      "Epoch 14203, Loss: 16.118, Final Batch Loss: 0.410\n",
      "Epoch 14204, Loss: 16.092, Final Batch Loss: 0.437\n",
      "Epoch 14205, Loss: 16.042, Final Batch Loss: 0.411\n",
      "Epoch 14206, Loss: 16.351, Final Batch Loss: 0.454\n",
      "Epoch 14207, Loss: 16.246, Final Batch Loss: 0.465\n",
      "Epoch 14208, Loss: 16.222, Final Batch Loss: 0.561\n",
      "Epoch 14209, Loss: 16.280, Final Batch Loss: 0.438\n",
      "Epoch 14210, Loss: 16.370, Final Batch Loss: 0.551\n",
      "Epoch 14211, Loss: 16.418, Final Batch Loss: 0.444\n",
      "Epoch 14212, Loss: 16.017, Final Batch Loss: 0.443\n",
      "Epoch 14213, Loss: 16.171, Final Batch Loss: 0.496\n",
      "Epoch 14214, Loss: 16.267, Final Batch Loss: 0.454\n",
      "Epoch 14215, Loss: 16.151, Final Batch Loss: 0.497\n",
      "Epoch 14216, Loss: 16.206, Final Batch Loss: 0.391\n",
      "Epoch 14217, Loss: 16.208, Final Batch Loss: 0.520\n",
      "Epoch 14218, Loss: 16.072, Final Batch Loss: 0.456\n",
      "Epoch 14219, Loss: 16.110, Final Batch Loss: 0.378\n",
      "Epoch 14220, Loss: 16.338, Final Batch Loss: 0.419\n",
      "Epoch 14221, Loss: 16.403, Final Batch Loss: 0.446\n",
      "Epoch 14222, Loss: 16.329, Final Batch Loss: 0.576\n",
      "Epoch 14223, Loss: 16.242, Final Batch Loss: 0.459\n",
      "Epoch 14224, Loss: 16.218, Final Batch Loss: 0.353\n",
      "Epoch 14225, Loss: 16.318, Final Batch Loss: 0.449\n",
      "Epoch 14226, Loss: 16.888, Final Batch Loss: 0.404\n",
      "Epoch 14227, Loss: 16.306, Final Batch Loss: 0.450\n",
      "Epoch 14228, Loss: 16.078, Final Batch Loss: 0.380\n",
      "Epoch 14229, Loss: 16.063, Final Batch Loss: 0.414\n",
      "Epoch 14230, Loss: 16.418, Final Batch Loss: 0.501\n",
      "Epoch 14231, Loss: 15.987, Final Batch Loss: 0.544\n",
      "Epoch 14232, Loss: 16.279, Final Batch Loss: 0.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14233, Loss: 16.388, Final Batch Loss: 0.556\n",
      "Epoch 14234, Loss: 16.122, Final Batch Loss: 0.428\n",
      "Epoch 14235, Loss: 16.362, Final Batch Loss: 0.539\n",
      "Epoch 14236, Loss: 16.256, Final Batch Loss: 0.434\n",
      "Epoch 14237, Loss: 16.267, Final Batch Loss: 0.414\n",
      "Epoch 14238, Loss: 16.140, Final Batch Loss: 0.422\n",
      "Epoch 14239, Loss: 16.069, Final Batch Loss: 0.567\n",
      "Epoch 14240, Loss: 16.059, Final Batch Loss: 0.543\n",
      "Epoch 14241, Loss: 16.696, Final Batch Loss: 0.408\n",
      "Epoch 14242, Loss: 16.073, Final Batch Loss: 0.460\n",
      "Epoch 14243, Loss: 16.040, Final Batch Loss: 0.477\n",
      "Epoch 14244, Loss: 16.607, Final Batch Loss: 0.567\n",
      "Epoch 14245, Loss: 16.222, Final Batch Loss: 0.552\n",
      "Epoch 14246, Loss: 16.210, Final Batch Loss: 0.534\n",
      "Epoch 14247, Loss: 16.222, Final Batch Loss: 0.438\n",
      "Epoch 14248, Loss: 16.273, Final Batch Loss: 0.364\n",
      "Epoch 14249, Loss: 16.194, Final Batch Loss: 0.442\n",
      "Epoch 14250, Loss: 16.125, Final Batch Loss: 0.466\n",
      "Epoch 14251, Loss: 16.344, Final Batch Loss: 0.489\n",
      "Epoch 14252, Loss: 16.341, Final Batch Loss: 0.497\n",
      "Epoch 14253, Loss: 16.122, Final Batch Loss: 0.428\n",
      "Epoch 14254, Loss: 16.204, Final Batch Loss: 0.440\n",
      "Epoch 14255, Loss: 16.237, Final Batch Loss: 0.489\n",
      "Epoch 14256, Loss: 16.285, Final Batch Loss: 0.414\n",
      "Epoch 14257, Loss: 16.303, Final Batch Loss: 0.411\n",
      "Epoch 14258, Loss: 16.261, Final Batch Loss: 0.479\n",
      "Epoch 14259, Loss: 16.344, Final Batch Loss: 0.379\n",
      "Epoch 14260, Loss: 16.221, Final Batch Loss: 0.432\n",
      "Epoch 14261, Loss: 16.166, Final Batch Loss: 0.421\n",
      "Epoch 14262, Loss: 16.572, Final Batch Loss: 0.475\n",
      "Epoch 14263, Loss: 16.477, Final Batch Loss: 0.420\n",
      "Epoch 14264, Loss: 16.198, Final Batch Loss: 0.509\n",
      "Epoch 14265, Loss: 16.142, Final Batch Loss: 0.464\n",
      "Epoch 14266, Loss: 16.396, Final Batch Loss: 0.421\n",
      "Epoch 14267, Loss: 16.212, Final Batch Loss: 0.368\n",
      "Epoch 14268, Loss: 16.243, Final Batch Loss: 0.492\n",
      "Epoch 14269, Loss: 16.076, Final Batch Loss: 0.357\n",
      "Epoch 14270, Loss: 16.031, Final Batch Loss: 0.437\n",
      "Epoch 14271, Loss: 16.211, Final Batch Loss: 0.503\n",
      "Epoch 14272, Loss: 16.057, Final Batch Loss: 0.464\n",
      "Epoch 14273, Loss: 16.265, Final Batch Loss: 0.514\n",
      "Epoch 14274, Loss: 16.527, Final Batch Loss: 0.486\n",
      "Epoch 14275, Loss: 16.081, Final Batch Loss: 0.456\n",
      "Epoch 14276, Loss: 16.230, Final Batch Loss: 0.390\n",
      "Epoch 14277, Loss: 16.129, Final Batch Loss: 0.495\n",
      "Epoch 14278, Loss: 16.378, Final Batch Loss: 0.498\n",
      "Epoch 14279, Loss: 16.080, Final Batch Loss: 0.427\n",
      "Epoch 14280, Loss: 16.407, Final Batch Loss: 0.397\n",
      "Epoch 14281, Loss: 16.366, Final Batch Loss: 0.404\n",
      "Epoch 14282, Loss: 16.129, Final Batch Loss: 0.553\n",
      "Epoch 14283, Loss: 16.125, Final Batch Loss: 0.415\n",
      "Epoch 14284, Loss: 16.575, Final Batch Loss: 0.512\n",
      "Epoch 14285, Loss: 16.173, Final Batch Loss: 0.370\n",
      "Epoch 14286, Loss: 16.350, Final Batch Loss: 0.476\n",
      "Epoch 14287, Loss: 16.335, Final Batch Loss: 0.385\n",
      "Epoch 14288, Loss: 16.198, Final Batch Loss: 0.336\n",
      "Epoch 14289, Loss: 16.129, Final Batch Loss: 0.434\n",
      "Epoch 14290, Loss: 16.153, Final Batch Loss: 0.460\n",
      "Epoch 14291, Loss: 16.425, Final Batch Loss: 0.519\n",
      "Epoch 14292, Loss: 16.156, Final Batch Loss: 0.442\n",
      "Epoch 14293, Loss: 16.123, Final Batch Loss: 0.439\n",
      "Epoch 14294, Loss: 16.559, Final Batch Loss: 0.352\n",
      "Epoch 14295, Loss: 16.248, Final Batch Loss: 0.464\n",
      "Epoch 14296, Loss: 16.136, Final Batch Loss: 0.399\n",
      "Epoch 14297, Loss: 16.408, Final Batch Loss: 0.444\n",
      "Epoch 14298, Loss: 16.309, Final Batch Loss: 0.431\n",
      "Epoch 14299, Loss: 16.350, Final Batch Loss: 0.434\n",
      "Epoch 14300, Loss: 16.315, Final Batch Loss: 0.407\n",
      "Epoch 14301, Loss: 16.182, Final Batch Loss: 0.399\n",
      "Epoch 14302, Loss: 16.317, Final Batch Loss: 0.567\n",
      "Epoch 14303, Loss: 15.933, Final Batch Loss: 0.440\n",
      "Epoch 14304, Loss: 16.324, Final Batch Loss: 0.390\n",
      "Epoch 14305, Loss: 16.340, Final Batch Loss: 0.382\n",
      "Epoch 14306, Loss: 16.374, Final Batch Loss: 0.439\n",
      "Epoch 14307, Loss: 16.061, Final Batch Loss: 0.371\n",
      "Epoch 14308, Loss: 16.480, Final Batch Loss: 0.668\n",
      "Epoch 14309, Loss: 16.213, Final Batch Loss: 0.471\n",
      "Epoch 14310, Loss: 16.532, Final Batch Loss: 0.450\n",
      "Epoch 14311, Loss: 16.003, Final Batch Loss: 0.392\n",
      "Epoch 14312, Loss: 16.485, Final Batch Loss: 0.611\n",
      "Epoch 14313, Loss: 16.162, Final Batch Loss: 0.516\n",
      "Epoch 14314, Loss: 16.185, Final Batch Loss: 0.494\n",
      "Epoch 14315, Loss: 16.299, Final Batch Loss: 0.435\n",
      "Epoch 14316, Loss: 16.278, Final Batch Loss: 0.325\n",
      "Epoch 14317, Loss: 16.409, Final Batch Loss: 0.419\n",
      "Epoch 14318, Loss: 16.121, Final Batch Loss: 0.420\n",
      "Epoch 14319, Loss: 16.137, Final Batch Loss: 0.371\n",
      "Epoch 14320, Loss: 16.323, Final Batch Loss: 0.455\n",
      "Epoch 14321, Loss: 16.475, Final Batch Loss: 0.501\n",
      "Epoch 14322, Loss: 16.330, Final Batch Loss: 0.566\n",
      "Epoch 14323, Loss: 16.284, Final Batch Loss: 0.444\n",
      "Epoch 14324, Loss: 16.469, Final Batch Loss: 0.413\n",
      "Epoch 14325, Loss: 16.316, Final Batch Loss: 0.395\n",
      "Epoch 14326, Loss: 16.426, Final Batch Loss: 0.540\n",
      "Epoch 14327, Loss: 16.302, Final Batch Loss: 0.430\n",
      "Epoch 14328, Loss: 16.421, Final Batch Loss: 0.525\n",
      "Epoch 14329, Loss: 16.395, Final Batch Loss: 0.466\n",
      "Epoch 14330, Loss: 16.367, Final Batch Loss: 0.405\n",
      "Epoch 14331, Loss: 16.094, Final Batch Loss: 0.459\n",
      "Epoch 14332, Loss: 16.381, Final Batch Loss: 0.414\n",
      "Epoch 14333, Loss: 16.066, Final Batch Loss: 0.451\n",
      "Epoch 14334, Loss: 16.043, Final Batch Loss: 0.577\n",
      "Epoch 14335, Loss: 16.285, Final Batch Loss: 0.504\n",
      "Epoch 14336, Loss: 16.073, Final Batch Loss: 0.368\n",
      "Epoch 14337, Loss: 16.284, Final Batch Loss: 0.410\n",
      "Epoch 14338, Loss: 16.454, Final Batch Loss: 0.392\n",
      "Epoch 14339, Loss: 16.054, Final Batch Loss: 0.442\n",
      "Epoch 14340, Loss: 16.173, Final Batch Loss: 0.483\n",
      "Epoch 14341, Loss: 16.247, Final Batch Loss: 0.456\n",
      "Epoch 14342, Loss: 16.312, Final Batch Loss: 0.434\n",
      "Epoch 14343, Loss: 16.363, Final Batch Loss: 0.406\n",
      "Epoch 14344, Loss: 16.259, Final Batch Loss: 0.469\n",
      "Epoch 14345, Loss: 16.275, Final Batch Loss: 0.454\n",
      "Epoch 14346, Loss: 16.216, Final Batch Loss: 0.416\n",
      "Epoch 14347, Loss: 16.367, Final Batch Loss: 0.401\n",
      "Epoch 14348, Loss: 16.312, Final Batch Loss: 0.386\n",
      "Epoch 14349, Loss: 16.164, Final Batch Loss: 0.494\n",
      "Epoch 14350, Loss: 16.083, Final Batch Loss: 0.391\n",
      "Epoch 14351, Loss: 16.202, Final Batch Loss: 0.414\n",
      "Epoch 14352, Loss: 16.220, Final Batch Loss: 0.405\n",
      "Epoch 14353, Loss: 16.291, Final Batch Loss: 0.465\n",
      "Epoch 14354, Loss: 16.186, Final Batch Loss: 0.409\n",
      "Epoch 14355, Loss: 16.055, Final Batch Loss: 0.278\n",
      "Epoch 14356, Loss: 16.408, Final Batch Loss: 0.440\n",
      "Epoch 14357, Loss: 16.067, Final Batch Loss: 0.503\n",
      "Epoch 14358, Loss: 16.507, Final Batch Loss: 0.481\n",
      "Epoch 14359, Loss: 16.106, Final Batch Loss: 0.573\n",
      "Epoch 14360, Loss: 16.490, Final Batch Loss: 0.486\n",
      "Epoch 14361, Loss: 16.213, Final Batch Loss: 0.410\n",
      "Epoch 14362, Loss: 16.314, Final Batch Loss: 0.414\n",
      "Epoch 14363, Loss: 15.945, Final Batch Loss: 0.440\n",
      "Epoch 14364, Loss: 16.163, Final Batch Loss: 0.519\n",
      "Epoch 14365, Loss: 16.290, Final Batch Loss: 0.435\n",
      "Epoch 14366, Loss: 16.076, Final Batch Loss: 0.552\n",
      "Epoch 14367, Loss: 16.459, Final Batch Loss: 0.373\n",
      "Epoch 14368, Loss: 16.304, Final Batch Loss: 0.497\n",
      "Epoch 14369, Loss: 16.159, Final Batch Loss: 0.548\n",
      "Epoch 14370, Loss: 16.068, Final Batch Loss: 0.467\n",
      "Epoch 14371, Loss: 16.345, Final Batch Loss: 0.463\n",
      "Epoch 14372, Loss: 16.299, Final Batch Loss: 0.421\n",
      "Epoch 14373, Loss: 16.179, Final Batch Loss: 0.506\n",
      "Epoch 14374, Loss: 16.101, Final Batch Loss: 0.420\n",
      "Epoch 14375, Loss: 16.145, Final Batch Loss: 0.412\n",
      "Epoch 14376, Loss: 16.253, Final Batch Loss: 0.496\n",
      "Epoch 14377, Loss: 16.446, Final Batch Loss: 0.483\n",
      "Epoch 14378, Loss: 16.126, Final Batch Loss: 0.469\n",
      "Epoch 14379, Loss: 16.302, Final Batch Loss: 0.498\n",
      "Epoch 14380, Loss: 16.368, Final Batch Loss: 0.413\n",
      "Epoch 14381, Loss: 16.159, Final Batch Loss: 0.522\n",
      "Epoch 14382, Loss: 16.177, Final Batch Loss: 0.506\n",
      "Epoch 14383, Loss: 16.071, Final Batch Loss: 0.402\n",
      "Epoch 14384, Loss: 16.288, Final Batch Loss: 0.418\n",
      "Epoch 14385, Loss: 16.091, Final Batch Loss: 0.468\n",
      "Epoch 14386, Loss: 16.379, Final Batch Loss: 0.619\n",
      "Epoch 14387, Loss: 16.385, Final Batch Loss: 0.549\n",
      "Epoch 14388, Loss: 16.342, Final Batch Loss: 0.452\n",
      "Epoch 14389, Loss: 16.078, Final Batch Loss: 0.415\n",
      "Epoch 14390, Loss: 16.220, Final Batch Loss: 0.530\n",
      "Epoch 14391, Loss: 16.475, Final Batch Loss: 0.558\n",
      "Epoch 14392, Loss: 16.325, Final Batch Loss: 0.409\n",
      "Epoch 14393, Loss: 16.262, Final Batch Loss: 0.449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14394, Loss: 16.737, Final Batch Loss: 0.558\n",
      "Epoch 14395, Loss: 16.380, Final Batch Loss: 0.496\n",
      "Epoch 14396, Loss: 16.151, Final Batch Loss: 0.504\n",
      "Epoch 14397, Loss: 16.119, Final Batch Loss: 0.423\n",
      "Epoch 14398, Loss: 16.225, Final Batch Loss: 0.518\n",
      "Epoch 14399, Loss: 16.409, Final Batch Loss: 0.385\n",
      "Epoch 14400, Loss: 16.252, Final Batch Loss: 0.506\n",
      "Epoch 14401, Loss: 16.200, Final Batch Loss: 0.428\n",
      "Epoch 14402, Loss: 16.589, Final Batch Loss: 0.453\n",
      "Epoch 14403, Loss: 16.386, Final Batch Loss: 0.652\n",
      "Epoch 14404, Loss: 16.181, Final Batch Loss: 0.382\n",
      "Epoch 14405, Loss: 15.788, Final Batch Loss: 0.541\n",
      "Epoch 14406, Loss: 16.262, Final Batch Loss: 0.433\n",
      "Epoch 14407, Loss: 16.397, Final Batch Loss: 0.481\n",
      "Epoch 14408, Loss: 16.095, Final Batch Loss: 0.358\n",
      "Epoch 14409, Loss: 16.054, Final Batch Loss: 0.378\n",
      "Epoch 14410, Loss: 16.359, Final Batch Loss: 0.467\n",
      "Epoch 14411, Loss: 16.175, Final Batch Loss: 0.417\n",
      "Epoch 14412, Loss: 16.224, Final Batch Loss: 0.439\n",
      "Epoch 14413, Loss: 16.188, Final Batch Loss: 0.391\n",
      "Epoch 14414, Loss: 16.368, Final Batch Loss: 0.466\n",
      "Epoch 14415, Loss: 16.317, Final Batch Loss: 0.397\n",
      "Epoch 14416, Loss: 16.211, Final Batch Loss: 0.486\n",
      "Epoch 14417, Loss: 16.151, Final Batch Loss: 0.438\n",
      "Epoch 14418, Loss: 16.326, Final Batch Loss: 0.436\n",
      "Epoch 14419, Loss: 16.125, Final Batch Loss: 0.430\n",
      "Epoch 14420, Loss: 16.357, Final Batch Loss: 0.558\n",
      "Epoch 14421, Loss: 16.198, Final Batch Loss: 0.510\n",
      "Epoch 14422, Loss: 16.316, Final Batch Loss: 0.466\n",
      "Epoch 14423, Loss: 16.514, Final Batch Loss: 0.362\n",
      "Epoch 14424, Loss: 16.169, Final Batch Loss: 0.443\n",
      "Epoch 14425, Loss: 16.711, Final Batch Loss: 0.476\n",
      "Epoch 14426, Loss: 16.042, Final Batch Loss: 0.439\n",
      "Epoch 14427, Loss: 16.486, Final Batch Loss: 0.571\n",
      "Epoch 14428, Loss: 16.289, Final Batch Loss: 0.587\n",
      "Epoch 14429, Loss: 16.250, Final Batch Loss: 0.401\n",
      "Epoch 14430, Loss: 16.262, Final Batch Loss: 0.515\n",
      "Epoch 14431, Loss: 16.211, Final Batch Loss: 0.478\n",
      "Epoch 14432, Loss: 16.279, Final Batch Loss: 0.367\n",
      "Epoch 14433, Loss: 16.147, Final Batch Loss: 0.432\n",
      "Epoch 14434, Loss: 15.824, Final Batch Loss: 0.358\n",
      "Epoch 14435, Loss: 16.501, Final Batch Loss: 0.435\n",
      "Epoch 14436, Loss: 16.006, Final Batch Loss: 0.419\n",
      "Epoch 14437, Loss: 16.308, Final Batch Loss: 0.463\n",
      "Epoch 14438, Loss: 16.286, Final Batch Loss: 0.427\n",
      "Epoch 14439, Loss: 16.015, Final Batch Loss: 0.510\n",
      "Epoch 14440, Loss: 16.330, Final Batch Loss: 0.542\n",
      "Epoch 14441, Loss: 16.490, Final Batch Loss: 0.543\n",
      "Epoch 14442, Loss: 16.258, Final Batch Loss: 0.464\n",
      "Epoch 14443, Loss: 16.513, Final Batch Loss: 0.500\n",
      "Epoch 14444, Loss: 16.150, Final Batch Loss: 0.348\n",
      "Epoch 14445, Loss: 16.286, Final Batch Loss: 0.529\n",
      "Epoch 14446, Loss: 16.331, Final Batch Loss: 0.462\n",
      "Epoch 14447, Loss: 15.996, Final Batch Loss: 0.344\n",
      "Epoch 14448, Loss: 16.346, Final Batch Loss: 0.442\n",
      "Epoch 14449, Loss: 16.296, Final Batch Loss: 0.531\n",
      "Epoch 14450, Loss: 15.966, Final Batch Loss: 0.411\n",
      "Epoch 14451, Loss: 16.435, Final Batch Loss: 0.455\n",
      "Epoch 14452, Loss: 16.289, Final Batch Loss: 0.477\n",
      "Epoch 14453, Loss: 16.089, Final Batch Loss: 0.450\n",
      "Epoch 14454, Loss: 16.355, Final Batch Loss: 0.495\n",
      "Epoch 14455, Loss: 16.184, Final Batch Loss: 0.488\n",
      "Epoch 14456, Loss: 16.241, Final Batch Loss: 0.354\n",
      "Epoch 14457, Loss: 16.269, Final Batch Loss: 0.463\n",
      "Epoch 14458, Loss: 16.436, Final Batch Loss: 0.377\n",
      "Epoch 14459, Loss: 16.206, Final Batch Loss: 0.398\n",
      "Epoch 14460, Loss: 16.250, Final Batch Loss: 0.413\n",
      "Epoch 14461, Loss: 16.359, Final Batch Loss: 0.510\n",
      "Epoch 14462, Loss: 16.581, Final Batch Loss: 0.553\n",
      "Epoch 14463, Loss: 16.286, Final Batch Loss: 0.410\n",
      "Epoch 14464, Loss: 16.028, Final Batch Loss: 0.438\n",
      "Epoch 14465, Loss: 16.344, Final Batch Loss: 0.467\n",
      "Epoch 14466, Loss: 16.369, Final Batch Loss: 0.445\n",
      "Epoch 14467, Loss: 16.246, Final Batch Loss: 0.397\n",
      "Epoch 14468, Loss: 16.158, Final Batch Loss: 0.450\n",
      "Epoch 14469, Loss: 16.098, Final Batch Loss: 0.473\n",
      "Epoch 14470, Loss: 16.291, Final Batch Loss: 0.569\n",
      "Epoch 14471, Loss: 16.303, Final Batch Loss: 0.449\n",
      "Epoch 14472, Loss: 16.579, Final Batch Loss: 0.565\n",
      "Epoch 14473, Loss: 16.106, Final Batch Loss: 0.481\n",
      "Epoch 14474, Loss: 16.316, Final Batch Loss: 0.419\n",
      "Epoch 14475, Loss: 16.415, Final Batch Loss: 0.448\n",
      "Epoch 14476, Loss: 16.192, Final Batch Loss: 0.477\n",
      "Epoch 14477, Loss: 16.631, Final Batch Loss: 0.483\n",
      "Epoch 14478, Loss: 16.571, Final Batch Loss: 0.438\n",
      "Epoch 14479, Loss: 16.559, Final Batch Loss: 0.466\n",
      "Epoch 14480, Loss: 16.360, Final Batch Loss: 0.376\n",
      "Epoch 14481, Loss: 16.386, Final Batch Loss: 0.439\n",
      "Epoch 14482, Loss: 16.098, Final Batch Loss: 0.393\n",
      "Epoch 14483, Loss: 16.373, Final Batch Loss: 0.388\n",
      "Epoch 14484, Loss: 16.105, Final Batch Loss: 0.426\n",
      "Epoch 14485, Loss: 16.258, Final Batch Loss: 0.388\n",
      "Epoch 14486, Loss: 16.166, Final Batch Loss: 0.429\n",
      "Epoch 14487, Loss: 16.294, Final Batch Loss: 0.372\n",
      "Epoch 14488, Loss: 16.061, Final Batch Loss: 0.372\n",
      "Epoch 14489, Loss: 15.938, Final Batch Loss: 0.429\n",
      "Epoch 14490, Loss: 16.227, Final Batch Loss: 0.476\n",
      "Epoch 14491, Loss: 16.038, Final Batch Loss: 0.493\n",
      "Epoch 14492, Loss: 16.162, Final Batch Loss: 0.391\n",
      "Epoch 14493, Loss: 16.170, Final Batch Loss: 0.510\n",
      "Epoch 14494, Loss: 16.339, Final Batch Loss: 0.568\n",
      "Epoch 14495, Loss: 16.310, Final Batch Loss: 0.471\n",
      "Epoch 14496, Loss: 16.120, Final Batch Loss: 0.432\n",
      "Epoch 14497, Loss: 16.269, Final Batch Loss: 0.522\n",
      "Epoch 14498, Loss: 16.139, Final Batch Loss: 0.362\n",
      "Epoch 14499, Loss: 16.645, Final Batch Loss: 0.498\n",
      "Epoch 14500, Loss: 16.448, Final Batch Loss: 0.427\n",
      "Epoch 14501, Loss: 16.101, Final Batch Loss: 0.425\n",
      "Epoch 14502, Loss: 16.157, Final Batch Loss: 0.449\n",
      "Epoch 14503, Loss: 15.988, Final Batch Loss: 0.402\n",
      "Epoch 14504, Loss: 16.435, Final Batch Loss: 0.484\n",
      "Epoch 14505, Loss: 16.260, Final Batch Loss: 0.391\n",
      "Epoch 14506, Loss: 16.221, Final Batch Loss: 0.478\n",
      "Epoch 14507, Loss: 16.153, Final Batch Loss: 0.468\n",
      "Epoch 14508, Loss: 16.270, Final Batch Loss: 0.501\n",
      "Epoch 14509, Loss: 16.320, Final Batch Loss: 0.408\n",
      "Epoch 14510, Loss: 16.440, Final Batch Loss: 0.533\n",
      "Epoch 14511, Loss: 16.211, Final Batch Loss: 0.363\n",
      "Epoch 14512, Loss: 16.286, Final Batch Loss: 0.437\n",
      "Epoch 14513, Loss: 16.245, Final Batch Loss: 0.426\n",
      "Epoch 14514, Loss: 16.154, Final Batch Loss: 0.409\n",
      "Epoch 14515, Loss: 16.140, Final Batch Loss: 0.473\n",
      "Epoch 14516, Loss: 16.073, Final Batch Loss: 0.492\n",
      "Epoch 14517, Loss: 16.350, Final Batch Loss: 0.427\n",
      "Epoch 14518, Loss: 16.184, Final Batch Loss: 0.469\n",
      "Epoch 14519, Loss: 15.967, Final Batch Loss: 0.477\n",
      "Epoch 14520, Loss: 16.083, Final Batch Loss: 0.429\n",
      "Epoch 14521, Loss: 16.298, Final Batch Loss: 0.459\n",
      "Epoch 14522, Loss: 16.545, Final Batch Loss: 0.396\n",
      "Epoch 14523, Loss: 16.137, Final Batch Loss: 0.370\n",
      "Epoch 14524, Loss: 16.306, Final Batch Loss: 0.535\n",
      "Epoch 14525, Loss: 16.250, Final Batch Loss: 0.470\n",
      "Epoch 14526, Loss: 15.969, Final Batch Loss: 0.476\n",
      "Epoch 14527, Loss: 16.059, Final Batch Loss: 0.429\n",
      "Epoch 14528, Loss: 16.183, Final Batch Loss: 0.396\n",
      "Epoch 14529, Loss: 16.600, Final Batch Loss: 0.513\n",
      "Epoch 14530, Loss: 16.317, Final Batch Loss: 0.516\n",
      "Epoch 14531, Loss: 15.956, Final Batch Loss: 0.422\n",
      "Epoch 14532, Loss: 16.403, Final Batch Loss: 0.402\n",
      "Epoch 14533, Loss: 16.110, Final Batch Loss: 0.433\n",
      "Epoch 14534, Loss: 16.346, Final Batch Loss: 0.490\n",
      "Epoch 14535, Loss: 16.401, Final Batch Loss: 0.433\n",
      "Epoch 14536, Loss: 15.998, Final Batch Loss: 0.415\n",
      "Epoch 14537, Loss: 16.331, Final Batch Loss: 0.551\n",
      "Epoch 14538, Loss: 16.398, Final Batch Loss: 0.487\n",
      "Epoch 14539, Loss: 16.090, Final Batch Loss: 0.404\n",
      "Epoch 14540, Loss: 16.264, Final Batch Loss: 0.380\n",
      "Epoch 14541, Loss: 16.143, Final Batch Loss: 0.344\n",
      "Epoch 14542, Loss: 16.202, Final Batch Loss: 0.376\n",
      "Epoch 14543, Loss: 16.231, Final Batch Loss: 0.477\n",
      "Epoch 14544, Loss: 16.206, Final Batch Loss: 0.423\n",
      "Epoch 14545, Loss: 16.131, Final Batch Loss: 0.481\n",
      "Epoch 14546, Loss: 16.276, Final Batch Loss: 0.431\n",
      "Epoch 14547, Loss: 16.504, Final Batch Loss: 0.370\n",
      "Epoch 14548, Loss: 16.474, Final Batch Loss: 0.660\n",
      "Epoch 14549, Loss: 16.225, Final Batch Loss: 0.367\n",
      "Epoch 14550, Loss: 16.207, Final Batch Loss: 0.350\n",
      "Epoch 14551, Loss: 16.016, Final Batch Loss: 0.552\n",
      "Epoch 14552, Loss: 16.126, Final Batch Loss: 0.466\n",
      "Epoch 14553, Loss: 16.154, Final Batch Loss: 0.482\n",
      "Epoch 14554, Loss: 15.991, Final Batch Loss: 0.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14555, Loss: 15.780, Final Batch Loss: 0.487\n",
      "Epoch 14556, Loss: 16.263, Final Batch Loss: 0.484\n",
      "Epoch 14557, Loss: 16.124, Final Batch Loss: 0.338\n",
      "Epoch 14558, Loss: 16.238, Final Batch Loss: 0.429\n",
      "Epoch 14559, Loss: 15.983, Final Batch Loss: 0.396\n",
      "Epoch 14560, Loss: 16.098, Final Batch Loss: 0.410\n",
      "Epoch 14561, Loss: 16.232, Final Batch Loss: 0.419\n",
      "Epoch 14562, Loss: 16.242, Final Batch Loss: 0.382\n",
      "Epoch 14563, Loss: 16.267, Final Batch Loss: 0.500\n",
      "Epoch 14564, Loss: 16.082, Final Batch Loss: 0.452\n",
      "Epoch 14565, Loss: 15.977, Final Batch Loss: 0.503\n",
      "Epoch 14566, Loss: 16.405, Final Batch Loss: 0.420\n",
      "Epoch 14567, Loss: 15.995, Final Batch Loss: 0.409\n",
      "Epoch 14568, Loss: 16.134, Final Batch Loss: 0.370\n",
      "Epoch 14569, Loss: 16.378, Final Batch Loss: 0.380\n",
      "Epoch 14570, Loss: 16.469, Final Batch Loss: 0.429\n",
      "Epoch 14571, Loss: 16.201, Final Batch Loss: 0.437\n",
      "Epoch 14572, Loss: 16.077, Final Batch Loss: 0.403\n",
      "Epoch 14573, Loss: 16.242, Final Batch Loss: 0.441\n",
      "Epoch 14574, Loss: 16.269, Final Batch Loss: 0.401\n",
      "Epoch 14575, Loss: 16.523, Final Batch Loss: 0.441\n",
      "Epoch 14576, Loss: 16.243, Final Batch Loss: 0.435\n",
      "Epoch 14577, Loss: 16.319, Final Batch Loss: 0.504\n",
      "Epoch 14578, Loss: 16.429, Final Batch Loss: 0.426\n",
      "Epoch 14579, Loss: 16.272, Final Batch Loss: 0.467\n",
      "Epoch 14580, Loss: 16.578, Final Batch Loss: 0.521\n",
      "Epoch 14581, Loss: 16.130, Final Batch Loss: 0.359\n",
      "Epoch 14582, Loss: 16.199, Final Batch Loss: 0.476\n",
      "Epoch 14583, Loss: 16.351, Final Batch Loss: 0.465\n",
      "Epoch 14584, Loss: 16.177, Final Batch Loss: 0.472\n",
      "Epoch 14585, Loss: 16.141, Final Batch Loss: 0.609\n",
      "Epoch 14586, Loss: 16.567, Final Batch Loss: 0.454\n",
      "Epoch 14587, Loss: 16.602, Final Batch Loss: 0.360\n",
      "Epoch 14588, Loss: 16.561, Final Batch Loss: 0.449\n",
      "Epoch 14589, Loss: 16.141, Final Batch Loss: 0.440\n",
      "Epoch 14590, Loss: 16.362, Final Batch Loss: 0.597\n",
      "Epoch 14591, Loss: 16.154, Final Batch Loss: 0.512\n",
      "Epoch 14592, Loss: 16.435, Final Batch Loss: 0.459\n",
      "Epoch 14593, Loss: 16.364, Final Batch Loss: 0.482\n",
      "Epoch 14594, Loss: 16.351, Final Batch Loss: 0.475\n",
      "Epoch 14595, Loss: 16.131, Final Batch Loss: 0.505\n",
      "Epoch 14596, Loss: 16.430, Final Batch Loss: 0.461\n",
      "Epoch 14597, Loss: 16.334, Final Batch Loss: 0.505\n",
      "Epoch 14598, Loss: 16.208, Final Batch Loss: 0.406\n",
      "Epoch 14599, Loss: 16.359, Final Batch Loss: 0.609\n",
      "Epoch 14600, Loss: 15.982, Final Batch Loss: 0.404\n",
      "Epoch 14601, Loss: 16.321, Final Batch Loss: 0.342\n",
      "Epoch 14602, Loss: 16.263, Final Batch Loss: 0.463\n",
      "Epoch 14603, Loss: 16.329, Final Batch Loss: 0.431\n",
      "Epoch 14604, Loss: 16.135, Final Batch Loss: 0.388\n",
      "Epoch 14605, Loss: 15.998, Final Batch Loss: 0.397\n",
      "Epoch 14606, Loss: 16.285, Final Batch Loss: 0.477\n",
      "Epoch 14607, Loss: 16.468, Final Batch Loss: 0.489\n",
      "Epoch 14608, Loss: 16.352, Final Batch Loss: 0.438\n",
      "Epoch 14609, Loss: 16.380, Final Batch Loss: 0.546\n",
      "Epoch 14610, Loss: 15.999, Final Batch Loss: 0.501\n",
      "Epoch 14611, Loss: 15.978, Final Batch Loss: 0.485\n",
      "Epoch 14612, Loss: 16.176, Final Batch Loss: 0.458\n",
      "Epoch 14613, Loss: 16.105, Final Batch Loss: 0.555\n",
      "Epoch 14614, Loss: 16.204, Final Batch Loss: 0.413\n",
      "Epoch 14615, Loss: 16.154, Final Batch Loss: 0.463\n",
      "Epoch 14616, Loss: 16.259, Final Batch Loss: 0.446\n",
      "Epoch 14617, Loss: 16.275, Final Batch Loss: 0.428\n",
      "Epoch 14618, Loss: 16.292, Final Batch Loss: 0.406\n",
      "Epoch 14619, Loss: 16.367, Final Batch Loss: 0.425\n",
      "Epoch 14620, Loss: 16.037, Final Batch Loss: 0.427\n",
      "Epoch 14621, Loss: 16.267, Final Batch Loss: 0.491\n",
      "Epoch 14622, Loss: 16.179, Final Batch Loss: 0.469\n",
      "Epoch 14623, Loss: 16.270, Final Batch Loss: 0.487\n",
      "Epoch 14624, Loss: 16.256, Final Batch Loss: 0.393\n",
      "Epoch 14625, Loss: 16.184, Final Batch Loss: 0.398\n",
      "Epoch 14626, Loss: 15.984, Final Batch Loss: 0.440\n",
      "Epoch 14627, Loss: 16.099, Final Batch Loss: 0.504\n",
      "Epoch 14628, Loss: 16.243, Final Batch Loss: 0.422\n",
      "Epoch 14629, Loss: 16.212, Final Batch Loss: 0.432\n",
      "Epoch 14630, Loss: 16.384, Final Batch Loss: 0.497\n",
      "Epoch 14631, Loss: 16.055, Final Batch Loss: 0.470\n",
      "Epoch 14632, Loss: 16.098, Final Batch Loss: 0.523\n",
      "Epoch 14633, Loss: 15.999, Final Batch Loss: 0.423\n",
      "Epoch 14634, Loss: 15.961, Final Batch Loss: 0.395\n",
      "Epoch 14635, Loss: 16.156, Final Batch Loss: 0.467\n",
      "Epoch 14636, Loss: 16.192, Final Batch Loss: 0.527\n",
      "Epoch 14637, Loss: 16.322, Final Batch Loss: 0.399\n",
      "Epoch 14638, Loss: 16.311, Final Batch Loss: 0.375\n",
      "Epoch 14639, Loss: 16.249, Final Batch Loss: 0.540\n",
      "Epoch 14640, Loss: 16.022, Final Batch Loss: 0.385\n",
      "Epoch 14641, Loss: 16.400, Final Batch Loss: 0.549\n",
      "Epoch 14642, Loss: 15.941, Final Batch Loss: 0.392\n",
      "Epoch 14643, Loss: 16.189, Final Batch Loss: 0.465\n",
      "Epoch 14644, Loss: 16.287, Final Batch Loss: 0.392\n",
      "Epoch 14645, Loss: 16.250, Final Batch Loss: 0.458\n",
      "Epoch 14646, Loss: 16.354, Final Batch Loss: 0.481\n",
      "Epoch 14647, Loss: 16.251, Final Batch Loss: 0.376\n",
      "Epoch 14648, Loss: 16.214, Final Batch Loss: 0.482\n",
      "Epoch 14649, Loss: 16.020, Final Batch Loss: 0.412\n",
      "Epoch 14650, Loss: 16.151, Final Batch Loss: 0.421\n",
      "Epoch 14651, Loss: 16.149, Final Batch Loss: 0.442\n",
      "Epoch 14652, Loss: 16.285, Final Batch Loss: 0.469\n",
      "Epoch 14653, Loss: 16.217, Final Batch Loss: 0.417\n",
      "Epoch 14654, Loss: 16.456, Final Batch Loss: 0.524\n",
      "Epoch 14655, Loss: 16.359, Final Batch Loss: 0.493\n",
      "Epoch 14656, Loss: 16.266, Final Batch Loss: 0.408\n",
      "Epoch 14657, Loss: 16.213, Final Batch Loss: 0.472\n",
      "Epoch 14658, Loss: 16.141, Final Batch Loss: 0.428\n",
      "Epoch 14659, Loss: 16.162, Final Batch Loss: 0.613\n",
      "Epoch 14660, Loss: 15.995, Final Batch Loss: 0.359\n",
      "Epoch 14661, Loss: 16.278, Final Batch Loss: 0.455\n",
      "Epoch 14662, Loss: 16.202, Final Batch Loss: 0.330\n",
      "Epoch 14663, Loss: 15.985, Final Batch Loss: 0.442\n",
      "Epoch 14664, Loss: 16.276, Final Batch Loss: 0.410\n",
      "Epoch 14665, Loss: 16.300, Final Batch Loss: 0.449\n",
      "Epoch 14666, Loss: 16.134, Final Batch Loss: 0.433\n",
      "Epoch 14667, Loss: 16.299, Final Batch Loss: 0.412\n",
      "Epoch 14668, Loss: 16.465, Final Batch Loss: 0.409\n",
      "Epoch 14669, Loss: 16.215, Final Batch Loss: 0.548\n",
      "Epoch 14670, Loss: 16.226, Final Batch Loss: 0.401\n",
      "Epoch 14671, Loss: 16.534, Final Batch Loss: 0.499\n",
      "Epoch 14672, Loss: 16.205, Final Batch Loss: 0.499\n",
      "Epoch 14673, Loss: 16.238, Final Batch Loss: 0.457\n",
      "Epoch 14674, Loss: 16.076, Final Batch Loss: 0.405\n",
      "Epoch 14675, Loss: 16.108, Final Batch Loss: 0.422\n",
      "Epoch 14676, Loss: 16.481, Final Batch Loss: 0.487\n",
      "Epoch 14677, Loss: 16.043, Final Batch Loss: 0.318\n",
      "Epoch 14678, Loss: 15.960, Final Batch Loss: 0.448\n",
      "Epoch 14679, Loss: 16.222, Final Batch Loss: 0.449\n",
      "Epoch 14680, Loss: 16.189, Final Batch Loss: 0.401\n",
      "Epoch 14681, Loss: 16.351, Final Batch Loss: 0.415\n",
      "Epoch 14682, Loss: 16.452, Final Batch Loss: 0.522\n",
      "Epoch 14683, Loss: 16.082, Final Batch Loss: 0.431\n",
      "Epoch 14684, Loss: 16.297, Final Batch Loss: 0.498\n",
      "Epoch 14685, Loss: 16.064, Final Batch Loss: 0.527\n",
      "Epoch 14686, Loss: 15.957, Final Batch Loss: 0.410\n",
      "Epoch 14687, Loss: 16.430, Final Batch Loss: 0.574\n",
      "Epoch 14688, Loss: 16.236, Final Batch Loss: 0.405\n",
      "Epoch 14689, Loss: 16.251, Final Batch Loss: 0.470\n",
      "Epoch 14690, Loss: 16.249, Final Batch Loss: 0.364\n",
      "Epoch 14691, Loss: 16.143, Final Batch Loss: 0.381\n",
      "Epoch 14692, Loss: 16.386, Final Batch Loss: 0.489\n",
      "Epoch 14693, Loss: 16.443, Final Batch Loss: 0.426\n",
      "Epoch 14694, Loss: 16.297, Final Batch Loss: 0.450\n",
      "Epoch 14695, Loss: 16.084, Final Batch Loss: 0.497\n",
      "Epoch 14696, Loss: 16.197, Final Batch Loss: 0.499\n",
      "Epoch 14697, Loss: 16.300, Final Batch Loss: 0.426\n",
      "Epoch 14698, Loss: 16.316, Final Batch Loss: 0.438\n",
      "Epoch 14699, Loss: 16.253, Final Batch Loss: 0.469\n",
      "Epoch 14700, Loss: 16.298, Final Batch Loss: 0.438\n",
      "Epoch 14701, Loss: 16.207, Final Batch Loss: 0.533\n",
      "Epoch 14702, Loss: 16.274, Final Batch Loss: 0.438\n",
      "Epoch 14703, Loss: 16.189, Final Batch Loss: 0.541\n",
      "Epoch 14704, Loss: 16.139, Final Batch Loss: 0.354\n",
      "Epoch 14705, Loss: 16.569, Final Batch Loss: 0.526\n",
      "Epoch 14706, Loss: 15.959, Final Batch Loss: 0.449\n",
      "Epoch 14707, Loss: 16.204, Final Batch Loss: 0.412\n",
      "Epoch 14708, Loss: 16.205, Final Batch Loss: 0.528\n",
      "Epoch 14709, Loss: 15.942, Final Batch Loss: 0.378\n",
      "Epoch 14710, Loss: 16.279, Final Batch Loss: 0.522\n",
      "Epoch 14711, Loss: 16.442, Final Batch Loss: 0.496\n",
      "Epoch 14712, Loss: 16.491, Final Batch Loss: 0.603\n",
      "Epoch 14713, Loss: 16.506, Final Batch Loss: 0.462\n",
      "Epoch 14714, Loss: 16.439, Final Batch Loss: 0.520\n",
      "Epoch 14715, Loss: 16.144, Final Batch Loss: 0.507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14716, Loss: 16.216, Final Batch Loss: 0.360\n",
      "Epoch 14717, Loss: 16.387, Final Batch Loss: 0.457\n",
      "Epoch 14718, Loss: 16.391, Final Batch Loss: 0.417\n",
      "Epoch 14719, Loss: 16.096, Final Batch Loss: 0.546\n",
      "Epoch 14720, Loss: 16.160, Final Batch Loss: 0.422\n",
      "Epoch 14721, Loss: 16.014, Final Batch Loss: 0.511\n",
      "Epoch 14722, Loss: 16.061, Final Batch Loss: 0.363\n",
      "Epoch 14723, Loss: 16.188, Final Batch Loss: 0.641\n",
      "Epoch 14724, Loss: 16.256, Final Batch Loss: 0.366\n",
      "Epoch 14725, Loss: 16.106, Final Batch Loss: 0.448\n",
      "Epoch 14726, Loss: 16.281, Final Batch Loss: 0.393\n",
      "Epoch 14727, Loss: 16.331, Final Batch Loss: 0.424\n",
      "Epoch 14728, Loss: 16.489, Final Batch Loss: 0.498\n",
      "Epoch 14729, Loss: 16.020, Final Batch Loss: 0.401\n",
      "Epoch 14730, Loss: 16.050, Final Batch Loss: 0.389\n",
      "Epoch 14731, Loss: 16.279, Final Batch Loss: 0.511\n",
      "Epoch 14732, Loss: 16.034, Final Batch Loss: 0.408\n",
      "Epoch 14733, Loss: 16.541, Final Batch Loss: 0.444\n",
      "Epoch 14734, Loss: 16.301, Final Batch Loss: 0.464\n",
      "Epoch 14735, Loss: 16.396, Final Batch Loss: 0.504\n",
      "Epoch 14736, Loss: 16.245, Final Batch Loss: 0.551\n",
      "Epoch 14737, Loss: 16.245, Final Batch Loss: 0.545\n",
      "Epoch 14738, Loss: 16.479, Final Batch Loss: 0.481\n",
      "Epoch 14739, Loss: 16.336, Final Batch Loss: 0.444\n",
      "Epoch 14740, Loss: 16.410, Final Batch Loss: 0.524\n",
      "Epoch 14741, Loss: 15.992, Final Batch Loss: 0.389\n",
      "Epoch 14742, Loss: 16.154, Final Batch Loss: 0.360\n",
      "Epoch 14743, Loss: 15.917, Final Batch Loss: 0.321\n",
      "Epoch 14744, Loss: 16.159, Final Batch Loss: 0.503\n",
      "Epoch 14745, Loss: 16.329, Final Batch Loss: 0.507\n",
      "Epoch 14746, Loss: 16.210, Final Batch Loss: 0.440\n",
      "Epoch 14747, Loss: 16.285, Final Batch Loss: 0.394\n",
      "Epoch 14748, Loss: 16.326, Final Batch Loss: 0.471\n",
      "Epoch 14749, Loss: 16.021, Final Batch Loss: 0.432\n",
      "Epoch 14750, Loss: 16.069, Final Batch Loss: 0.475\n",
      "Epoch 14751, Loss: 16.245, Final Batch Loss: 0.577\n",
      "Epoch 14752, Loss: 16.325, Final Batch Loss: 0.484\n",
      "Epoch 14753, Loss: 16.308, Final Batch Loss: 0.551\n",
      "Epoch 14754, Loss: 16.269, Final Batch Loss: 0.468\n",
      "Epoch 14755, Loss: 16.146, Final Batch Loss: 0.376\n",
      "Epoch 14756, Loss: 16.212, Final Batch Loss: 0.450\n",
      "Epoch 14757, Loss: 15.929, Final Batch Loss: 0.369\n",
      "Epoch 14758, Loss: 16.032, Final Batch Loss: 0.418\n",
      "Epoch 14759, Loss: 16.288, Final Batch Loss: 0.472\n",
      "Epoch 14760, Loss: 16.396, Final Batch Loss: 0.524\n",
      "Epoch 14761, Loss: 16.135, Final Batch Loss: 0.387\n",
      "Epoch 14762, Loss: 16.253, Final Batch Loss: 0.432\n",
      "Epoch 14763, Loss: 16.713, Final Batch Loss: 0.488\n",
      "Epoch 14764, Loss: 16.151, Final Batch Loss: 0.521\n",
      "Epoch 14765, Loss: 16.262, Final Batch Loss: 0.482\n",
      "Epoch 14766, Loss: 16.233, Final Batch Loss: 0.367\n",
      "Epoch 14767, Loss: 16.207, Final Batch Loss: 0.399\n",
      "Epoch 14768, Loss: 16.234, Final Batch Loss: 0.451\n",
      "Epoch 14769, Loss: 15.962, Final Batch Loss: 0.406\n",
      "Epoch 14770, Loss: 16.082, Final Batch Loss: 0.373\n",
      "Epoch 14771, Loss: 16.032, Final Batch Loss: 0.402\n",
      "Epoch 14772, Loss: 16.147, Final Batch Loss: 0.502\n",
      "Epoch 14773, Loss: 16.138, Final Batch Loss: 0.385\n",
      "Epoch 14774, Loss: 16.347, Final Batch Loss: 0.457\n",
      "Epoch 14775, Loss: 16.518, Final Batch Loss: 0.468\n",
      "Epoch 14776, Loss: 16.528, Final Batch Loss: 0.571\n",
      "Epoch 14777, Loss: 16.167, Final Batch Loss: 0.538\n",
      "Epoch 14778, Loss: 15.872, Final Batch Loss: 0.462\n",
      "Epoch 14779, Loss: 16.367, Final Batch Loss: 0.369\n",
      "Epoch 14780, Loss: 16.341, Final Batch Loss: 0.496\n",
      "Epoch 14781, Loss: 16.414, Final Batch Loss: 0.510\n",
      "Epoch 14782, Loss: 16.207, Final Batch Loss: 0.498\n",
      "Epoch 14783, Loss: 16.180, Final Batch Loss: 0.394\n",
      "Epoch 14784, Loss: 16.429, Final Batch Loss: 0.394\n",
      "Epoch 14785, Loss: 16.361, Final Batch Loss: 0.399\n",
      "Epoch 14786, Loss: 16.136, Final Batch Loss: 0.397\n",
      "Epoch 14787, Loss: 16.203, Final Batch Loss: 0.570\n",
      "Epoch 14788, Loss: 16.453, Final Batch Loss: 0.434\n",
      "Epoch 14789, Loss: 16.193, Final Batch Loss: 0.445\n",
      "Epoch 14790, Loss: 15.843, Final Batch Loss: 0.443\n",
      "Epoch 14791, Loss: 16.399, Final Batch Loss: 0.475\n",
      "Epoch 14792, Loss: 16.415, Final Batch Loss: 0.560\n",
      "Epoch 14793, Loss: 15.931, Final Batch Loss: 0.484\n",
      "Epoch 14794, Loss: 16.374, Final Batch Loss: 0.447\n",
      "Epoch 14795, Loss: 16.069, Final Batch Loss: 0.530\n",
      "Epoch 14796, Loss: 16.097, Final Batch Loss: 0.442\n",
      "Epoch 14797, Loss: 16.385, Final Batch Loss: 0.455\n",
      "Epoch 14798, Loss: 16.190, Final Batch Loss: 0.413\n",
      "Epoch 14799, Loss: 16.491, Final Batch Loss: 0.493\n",
      "Epoch 14800, Loss: 16.230, Final Batch Loss: 0.377\n",
      "Epoch 14801, Loss: 16.381, Final Batch Loss: 0.361\n",
      "Epoch 14802, Loss: 16.043, Final Batch Loss: 0.426\n",
      "Epoch 14803, Loss: 16.314, Final Batch Loss: 0.412\n",
      "Epoch 14804, Loss: 16.083, Final Batch Loss: 0.444\n",
      "Epoch 14805, Loss: 16.372, Final Batch Loss: 0.453\n",
      "Epoch 14806, Loss: 16.195, Final Batch Loss: 0.448\n",
      "Epoch 14807, Loss: 16.360, Final Batch Loss: 0.512\n",
      "Epoch 14808, Loss: 16.181, Final Batch Loss: 0.568\n",
      "Epoch 14809, Loss: 16.034, Final Batch Loss: 0.342\n",
      "Epoch 14810, Loss: 16.081, Final Batch Loss: 0.458\n",
      "Epoch 14811, Loss: 16.029, Final Batch Loss: 0.396\n",
      "Epoch 14812, Loss: 16.253, Final Batch Loss: 0.557\n",
      "Epoch 14813, Loss: 16.128, Final Batch Loss: 0.490\n",
      "Epoch 14814, Loss: 16.127, Final Batch Loss: 0.509\n",
      "Epoch 14815, Loss: 16.341, Final Batch Loss: 0.443\n",
      "Epoch 14816, Loss: 16.216, Final Batch Loss: 0.427\n",
      "Epoch 14817, Loss: 15.863, Final Batch Loss: 0.383\n",
      "Epoch 14818, Loss: 16.367, Final Batch Loss: 0.460\n",
      "Epoch 14819, Loss: 16.232, Final Batch Loss: 0.415\n",
      "Epoch 14820, Loss: 16.259, Final Batch Loss: 0.428\n",
      "Epoch 14821, Loss: 16.229, Final Batch Loss: 0.462\n",
      "Epoch 14822, Loss: 16.511, Final Batch Loss: 0.530\n",
      "Epoch 14823, Loss: 16.197, Final Batch Loss: 0.497\n",
      "Epoch 14824, Loss: 16.515, Final Batch Loss: 0.406\n",
      "Epoch 14825, Loss: 16.391, Final Batch Loss: 0.342\n",
      "Epoch 14826, Loss: 16.348, Final Batch Loss: 0.592\n",
      "Epoch 14827, Loss: 16.363, Final Batch Loss: 0.394\n",
      "Epoch 14828, Loss: 16.657, Final Batch Loss: 0.441\n",
      "Epoch 14829, Loss: 16.606, Final Batch Loss: 0.480\n",
      "Epoch 14830, Loss: 15.889, Final Batch Loss: 0.396\n",
      "Epoch 14831, Loss: 16.319, Final Batch Loss: 0.612\n",
      "Epoch 14832, Loss: 15.849, Final Batch Loss: 0.353\n",
      "Epoch 14833, Loss: 16.247, Final Batch Loss: 0.435\n",
      "Epoch 14834, Loss: 16.049, Final Batch Loss: 0.320\n",
      "Epoch 14835, Loss: 16.172, Final Batch Loss: 0.389\n",
      "Epoch 14836, Loss: 16.092, Final Batch Loss: 0.436\n",
      "Epoch 14837, Loss: 16.165, Final Batch Loss: 0.445\n",
      "Epoch 14838, Loss: 16.491, Final Batch Loss: 0.461\n",
      "Epoch 14839, Loss: 16.245, Final Batch Loss: 0.389\n",
      "Epoch 14840, Loss: 16.409, Final Batch Loss: 0.484\n",
      "Epoch 14841, Loss: 16.365, Final Batch Loss: 0.541\n",
      "Epoch 14842, Loss: 16.199, Final Batch Loss: 0.395\n",
      "Epoch 14843, Loss: 16.171, Final Batch Loss: 0.396\n",
      "Epoch 14844, Loss: 16.282, Final Batch Loss: 0.480\n",
      "Epoch 14845, Loss: 16.008, Final Batch Loss: 0.313\n",
      "Epoch 14846, Loss: 16.263, Final Batch Loss: 0.455\n",
      "Epoch 14847, Loss: 16.151, Final Batch Loss: 0.380\n",
      "Epoch 14848, Loss: 16.127, Final Batch Loss: 0.370\n",
      "Epoch 14849, Loss: 15.906, Final Batch Loss: 0.454\n",
      "Epoch 14850, Loss: 16.353, Final Batch Loss: 0.605\n",
      "Epoch 14851, Loss: 15.988, Final Batch Loss: 0.421\n",
      "Epoch 14852, Loss: 16.056, Final Batch Loss: 0.354\n",
      "Epoch 14853, Loss: 16.477, Final Batch Loss: 0.413\n",
      "Epoch 14854, Loss: 16.370, Final Batch Loss: 0.456\n",
      "Epoch 14855, Loss: 16.061, Final Batch Loss: 0.403\n",
      "Epoch 14856, Loss: 16.266, Final Batch Loss: 0.425\n",
      "Epoch 14857, Loss: 16.129, Final Batch Loss: 0.572\n",
      "Epoch 14858, Loss: 16.029, Final Batch Loss: 0.407\n",
      "Epoch 14859, Loss: 16.097, Final Batch Loss: 0.449\n",
      "Epoch 14860, Loss: 16.354, Final Batch Loss: 0.574\n",
      "Epoch 14861, Loss: 16.137, Final Batch Loss: 0.460\n",
      "Epoch 14862, Loss: 16.384, Final Batch Loss: 0.439\n",
      "Epoch 14863, Loss: 16.116, Final Batch Loss: 0.431\n",
      "Epoch 14864, Loss: 16.266, Final Batch Loss: 0.438\n",
      "Epoch 14865, Loss: 16.083, Final Batch Loss: 0.445\n",
      "Epoch 14866, Loss: 16.078, Final Batch Loss: 0.479\n",
      "Epoch 14867, Loss: 16.290, Final Batch Loss: 0.460\n",
      "Epoch 14868, Loss: 16.077, Final Batch Loss: 0.476\n",
      "Epoch 14869, Loss: 16.332, Final Batch Loss: 0.504\n",
      "Epoch 14870, Loss: 16.128, Final Batch Loss: 0.462\n",
      "Epoch 14871, Loss: 16.391, Final Batch Loss: 0.417\n",
      "Epoch 14872, Loss: 16.538, Final Batch Loss: 0.533\n",
      "Epoch 14873, Loss: 16.112, Final Batch Loss: 0.420\n",
      "Epoch 14874, Loss: 16.087, Final Batch Loss: 0.537\n",
      "Epoch 14875, Loss: 16.024, Final Batch Loss: 0.430\n",
      "Epoch 14876, Loss: 16.156, Final Batch Loss: 0.463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14877, Loss: 16.331, Final Batch Loss: 0.493\n",
      "Epoch 14878, Loss: 16.416, Final Batch Loss: 0.420\n",
      "Epoch 14879, Loss: 15.994, Final Batch Loss: 0.485\n",
      "Epoch 14880, Loss: 16.180, Final Batch Loss: 0.458\n",
      "Epoch 14881, Loss: 16.322, Final Batch Loss: 0.459\n",
      "Epoch 14882, Loss: 16.018, Final Batch Loss: 0.446\n",
      "Epoch 14883, Loss: 16.141, Final Batch Loss: 0.457\n",
      "Epoch 14884, Loss: 16.339, Final Batch Loss: 0.407\n",
      "Epoch 14885, Loss: 16.183, Final Batch Loss: 0.434\n",
      "Epoch 14886, Loss: 16.191, Final Batch Loss: 0.372\n",
      "Epoch 14887, Loss: 16.529, Final Batch Loss: 0.558\n",
      "Epoch 14888, Loss: 16.500, Final Batch Loss: 0.453\n",
      "Epoch 14889, Loss: 16.344, Final Batch Loss: 0.487\n",
      "Epoch 14890, Loss: 16.152, Final Batch Loss: 0.516\n",
      "Epoch 14891, Loss: 16.409, Final Batch Loss: 0.436\n",
      "Epoch 14892, Loss: 16.151, Final Batch Loss: 0.348\n",
      "Epoch 14893, Loss: 16.193, Final Batch Loss: 0.545\n",
      "Epoch 14894, Loss: 16.140, Final Batch Loss: 0.460\n",
      "Epoch 14895, Loss: 16.207, Final Batch Loss: 0.371\n",
      "Epoch 14896, Loss: 16.103, Final Batch Loss: 0.384\n",
      "Epoch 14897, Loss: 16.130, Final Batch Loss: 0.478\n",
      "Epoch 14898, Loss: 15.975, Final Batch Loss: 0.404\n",
      "Epoch 14899, Loss: 16.445, Final Batch Loss: 0.403\n",
      "Epoch 14900, Loss: 15.985, Final Batch Loss: 0.496\n",
      "Epoch 14901, Loss: 16.235, Final Batch Loss: 0.478\n",
      "Epoch 14902, Loss: 16.197, Final Batch Loss: 0.433\n",
      "Epoch 14903, Loss: 16.152, Final Batch Loss: 0.471\n",
      "Epoch 14904, Loss: 16.139, Final Batch Loss: 0.411\n",
      "Epoch 14905, Loss: 16.022, Final Batch Loss: 0.518\n",
      "Epoch 14906, Loss: 16.073, Final Batch Loss: 0.426\n",
      "Epoch 14907, Loss: 16.324, Final Batch Loss: 0.440\n",
      "Epoch 14908, Loss: 16.436, Final Batch Loss: 0.626\n",
      "Epoch 14909, Loss: 15.915, Final Batch Loss: 0.479\n",
      "Epoch 14910, Loss: 16.131, Final Batch Loss: 0.456\n",
      "Epoch 14911, Loss: 16.050, Final Batch Loss: 0.392\n",
      "Epoch 14912, Loss: 16.445, Final Batch Loss: 0.447\n",
      "Epoch 14913, Loss: 16.181, Final Batch Loss: 0.443\n",
      "Epoch 14914, Loss: 16.082, Final Batch Loss: 0.594\n",
      "Epoch 14915, Loss: 16.321, Final Batch Loss: 0.509\n",
      "Epoch 14916, Loss: 16.292, Final Batch Loss: 0.356\n",
      "Epoch 14917, Loss: 16.495, Final Batch Loss: 0.450\n",
      "Epoch 14918, Loss: 16.291, Final Batch Loss: 0.402\n",
      "Epoch 14919, Loss: 16.681, Final Batch Loss: 0.413\n",
      "Epoch 14920, Loss: 16.356, Final Batch Loss: 0.518\n",
      "Epoch 14921, Loss: 16.157, Final Batch Loss: 0.344\n",
      "Epoch 14922, Loss: 16.088, Final Batch Loss: 0.389\n",
      "Epoch 14923, Loss: 16.072, Final Batch Loss: 0.470\n",
      "Epoch 14924, Loss: 16.199, Final Batch Loss: 0.386\n",
      "Epoch 14925, Loss: 16.270, Final Batch Loss: 0.429\n",
      "Epoch 14926, Loss: 16.103, Final Batch Loss: 0.542\n",
      "Epoch 14927, Loss: 16.142, Final Batch Loss: 0.419\n",
      "Epoch 14928, Loss: 16.284, Final Batch Loss: 0.606\n",
      "Epoch 14929, Loss: 16.337, Final Batch Loss: 0.402\n",
      "Epoch 14930, Loss: 16.058, Final Batch Loss: 0.418\n",
      "Epoch 14931, Loss: 15.930, Final Batch Loss: 0.381\n",
      "Epoch 14932, Loss: 16.307, Final Batch Loss: 0.499\n",
      "Epoch 14933, Loss: 16.061, Final Batch Loss: 0.563\n",
      "Epoch 14934, Loss: 16.115, Final Batch Loss: 0.400\n",
      "Epoch 14935, Loss: 16.677, Final Batch Loss: 0.424\n",
      "Epoch 14936, Loss: 16.276, Final Batch Loss: 0.477\n",
      "Epoch 14937, Loss: 16.275, Final Batch Loss: 0.479\n",
      "Epoch 14938, Loss: 16.224, Final Batch Loss: 0.439\n",
      "Epoch 14939, Loss: 16.200, Final Batch Loss: 0.448\n",
      "Epoch 14940, Loss: 16.098, Final Batch Loss: 0.411\n",
      "Epoch 14941, Loss: 16.034, Final Batch Loss: 0.438\n",
      "Epoch 14942, Loss: 16.491, Final Batch Loss: 0.461\n",
      "Epoch 14943, Loss: 16.201, Final Batch Loss: 0.481\n",
      "Epoch 14944, Loss: 16.157, Final Batch Loss: 0.367\n",
      "Epoch 14945, Loss: 16.005, Final Batch Loss: 0.435\n",
      "Epoch 14946, Loss: 16.183, Final Batch Loss: 0.467\n",
      "Epoch 14947, Loss: 16.180, Final Batch Loss: 0.543\n",
      "Epoch 14948, Loss: 16.373, Final Batch Loss: 0.445\n",
      "Epoch 14949, Loss: 16.030, Final Batch Loss: 0.447\n",
      "Epoch 14950, Loss: 16.141, Final Batch Loss: 0.394\n",
      "Epoch 14951, Loss: 16.121, Final Batch Loss: 0.527\n",
      "Epoch 14952, Loss: 16.129, Final Batch Loss: 0.349\n",
      "Epoch 14953, Loss: 16.339, Final Batch Loss: 0.579\n",
      "Epoch 14954, Loss: 16.117, Final Batch Loss: 0.495\n",
      "Epoch 14955, Loss: 16.217, Final Batch Loss: 0.482\n",
      "Epoch 14956, Loss: 16.361, Final Batch Loss: 0.426\n",
      "Epoch 14957, Loss: 16.163, Final Batch Loss: 0.383\n",
      "Epoch 14958, Loss: 16.119, Final Batch Loss: 0.544\n",
      "Epoch 14959, Loss: 16.167, Final Batch Loss: 0.428\n",
      "Epoch 14960, Loss: 16.302, Final Batch Loss: 0.613\n",
      "Epoch 14961, Loss: 16.355, Final Batch Loss: 0.442\n",
      "Epoch 14962, Loss: 15.990, Final Batch Loss: 0.433\n",
      "Epoch 14963, Loss: 16.203, Final Batch Loss: 0.411\n",
      "Epoch 14964, Loss: 16.474, Final Batch Loss: 0.594\n",
      "Epoch 14965, Loss: 16.529, Final Batch Loss: 0.505\n",
      "Epoch 14966, Loss: 16.070, Final Batch Loss: 0.324\n",
      "Epoch 14967, Loss: 16.151, Final Batch Loss: 0.471\n",
      "Epoch 14968, Loss: 15.963, Final Batch Loss: 0.321\n",
      "Epoch 14969, Loss: 16.459, Final Batch Loss: 0.441\n",
      "Epoch 14970, Loss: 16.040, Final Batch Loss: 0.513\n",
      "Epoch 14971, Loss: 16.254, Final Batch Loss: 0.495\n",
      "Epoch 14972, Loss: 16.012, Final Batch Loss: 0.512\n",
      "Epoch 14973, Loss: 16.296, Final Batch Loss: 0.531\n",
      "Epoch 14974, Loss: 16.334, Final Batch Loss: 0.464\n",
      "Epoch 14975, Loss: 16.351, Final Batch Loss: 0.469\n",
      "Epoch 14976, Loss: 16.077, Final Batch Loss: 0.518\n",
      "Epoch 14977, Loss: 16.217, Final Batch Loss: 0.548\n",
      "Epoch 14978, Loss: 16.458, Final Batch Loss: 0.527\n",
      "Epoch 14979, Loss: 16.179, Final Batch Loss: 0.429\n",
      "Epoch 14980, Loss: 16.132, Final Batch Loss: 0.516\n",
      "Epoch 14981, Loss: 16.349, Final Batch Loss: 0.515\n",
      "Epoch 14982, Loss: 16.318, Final Batch Loss: 0.438\n",
      "Epoch 14983, Loss: 16.021, Final Batch Loss: 0.480\n",
      "Epoch 14984, Loss: 16.080, Final Batch Loss: 0.399\n",
      "Epoch 14985, Loss: 16.178, Final Batch Loss: 0.531\n",
      "Epoch 14986, Loss: 15.873, Final Batch Loss: 0.424\n",
      "Epoch 14987, Loss: 16.234, Final Batch Loss: 0.414\n",
      "Epoch 14988, Loss: 16.195, Final Batch Loss: 0.472\n",
      "Epoch 14989, Loss: 16.149, Final Batch Loss: 0.408\n",
      "Epoch 14990, Loss: 16.058, Final Batch Loss: 0.384\n",
      "Epoch 14991, Loss: 16.037, Final Batch Loss: 0.385\n",
      "Epoch 14992, Loss: 16.353, Final Batch Loss: 0.509\n",
      "Epoch 14993, Loss: 16.131, Final Batch Loss: 0.470\n",
      "Epoch 14994, Loss: 16.287, Final Batch Loss: 0.402\n",
      "Epoch 14995, Loss: 16.355, Final Batch Loss: 0.582\n",
      "Epoch 14996, Loss: 16.140, Final Batch Loss: 0.338\n",
      "Epoch 14997, Loss: 16.428, Final Batch Loss: 0.419\n",
      "Epoch 14998, Loss: 16.139, Final Batch Loss: 0.476\n",
      "Epoch 14999, Loss: 16.193, Final Batch Loss: 0.487\n",
      "Epoch 15000, Loss: 16.132, Final Batch Loss: 0.496\n",
      "Epoch 15001, Loss: 16.056, Final Batch Loss: 0.486\n",
      "Epoch 15002, Loss: 15.997, Final Batch Loss: 0.415\n",
      "Epoch 15003, Loss: 16.203, Final Batch Loss: 0.494\n",
      "Epoch 15004, Loss: 16.161, Final Batch Loss: 0.428\n",
      "Epoch 15005, Loss: 16.416, Final Batch Loss: 0.576\n",
      "Epoch 15006, Loss: 16.093, Final Batch Loss: 0.395\n",
      "Epoch 15007, Loss: 16.134, Final Batch Loss: 0.413\n",
      "Epoch 15008, Loss: 16.303, Final Batch Loss: 0.480\n",
      "Epoch 15009, Loss: 16.180, Final Batch Loss: 0.458\n",
      "Epoch 15010, Loss: 16.373, Final Batch Loss: 0.539\n",
      "Epoch 15011, Loss: 16.161, Final Batch Loss: 0.389\n",
      "Epoch 15012, Loss: 16.267, Final Batch Loss: 0.395\n",
      "Epoch 15013, Loss: 16.002, Final Batch Loss: 0.414\n",
      "Epoch 15014, Loss: 16.237, Final Batch Loss: 0.467\n",
      "Epoch 15015, Loss: 16.234, Final Batch Loss: 0.374\n",
      "Epoch 15016, Loss: 16.402, Final Batch Loss: 0.485\n",
      "Epoch 15017, Loss: 16.125, Final Batch Loss: 0.406\n",
      "Epoch 15018, Loss: 16.228, Final Batch Loss: 0.415\n",
      "Epoch 15019, Loss: 16.368, Final Batch Loss: 0.411\n",
      "Epoch 15020, Loss: 16.330, Final Batch Loss: 0.453\n",
      "Epoch 15021, Loss: 16.398, Final Batch Loss: 0.442\n",
      "Epoch 15022, Loss: 16.224, Final Batch Loss: 0.428\n",
      "Epoch 15023, Loss: 16.328, Final Batch Loss: 0.435\n",
      "Epoch 15024, Loss: 16.258, Final Batch Loss: 0.352\n",
      "Epoch 15025, Loss: 16.233, Final Batch Loss: 0.349\n",
      "Epoch 15026, Loss: 16.183, Final Batch Loss: 0.495\n",
      "Epoch 15027, Loss: 16.303, Final Batch Loss: 0.486\n",
      "Epoch 15028, Loss: 16.370, Final Batch Loss: 0.494\n",
      "Epoch 15029, Loss: 15.954, Final Batch Loss: 0.450\n",
      "Epoch 15030, Loss: 16.143, Final Batch Loss: 0.456\n",
      "Epoch 15031, Loss: 16.274, Final Batch Loss: 0.381\n",
      "Epoch 15032, Loss: 16.113, Final Batch Loss: 0.362\n",
      "Epoch 15033, Loss: 16.357, Final Batch Loss: 0.437\n",
      "Epoch 15034, Loss: 16.010, Final Batch Loss: 0.431\n",
      "Epoch 15035, Loss: 16.192, Final Batch Loss: 0.423\n",
      "Epoch 15036, Loss: 16.212, Final Batch Loss: 0.382\n",
      "Epoch 15037, Loss: 16.494, Final Batch Loss: 0.468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15038, Loss: 16.320, Final Batch Loss: 0.398\n",
      "Epoch 15039, Loss: 16.012, Final Batch Loss: 0.401\n",
      "Epoch 15040, Loss: 16.048, Final Batch Loss: 0.400\n",
      "Epoch 15041, Loss: 16.068, Final Batch Loss: 0.438\n",
      "Epoch 15042, Loss: 16.392, Final Batch Loss: 0.600\n",
      "Epoch 15043, Loss: 16.255, Final Batch Loss: 0.395\n",
      "Epoch 15044, Loss: 16.248, Final Batch Loss: 0.523\n",
      "Epoch 15045, Loss: 16.234, Final Batch Loss: 0.507\n",
      "Epoch 15046, Loss: 16.291, Final Batch Loss: 0.500\n",
      "Epoch 15047, Loss: 16.194, Final Batch Loss: 0.438\n",
      "Epoch 15048, Loss: 16.159, Final Batch Loss: 0.452\n",
      "Epoch 15049, Loss: 16.254, Final Batch Loss: 0.485\n",
      "Epoch 15050, Loss: 16.549, Final Batch Loss: 0.438\n",
      "Epoch 15051, Loss: 16.476, Final Batch Loss: 0.499\n",
      "Epoch 15052, Loss: 15.989, Final Batch Loss: 0.413\n",
      "Epoch 15053, Loss: 16.117, Final Batch Loss: 0.468\n",
      "Epoch 15054, Loss: 15.900, Final Batch Loss: 0.409\n",
      "Epoch 15055, Loss: 16.494, Final Batch Loss: 0.544\n",
      "Epoch 15056, Loss: 16.351, Final Batch Loss: 0.370\n",
      "Epoch 15057, Loss: 16.277, Final Batch Loss: 0.495\n",
      "Epoch 15058, Loss: 16.185, Final Batch Loss: 0.408\n",
      "Epoch 15059, Loss: 16.225, Final Batch Loss: 0.431\n",
      "Epoch 15060, Loss: 16.240, Final Batch Loss: 0.486\n",
      "Epoch 15061, Loss: 16.125, Final Batch Loss: 0.391\n",
      "Epoch 15062, Loss: 16.183, Final Batch Loss: 0.428\n",
      "Epoch 15063, Loss: 16.305, Final Batch Loss: 0.483\n",
      "Epoch 15064, Loss: 16.040, Final Batch Loss: 0.471\n",
      "Epoch 15065, Loss: 16.065, Final Batch Loss: 0.490\n",
      "Epoch 15066, Loss: 16.061, Final Batch Loss: 0.488\n",
      "Epoch 15067, Loss: 16.274, Final Batch Loss: 0.444\n",
      "Epoch 15068, Loss: 16.216, Final Batch Loss: 0.521\n",
      "Epoch 15069, Loss: 16.032, Final Batch Loss: 0.479\n",
      "Epoch 15070, Loss: 16.210, Final Batch Loss: 0.442\n",
      "Epoch 15071, Loss: 16.145, Final Batch Loss: 0.469\n",
      "Epoch 15072, Loss: 16.242, Final Batch Loss: 0.453\n",
      "Epoch 15073, Loss: 16.150, Final Batch Loss: 0.396\n",
      "Epoch 15074, Loss: 16.089, Final Batch Loss: 0.452\n",
      "Epoch 15075, Loss: 16.110, Final Batch Loss: 0.467\n",
      "Epoch 15076, Loss: 16.316, Final Batch Loss: 0.386\n",
      "Epoch 15077, Loss: 16.162, Final Batch Loss: 0.460\n",
      "Epoch 15078, Loss: 16.249, Final Batch Loss: 0.513\n",
      "Epoch 15079, Loss: 15.909, Final Batch Loss: 0.290\n",
      "Epoch 15080, Loss: 16.211, Final Batch Loss: 0.452\n",
      "Epoch 15081, Loss: 16.054, Final Batch Loss: 0.407\n",
      "Epoch 15082, Loss: 16.056, Final Batch Loss: 0.448\n",
      "Epoch 15083, Loss: 16.191, Final Batch Loss: 0.434\n",
      "Epoch 15084, Loss: 16.180, Final Batch Loss: 0.422\n",
      "Epoch 15085, Loss: 16.179, Final Batch Loss: 0.495\n",
      "Epoch 15086, Loss: 16.096, Final Batch Loss: 0.433\n",
      "Epoch 15087, Loss: 16.358, Final Batch Loss: 0.514\n",
      "Epoch 15088, Loss: 16.174, Final Batch Loss: 0.365\n",
      "Epoch 15089, Loss: 16.399, Final Batch Loss: 0.576\n",
      "Epoch 15090, Loss: 16.439, Final Batch Loss: 0.538\n",
      "Epoch 15091, Loss: 16.171, Final Batch Loss: 0.429\n",
      "Epoch 15092, Loss: 16.219, Final Batch Loss: 0.354\n",
      "Epoch 15093, Loss: 16.430, Final Batch Loss: 0.610\n",
      "Epoch 15094, Loss: 16.207, Final Batch Loss: 0.484\n",
      "Epoch 15095, Loss: 16.056, Final Batch Loss: 0.433\n",
      "Epoch 15096, Loss: 16.037, Final Batch Loss: 0.372\n",
      "Epoch 15097, Loss: 16.381, Final Batch Loss: 0.531\n",
      "Epoch 15098, Loss: 16.010, Final Batch Loss: 0.418\n",
      "Epoch 15099, Loss: 16.192, Final Batch Loss: 0.388\n",
      "Epoch 15100, Loss: 15.958, Final Batch Loss: 0.412\n",
      "Epoch 15101, Loss: 16.197, Final Batch Loss: 0.457\n",
      "Epoch 15102, Loss: 16.418, Final Batch Loss: 0.616\n",
      "Epoch 15103, Loss: 16.028, Final Batch Loss: 0.389\n",
      "Epoch 15104, Loss: 15.792, Final Batch Loss: 0.556\n",
      "Epoch 15105, Loss: 16.460, Final Batch Loss: 0.419\n",
      "Epoch 15106, Loss: 16.176, Final Batch Loss: 0.559\n",
      "Epoch 15107, Loss: 15.839, Final Batch Loss: 0.438\n",
      "Epoch 15108, Loss: 16.300, Final Batch Loss: 0.490\n",
      "Epoch 15109, Loss: 15.848, Final Batch Loss: 0.440\n",
      "Epoch 15110, Loss: 16.252, Final Batch Loss: 0.439\n",
      "Epoch 15111, Loss: 16.440, Final Batch Loss: 0.400\n",
      "Epoch 15112, Loss: 15.887, Final Batch Loss: 0.443\n",
      "Epoch 15113, Loss: 16.148, Final Batch Loss: 0.494\n",
      "Epoch 15114, Loss: 16.081, Final Batch Loss: 0.456\n",
      "Epoch 15115, Loss: 16.068, Final Batch Loss: 0.500\n",
      "Epoch 15116, Loss: 16.312, Final Batch Loss: 0.578\n",
      "Epoch 15117, Loss: 16.406, Final Batch Loss: 0.520\n",
      "Epoch 15118, Loss: 16.093, Final Batch Loss: 0.347\n",
      "Epoch 15119, Loss: 15.942, Final Batch Loss: 0.366\n",
      "Epoch 15120, Loss: 16.122, Final Batch Loss: 0.438\n",
      "Epoch 15121, Loss: 15.923, Final Batch Loss: 0.445\n",
      "Epoch 15122, Loss: 16.199, Final Batch Loss: 0.487\n",
      "Epoch 15123, Loss: 16.292, Final Batch Loss: 0.383\n",
      "Epoch 15124, Loss: 16.233, Final Batch Loss: 0.454\n",
      "Epoch 15125, Loss: 16.264, Final Batch Loss: 0.493\n",
      "Epoch 15126, Loss: 15.930, Final Batch Loss: 0.320\n",
      "Epoch 15127, Loss: 16.156, Final Batch Loss: 0.478\n",
      "Epoch 15128, Loss: 16.367, Final Batch Loss: 0.528\n",
      "Epoch 15129, Loss: 16.483, Final Batch Loss: 0.563\n",
      "Epoch 15130, Loss: 16.141, Final Batch Loss: 0.407\n",
      "Epoch 15131, Loss: 16.108, Final Batch Loss: 0.460\n",
      "Epoch 15132, Loss: 15.875, Final Batch Loss: 0.491\n",
      "Epoch 15133, Loss: 16.347, Final Batch Loss: 0.464\n",
      "Epoch 15134, Loss: 16.184, Final Batch Loss: 0.507\n",
      "Epoch 15135, Loss: 15.969, Final Batch Loss: 0.434\n",
      "Epoch 15136, Loss: 16.118, Final Batch Loss: 0.419\n",
      "Epoch 15137, Loss: 16.023, Final Batch Loss: 0.473\n",
      "Epoch 15138, Loss: 16.172, Final Batch Loss: 0.408\n",
      "Epoch 15139, Loss: 16.042, Final Batch Loss: 0.448\n",
      "Epoch 15140, Loss: 16.131, Final Batch Loss: 0.512\n",
      "Epoch 15141, Loss: 16.142, Final Batch Loss: 0.459\n",
      "Epoch 15142, Loss: 15.996, Final Batch Loss: 0.500\n",
      "Epoch 15143, Loss: 16.306, Final Batch Loss: 0.449\n",
      "Epoch 15144, Loss: 16.250, Final Batch Loss: 0.522\n",
      "Epoch 15145, Loss: 16.016, Final Batch Loss: 0.508\n",
      "Epoch 15146, Loss: 16.195, Final Batch Loss: 0.366\n",
      "Epoch 15147, Loss: 16.031, Final Batch Loss: 0.462\n",
      "Epoch 15148, Loss: 16.271, Final Batch Loss: 0.445\n",
      "Epoch 15149, Loss: 16.245, Final Batch Loss: 0.512\n",
      "Epoch 15150, Loss: 16.246, Final Batch Loss: 0.478\n",
      "Epoch 15151, Loss: 16.147, Final Batch Loss: 0.450\n",
      "Epoch 15152, Loss: 16.327, Final Batch Loss: 0.529\n",
      "Epoch 15153, Loss: 16.429, Final Batch Loss: 0.501\n",
      "Epoch 15154, Loss: 15.942, Final Batch Loss: 0.381\n",
      "Epoch 15155, Loss: 16.359, Final Batch Loss: 0.471\n",
      "Epoch 15156, Loss: 15.974, Final Batch Loss: 0.452\n",
      "Epoch 15157, Loss: 16.375, Final Batch Loss: 0.509\n",
      "Epoch 15158, Loss: 16.495, Final Batch Loss: 0.558\n",
      "Epoch 15159, Loss: 16.160, Final Batch Loss: 0.501\n",
      "Epoch 15160, Loss: 16.292, Final Batch Loss: 0.487\n",
      "Epoch 15161, Loss: 16.001, Final Batch Loss: 0.327\n",
      "Epoch 15162, Loss: 16.008, Final Batch Loss: 0.500\n",
      "Epoch 15163, Loss: 16.249, Final Batch Loss: 0.438\n",
      "Epoch 15164, Loss: 16.278, Final Batch Loss: 0.388\n",
      "Epoch 15165, Loss: 16.158, Final Batch Loss: 0.382\n",
      "Epoch 15166, Loss: 16.073, Final Batch Loss: 0.421\n",
      "Epoch 15167, Loss: 16.067, Final Batch Loss: 0.413\n",
      "Epoch 15168, Loss: 16.009, Final Batch Loss: 0.501\n",
      "Epoch 15169, Loss: 16.184, Final Batch Loss: 0.509\n",
      "Epoch 15170, Loss: 16.141, Final Batch Loss: 0.439\n",
      "Epoch 15171, Loss: 15.895, Final Batch Loss: 0.324\n",
      "Epoch 15172, Loss: 16.200, Final Batch Loss: 0.578\n",
      "Epoch 15173, Loss: 16.150, Final Batch Loss: 0.468\n",
      "Epoch 15174, Loss: 16.326, Final Batch Loss: 0.399\n",
      "Epoch 15175, Loss: 16.085, Final Batch Loss: 0.502\n",
      "Epoch 15176, Loss: 16.254, Final Batch Loss: 0.461\n",
      "Epoch 15177, Loss: 16.043, Final Batch Loss: 0.495\n",
      "Epoch 15178, Loss: 16.174, Final Batch Loss: 0.448\n",
      "Epoch 15179, Loss: 16.132, Final Batch Loss: 0.410\n",
      "Epoch 15180, Loss: 16.394, Final Batch Loss: 0.586\n",
      "Epoch 15181, Loss: 16.169, Final Batch Loss: 0.480\n",
      "Epoch 15182, Loss: 16.245, Final Batch Loss: 0.424\n",
      "Epoch 15183, Loss: 16.096, Final Batch Loss: 0.428\n",
      "Epoch 15184, Loss: 16.497, Final Batch Loss: 0.487\n",
      "Epoch 15185, Loss: 15.909, Final Batch Loss: 0.417\n",
      "Epoch 15186, Loss: 16.309, Final Batch Loss: 0.525\n",
      "Epoch 15187, Loss: 16.366, Final Batch Loss: 0.402\n",
      "Epoch 15188, Loss: 16.475, Final Batch Loss: 0.512\n",
      "Epoch 15189, Loss: 16.262, Final Batch Loss: 0.483\n",
      "Epoch 15190, Loss: 16.282, Final Batch Loss: 0.458\n",
      "Epoch 15191, Loss: 16.205, Final Batch Loss: 0.390\n",
      "Epoch 15192, Loss: 16.380, Final Batch Loss: 0.451\n",
      "Epoch 15193, Loss: 16.093, Final Batch Loss: 0.415\n",
      "Epoch 15194, Loss: 16.098, Final Batch Loss: 0.395\n",
      "Epoch 15195, Loss: 15.742, Final Batch Loss: 0.451\n",
      "Epoch 15196, Loss: 16.216, Final Batch Loss: 0.379\n",
      "Epoch 15197, Loss: 16.155, Final Batch Loss: 0.430\n",
      "Epoch 15198, Loss: 16.027, Final Batch Loss: 0.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15199, Loss: 16.303, Final Batch Loss: 0.383\n",
      "Epoch 15200, Loss: 16.208, Final Batch Loss: 0.536\n",
      "Epoch 15201, Loss: 16.097, Final Batch Loss: 0.414\n",
      "Epoch 15202, Loss: 16.081, Final Batch Loss: 0.411\n",
      "Epoch 15203, Loss: 16.208, Final Batch Loss: 0.458\n",
      "Epoch 15204, Loss: 16.366, Final Batch Loss: 0.383\n",
      "Epoch 15205, Loss: 16.060, Final Batch Loss: 0.389\n",
      "Epoch 15206, Loss: 16.263, Final Batch Loss: 0.382\n",
      "Epoch 15207, Loss: 16.028, Final Batch Loss: 0.473\n",
      "Epoch 15208, Loss: 16.092, Final Batch Loss: 0.469\n",
      "Epoch 15209, Loss: 16.320, Final Batch Loss: 0.446\n",
      "Epoch 15210, Loss: 16.436, Final Batch Loss: 0.484\n",
      "Epoch 15211, Loss: 16.164, Final Batch Loss: 0.433\n",
      "Epoch 15212, Loss: 16.029, Final Batch Loss: 0.363\n",
      "Epoch 15213, Loss: 16.204, Final Batch Loss: 0.368\n",
      "Epoch 15214, Loss: 16.152, Final Batch Loss: 0.483\n",
      "Epoch 15215, Loss: 16.181, Final Batch Loss: 0.536\n",
      "Epoch 15216, Loss: 16.222, Final Batch Loss: 0.481\n",
      "Epoch 15217, Loss: 16.059, Final Batch Loss: 0.480\n",
      "Epoch 15218, Loss: 16.178, Final Batch Loss: 0.474\n",
      "Epoch 15219, Loss: 16.243, Final Batch Loss: 0.349\n",
      "Epoch 15220, Loss: 16.424, Final Batch Loss: 0.514\n",
      "Epoch 15221, Loss: 16.247, Final Batch Loss: 0.490\n",
      "Epoch 15222, Loss: 15.832, Final Batch Loss: 0.416\n",
      "Epoch 15223, Loss: 16.007, Final Batch Loss: 0.482\n",
      "Epoch 15224, Loss: 16.070, Final Batch Loss: 0.415\n",
      "Epoch 15225, Loss: 16.302, Final Batch Loss: 0.463\n",
      "Epoch 15226, Loss: 16.046, Final Batch Loss: 0.519\n",
      "Epoch 15227, Loss: 16.089, Final Batch Loss: 0.451\n",
      "Epoch 15228, Loss: 16.023, Final Batch Loss: 0.450\n",
      "Epoch 15229, Loss: 16.508, Final Batch Loss: 0.511\n",
      "Epoch 15230, Loss: 16.203, Final Batch Loss: 0.427\n",
      "Epoch 15231, Loss: 16.149, Final Batch Loss: 0.359\n",
      "Epoch 15232, Loss: 16.219, Final Batch Loss: 0.492\n",
      "Epoch 15233, Loss: 16.123, Final Batch Loss: 0.361\n",
      "Epoch 15234, Loss: 16.020, Final Batch Loss: 0.444\n",
      "Epoch 15235, Loss: 16.164, Final Batch Loss: 0.423\n",
      "Epoch 15236, Loss: 16.070, Final Batch Loss: 0.578\n",
      "Epoch 15237, Loss: 16.295, Final Batch Loss: 0.406\n",
      "Epoch 15238, Loss: 16.316, Final Batch Loss: 0.483\n",
      "Epoch 15239, Loss: 16.068, Final Batch Loss: 0.395\n",
      "Epoch 15240, Loss: 16.102, Final Batch Loss: 0.497\n",
      "Epoch 15241, Loss: 16.270, Final Batch Loss: 0.495\n",
      "Epoch 15242, Loss: 16.057, Final Batch Loss: 0.419\n",
      "Epoch 15243, Loss: 16.323, Final Batch Loss: 0.430\n",
      "Epoch 15244, Loss: 16.027, Final Batch Loss: 0.457\n",
      "Epoch 15245, Loss: 16.503, Final Batch Loss: 0.593\n",
      "Epoch 15246, Loss: 16.066, Final Batch Loss: 0.511\n",
      "Epoch 15247, Loss: 16.600, Final Batch Loss: 0.442\n",
      "Epoch 15248, Loss: 16.373, Final Batch Loss: 0.478\n",
      "Epoch 15249, Loss: 16.169, Final Batch Loss: 0.402\n",
      "Epoch 15250, Loss: 16.100, Final Batch Loss: 0.399\n",
      "Epoch 15251, Loss: 16.154, Final Batch Loss: 0.468\n",
      "Epoch 15252, Loss: 15.973, Final Batch Loss: 0.356\n",
      "Epoch 15253, Loss: 16.380, Final Batch Loss: 0.378\n",
      "Epoch 15254, Loss: 16.291, Final Batch Loss: 0.389\n",
      "Epoch 15255, Loss: 15.929, Final Batch Loss: 0.446\n",
      "Epoch 15256, Loss: 16.333, Final Batch Loss: 0.499\n",
      "Epoch 15257, Loss: 16.279, Final Batch Loss: 0.403\n",
      "Epoch 15258, Loss: 16.309, Final Batch Loss: 0.392\n",
      "Epoch 15259, Loss: 15.887, Final Batch Loss: 0.304\n",
      "Epoch 15260, Loss: 16.194, Final Batch Loss: 0.470\n",
      "Epoch 15261, Loss: 16.140, Final Batch Loss: 0.434\n",
      "Epoch 15262, Loss: 15.959, Final Batch Loss: 0.491\n",
      "Epoch 15263, Loss: 16.069, Final Batch Loss: 0.381\n",
      "Epoch 15264, Loss: 16.136, Final Batch Loss: 0.445\n",
      "Epoch 15265, Loss: 16.314, Final Batch Loss: 0.485\n",
      "Epoch 15266, Loss: 16.124, Final Batch Loss: 0.427\n",
      "Epoch 15267, Loss: 16.152, Final Batch Loss: 0.443\n",
      "Epoch 15268, Loss: 15.968, Final Batch Loss: 0.347\n",
      "Epoch 15269, Loss: 16.257, Final Batch Loss: 0.351\n",
      "Epoch 15270, Loss: 16.254, Final Batch Loss: 0.524\n",
      "Epoch 15271, Loss: 16.417, Final Batch Loss: 0.478\n",
      "Epoch 15272, Loss: 15.964, Final Batch Loss: 0.473\n",
      "Epoch 15273, Loss: 15.984, Final Batch Loss: 0.501\n",
      "Epoch 15274, Loss: 16.238, Final Batch Loss: 0.545\n",
      "Epoch 15275, Loss: 16.188, Final Batch Loss: 0.356\n",
      "Epoch 15276, Loss: 15.804, Final Batch Loss: 0.393\n",
      "Epoch 15277, Loss: 16.184, Final Batch Loss: 0.486\n",
      "Epoch 15278, Loss: 16.183, Final Batch Loss: 0.457\n",
      "Epoch 15279, Loss: 16.445, Final Batch Loss: 0.395\n",
      "Epoch 15280, Loss: 16.018, Final Batch Loss: 0.406\n",
      "Epoch 15281, Loss: 16.139, Final Batch Loss: 0.507\n",
      "Epoch 15282, Loss: 16.003, Final Batch Loss: 0.409\n",
      "Epoch 15283, Loss: 15.881, Final Batch Loss: 0.377\n",
      "Epoch 15284, Loss: 15.935, Final Batch Loss: 0.393\n",
      "Epoch 15285, Loss: 16.057, Final Batch Loss: 0.377\n",
      "Epoch 15286, Loss: 16.244, Final Batch Loss: 0.368\n",
      "Epoch 15287, Loss: 16.109, Final Batch Loss: 0.507\n",
      "Epoch 15288, Loss: 16.270, Final Batch Loss: 0.491\n",
      "Epoch 15289, Loss: 16.159, Final Batch Loss: 0.519\n",
      "Epoch 15290, Loss: 16.133, Final Batch Loss: 0.432\n",
      "Epoch 15291, Loss: 16.268, Final Batch Loss: 0.519\n",
      "Epoch 15292, Loss: 16.346, Final Batch Loss: 0.413\n",
      "Epoch 15293, Loss: 16.283, Final Batch Loss: 0.455\n",
      "Epoch 15294, Loss: 15.988, Final Batch Loss: 0.428\n",
      "Epoch 15295, Loss: 16.435, Final Batch Loss: 0.479\n",
      "Epoch 15296, Loss: 16.179, Final Batch Loss: 0.365\n",
      "Epoch 15297, Loss: 15.799, Final Batch Loss: 0.389\n",
      "Epoch 15298, Loss: 16.153, Final Batch Loss: 0.466\n",
      "Epoch 15299, Loss: 15.954, Final Batch Loss: 0.420\n",
      "Epoch 15300, Loss: 16.312, Final Batch Loss: 0.461\n",
      "Epoch 15301, Loss: 16.391, Final Batch Loss: 0.459\n",
      "Epoch 15302, Loss: 16.275, Final Batch Loss: 0.491\n",
      "Epoch 15303, Loss: 16.275, Final Batch Loss: 0.541\n",
      "Epoch 15304, Loss: 16.246, Final Batch Loss: 0.488\n",
      "Epoch 15305, Loss: 16.063, Final Batch Loss: 0.471\n",
      "Epoch 15306, Loss: 16.236, Final Batch Loss: 0.382\n",
      "Epoch 15307, Loss: 16.136, Final Batch Loss: 0.407\n",
      "Epoch 15308, Loss: 16.286, Final Batch Loss: 0.480\n",
      "Epoch 15309, Loss: 16.332, Final Batch Loss: 0.542\n",
      "Epoch 15310, Loss: 16.041, Final Batch Loss: 0.461\n",
      "Epoch 15311, Loss: 16.193, Final Batch Loss: 0.457\n",
      "Epoch 15312, Loss: 16.265, Final Batch Loss: 0.439\n",
      "Epoch 15313, Loss: 16.400, Final Batch Loss: 0.442\n",
      "Epoch 15314, Loss: 15.808, Final Batch Loss: 0.516\n",
      "Epoch 15315, Loss: 16.240, Final Batch Loss: 0.472\n",
      "Epoch 15316, Loss: 16.090, Final Batch Loss: 0.446\n",
      "Epoch 15317, Loss: 16.320, Final Batch Loss: 0.473\n",
      "Epoch 15318, Loss: 16.315, Final Batch Loss: 0.543\n",
      "Epoch 15319, Loss: 16.484, Final Batch Loss: 0.539\n",
      "Epoch 15320, Loss: 16.405, Final Batch Loss: 0.476\n",
      "Epoch 15321, Loss: 16.085, Final Batch Loss: 0.476\n",
      "Epoch 15322, Loss: 16.467, Final Batch Loss: 0.504\n",
      "Epoch 15323, Loss: 16.036, Final Batch Loss: 0.482\n",
      "Epoch 15324, Loss: 16.303, Final Batch Loss: 0.418\n",
      "Epoch 15325, Loss: 16.318, Final Batch Loss: 0.483\n",
      "Epoch 15326, Loss: 16.043, Final Batch Loss: 0.418\n",
      "Epoch 15327, Loss: 16.096, Final Batch Loss: 0.484\n",
      "Epoch 15328, Loss: 16.126, Final Batch Loss: 0.500\n",
      "Epoch 15329, Loss: 16.281, Final Batch Loss: 0.508\n",
      "Epoch 15330, Loss: 16.125, Final Batch Loss: 0.361\n",
      "Epoch 15331, Loss: 16.112, Final Batch Loss: 0.525\n",
      "Epoch 15332, Loss: 16.409, Final Batch Loss: 0.428\n",
      "Epoch 15333, Loss: 16.101, Final Batch Loss: 0.471\n",
      "Epoch 15334, Loss: 16.155, Final Batch Loss: 0.527\n",
      "Epoch 15335, Loss: 16.020, Final Batch Loss: 0.495\n",
      "Epoch 15336, Loss: 16.298, Final Batch Loss: 0.541\n",
      "Epoch 15337, Loss: 16.205, Final Batch Loss: 0.474\n",
      "Epoch 15338, Loss: 16.079, Final Batch Loss: 0.397\n",
      "Epoch 15339, Loss: 16.448, Final Batch Loss: 0.549\n",
      "Epoch 15340, Loss: 16.228, Final Batch Loss: 0.468\n",
      "Epoch 15341, Loss: 16.228, Final Batch Loss: 0.463\n",
      "Epoch 15342, Loss: 16.094, Final Batch Loss: 0.467\n",
      "Epoch 15343, Loss: 16.220, Final Batch Loss: 0.418\n",
      "Epoch 15344, Loss: 16.039, Final Batch Loss: 0.408\n",
      "Epoch 15345, Loss: 16.216, Final Batch Loss: 0.493\n",
      "Epoch 15346, Loss: 16.467, Final Batch Loss: 0.554\n",
      "Epoch 15347, Loss: 16.125, Final Batch Loss: 0.405\n",
      "Epoch 15348, Loss: 16.454, Final Batch Loss: 0.463\n",
      "Epoch 15349, Loss: 16.128, Final Batch Loss: 0.405\n",
      "Epoch 15350, Loss: 16.339, Final Batch Loss: 0.392\n",
      "Epoch 15351, Loss: 15.942, Final Batch Loss: 0.419\n",
      "Epoch 15352, Loss: 16.246, Final Batch Loss: 0.492\n",
      "Epoch 15353, Loss: 15.987, Final Batch Loss: 0.371\n",
      "Epoch 15354, Loss: 16.274, Final Batch Loss: 0.386\n",
      "Epoch 15355, Loss: 16.469, Final Batch Loss: 0.389\n",
      "Epoch 15356, Loss: 16.188, Final Batch Loss: 0.410\n",
      "Epoch 15357, Loss: 16.395, Final Batch Loss: 0.409\n",
      "Epoch 15358, Loss: 16.303, Final Batch Loss: 0.400\n",
      "Epoch 15359, Loss: 16.366, Final Batch Loss: 0.493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15360, Loss: 16.168, Final Batch Loss: 0.473\n",
      "Epoch 15361, Loss: 16.405, Final Batch Loss: 0.445\n",
      "Epoch 15362, Loss: 16.134, Final Batch Loss: 0.465\n",
      "Epoch 15363, Loss: 16.178, Final Batch Loss: 0.457\n",
      "Epoch 15364, Loss: 16.431, Final Batch Loss: 0.560\n",
      "Epoch 15365, Loss: 16.046, Final Batch Loss: 0.433\n",
      "Epoch 15366, Loss: 16.250, Final Batch Loss: 0.503\n",
      "Epoch 15367, Loss: 16.382, Final Batch Loss: 0.511\n",
      "Epoch 15368, Loss: 16.178, Final Batch Loss: 0.506\n",
      "Epoch 15369, Loss: 16.247, Final Batch Loss: 0.399\n",
      "Epoch 15370, Loss: 16.083, Final Batch Loss: 0.401\n",
      "Epoch 15371, Loss: 15.994, Final Batch Loss: 0.403\n",
      "Epoch 15372, Loss: 16.428, Final Batch Loss: 0.450\n",
      "Epoch 15373, Loss: 16.445, Final Batch Loss: 0.379\n",
      "Epoch 15374, Loss: 16.120, Final Batch Loss: 0.439\n",
      "Epoch 15375, Loss: 16.196, Final Batch Loss: 0.419\n",
      "Epoch 15376, Loss: 16.346, Final Batch Loss: 0.463\n",
      "Epoch 15377, Loss: 16.079, Final Batch Loss: 0.289\n",
      "Epoch 15378, Loss: 16.267, Final Batch Loss: 0.443\n",
      "Epoch 15379, Loss: 16.166, Final Batch Loss: 0.431\n",
      "Epoch 15380, Loss: 16.014, Final Batch Loss: 0.446\n",
      "Epoch 15381, Loss: 16.305, Final Batch Loss: 0.388\n",
      "Epoch 15382, Loss: 15.963, Final Batch Loss: 0.498\n",
      "Epoch 15383, Loss: 16.210, Final Batch Loss: 0.550\n",
      "Epoch 15384, Loss: 16.411, Final Batch Loss: 0.426\n",
      "Epoch 15385, Loss: 16.259, Final Batch Loss: 0.367\n",
      "Epoch 15386, Loss: 16.127, Final Batch Loss: 0.456\n",
      "Epoch 15387, Loss: 16.259, Final Batch Loss: 0.417\n",
      "Epoch 15388, Loss: 16.321, Final Batch Loss: 0.615\n",
      "Epoch 15389, Loss: 16.358, Final Batch Loss: 0.508\n",
      "Epoch 15390, Loss: 16.476, Final Batch Loss: 0.445\n",
      "Epoch 15391, Loss: 16.218, Final Batch Loss: 0.414\n",
      "Epoch 15392, Loss: 16.213, Final Batch Loss: 0.520\n",
      "Epoch 15393, Loss: 16.310, Final Batch Loss: 0.424\n",
      "Epoch 15394, Loss: 16.282, Final Batch Loss: 0.561\n",
      "Epoch 15395, Loss: 16.138, Final Batch Loss: 0.443\n",
      "Epoch 15396, Loss: 16.232, Final Batch Loss: 0.532\n",
      "Epoch 15397, Loss: 16.099, Final Batch Loss: 0.499\n",
      "Epoch 15398, Loss: 16.339, Final Batch Loss: 0.440\n",
      "Epoch 15399, Loss: 16.345, Final Batch Loss: 0.412\n",
      "Epoch 15400, Loss: 16.278, Final Batch Loss: 0.438\n",
      "Epoch 15401, Loss: 16.161, Final Batch Loss: 0.452\n",
      "Epoch 15402, Loss: 16.406, Final Batch Loss: 0.473\n",
      "Epoch 15403, Loss: 16.037, Final Batch Loss: 0.387\n",
      "Epoch 15404, Loss: 15.894, Final Batch Loss: 0.497\n",
      "Epoch 15405, Loss: 15.966, Final Batch Loss: 0.402\n",
      "Epoch 15406, Loss: 15.920, Final Batch Loss: 0.434\n",
      "Epoch 15407, Loss: 16.330, Final Batch Loss: 0.416\n",
      "Epoch 15408, Loss: 16.231, Final Batch Loss: 0.519\n",
      "Epoch 15409, Loss: 16.125, Final Batch Loss: 0.457\n",
      "Epoch 15410, Loss: 16.294, Final Batch Loss: 0.591\n",
      "Epoch 15411, Loss: 16.182, Final Batch Loss: 0.453\n",
      "Epoch 15412, Loss: 16.467, Final Batch Loss: 0.492\n",
      "Epoch 15413, Loss: 16.162, Final Batch Loss: 0.483\n",
      "Epoch 15414, Loss: 15.833, Final Batch Loss: 0.464\n",
      "Epoch 15415, Loss: 16.312, Final Batch Loss: 0.492\n",
      "Epoch 15416, Loss: 16.093, Final Batch Loss: 0.394\n",
      "Epoch 15417, Loss: 16.111, Final Batch Loss: 0.472\n",
      "Epoch 15418, Loss: 16.142, Final Batch Loss: 0.531\n",
      "Epoch 15419, Loss: 16.110, Final Batch Loss: 0.453\n",
      "Epoch 15420, Loss: 16.311, Final Batch Loss: 0.449\n",
      "Epoch 15421, Loss: 15.959, Final Batch Loss: 0.374\n",
      "Epoch 15422, Loss: 15.947, Final Batch Loss: 0.406\n",
      "Epoch 15423, Loss: 16.109, Final Batch Loss: 0.337\n",
      "Epoch 15424, Loss: 16.297, Final Batch Loss: 0.522\n",
      "Epoch 15425, Loss: 16.299, Final Batch Loss: 0.450\n",
      "Epoch 15426, Loss: 16.143, Final Batch Loss: 0.394\n",
      "Epoch 15427, Loss: 15.983, Final Batch Loss: 0.523\n",
      "Epoch 15428, Loss: 15.936, Final Batch Loss: 0.491\n",
      "Epoch 15429, Loss: 16.319, Final Batch Loss: 0.345\n",
      "Epoch 15430, Loss: 16.431, Final Batch Loss: 0.458\n",
      "Epoch 15431, Loss: 16.157, Final Batch Loss: 0.370\n",
      "Epoch 15432, Loss: 16.297, Final Batch Loss: 0.473\n",
      "Epoch 15433, Loss: 16.250, Final Batch Loss: 0.469\n",
      "Epoch 15434, Loss: 16.119, Final Batch Loss: 0.406\n",
      "Epoch 15435, Loss: 16.455, Final Batch Loss: 0.544\n",
      "Epoch 15436, Loss: 15.977, Final Batch Loss: 0.285\n",
      "Epoch 15437, Loss: 16.233, Final Batch Loss: 0.453\n",
      "Epoch 15438, Loss: 16.445, Final Batch Loss: 0.520\n",
      "Epoch 15439, Loss: 16.163, Final Batch Loss: 0.367\n",
      "Epoch 15440, Loss: 16.220, Final Batch Loss: 0.319\n",
      "Epoch 15441, Loss: 16.349, Final Batch Loss: 0.399\n",
      "Epoch 15442, Loss: 16.108, Final Batch Loss: 0.412\n",
      "Epoch 15443, Loss: 16.280, Final Batch Loss: 0.400\n",
      "Epoch 15444, Loss: 16.124, Final Batch Loss: 0.416\n",
      "Epoch 15445, Loss: 16.261, Final Batch Loss: 0.426\n",
      "Epoch 15446, Loss: 16.079, Final Batch Loss: 0.415\n",
      "Epoch 15447, Loss: 16.103, Final Batch Loss: 0.431\n",
      "Epoch 15448, Loss: 16.505, Final Batch Loss: 0.413\n",
      "Epoch 15449, Loss: 16.074, Final Batch Loss: 0.455\n",
      "Epoch 15450, Loss: 16.150, Final Batch Loss: 0.389\n",
      "Epoch 15451, Loss: 16.281, Final Batch Loss: 0.429\n",
      "Epoch 15452, Loss: 16.348, Final Batch Loss: 0.480\n",
      "Epoch 15453, Loss: 15.980, Final Batch Loss: 0.353\n",
      "Epoch 15454, Loss: 16.333, Final Batch Loss: 0.414\n",
      "Epoch 15455, Loss: 16.038, Final Batch Loss: 0.528\n",
      "Epoch 15456, Loss: 16.114, Final Batch Loss: 0.459\n",
      "Epoch 15457, Loss: 16.375, Final Batch Loss: 0.431\n",
      "Epoch 15458, Loss: 16.007, Final Batch Loss: 0.451\n",
      "Epoch 15459, Loss: 16.286, Final Batch Loss: 0.564\n",
      "Epoch 15460, Loss: 15.998, Final Batch Loss: 0.435\n",
      "Epoch 15461, Loss: 16.231, Final Batch Loss: 0.384\n",
      "Epoch 15462, Loss: 16.369, Final Batch Loss: 0.541\n",
      "Epoch 15463, Loss: 16.365, Final Batch Loss: 0.434\n",
      "Epoch 15464, Loss: 16.130, Final Batch Loss: 0.424\n",
      "Epoch 15465, Loss: 16.365, Final Batch Loss: 0.551\n",
      "Epoch 15466, Loss: 16.020, Final Batch Loss: 0.466\n",
      "Epoch 15467, Loss: 16.289, Final Batch Loss: 0.474\n",
      "Epoch 15468, Loss: 16.418, Final Batch Loss: 0.518\n",
      "Epoch 15469, Loss: 16.235, Final Batch Loss: 0.420\n",
      "Epoch 15470, Loss: 16.266, Final Batch Loss: 0.429\n",
      "Epoch 15471, Loss: 16.237, Final Batch Loss: 0.514\n",
      "Epoch 15472, Loss: 16.461, Final Batch Loss: 0.441\n",
      "Epoch 15473, Loss: 16.326, Final Batch Loss: 0.361\n",
      "Epoch 15474, Loss: 16.215, Final Batch Loss: 0.493\n",
      "Epoch 15475, Loss: 16.095, Final Batch Loss: 0.422\n",
      "Epoch 15476, Loss: 16.128, Final Batch Loss: 0.433\n",
      "Epoch 15477, Loss: 15.913, Final Batch Loss: 0.448\n",
      "Epoch 15478, Loss: 16.560, Final Batch Loss: 0.557\n",
      "Epoch 15479, Loss: 16.032, Final Batch Loss: 0.442\n",
      "Epoch 15480, Loss: 16.316, Final Batch Loss: 0.398\n",
      "Epoch 15481, Loss: 16.004, Final Batch Loss: 0.475\n",
      "Epoch 15482, Loss: 16.243, Final Batch Loss: 0.516\n",
      "Epoch 15483, Loss: 16.105, Final Batch Loss: 0.418\n",
      "Epoch 15484, Loss: 16.240, Final Batch Loss: 0.604\n",
      "Epoch 15485, Loss: 16.237, Final Batch Loss: 0.516\n",
      "Epoch 15486, Loss: 16.097, Final Batch Loss: 0.366\n",
      "Epoch 15487, Loss: 16.099, Final Batch Loss: 0.499\n",
      "Epoch 15488, Loss: 16.500, Final Batch Loss: 0.467\n",
      "Epoch 15489, Loss: 16.306, Final Batch Loss: 0.465\n",
      "Epoch 15490, Loss: 16.215, Final Batch Loss: 0.387\n",
      "Epoch 15491, Loss: 16.195, Final Batch Loss: 0.434\n",
      "Epoch 15492, Loss: 16.396, Final Batch Loss: 0.526\n",
      "Epoch 15493, Loss: 16.331, Final Batch Loss: 0.412\n",
      "Epoch 15494, Loss: 16.263, Final Batch Loss: 0.446\n",
      "Epoch 15495, Loss: 16.285, Final Batch Loss: 0.460\n",
      "Epoch 15496, Loss: 16.229, Final Batch Loss: 0.471\n",
      "Epoch 15497, Loss: 16.191, Final Batch Loss: 0.345\n",
      "Epoch 15498, Loss: 16.442, Final Batch Loss: 0.484\n",
      "Epoch 15499, Loss: 15.914, Final Batch Loss: 0.335\n",
      "Epoch 15500, Loss: 15.773, Final Batch Loss: 0.365\n",
      "Epoch 15501, Loss: 16.167, Final Batch Loss: 0.462\n",
      "Epoch 15502, Loss: 16.331, Final Batch Loss: 0.422\n",
      "Epoch 15503, Loss: 16.219, Final Batch Loss: 0.431\n",
      "Epoch 15504, Loss: 16.188, Final Batch Loss: 0.527\n",
      "Epoch 15505, Loss: 16.310, Final Batch Loss: 0.482\n",
      "Epoch 15506, Loss: 16.148, Final Batch Loss: 0.521\n",
      "Epoch 15507, Loss: 16.050, Final Batch Loss: 0.368\n",
      "Epoch 15508, Loss: 16.467, Final Batch Loss: 0.595\n",
      "Epoch 15509, Loss: 16.242, Final Batch Loss: 0.520\n",
      "Epoch 15510, Loss: 16.436, Final Batch Loss: 0.493\n",
      "Epoch 15511, Loss: 16.077, Final Batch Loss: 0.375\n",
      "Epoch 15512, Loss: 16.135, Final Batch Loss: 0.438\n",
      "Epoch 15513, Loss: 16.065, Final Batch Loss: 0.353\n",
      "Epoch 15514, Loss: 16.095, Final Batch Loss: 0.339\n",
      "Epoch 15515, Loss: 16.281, Final Batch Loss: 0.440\n",
      "Epoch 15516, Loss: 16.268, Final Batch Loss: 0.372\n",
      "Epoch 15517, Loss: 16.153, Final Batch Loss: 0.528\n",
      "Epoch 15518, Loss: 16.184, Final Batch Loss: 0.515\n",
      "Epoch 15519, Loss: 16.115, Final Batch Loss: 0.443\n",
      "Epoch 15520, Loss: 16.103, Final Batch Loss: 0.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15521, Loss: 16.228, Final Batch Loss: 0.581\n",
      "Epoch 15522, Loss: 16.234, Final Batch Loss: 0.366\n",
      "Epoch 15523, Loss: 16.113, Final Batch Loss: 0.410\n",
      "Epoch 15524, Loss: 16.091, Final Batch Loss: 0.504\n",
      "Epoch 15525, Loss: 16.096, Final Batch Loss: 0.395\n",
      "Epoch 15526, Loss: 16.161, Final Batch Loss: 0.472\n",
      "Epoch 15527, Loss: 16.116, Final Batch Loss: 0.385\n",
      "Epoch 15528, Loss: 16.248, Final Batch Loss: 0.356\n",
      "Epoch 15529, Loss: 16.053, Final Batch Loss: 0.513\n",
      "Epoch 15530, Loss: 16.341, Final Batch Loss: 0.450\n",
      "Epoch 15531, Loss: 16.105, Final Batch Loss: 0.429\n",
      "Epoch 15532, Loss: 16.071, Final Batch Loss: 0.444\n",
      "Epoch 15533, Loss: 16.041, Final Batch Loss: 0.586\n",
      "Epoch 15534, Loss: 16.261, Final Batch Loss: 0.432\n",
      "Epoch 15535, Loss: 16.117, Final Batch Loss: 0.432\n",
      "Epoch 15536, Loss: 16.059, Final Batch Loss: 0.359\n",
      "Epoch 15537, Loss: 16.398, Final Batch Loss: 0.574\n",
      "Epoch 15538, Loss: 16.209, Final Batch Loss: 0.451\n",
      "Epoch 15539, Loss: 16.006, Final Batch Loss: 0.384\n",
      "Epoch 15540, Loss: 16.095, Final Batch Loss: 0.439\n",
      "Epoch 15541, Loss: 16.320, Final Batch Loss: 0.404\n",
      "Epoch 15542, Loss: 16.093, Final Batch Loss: 0.444\n",
      "Epoch 15543, Loss: 16.034, Final Batch Loss: 0.535\n",
      "Epoch 15544, Loss: 16.313, Final Batch Loss: 0.470\n",
      "Epoch 15545, Loss: 16.171, Final Batch Loss: 0.544\n",
      "Epoch 15546, Loss: 15.968, Final Batch Loss: 0.389\n",
      "Epoch 15547, Loss: 16.449, Final Batch Loss: 0.464\n",
      "Epoch 15548, Loss: 16.350, Final Batch Loss: 0.501\n",
      "Epoch 15549, Loss: 16.199, Final Batch Loss: 0.433\n",
      "Epoch 15550, Loss: 16.198, Final Batch Loss: 0.518\n",
      "Epoch 15551, Loss: 16.114, Final Batch Loss: 0.528\n",
      "Epoch 15552, Loss: 16.379, Final Batch Loss: 0.432\n",
      "Epoch 15553, Loss: 16.191, Final Batch Loss: 0.415\n",
      "Epoch 15554, Loss: 16.290, Final Batch Loss: 0.389\n",
      "Epoch 15555, Loss: 16.232, Final Batch Loss: 0.479\n",
      "Epoch 15556, Loss: 16.189, Final Batch Loss: 0.407\n",
      "Epoch 15557, Loss: 16.267, Final Batch Loss: 0.555\n",
      "Epoch 15558, Loss: 16.399, Final Batch Loss: 0.426\n",
      "Epoch 15559, Loss: 16.102, Final Batch Loss: 0.585\n",
      "Epoch 15560, Loss: 15.956, Final Batch Loss: 0.518\n",
      "Epoch 15561, Loss: 16.139, Final Batch Loss: 0.457\n",
      "Epoch 15562, Loss: 16.285, Final Batch Loss: 0.424\n",
      "Epoch 15563, Loss: 16.368, Final Batch Loss: 0.521\n",
      "Epoch 15564, Loss: 15.858, Final Batch Loss: 0.390\n",
      "Epoch 15565, Loss: 16.182, Final Batch Loss: 0.481\n",
      "Epoch 15566, Loss: 15.914, Final Batch Loss: 0.366\n",
      "Epoch 15567, Loss: 16.254, Final Batch Loss: 0.425\n",
      "Epoch 15568, Loss: 16.315, Final Batch Loss: 0.502\n",
      "Epoch 15569, Loss: 16.058, Final Batch Loss: 0.358\n",
      "Epoch 15570, Loss: 16.249, Final Batch Loss: 0.510\n",
      "Epoch 15571, Loss: 16.225, Final Batch Loss: 0.380\n",
      "Epoch 15572, Loss: 16.096, Final Batch Loss: 0.376\n",
      "Epoch 15573, Loss: 16.099, Final Batch Loss: 0.438\n",
      "Epoch 15574, Loss: 16.244, Final Batch Loss: 0.435\n",
      "Epoch 15575, Loss: 15.952, Final Batch Loss: 0.584\n",
      "Epoch 15576, Loss: 16.012, Final Batch Loss: 0.481\n",
      "Epoch 15577, Loss: 16.521, Final Batch Loss: 0.520\n",
      "Epoch 15578, Loss: 15.898, Final Batch Loss: 0.464\n",
      "Epoch 15579, Loss: 16.021, Final Batch Loss: 0.408\n",
      "Epoch 15580, Loss: 16.187, Final Batch Loss: 0.431\n",
      "Epoch 15581, Loss: 16.273, Final Batch Loss: 0.510\n",
      "Epoch 15582, Loss: 16.064, Final Batch Loss: 0.446\n",
      "Epoch 15583, Loss: 16.291, Final Batch Loss: 0.386\n",
      "Epoch 15584, Loss: 16.276, Final Batch Loss: 0.422\n",
      "Epoch 15585, Loss: 15.787, Final Batch Loss: 0.458\n",
      "Epoch 15586, Loss: 16.446, Final Batch Loss: 0.378\n",
      "Epoch 15587, Loss: 16.166, Final Batch Loss: 0.420\n",
      "Epoch 15588, Loss: 16.432, Final Batch Loss: 0.410\n",
      "Epoch 15589, Loss: 15.897, Final Batch Loss: 0.416\n",
      "Epoch 15590, Loss: 16.237, Final Batch Loss: 0.438\n",
      "Epoch 15591, Loss: 16.161, Final Batch Loss: 0.422\n",
      "Epoch 15592, Loss: 16.383, Final Batch Loss: 0.501\n",
      "Epoch 15593, Loss: 16.424, Final Batch Loss: 0.483\n",
      "Epoch 15594, Loss: 16.019, Final Batch Loss: 0.408\n",
      "Epoch 15595, Loss: 16.355, Final Batch Loss: 0.428\n",
      "Epoch 15596, Loss: 16.169, Final Batch Loss: 0.438\n",
      "Epoch 15597, Loss: 16.300, Final Batch Loss: 0.430\n",
      "Epoch 15598, Loss: 16.288, Final Batch Loss: 0.481\n",
      "Epoch 15599, Loss: 16.139, Final Batch Loss: 0.519\n",
      "Epoch 15600, Loss: 16.128, Final Batch Loss: 0.417\n",
      "Epoch 15601, Loss: 15.943, Final Batch Loss: 0.515\n",
      "Epoch 15602, Loss: 16.195, Final Batch Loss: 0.452\n",
      "Epoch 15603, Loss: 16.361, Final Batch Loss: 0.410\n",
      "Epoch 15604, Loss: 15.983, Final Batch Loss: 0.491\n",
      "Epoch 15605, Loss: 16.208, Final Batch Loss: 0.520\n",
      "Epoch 15606, Loss: 16.204, Final Batch Loss: 0.472\n",
      "Epoch 15607, Loss: 16.221, Final Batch Loss: 0.464\n",
      "Epoch 15608, Loss: 16.161, Final Batch Loss: 0.533\n",
      "Epoch 15609, Loss: 15.905, Final Batch Loss: 0.466\n",
      "Epoch 15610, Loss: 16.376, Final Batch Loss: 0.543\n",
      "Epoch 15611, Loss: 16.097, Final Batch Loss: 0.451\n",
      "Epoch 15612, Loss: 16.171, Final Batch Loss: 0.470\n",
      "Epoch 15613, Loss: 16.329, Final Batch Loss: 0.481\n",
      "Epoch 15614, Loss: 16.096, Final Batch Loss: 0.362\n",
      "Epoch 15615, Loss: 16.025, Final Batch Loss: 0.420\n",
      "Epoch 15616, Loss: 16.033, Final Batch Loss: 0.320\n",
      "Epoch 15617, Loss: 16.114, Final Batch Loss: 0.565\n",
      "Epoch 15618, Loss: 16.191, Final Batch Loss: 0.455\n",
      "Epoch 15619, Loss: 16.231, Final Batch Loss: 0.480\n",
      "Epoch 15620, Loss: 16.307, Final Batch Loss: 0.471\n",
      "Epoch 15621, Loss: 16.141, Final Batch Loss: 0.396\n",
      "Epoch 15622, Loss: 16.078, Final Batch Loss: 0.362\n",
      "Epoch 15623, Loss: 16.176, Final Batch Loss: 0.462\n",
      "Epoch 15624, Loss: 16.027, Final Batch Loss: 0.454\n",
      "Epoch 15625, Loss: 16.350, Final Batch Loss: 0.421\n",
      "Epoch 15626, Loss: 15.922, Final Batch Loss: 0.409\n",
      "Epoch 15627, Loss: 16.482, Final Batch Loss: 0.472\n",
      "Epoch 15628, Loss: 16.203, Final Batch Loss: 0.611\n",
      "Epoch 15629, Loss: 16.078, Final Batch Loss: 0.355\n",
      "Epoch 15630, Loss: 16.254, Final Batch Loss: 0.489\n",
      "Epoch 15631, Loss: 15.959, Final Batch Loss: 0.512\n",
      "Epoch 15632, Loss: 16.137, Final Batch Loss: 0.428\n",
      "Epoch 15633, Loss: 16.364, Final Batch Loss: 0.442\n",
      "Epoch 15634, Loss: 16.433, Final Batch Loss: 0.500\n",
      "Epoch 15635, Loss: 16.283, Final Batch Loss: 0.417\n",
      "Epoch 15636, Loss: 16.228, Final Batch Loss: 0.504\n",
      "Epoch 15637, Loss: 16.000, Final Batch Loss: 0.424\n",
      "Epoch 15638, Loss: 16.335, Final Batch Loss: 0.439\n",
      "Epoch 15639, Loss: 16.115, Final Batch Loss: 0.356\n",
      "Epoch 15640, Loss: 16.078, Final Batch Loss: 0.451\n",
      "Epoch 15641, Loss: 16.172, Final Batch Loss: 0.412\n",
      "Epoch 15642, Loss: 15.934, Final Batch Loss: 0.427\n",
      "Epoch 15643, Loss: 15.865, Final Batch Loss: 0.419\n",
      "Epoch 15644, Loss: 16.186, Final Batch Loss: 0.483\n",
      "Epoch 15645, Loss: 16.149, Final Batch Loss: 0.327\n",
      "Epoch 15646, Loss: 16.037, Final Batch Loss: 0.345\n",
      "Epoch 15647, Loss: 16.241, Final Batch Loss: 0.450\n",
      "Epoch 15648, Loss: 16.494, Final Batch Loss: 0.470\n",
      "Epoch 15649, Loss: 16.123, Final Batch Loss: 0.404\n",
      "Epoch 15650, Loss: 16.195, Final Batch Loss: 0.343\n",
      "Epoch 15651, Loss: 15.741, Final Batch Loss: 0.419\n",
      "Epoch 15652, Loss: 16.348, Final Batch Loss: 0.446\n",
      "Epoch 15653, Loss: 16.147, Final Batch Loss: 0.470\n",
      "Epoch 15654, Loss: 16.458, Final Batch Loss: 0.474\n",
      "Epoch 15655, Loss: 16.085, Final Batch Loss: 0.378\n",
      "Epoch 15656, Loss: 16.383, Final Batch Loss: 0.510\n",
      "Epoch 15657, Loss: 16.218, Final Batch Loss: 0.343\n",
      "Epoch 15658, Loss: 16.082, Final Batch Loss: 0.395\n",
      "Epoch 15659, Loss: 16.346, Final Batch Loss: 0.440\n",
      "Epoch 15660, Loss: 16.342, Final Batch Loss: 0.429\n",
      "Epoch 15661, Loss: 16.456, Final Batch Loss: 0.392\n",
      "Epoch 15662, Loss: 15.995, Final Batch Loss: 0.316\n",
      "Epoch 15663, Loss: 16.345, Final Batch Loss: 0.467\n",
      "Epoch 15664, Loss: 16.160, Final Batch Loss: 0.490\n",
      "Epoch 15665, Loss: 16.012, Final Batch Loss: 0.327\n",
      "Epoch 15666, Loss: 16.138, Final Batch Loss: 0.465\n",
      "Epoch 15667, Loss: 16.124, Final Batch Loss: 0.378\n",
      "Epoch 15668, Loss: 16.125, Final Batch Loss: 0.412\n",
      "Epoch 15669, Loss: 16.176, Final Batch Loss: 0.390\n",
      "Epoch 15670, Loss: 16.119, Final Batch Loss: 0.452\n",
      "Epoch 15671, Loss: 15.919, Final Batch Loss: 0.355\n",
      "Epoch 15672, Loss: 16.403, Final Batch Loss: 0.455\n",
      "Epoch 15673, Loss: 16.493, Final Batch Loss: 0.586\n",
      "Epoch 15674, Loss: 16.076, Final Batch Loss: 0.384\n",
      "Epoch 15675, Loss: 16.255, Final Batch Loss: 0.528\n",
      "Epoch 15676, Loss: 16.121, Final Batch Loss: 0.517\n",
      "Epoch 15677, Loss: 16.321, Final Batch Loss: 0.451\n",
      "Epoch 15678, Loss: 16.227, Final Batch Loss: 0.445\n",
      "Epoch 15679, Loss: 15.942, Final Batch Loss: 0.460\n",
      "Epoch 15680, Loss: 16.071, Final Batch Loss: 0.544\n",
      "Epoch 15681, Loss: 16.338, Final Batch Loss: 0.515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15682, Loss: 16.206, Final Batch Loss: 0.333\n",
      "Epoch 15683, Loss: 16.306, Final Batch Loss: 0.468\n",
      "Epoch 15684, Loss: 16.022, Final Batch Loss: 0.415\n",
      "Epoch 15685, Loss: 16.533, Final Batch Loss: 0.445\n",
      "Epoch 15686, Loss: 16.397, Final Batch Loss: 0.330\n",
      "Epoch 15687, Loss: 16.190, Final Batch Loss: 0.346\n",
      "Epoch 15688, Loss: 16.257, Final Batch Loss: 0.485\n",
      "Epoch 15689, Loss: 16.317, Final Batch Loss: 0.495\n",
      "Epoch 15690, Loss: 16.264, Final Batch Loss: 0.511\n",
      "Epoch 15691, Loss: 16.142, Final Batch Loss: 0.487\n",
      "Epoch 15692, Loss: 16.315, Final Batch Loss: 0.403\n",
      "Epoch 15693, Loss: 16.180, Final Batch Loss: 0.453\n",
      "Epoch 15694, Loss: 16.321, Final Batch Loss: 0.426\n",
      "Epoch 15695, Loss: 15.990, Final Batch Loss: 0.435\n",
      "Epoch 15696, Loss: 16.018, Final Batch Loss: 0.438\n",
      "Epoch 15697, Loss: 16.199, Final Batch Loss: 0.353\n",
      "Epoch 15698, Loss: 16.223, Final Batch Loss: 0.446\n",
      "Epoch 15699, Loss: 16.140, Final Batch Loss: 0.498\n",
      "Epoch 15700, Loss: 16.058, Final Batch Loss: 0.448\n",
      "Epoch 15701, Loss: 15.822, Final Batch Loss: 0.368\n",
      "Epoch 15702, Loss: 16.046, Final Batch Loss: 0.472\n",
      "Epoch 15703, Loss: 16.305, Final Batch Loss: 0.467\n",
      "Epoch 15704, Loss: 16.142, Final Batch Loss: 0.545\n",
      "Epoch 15705, Loss: 16.114, Final Batch Loss: 0.446\n",
      "Epoch 15706, Loss: 16.286, Final Batch Loss: 0.473\n",
      "Epoch 15707, Loss: 16.401, Final Batch Loss: 0.470\n",
      "Epoch 15708, Loss: 16.217, Final Batch Loss: 0.491\n",
      "Epoch 15709, Loss: 16.175, Final Batch Loss: 0.533\n",
      "Epoch 15710, Loss: 16.251, Final Batch Loss: 0.406\n",
      "Epoch 15711, Loss: 15.889, Final Batch Loss: 0.413\n",
      "Epoch 15712, Loss: 16.083, Final Batch Loss: 0.395\n",
      "Epoch 15713, Loss: 16.047, Final Batch Loss: 0.354\n",
      "Epoch 15714, Loss: 16.361, Final Batch Loss: 0.470\n",
      "Epoch 15715, Loss: 16.180, Final Batch Loss: 0.507\n",
      "Epoch 15716, Loss: 16.060, Final Batch Loss: 0.495\n",
      "Epoch 15717, Loss: 16.036, Final Batch Loss: 0.413\n",
      "Epoch 15718, Loss: 16.143, Final Batch Loss: 0.494\n",
      "Epoch 15719, Loss: 15.971, Final Batch Loss: 0.461\n",
      "Epoch 15720, Loss: 15.922, Final Batch Loss: 0.353\n",
      "Epoch 15721, Loss: 15.961, Final Batch Loss: 0.416\n",
      "Epoch 15722, Loss: 16.165, Final Batch Loss: 0.329\n",
      "Epoch 15723, Loss: 16.041, Final Batch Loss: 0.492\n",
      "Epoch 15724, Loss: 16.178, Final Batch Loss: 0.503\n",
      "Epoch 15725, Loss: 16.178, Final Batch Loss: 0.429\n",
      "Epoch 15726, Loss: 16.218, Final Batch Loss: 0.427\n",
      "Epoch 15727, Loss: 16.099, Final Batch Loss: 0.441\n",
      "Epoch 15728, Loss: 16.170, Final Batch Loss: 0.488\n",
      "Epoch 15729, Loss: 16.402, Final Batch Loss: 0.415\n",
      "Epoch 15730, Loss: 16.062, Final Batch Loss: 0.432\n",
      "Epoch 15731, Loss: 16.205, Final Batch Loss: 0.498\n",
      "Epoch 15732, Loss: 16.146, Final Batch Loss: 0.471\n",
      "Epoch 15733, Loss: 16.242, Final Batch Loss: 0.445\n",
      "Epoch 15734, Loss: 15.903, Final Batch Loss: 0.342\n",
      "Epoch 15735, Loss: 16.032, Final Batch Loss: 0.478\n",
      "Epoch 15736, Loss: 16.022, Final Batch Loss: 0.404\n",
      "Epoch 15737, Loss: 16.035, Final Batch Loss: 0.439\n",
      "Epoch 15738, Loss: 16.252, Final Batch Loss: 0.400\n",
      "Epoch 15739, Loss: 16.062, Final Batch Loss: 0.359\n",
      "Epoch 15740, Loss: 15.948, Final Batch Loss: 0.419\n",
      "Epoch 15741, Loss: 16.333, Final Batch Loss: 0.395\n",
      "Epoch 15742, Loss: 16.054, Final Batch Loss: 0.430\n",
      "Epoch 15743, Loss: 16.218, Final Batch Loss: 0.430\n",
      "Epoch 15744, Loss: 16.252, Final Batch Loss: 0.422\n",
      "Epoch 15745, Loss: 15.973, Final Batch Loss: 0.374\n",
      "Epoch 15746, Loss: 16.161, Final Batch Loss: 0.381\n",
      "Epoch 15747, Loss: 16.105, Final Batch Loss: 0.440\n",
      "Epoch 15748, Loss: 16.373, Final Batch Loss: 0.390\n",
      "Epoch 15749, Loss: 16.231, Final Batch Loss: 0.395\n",
      "Epoch 15750, Loss: 16.019, Final Batch Loss: 0.459\n",
      "Epoch 15751, Loss: 16.350, Final Batch Loss: 0.486\n",
      "Epoch 15752, Loss: 15.904, Final Batch Loss: 0.447\n",
      "Epoch 15753, Loss: 16.224, Final Batch Loss: 0.408\n",
      "Epoch 15754, Loss: 16.319, Final Batch Loss: 0.516\n",
      "Epoch 15755, Loss: 15.975, Final Batch Loss: 0.498\n",
      "Epoch 15756, Loss: 16.197, Final Batch Loss: 0.382\n",
      "Epoch 15757, Loss: 16.263, Final Batch Loss: 0.437\n",
      "Epoch 15758, Loss: 16.264, Final Batch Loss: 0.386\n",
      "Epoch 15759, Loss: 16.110, Final Batch Loss: 0.407\n",
      "Epoch 15760, Loss: 16.441, Final Batch Loss: 0.424\n",
      "Epoch 15761, Loss: 16.030, Final Batch Loss: 0.371\n",
      "Epoch 15762, Loss: 16.250, Final Batch Loss: 0.405\n",
      "Epoch 15763, Loss: 16.101, Final Batch Loss: 0.482\n",
      "Epoch 15764, Loss: 16.449, Final Batch Loss: 0.525\n",
      "Epoch 15765, Loss: 16.075, Final Batch Loss: 0.486\n",
      "Epoch 15766, Loss: 16.259, Final Batch Loss: 0.353\n",
      "Epoch 15767, Loss: 16.348, Final Batch Loss: 0.421\n",
      "Epoch 15768, Loss: 16.219, Final Batch Loss: 0.493\n",
      "Epoch 15769, Loss: 16.332, Final Batch Loss: 0.409\n",
      "Epoch 15770, Loss: 16.084, Final Batch Loss: 0.364\n",
      "Epoch 15771, Loss: 15.971, Final Batch Loss: 0.572\n",
      "Epoch 15772, Loss: 16.017, Final Batch Loss: 0.401\n",
      "Epoch 15773, Loss: 16.030, Final Batch Loss: 0.499\n",
      "Epoch 15774, Loss: 15.985, Final Batch Loss: 0.410\n",
      "Epoch 15775, Loss: 16.167, Final Batch Loss: 0.419\n",
      "Epoch 15776, Loss: 16.273, Final Batch Loss: 0.418\n",
      "Epoch 15777, Loss: 16.171, Final Batch Loss: 0.437\n",
      "Epoch 15778, Loss: 16.247, Final Batch Loss: 0.527\n",
      "Epoch 15779, Loss: 15.834, Final Batch Loss: 0.363\n",
      "Epoch 15780, Loss: 16.043, Final Batch Loss: 0.422\n",
      "Epoch 15781, Loss: 16.355, Final Batch Loss: 0.503\n",
      "Epoch 15782, Loss: 16.158, Final Batch Loss: 0.482\n",
      "Epoch 15783, Loss: 16.405, Final Batch Loss: 0.408\n",
      "Epoch 15784, Loss: 16.122, Final Batch Loss: 0.475\n",
      "Epoch 15785, Loss: 16.135, Final Batch Loss: 0.553\n",
      "Epoch 15786, Loss: 16.434, Final Batch Loss: 0.496\n",
      "Epoch 15787, Loss: 16.293, Final Batch Loss: 0.500\n",
      "Epoch 15788, Loss: 15.949, Final Batch Loss: 0.434\n",
      "Epoch 15789, Loss: 16.272, Final Batch Loss: 0.385\n",
      "Epoch 15790, Loss: 16.234, Final Batch Loss: 0.452\n",
      "Epoch 15791, Loss: 16.142, Final Batch Loss: 0.466\n",
      "Epoch 15792, Loss: 16.104, Final Batch Loss: 0.359\n",
      "Epoch 15793, Loss: 16.325, Final Batch Loss: 0.472\n",
      "Epoch 15794, Loss: 16.314, Final Batch Loss: 0.442\n",
      "Epoch 15795, Loss: 16.020, Final Batch Loss: 0.413\n",
      "Epoch 15796, Loss: 16.054, Final Batch Loss: 0.326\n",
      "Epoch 15797, Loss: 15.833, Final Batch Loss: 0.324\n",
      "Epoch 15798, Loss: 15.912, Final Batch Loss: 0.436\n",
      "Epoch 15799, Loss: 16.075, Final Batch Loss: 0.448\n",
      "Epoch 15800, Loss: 16.146, Final Batch Loss: 0.439\n",
      "Epoch 15801, Loss: 16.286, Final Batch Loss: 0.491\n",
      "Epoch 15802, Loss: 16.328, Final Batch Loss: 0.491\n",
      "Epoch 15803, Loss: 16.372, Final Batch Loss: 0.551\n",
      "Epoch 15804, Loss: 16.032, Final Batch Loss: 0.479\n",
      "Epoch 15805, Loss: 16.206, Final Batch Loss: 0.425\n",
      "Epoch 15806, Loss: 16.267, Final Batch Loss: 0.511\n",
      "Epoch 15807, Loss: 16.037, Final Batch Loss: 0.438\n",
      "Epoch 15808, Loss: 16.280, Final Batch Loss: 0.499\n",
      "Epoch 15809, Loss: 16.394, Final Batch Loss: 0.484\n",
      "Epoch 15810, Loss: 16.290, Final Batch Loss: 0.479\n",
      "Epoch 15811, Loss: 16.219, Final Batch Loss: 0.435\n",
      "Epoch 15812, Loss: 16.134, Final Batch Loss: 0.484\n",
      "Epoch 15813, Loss: 15.868, Final Batch Loss: 0.359\n",
      "Epoch 15814, Loss: 16.223, Final Batch Loss: 0.431\n",
      "Epoch 15815, Loss: 15.937, Final Batch Loss: 0.330\n",
      "Epoch 15816, Loss: 16.382, Final Batch Loss: 0.413\n",
      "Epoch 15817, Loss: 16.174, Final Batch Loss: 0.451\n",
      "Epoch 15818, Loss: 16.199, Final Batch Loss: 0.415\n",
      "Epoch 15819, Loss: 16.411, Final Batch Loss: 0.412\n",
      "Epoch 15820, Loss: 16.196, Final Batch Loss: 0.454\n",
      "Epoch 15821, Loss: 16.284, Final Batch Loss: 0.493\n",
      "Epoch 15822, Loss: 16.128, Final Batch Loss: 0.320\n",
      "Epoch 15823, Loss: 15.961, Final Batch Loss: 0.424\n",
      "Epoch 15824, Loss: 16.270, Final Batch Loss: 0.411\n",
      "Epoch 15825, Loss: 16.179, Final Batch Loss: 0.441\n",
      "Epoch 15826, Loss: 15.895, Final Batch Loss: 0.368\n",
      "Epoch 15827, Loss: 15.937, Final Batch Loss: 0.542\n",
      "Epoch 15828, Loss: 16.151, Final Batch Loss: 0.515\n",
      "Epoch 15829, Loss: 16.110, Final Batch Loss: 0.430\n",
      "Epoch 15830, Loss: 16.162, Final Batch Loss: 0.332\n",
      "Epoch 15831, Loss: 16.005, Final Batch Loss: 0.370\n",
      "Epoch 15832, Loss: 15.954, Final Batch Loss: 0.454\n",
      "Epoch 15833, Loss: 16.439, Final Batch Loss: 0.392\n",
      "Epoch 15834, Loss: 16.103, Final Batch Loss: 0.479\n",
      "Epoch 15835, Loss: 16.123, Final Batch Loss: 0.441\n",
      "Epoch 15836, Loss: 16.143, Final Batch Loss: 0.544\n",
      "Epoch 15837, Loss: 16.122, Final Batch Loss: 0.441\n",
      "Epoch 15838, Loss: 16.027, Final Batch Loss: 0.421\n",
      "Epoch 15839, Loss: 16.081, Final Batch Loss: 0.465\n",
      "Epoch 15840, Loss: 16.449, Final Batch Loss: 0.398\n",
      "Epoch 15841, Loss: 16.191, Final Batch Loss: 0.511\n",
      "Epoch 15842, Loss: 15.979, Final Batch Loss: 0.450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15843, Loss: 15.922, Final Batch Loss: 0.459\n",
      "Epoch 15844, Loss: 15.964, Final Batch Loss: 0.408\n",
      "Epoch 15845, Loss: 16.279, Final Batch Loss: 0.516\n",
      "Epoch 15846, Loss: 16.107, Final Batch Loss: 0.401\n",
      "Epoch 15847, Loss: 16.076, Final Batch Loss: 0.337\n",
      "Epoch 15848, Loss: 16.092, Final Batch Loss: 0.396\n",
      "Epoch 15849, Loss: 16.117, Final Batch Loss: 0.351\n",
      "Epoch 15850, Loss: 16.454, Final Batch Loss: 0.454\n",
      "Epoch 15851, Loss: 16.181, Final Batch Loss: 0.404\n",
      "Epoch 15852, Loss: 16.155, Final Batch Loss: 0.364\n",
      "Epoch 15853, Loss: 16.397, Final Batch Loss: 0.397\n",
      "Epoch 15854, Loss: 16.069, Final Batch Loss: 0.445\n",
      "Epoch 15855, Loss: 16.488, Final Batch Loss: 0.482\n",
      "Epoch 15856, Loss: 16.093, Final Batch Loss: 0.404\n",
      "Epoch 15857, Loss: 15.979, Final Batch Loss: 0.421\n",
      "Epoch 15858, Loss: 15.940, Final Batch Loss: 0.459\n",
      "Epoch 15859, Loss: 16.488, Final Batch Loss: 0.442\n",
      "Epoch 15860, Loss: 16.199, Final Batch Loss: 0.460\n",
      "Epoch 15861, Loss: 16.214, Final Batch Loss: 0.550\n",
      "Epoch 15862, Loss: 16.181, Final Batch Loss: 0.390\n",
      "Epoch 15863, Loss: 16.104, Final Batch Loss: 0.520\n",
      "Epoch 15864, Loss: 16.107, Final Batch Loss: 0.331\n",
      "Epoch 15865, Loss: 16.325, Final Batch Loss: 0.564\n",
      "Epoch 15866, Loss: 16.151, Final Batch Loss: 0.469\n",
      "Epoch 15867, Loss: 16.035, Final Batch Loss: 0.406\n",
      "Epoch 15868, Loss: 16.331, Final Batch Loss: 0.550\n",
      "Epoch 15869, Loss: 16.354, Final Batch Loss: 0.577\n",
      "Epoch 15870, Loss: 16.347, Final Batch Loss: 0.537\n",
      "Epoch 15871, Loss: 16.226, Final Batch Loss: 0.474\n",
      "Epoch 15872, Loss: 15.845, Final Batch Loss: 0.408\n",
      "Epoch 15873, Loss: 16.216, Final Batch Loss: 0.383\n",
      "Epoch 15874, Loss: 16.144, Final Batch Loss: 0.438\n",
      "Epoch 15875, Loss: 16.011, Final Batch Loss: 0.470\n",
      "Epoch 15876, Loss: 16.137, Final Batch Loss: 0.465\n",
      "Epoch 15877, Loss: 16.284, Final Batch Loss: 0.381\n",
      "Epoch 15878, Loss: 16.231, Final Batch Loss: 0.466\n",
      "Epoch 15879, Loss: 16.200, Final Batch Loss: 0.461\n",
      "Epoch 15880, Loss: 15.918, Final Batch Loss: 0.415\n",
      "Epoch 15881, Loss: 16.188, Final Batch Loss: 0.427\n",
      "Epoch 15882, Loss: 16.019, Final Batch Loss: 0.453\n",
      "Epoch 15883, Loss: 16.037, Final Batch Loss: 0.479\n",
      "Epoch 15884, Loss: 16.133, Final Batch Loss: 0.407\n",
      "Epoch 15885, Loss: 16.291, Final Batch Loss: 0.493\n",
      "Epoch 15886, Loss: 16.002, Final Batch Loss: 0.489\n",
      "Epoch 15887, Loss: 16.207, Final Batch Loss: 0.403\n",
      "Epoch 15888, Loss: 16.221, Final Batch Loss: 0.388\n",
      "Epoch 15889, Loss: 16.440, Final Batch Loss: 0.594\n",
      "Epoch 15890, Loss: 15.980, Final Batch Loss: 0.404\n",
      "Epoch 15891, Loss: 16.139, Final Batch Loss: 0.464\n",
      "Epoch 15892, Loss: 16.346, Final Batch Loss: 0.521\n",
      "Epoch 15893, Loss: 16.155, Final Batch Loss: 0.421\n",
      "Epoch 15894, Loss: 16.180, Final Batch Loss: 0.419\n",
      "Epoch 15895, Loss: 16.138, Final Batch Loss: 0.503\n",
      "Epoch 15896, Loss: 16.222, Final Batch Loss: 0.419\n",
      "Epoch 15897, Loss: 15.907, Final Batch Loss: 0.443\n",
      "Epoch 15898, Loss: 16.086, Final Batch Loss: 0.429\n",
      "Epoch 15899, Loss: 15.957, Final Batch Loss: 0.401\n",
      "Epoch 15900, Loss: 16.066, Final Batch Loss: 0.466\n",
      "Epoch 15901, Loss: 16.168, Final Batch Loss: 0.375\n",
      "Epoch 15902, Loss: 16.337, Final Batch Loss: 0.294\n",
      "Epoch 15903, Loss: 16.370, Final Batch Loss: 0.531\n",
      "Epoch 15904, Loss: 16.335, Final Batch Loss: 0.367\n",
      "Epoch 15905, Loss: 15.893, Final Batch Loss: 0.432\n",
      "Epoch 15906, Loss: 16.184, Final Batch Loss: 0.395\n",
      "Epoch 15907, Loss: 16.188, Final Batch Loss: 0.470\n",
      "Epoch 15908, Loss: 15.900, Final Batch Loss: 0.381\n",
      "Epoch 15909, Loss: 16.049, Final Batch Loss: 0.407\n",
      "Epoch 15910, Loss: 16.273, Final Batch Loss: 0.389\n",
      "Epoch 15911, Loss: 16.080, Final Batch Loss: 0.556\n",
      "Epoch 15912, Loss: 16.118, Final Batch Loss: 0.529\n",
      "Epoch 15913, Loss: 16.144, Final Batch Loss: 0.435\n",
      "Epoch 15914, Loss: 16.176, Final Batch Loss: 0.345\n",
      "Epoch 15915, Loss: 15.775, Final Batch Loss: 0.448\n",
      "Epoch 15916, Loss: 16.187, Final Batch Loss: 0.401\n",
      "Epoch 15917, Loss: 16.305, Final Batch Loss: 0.415\n",
      "Epoch 15918, Loss: 16.331, Final Batch Loss: 0.515\n",
      "Epoch 15919, Loss: 16.169, Final Batch Loss: 0.457\n",
      "Epoch 15920, Loss: 16.298, Final Batch Loss: 0.406\n",
      "Epoch 15921, Loss: 16.029, Final Batch Loss: 0.418\n",
      "Epoch 15922, Loss: 15.957, Final Batch Loss: 0.439\n",
      "Epoch 15923, Loss: 16.176, Final Batch Loss: 0.366\n",
      "Epoch 15924, Loss: 16.095, Final Batch Loss: 0.478\n",
      "Epoch 15925, Loss: 16.391, Final Batch Loss: 0.447\n",
      "Epoch 15926, Loss: 16.166, Final Batch Loss: 0.485\n",
      "Epoch 15927, Loss: 16.130, Final Batch Loss: 0.419\n",
      "Epoch 15928, Loss: 16.081, Final Batch Loss: 0.480\n",
      "Epoch 15929, Loss: 16.310, Final Batch Loss: 0.511\n",
      "Epoch 15930, Loss: 16.199, Final Batch Loss: 0.481\n",
      "Epoch 15931, Loss: 16.167, Final Batch Loss: 0.487\n",
      "Epoch 15932, Loss: 16.086, Final Batch Loss: 0.459\n",
      "Epoch 15933, Loss: 16.290, Final Batch Loss: 0.413\n",
      "Epoch 15934, Loss: 16.053, Final Batch Loss: 0.521\n",
      "Epoch 15935, Loss: 16.304, Final Batch Loss: 0.488\n",
      "Epoch 15936, Loss: 16.179, Final Batch Loss: 0.382\n",
      "Epoch 15937, Loss: 16.157, Final Batch Loss: 0.485\n",
      "Epoch 15938, Loss: 16.387, Final Batch Loss: 0.546\n",
      "Epoch 15939, Loss: 16.373, Final Batch Loss: 0.499\n",
      "Epoch 15940, Loss: 16.044, Final Batch Loss: 0.406\n",
      "Epoch 15941, Loss: 16.072, Final Batch Loss: 0.450\n",
      "Epoch 15942, Loss: 16.166, Final Batch Loss: 0.419\n",
      "Epoch 15943, Loss: 16.050, Final Batch Loss: 0.374\n",
      "Epoch 15944, Loss: 16.304, Final Batch Loss: 0.529\n",
      "Epoch 15945, Loss: 16.037, Final Batch Loss: 0.331\n",
      "Epoch 15946, Loss: 15.867, Final Batch Loss: 0.358\n",
      "Epoch 15947, Loss: 16.141, Final Batch Loss: 0.401\n",
      "Epoch 15948, Loss: 16.258, Final Batch Loss: 0.455\n",
      "Epoch 15949, Loss: 15.915, Final Batch Loss: 0.385\n",
      "Epoch 15950, Loss: 16.149, Final Batch Loss: 0.552\n",
      "Epoch 15951, Loss: 16.026, Final Batch Loss: 0.419\n",
      "Epoch 15952, Loss: 16.260, Final Batch Loss: 0.570\n",
      "Epoch 15953, Loss: 16.054, Final Batch Loss: 0.457\n",
      "Epoch 15954, Loss: 16.000, Final Batch Loss: 0.455\n",
      "Epoch 15955, Loss: 16.199, Final Batch Loss: 0.423\n",
      "Epoch 15956, Loss: 16.260, Final Batch Loss: 0.411\n",
      "Epoch 15957, Loss: 16.415, Final Batch Loss: 0.428\n",
      "Epoch 15958, Loss: 16.093, Final Batch Loss: 0.475\n",
      "Epoch 15959, Loss: 16.372, Final Batch Loss: 0.521\n",
      "Epoch 15960, Loss: 16.554, Final Batch Loss: 0.489\n",
      "Epoch 15961, Loss: 16.070, Final Batch Loss: 0.459\n",
      "Epoch 15962, Loss: 16.302, Final Batch Loss: 0.521\n",
      "Epoch 15963, Loss: 15.903, Final Batch Loss: 0.417\n",
      "Epoch 15964, Loss: 16.325, Final Batch Loss: 0.435\n",
      "Epoch 15965, Loss: 16.109, Final Batch Loss: 0.342\n",
      "Epoch 15966, Loss: 16.071, Final Batch Loss: 0.416\n",
      "Epoch 15967, Loss: 16.240, Final Batch Loss: 0.474\n",
      "Epoch 15968, Loss: 16.306, Final Batch Loss: 0.458\n",
      "Epoch 15969, Loss: 16.171, Final Batch Loss: 0.494\n",
      "Epoch 15970, Loss: 16.165, Final Batch Loss: 0.296\n",
      "Epoch 15971, Loss: 16.286, Final Batch Loss: 0.525\n",
      "Epoch 15972, Loss: 16.141, Final Batch Loss: 0.453\n",
      "Epoch 15973, Loss: 16.126, Final Batch Loss: 0.362\n",
      "Epoch 15974, Loss: 16.080, Final Batch Loss: 0.543\n",
      "Epoch 15975, Loss: 16.194, Final Batch Loss: 0.418\n",
      "Epoch 15976, Loss: 16.164, Final Batch Loss: 0.497\n",
      "Epoch 15977, Loss: 15.965, Final Batch Loss: 0.556\n",
      "Epoch 15978, Loss: 16.226, Final Batch Loss: 0.456\n",
      "Epoch 15979, Loss: 16.420, Final Batch Loss: 0.473\n",
      "Epoch 15980, Loss: 16.103, Final Batch Loss: 0.398\n",
      "Epoch 15981, Loss: 16.215, Final Batch Loss: 0.515\n",
      "Epoch 15982, Loss: 16.164, Final Batch Loss: 0.484\n",
      "Epoch 15983, Loss: 15.941, Final Batch Loss: 0.396\n",
      "Epoch 15984, Loss: 15.970, Final Batch Loss: 0.492\n",
      "Epoch 15985, Loss: 16.125, Final Batch Loss: 0.514\n",
      "Epoch 15986, Loss: 16.055, Final Batch Loss: 0.467\n",
      "Epoch 15987, Loss: 16.225, Final Batch Loss: 0.438\n",
      "Epoch 15988, Loss: 16.018, Final Batch Loss: 0.498\n",
      "Epoch 15989, Loss: 16.147, Final Batch Loss: 0.577\n",
      "Epoch 15990, Loss: 16.041, Final Batch Loss: 0.478\n",
      "Epoch 15991, Loss: 16.093, Final Batch Loss: 0.441\n",
      "Epoch 15992, Loss: 16.292, Final Batch Loss: 0.513\n",
      "Epoch 15993, Loss: 16.229, Final Batch Loss: 0.513\n",
      "Epoch 15994, Loss: 16.180, Final Batch Loss: 0.616\n",
      "Epoch 15995, Loss: 16.102, Final Batch Loss: 0.376\n",
      "Epoch 15996, Loss: 16.156, Final Batch Loss: 0.412\n",
      "Epoch 15997, Loss: 16.362, Final Batch Loss: 0.395\n",
      "Epoch 15998, Loss: 16.259, Final Batch Loss: 0.524\n",
      "Epoch 15999, Loss: 16.180, Final Batch Loss: 0.562\n",
      "Epoch 16000, Loss: 16.418, Final Batch Loss: 0.541\n",
      "Epoch 16001, Loss: 16.379, Final Batch Loss: 0.536\n",
      "Epoch 16002, Loss: 16.045, Final Batch Loss: 0.426\n",
      "Epoch 16003, Loss: 16.009, Final Batch Loss: 0.512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16004, Loss: 16.215, Final Batch Loss: 0.500\n",
      "Epoch 16005, Loss: 16.281, Final Batch Loss: 0.500\n",
      "Epoch 16006, Loss: 16.079, Final Batch Loss: 0.455\n",
      "Epoch 16007, Loss: 16.133, Final Batch Loss: 0.505\n",
      "Epoch 16008, Loss: 16.249, Final Batch Loss: 0.425\n",
      "Epoch 16009, Loss: 16.251, Final Batch Loss: 0.499\n",
      "Epoch 16010, Loss: 15.982, Final Batch Loss: 0.409\n",
      "Epoch 16011, Loss: 16.075, Final Batch Loss: 0.382\n",
      "Epoch 16012, Loss: 16.071, Final Batch Loss: 0.490\n",
      "Epoch 16013, Loss: 15.995, Final Batch Loss: 0.454\n",
      "Epoch 16014, Loss: 16.062, Final Batch Loss: 0.522\n",
      "Epoch 16015, Loss: 15.905, Final Batch Loss: 0.457\n",
      "Epoch 16016, Loss: 16.212, Final Batch Loss: 0.459\n",
      "Epoch 16017, Loss: 16.366, Final Batch Loss: 0.398\n",
      "Epoch 16018, Loss: 15.850, Final Batch Loss: 0.471\n",
      "Epoch 16019, Loss: 15.939, Final Batch Loss: 0.443\n",
      "Epoch 16020, Loss: 15.924, Final Batch Loss: 0.423\n",
      "Epoch 16021, Loss: 16.252, Final Batch Loss: 0.377\n",
      "Epoch 16022, Loss: 16.341, Final Batch Loss: 0.400\n",
      "Epoch 16023, Loss: 16.407, Final Batch Loss: 0.435\n",
      "Epoch 16024, Loss: 16.037, Final Batch Loss: 0.549\n",
      "Epoch 16025, Loss: 16.126, Final Batch Loss: 0.445\n",
      "Epoch 16026, Loss: 16.203, Final Batch Loss: 0.490\n",
      "Epoch 16027, Loss: 16.220, Final Batch Loss: 0.430\n",
      "Epoch 16028, Loss: 16.011, Final Batch Loss: 0.373\n",
      "Epoch 16029, Loss: 16.107, Final Batch Loss: 0.463\n",
      "Epoch 16030, Loss: 16.186, Final Batch Loss: 0.402\n",
      "Epoch 16031, Loss: 16.140, Final Batch Loss: 0.412\n",
      "Epoch 16032, Loss: 15.994, Final Batch Loss: 0.477\n",
      "Epoch 16033, Loss: 16.188, Final Batch Loss: 0.457\n",
      "Epoch 16034, Loss: 15.965, Final Batch Loss: 0.473\n",
      "Epoch 16035, Loss: 15.956, Final Batch Loss: 0.460\n",
      "Epoch 16036, Loss: 16.088, Final Batch Loss: 0.402\n",
      "Epoch 16037, Loss: 16.039, Final Batch Loss: 0.436\n",
      "Epoch 16038, Loss: 16.324, Final Batch Loss: 0.416\n",
      "Epoch 16039, Loss: 16.207, Final Batch Loss: 0.482\n",
      "Epoch 16040, Loss: 16.459, Final Batch Loss: 0.401\n",
      "Epoch 16041, Loss: 16.127, Final Batch Loss: 0.504\n",
      "Epoch 16042, Loss: 15.979, Final Batch Loss: 0.440\n",
      "Epoch 16043, Loss: 15.971, Final Batch Loss: 0.497\n",
      "Epoch 16044, Loss: 16.352, Final Batch Loss: 0.597\n",
      "Epoch 16045, Loss: 15.979, Final Batch Loss: 0.471\n",
      "Epoch 16046, Loss: 16.194, Final Batch Loss: 0.346\n",
      "Epoch 16047, Loss: 16.090, Final Batch Loss: 0.396\n",
      "Epoch 16048, Loss: 15.926, Final Batch Loss: 0.411\n",
      "Epoch 16049, Loss: 15.894, Final Batch Loss: 0.455\n",
      "Epoch 16050, Loss: 16.063, Final Batch Loss: 0.402\n",
      "Epoch 16051, Loss: 16.148, Final Batch Loss: 0.576\n",
      "Epoch 16052, Loss: 15.878, Final Batch Loss: 0.462\n",
      "Epoch 16053, Loss: 16.043, Final Batch Loss: 0.490\n",
      "Epoch 16054, Loss: 16.058, Final Batch Loss: 0.414\n",
      "Epoch 16055, Loss: 16.234, Final Batch Loss: 0.474\n",
      "Epoch 16056, Loss: 15.817, Final Batch Loss: 0.363\n",
      "Epoch 16057, Loss: 16.067, Final Batch Loss: 0.473\n",
      "Epoch 16058, Loss: 16.234, Final Batch Loss: 0.537\n",
      "Epoch 16059, Loss: 16.235, Final Batch Loss: 0.427\n",
      "Epoch 16060, Loss: 15.938, Final Batch Loss: 0.462\n",
      "Epoch 16061, Loss: 16.165, Final Batch Loss: 0.488\n",
      "Epoch 16062, Loss: 15.918, Final Batch Loss: 0.416\n",
      "Epoch 16063, Loss: 16.378, Final Batch Loss: 0.470\n",
      "Epoch 16064, Loss: 16.195, Final Batch Loss: 0.462\n",
      "Epoch 16065, Loss: 16.211, Final Batch Loss: 0.529\n",
      "Epoch 16066, Loss: 16.261, Final Batch Loss: 0.394\n",
      "Epoch 16067, Loss: 16.084, Final Batch Loss: 0.415\n",
      "Epoch 16068, Loss: 16.262, Final Batch Loss: 0.373\n",
      "Epoch 16069, Loss: 16.150, Final Batch Loss: 0.453\n",
      "Epoch 16070, Loss: 16.135, Final Batch Loss: 0.466\n",
      "Epoch 16071, Loss: 16.123, Final Batch Loss: 0.477\n",
      "Epoch 16072, Loss: 16.333, Final Batch Loss: 0.533\n",
      "Epoch 16073, Loss: 16.085, Final Batch Loss: 0.405\n",
      "Epoch 16074, Loss: 16.149, Final Batch Loss: 0.481\n",
      "Epoch 16075, Loss: 16.182, Final Batch Loss: 0.454\n",
      "Epoch 16076, Loss: 16.211, Final Batch Loss: 0.428\n",
      "Epoch 16077, Loss: 16.114, Final Batch Loss: 0.478\n",
      "Epoch 16078, Loss: 16.257, Final Batch Loss: 0.462\n",
      "Epoch 16079, Loss: 16.090, Final Batch Loss: 0.466\n",
      "Epoch 16080, Loss: 16.152, Final Batch Loss: 0.363\n",
      "Epoch 16081, Loss: 16.422, Final Batch Loss: 0.458\n",
      "Epoch 16082, Loss: 15.998, Final Batch Loss: 0.351\n",
      "Epoch 16083, Loss: 16.364, Final Batch Loss: 0.399\n",
      "Epoch 16084, Loss: 16.288, Final Batch Loss: 0.421\n",
      "Epoch 16085, Loss: 16.062, Final Batch Loss: 0.357\n",
      "Epoch 16086, Loss: 16.277, Final Batch Loss: 0.435\n",
      "Epoch 16087, Loss: 16.143, Final Batch Loss: 0.459\n",
      "Epoch 16088, Loss: 16.118, Final Batch Loss: 0.413\n",
      "Epoch 16089, Loss: 16.189, Final Batch Loss: 0.354\n",
      "Epoch 16090, Loss: 16.260, Final Batch Loss: 0.495\n",
      "Epoch 16091, Loss: 15.877, Final Batch Loss: 0.364\n",
      "Epoch 16092, Loss: 16.314, Final Batch Loss: 0.415\n",
      "Epoch 16093, Loss: 16.216, Final Batch Loss: 0.371\n",
      "Epoch 16094, Loss: 16.208, Final Batch Loss: 0.474\n",
      "Epoch 16095, Loss: 16.060, Final Batch Loss: 0.378\n",
      "Epoch 16096, Loss: 15.943, Final Batch Loss: 0.496\n",
      "Epoch 16097, Loss: 16.011, Final Batch Loss: 0.392\n",
      "Epoch 16098, Loss: 16.131, Final Batch Loss: 0.399\n",
      "Epoch 16099, Loss: 16.259, Final Batch Loss: 0.356\n",
      "Epoch 16100, Loss: 16.121, Final Batch Loss: 0.401\n",
      "Epoch 16101, Loss: 16.254, Final Batch Loss: 0.543\n",
      "Epoch 16102, Loss: 16.276, Final Batch Loss: 0.389\n",
      "Epoch 16103, Loss: 16.072, Final Batch Loss: 0.449\n",
      "Epoch 16104, Loss: 16.111, Final Batch Loss: 0.523\n",
      "Epoch 16105, Loss: 16.074, Final Batch Loss: 0.561\n",
      "Epoch 16106, Loss: 16.351, Final Batch Loss: 0.528\n",
      "Epoch 16107, Loss: 15.982, Final Batch Loss: 0.367\n",
      "Epoch 16108, Loss: 16.065, Final Batch Loss: 0.451\n",
      "Epoch 16109, Loss: 16.137, Final Batch Loss: 0.409\n",
      "Epoch 16110, Loss: 16.223, Final Batch Loss: 0.439\n",
      "Epoch 16111, Loss: 16.328, Final Batch Loss: 0.524\n",
      "Epoch 16112, Loss: 16.070, Final Batch Loss: 0.383\n",
      "Epoch 16113, Loss: 16.313, Final Batch Loss: 0.434\n",
      "Epoch 16114, Loss: 16.228, Final Batch Loss: 0.506\n",
      "Epoch 16115, Loss: 16.331, Final Batch Loss: 0.522\n",
      "Epoch 16116, Loss: 15.943, Final Batch Loss: 0.421\n",
      "Epoch 16117, Loss: 16.330, Final Batch Loss: 0.370\n",
      "Epoch 16118, Loss: 16.097, Final Batch Loss: 0.378\n",
      "Epoch 16119, Loss: 16.182, Final Batch Loss: 0.466\n",
      "Epoch 16120, Loss: 16.128, Final Batch Loss: 0.440\n",
      "Epoch 16121, Loss: 16.268, Final Batch Loss: 0.443\n",
      "Epoch 16122, Loss: 16.096, Final Batch Loss: 0.460\n",
      "Epoch 16123, Loss: 16.361, Final Batch Loss: 0.434\n",
      "Epoch 16124, Loss: 16.158, Final Batch Loss: 0.540\n",
      "Epoch 16125, Loss: 15.856, Final Batch Loss: 0.412\n",
      "Epoch 16126, Loss: 16.202, Final Batch Loss: 0.499\n",
      "Epoch 16127, Loss: 16.223, Final Batch Loss: 0.369\n",
      "Epoch 16128, Loss: 16.178, Final Batch Loss: 0.497\n",
      "Epoch 16129, Loss: 16.472, Final Batch Loss: 0.451\n",
      "Epoch 16130, Loss: 15.915, Final Batch Loss: 0.456\n",
      "Epoch 16131, Loss: 16.179, Final Batch Loss: 0.384\n",
      "Epoch 16132, Loss: 16.256, Final Batch Loss: 0.509\n",
      "Epoch 16133, Loss: 16.329, Final Batch Loss: 0.442\n",
      "Epoch 16134, Loss: 16.289, Final Batch Loss: 0.396\n",
      "Epoch 16135, Loss: 15.977, Final Batch Loss: 0.410\n",
      "Epoch 16136, Loss: 16.110, Final Batch Loss: 0.485\n",
      "Epoch 16137, Loss: 15.874, Final Batch Loss: 0.457\n",
      "Epoch 16138, Loss: 16.236, Final Batch Loss: 0.417\n",
      "Epoch 16139, Loss: 16.308, Final Batch Loss: 0.408\n",
      "Epoch 16140, Loss: 16.425, Final Batch Loss: 0.458\n",
      "Epoch 16141, Loss: 16.036, Final Batch Loss: 0.447\n",
      "Epoch 16142, Loss: 16.134, Final Batch Loss: 0.463\n",
      "Epoch 16143, Loss: 16.344, Final Batch Loss: 0.515\n",
      "Epoch 16144, Loss: 15.958, Final Batch Loss: 0.437\n",
      "Epoch 16145, Loss: 16.350, Final Batch Loss: 0.356\n",
      "Epoch 16146, Loss: 16.212, Final Batch Loss: 0.589\n",
      "Epoch 16147, Loss: 16.148, Final Batch Loss: 0.451\n",
      "Epoch 16148, Loss: 16.226, Final Batch Loss: 0.523\n",
      "Epoch 16149, Loss: 15.889, Final Batch Loss: 0.400\n",
      "Epoch 16150, Loss: 16.199, Final Batch Loss: 0.321\n",
      "Epoch 16151, Loss: 16.145, Final Batch Loss: 0.466\n",
      "Epoch 16152, Loss: 15.780, Final Batch Loss: 0.514\n",
      "Epoch 16153, Loss: 16.078, Final Batch Loss: 0.359\n",
      "Epoch 16154, Loss: 16.346, Final Batch Loss: 0.516\n",
      "Epoch 16155, Loss: 16.206, Final Batch Loss: 0.520\n",
      "Epoch 16156, Loss: 16.247, Final Batch Loss: 0.552\n",
      "Epoch 16157, Loss: 15.875, Final Batch Loss: 0.398\n",
      "Epoch 16158, Loss: 16.453, Final Batch Loss: 0.527\n",
      "Epoch 16159, Loss: 16.256, Final Batch Loss: 0.605\n",
      "Epoch 16160, Loss: 16.206, Final Batch Loss: 0.420\n",
      "Epoch 16161, Loss: 16.187, Final Batch Loss: 0.380\n",
      "Epoch 16162, Loss: 15.980, Final Batch Loss: 0.415\n",
      "Epoch 16163, Loss: 16.154, Final Batch Loss: 0.494\n",
      "Epoch 16164, Loss: 15.992, Final Batch Loss: 0.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16165, Loss: 16.100, Final Batch Loss: 0.510\n",
      "Epoch 16166, Loss: 16.303, Final Batch Loss: 0.549\n",
      "Epoch 16167, Loss: 16.120, Final Batch Loss: 0.384\n",
      "Epoch 16168, Loss: 16.030, Final Batch Loss: 0.426\n",
      "Epoch 16169, Loss: 16.232, Final Batch Loss: 0.551\n",
      "Epoch 16170, Loss: 16.032, Final Batch Loss: 0.381\n",
      "Epoch 16171, Loss: 16.012, Final Batch Loss: 0.376\n",
      "Epoch 16172, Loss: 16.139, Final Batch Loss: 0.471\n",
      "Epoch 16173, Loss: 16.082, Final Batch Loss: 0.387\n",
      "Epoch 16174, Loss: 15.889, Final Batch Loss: 0.465\n",
      "Epoch 16175, Loss: 16.183, Final Batch Loss: 0.430\n",
      "Epoch 16176, Loss: 16.021, Final Batch Loss: 0.331\n",
      "Epoch 16177, Loss: 15.855, Final Batch Loss: 0.504\n",
      "Epoch 16178, Loss: 16.109, Final Batch Loss: 0.545\n",
      "Epoch 16179, Loss: 15.956, Final Batch Loss: 0.396\n",
      "Epoch 16180, Loss: 16.302, Final Batch Loss: 0.401\n",
      "Epoch 16181, Loss: 16.158, Final Batch Loss: 0.532\n",
      "Epoch 16182, Loss: 16.156, Final Batch Loss: 0.467\n",
      "Epoch 16183, Loss: 16.178, Final Batch Loss: 0.392\n",
      "Epoch 16184, Loss: 16.345, Final Batch Loss: 0.410\n",
      "Epoch 16185, Loss: 16.149, Final Batch Loss: 0.439\n",
      "Epoch 16186, Loss: 16.009, Final Batch Loss: 0.406\n",
      "Epoch 16187, Loss: 16.017, Final Batch Loss: 0.479\n",
      "Epoch 16188, Loss: 16.017, Final Batch Loss: 0.434\n",
      "Epoch 16189, Loss: 16.165, Final Batch Loss: 0.464\n",
      "Epoch 16190, Loss: 16.038, Final Batch Loss: 0.416\n",
      "Epoch 16191, Loss: 16.219, Final Batch Loss: 0.425\n",
      "Epoch 16192, Loss: 15.696, Final Batch Loss: 0.370\n",
      "Epoch 16193, Loss: 16.026, Final Batch Loss: 0.437\n",
      "Epoch 16194, Loss: 16.261, Final Batch Loss: 0.560\n",
      "Epoch 16195, Loss: 15.908, Final Batch Loss: 0.338\n",
      "Epoch 16196, Loss: 16.134, Final Batch Loss: 0.350\n",
      "Epoch 16197, Loss: 16.403, Final Batch Loss: 0.515\n",
      "Epoch 16198, Loss: 16.208, Final Batch Loss: 0.445\n",
      "Epoch 16199, Loss: 15.901, Final Batch Loss: 0.433\n",
      "Epoch 16200, Loss: 16.042, Final Batch Loss: 0.378\n",
      "Epoch 16201, Loss: 16.213, Final Batch Loss: 0.380\n",
      "Epoch 16202, Loss: 16.198, Final Batch Loss: 0.530\n",
      "Epoch 16203, Loss: 16.178, Final Batch Loss: 0.396\n",
      "Epoch 16204, Loss: 16.075, Final Batch Loss: 0.482\n",
      "Epoch 16205, Loss: 16.085, Final Batch Loss: 0.439\n",
      "Epoch 16206, Loss: 16.077, Final Batch Loss: 0.456\n",
      "Epoch 16207, Loss: 15.893, Final Batch Loss: 0.468\n",
      "Epoch 16208, Loss: 16.059, Final Batch Loss: 0.404\n",
      "Epoch 16209, Loss: 16.443, Final Batch Loss: 0.390\n",
      "Epoch 16210, Loss: 16.121, Final Batch Loss: 0.302\n",
      "Epoch 16211, Loss: 16.222, Final Batch Loss: 0.370\n",
      "Epoch 16212, Loss: 16.391, Final Batch Loss: 0.491\n",
      "Epoch 16213, Loss: 16.086, Final Batch Loss: 0.482\n",
      "Epoch 16214, Loss: 15.882, Final Batch Loss: 0.429\n",
      "Epoch 16215, Loss: 16.014, Final Batch Loss: 0.449\n",
      "Epoch 16216, Loss: 16.140, Final Batch Loss: 0.376\n",
      "Epoch 16217, Loss: 16.103, Final Batch Loss: 0.400\n",
      "Epoch 16218, Loss: 16.185, Final Batch Loss: 0.413\n",
      "Epoch 16219, Loss: 16.109, Final Batch Loss: 0.440\n",
      "Epoch 16220, Loss: 16.077, Final Batch Loss: 0.449\n",
      "Epoch 16221, Loss: 16.322, Final Batch Loss: 0.481\n",
      "Epoch 16222, Loss: 16.183, Final Batch Loss: 0.431\n",
      "Epoch 16223, Loss: 15.940, Final Batch Loss: 0.345\n",
      "Epoch 16224, Loss: 16.133, Final Batch Loss: 0.520\n",
      "Epoch 16225, Loss: 16.130, Final Batch Loss: 0.411\n",
      "Epoch 16226, Loss: 16.251, Final Batch Loss: 0.375\n",
      "Epoch 16227, Loss: 16.358, Final Batch Loss: 0.519\n",
      "Epoch 16228, Loss: 16.077, Final Batch Loss: 0.380\n",
      "Epoch 16229, Loss: 16.240, Final Batch Loss: 0.474\n",
      "Epoch 16230, Loss: 16.028, Final Batch Loss: 0.572\n",
      "Epoch 16231, Loss: 16.137, Final Batch Loss: 0.533\n",
      "Epoch 16232, Loss: 15.976, Final Batch Loss: 0.420\n",
      "Epoch 16233, Loss: 16.227, Final Batch Loss: 0.372\n",
      "Epoch 16234, Loss: 16.374, Final Batch Loss: 0.356\n",
      "Epoch 16235, Loss: 16.246, Final Batch Loss: 0.461\n",
      "Epoch 16236, Loss: 16.212, Final Batch Loss: 0.384\n",
      "Epoch 16237, Loss: 16.136, Final Batch Loss: 0.449\n",
      "Epoch 16238, Loss: 16.094, Final Batch Loss: 0.507\n",
      "Epoch 16239, Loss: 15.984, Final Batch Loss: 0.355\n",
      "Epoch 16240, Loss: 16.092, Final Batch Loss: 0.426\n",
      "Epoch 16241, Loss: 16.067, Final Batch Loss: 0.439\n",
      "Epoch 16242, Loss: 15.956, Final Batch Loss: 0.423\n",
      "Epoch 16243, Loss: 16.458, Final Batch Loss: 0.389\n",
      "Epoch 16244, Loss: 16.455, Final Batch Loss: 0.488\n",
      "Epoch 16245, Loss: 16.061, Final Batch Loss: 0.500\n",
      "Epoch 16246, Loss: 16.230, Final Batch Loss: 0.411\n",
      "Epoch 16247, Loss: 16.192, Final Batch Loss: 0.426\n",
      "Epoch 16248, Loss: 15.933, Final Batch Loss: 0.397\n",
      "Epoch 16249, Loss: 16.032, Final Batch Loss: 0.371\n",
      "Epoch 16250, Loss: 16.244, Final Batch Loss: 0.393\n",
      "Epoch 16251, Loss: 16.052, Final Batch Loss: 0.604\n",
      "Epoch 16252, Loss: 15.955, Final Batch Loss: 0.390\n",
      "Epoch 16253, Loss: 16.237, Final Batch Loss: 0.449\n",
      "Epoch 16254, Loss: 16.583, Final Batch Loss: 0.539\n",
      "Epoch 16255, Loss: 16.165, Final Batch Loss: 0.391\n",
      "Epoch 16256, Loss: 15.929, Final Batch Loss: 0.433\n",
      "Epoch 16257, Loss: 16.118, Final Batch Loss: 0.422\n",
      "Epoch 16258, Loss: 16.154, Final Batch Loss: 0.472\n",
      "Epoch 16259, Loss: 16.002, Final Batch Loss: 0.413\n",
      "Epoch 16260, Loss: 16.102, Final Batch Loss: 0.430\n",
      "Epoch 16261, Loss: 16.210, Final Batch Loss: 0.561\n",
      "Epoch 16262, Loss: 16.301, Final Batch Loss: 0.384\n",
      "Epoch 16263, Loss: 16.024, Final Batch Loss: 0.439\n",
      "Epoch 16264, Loss: 15.991, Final Batch Loss: 0.334\n",
      "Epoch 16265, Loss: 16.051, Final Batch Loss: 0.442\n",
      "Epoch 16266, Loss: 16.091, Final Batch Loss: 0.392\n",
      "Epoch 16267, Loss: 15.954, Final Batch Loss: 0.398\n",
      "Epoch 16268, Loss: 16.175, Final Batch Loss: 0.393\n",
      "Epoch 16269, Loss: 16.168, Final Batch Loss: 0.458\n",
      "Epoch 16270, Loss: 16.202, Final Batch Loss: 0.487\n",
      "Epoch 16271, Loss: 15.998, Final Batch Loss: 0.499\n",
      "Epoch 16272, Loss: 15.958, Final Batch Loss: 0.419\n",
      "Epoch 16273, Loss: 16.071, Final Batch Loss: 0.536\n",
      "Epoch 16274, Loss: 16.130, Final Batch Loss: 0.401\n",
      "Epoch 16275, Loss: 16.029, Final Batch Loss: 0.450\n",
      "Epoch 16276, Loss: 16.190, Final Batch Loss: 0.496\n",
      "Epoch 16277, Loss: 16.079, Final Batch Loss: 0.569\n",
      "Epoch 16278, Loss: 16.165, Final Batch Loss: 0.461\n",
      "Epoch 16279, Loss: 16.058, Final Batch Loss: 0.477\n",
      "Epoch 16280, Loss: 15.932, Final Batch Loss: 0.530\n",
      "Epoch 16281, Loss: 16.312, Final Batch Loss: 0.452\n",
      "Epoch 16282, Loss: 16.025, Final Batch Loss: 0.503\n",
      "Epoch 16283, Loss: 16.233, Final Batch Loss: 0.591\n",
      "Epoch 16284, Loss: 16.224, Final Batch Loss: 0.454\n",
      "Epoch 16285, Loss: 16.186, Final Batch Loss: 0.566\n",
      "Epoch 16286, Loss: 16.191, Final Batch Loss: 0.400\n",
      "Epoch 16287, Loss: 16.188, Final Batch Loss: 0.419\n",
      "Epoch 16288, Loss: 16.383, Final Batch Loss: 0.443\n",
      "Epoch 16289, Loss: 16.040, Final Batch Loss: 0.440\n",
      "Epoch 16290, Loss: 16.271, Final Batch Loss: 0.426\n",
      "Epoch 16291, Loss: 16.097, Final Batch Loss: 0.305\n",
      "Epoch 16292, Loss: 16.122, Final Batch Loss: 0.458\n",
      "Epoch 16293, Loss: 16.068, Final Batch Loss: 0.453\n",
      "Epoch 16294, Loss: 15.922, Final Batch Loss: 0.416\n",
      "Epoch 16295, Loss: 16.215, Final Batch Loss: 0.551\n",
      "Epoch 16296, Loss: 15.992, Final Batch Loss: 0.370\n",
      "Epoch 16297, Loss: 16.318, Final Batch Loss: 0.534\n",
      "Epoch 16298, Loss: 16.323, Final Batch Loss: 0.376\n",
      "Epoch 16299, Loss: 16.204, Final Batch Loss: 0.458\n",
      "Epoch 16300, Loss: 16.414, Final Batch Loss: 0.420\n",
      "Epoch 16301, Loss: 16.389, Final Batch Loss: 0.430\n",
      "Epoch 16302, Loss: 16.280, Final Batch Loss: 0.564\n",
      "Epoch 16303, Loss: 16.065, Final Batch Loss: 0.415\n",
      "Epoch 16304, Loss: 15.988, Final Batch Loss: 0.444\n",
      "Epoch 16305, Loss: 16.095, Final Batch Loss: 0.423\n",
      "Epoch 16306, Loss: 16.116, Final Batch Loss: 0.441\n",
      "Epoch 16307, Loss: 16.019, Final Batch Loss: 0.414\n",
      "Epoch 16308, Loss: 15.982, Final Batch Loss: 0.492\n",
      "Epoch 16309, Loss: 15.856, Final Batch Loss: 0.451\n",
      "Epoch 16310, Loss: 16.282, Final Batch Loss: 0.404\n",
      "Epoch 16311, Loss: 16.116, Final Batch Loss: 0.476\n",
      "Epoch 16312, Loss: 16.118, Final Batch Loss: 0.506\n",
      "Epoch 16313, Loss: 16.170, Final Batch Loss: 0.332\n",
      "Epoch 16314, Loss: 16.126, Final Batch Loss: 0.473\n",
      "Epoch 16315, Loss: 16.258, Final Batch Loss: 0.656\n",
      "Epoch 16316, Loss: 16.143, Final Batch Loss: 0.560\n",
      "Epoch 16317, Loss: 16.365, Final Batch Loss: 0.573\n",
      "Epoch 16318, Loss: 15.982, Final Batch Loss: 0.484\n",
      "Epoch 16319, Loss: 16.195, Final Batch Loss: 0.466\n",
      "Epoch 16320, Loss: 16.247, Final Batch Loss: 0.445\n",
      "Epoch 16321, Loss: 15.989, Final Batch Loss: 0.450\n",
      "Epoch 16322, Loss: 16.348, Final Batch Loss: 0.500\n",
      "Epoch 16323, Loss: 16.158, Final Batch Loss: 0.461\n",
      "Epoch 16324, Loss: 15.908, Final Batch Loss: 0.435\n",
      "Epoch 16325, Loss: 15.936, Final Batch Loss: 0.405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16326, Loss: 16.187, Final Batch Loss: 0.362\n",
      "Epoch 16327, Loss: 16.094, Final Batch Loss: 0.347\n",
      "Epoch 16328, Loss: 15.946, Final Batch Loss: 0.399\n",
      "Epoch 16329, Loss: 16.284, Final Batch Loss: 0.402\n",
      "Epoch 16330, Loss: 16.064, Final Batch Loss: 0.421\n",
      "Epoch 16331, Loss: 16.223, Final Batch Loss: 0.409\n",
      "Epoch 16332, Loss: 15.890, Final Batch Loss: 0.474\n",
      "Epoch 16333, Loss: 15.991, Final Batch Loss: 0.451\n",
      "Epoch 16334, Loss: 16.308, Final Batch Loss: 0.467\n",
      "Epoch 16335, Loss: 16.030, Final Batch Loss: 0.498\n",
      "Epoch 16336, Loss: 16.422, Final Batch Loss: 0.531\n",
      "Epoch 16337, Loss: 16.204, Final Batch Loss: 0.416\n",
      "Epoch 16338, Loss: 15.793, Final Batch Loss: 0.392\n",
      "Epoch 16339, Loss: 16.669, Final Batch Loss: 0.361\n",
      "Epoch 16340, Loss: 16.325, Final Batch Loss: 0.537\n",
      "Epoch 16341, Loss: 16.207, Final Batch Loss: 0.489\n",
      "Epoch 16342, Loss: 15.985, Final Batch Loss: 0.444\n",
      "Epoch 16343, Loss: 16.282, Final Batch Loss: 0.478\n",
      "Epoch 16344, Loss: 16.142, Final Batch Loss: 0.495\n",
      "Epoch 16345, Loss: 16.033, Final Batch Loss: 0.530\n",
      "Epoch 16346, Loss: 16.185, Final Batch Loss: 0.462\n",
      "Epoch 16347, Loss: 16.123, Final Batch Loss: 0.451\n",
      "Epoch 16348, Loss: 16.261, Final Batch Loss: 0.460\n",
      "Epoch 16349, Loss: 16.368, Final Batch Loss: 0.409\n",
      "Epoch 16350, Loss: 16.198, Final Batch Loss: 0.524\n",
      "Epoch 16351, Loss: 16.072, Final Batch Loss: 0.383\n",
      "Epoch 16352, Loss: 15.873, Final Batch Loss: 0.452\n",
      "Epoch 16353, Loss: 16.193, Final Batch Loss: 0.388\n",
      "Epoch 16354, Loss: 16.290, Final Batch Loss: 0.375\n",
      "Epoch 16355, Loss: 16.427, Final Batch Loss: 0.562\n",
      "Epoch 16356, Loss: 15.898, Final Batch Loss: 0.383\n",
      "Epoch 16357, Loss: 16.202, Final Batch Loss: 0.477\n",
      "Epoch 16358, Loss: 15.989, Final Batch Loss: 0.409\n",
      "Epoch 16359, Loss: 15.853, Final Batch Loss: 0.449\n",
      "Epoch 16360, Loss: 16.178, Final Batch Loss: 0.432\n",
      "Epoch 16361, Loss: 16.036, Final Batch Loss: 0.421\n",
      "Epoch 16362, Loss: 15.989, Final Batch Loss: 0.391\n",
      "Epoch 16363, Loss: 16.160, Final Batch Loss: 0.474\n",
      "Epoch 16364, Loss: 16.234, Final Batch Loss: 0.488\n",
      "Epoch 16365, Loss: 16.144, Final Batch Loss: 0.541\n",
      "Epoch 16366, Loss: 16.066, Final Batch Loss: 0.530\n",
      "Epoch 16367, Loss: 16.112, Final Batch Loss: 0.457\n",
      "Epoch 16368, Loss: 15.978, Final Batch Loss: 0.468\n",
      "Epoch 16369, Loss: 15.907, Final Batch Loss: 0.500\n",
      "Epoch 16370, Loss: 15.958, Final Batch Loss: 0.494\n",
      "Epoch 16371, Loss: 16.138, Final Batch Loss: 0.533\n",
      "Epoch 16372, Loss: 15.954, Final Batch Loss: 0.410\n",
      "Epoch 16373, Loss: 16.254, Final Batch Loss: 0.469\n",
      "Epoch 16374, Loss: 16.195, Final Batch Loss: 0.490\n",
      "Epoch 16375, Loss: 15.979, Final Batch Loss: 0.494\n",
      "Epoch 16376, Loss: 16.385, Final Batch Loss: 0.445\n",
      "Epoch 16377, Loss: 15.987, Final Batch Loss: 0.436\n",
      "Epoch 16378, Loss: 16.385, Final Batch Loss: 0.471\n",
      "Epoch 16379, Loss: 16.220, Final Batch Loss: 0.588\n",
      "Epoch 16380, Loss: 16.043, Final Batch Loss: 0.439\n",
      "Epoch 16381, Loss: 16.159, Final Batch Loss: 0.359\n",
      "Epoch 16382, Loss: 16.119, Final Batch Loss: 0.426\n",
      "Epoch 16383, Loss: 16.106, Final Batch Loss: 0.285\n",
      "Epoch 16384, Loss: 15.974, Final Batch Loss: 0.403\n",
      "Epoch 16385, Loss: 16.169, Final Batch Loss: 0.388\n",
      "Epoch 16386, Loss: 16.261, Final Batch Loss: 0.526\n",
      "Epoch 16387, Loss: 16.391, Final Batch Loss: 0.382\n",
      "Epoch 16388, Loss: 16.229, Final Batch Loss: 0.436\n",
      "Epoch 16389, Loss: 16.049, Final Batch Loss: 0.405\n",
      "Epoch 16390, Loss: 16.109, Final Batch Loss: 0.442\n",
      "Epoch 16391, Loss: 16.296, Final Batch Loss: 0.544\n",
      "Epoch 16392, Loss: 16.141, Final Batch Loss: 0.396\n",
      "Epoch 16393, Loss: 16.000, Final Batch Loss: 0.535\n",
      "Epoch 16394, Loss: 16.139, Final Batch Loss: 0.491\n",
      "Epoch 16395, Loss: 15.849, Final Batch Loss: 0.432\n",
      "Epoch 16396, Loss: 16.125, Final Batch Loss: 0.414\n",
      "Epoch 16397, Loss: 16.110, Final Batch Loss: 0.394\n",
      "Epoch 16398, Loss: 16.050, Final Batch Loss: 0.422\n",
      "Epoch 16399, Loss: 16.073, Final Batch Loss: 0.412\n",
      "Epoch 16400, Loss: 16.128, Final Batch Loss: 0.551\n",
      "Epoch 16401, Loss: 16.095, Final Batch Loss: 0.333\n",
      "Epoch 16402, Loss: 16.266, Final Batch Loss: 0.584\n",
      "Epoch 16403, Loss: 16.191, Final Batch Loss: 0.411\n",
      "Epoch 16404, Loss: 16.113, Final Batch Loss: 0.534\n",
      "Epoch 16405, Loss: 16.044, Final Batch Loss: 0.450\n",
      "Epoch 16406, Loss: 16.224, Final Batch Loss: 0.558\n",
      "Epoch 16407, Loss: 16.225, Final Batch Loss: 0.462\n",
      "Epoch 16408, Loss: 16.043, Final Batch Loss: 0.472\n",
      "Epoch 16409, Loss: 15.859, Final Batch Loss: 0.536\n",
      "Epoch 16410, Loss: 15.774, Final Batch Loss: 0.445\n",
      "Epoch 16411, Loss: 16.145, Final Batch Loss: 0.323\n",
      "Epoch 16412, Loss: 16.036, Final Batch Loss: 0.502\n",
      "Epoch 16413, Loss: 16.040, Final Batch Loss: 0.396\n",
      "Epoch 16414, Loss: 16.299, Final Batch Loss: 0.468\n",
      "Epoch 16415, Loss: 15.989, Final Batch Loss: 0.394\n",
      "Epoch 16416, Loss: 16.329, Final Batch Loss: 0.436\n",
      "Epoch 16417, Loss: 16.288, Final Batch Loss: 0.535\n",
      "Epoch 16418, Loss: 16.170, Final Batch Loss: 0.532\n",
      "Epoch 16419, Loss: 15.996, Final Batch Loss: 0.570\n",
      "Epoch 16420, Loss: 16.081, Final Batch Loss: 0.483\n",
      "Epoch 16421, Loss: 16.356, Final Batch Loss: 0.367\n",
      "Epoch 16422, Loss: 16.095, Final Batch Loss: 0.467\n",
      "Epoch 16423, Loss: 16.489, Final Batch Loss: 0.511\n",
      "Epoch 16424, Loss: 16.246, Final Batch Loss: 0.391\n",
      "Epoch 16425, Loss: 16.224, Final Batch Loss: 0.429\n",
      "Epoch 16426, Loss: 16.067, Final Batch Loss: 0.470\n",
      "Epoch 16427, Loss: 15.962, Final Batch Loss: 0.467\n",
      "Epoch 16428, Loss: 16.035, Final Batch Loss: 0.344\n",
      "Epoch 16429, Loss: 15.982, Final Batch Loss: 0.468\n",
      "Epoch 16430, Loss: 16.444, Final Batch Loss: 0.412\n",
      "Epoch 16431, Loss: 15.890, Final Batch Loss: 0.440\n",
      "Epoch 16432, Loss: 16.294, Final Batch Loss: 0.449\n",
      "Epoch 16433, Loss: 16.114, Final Batch Loss: 0.479\n",
      "Epoch 16434, Loss: 16.066, Final Batch Loss: 0.438\n",
      "Epoch 16435, Loss: 15.961, Final Batch Loss: 0.359\n",
      "Epoch 16436, Loss: 16.139, Final Batch Loss: 0.555\n",
      "Epoch 16437, Loss: 16.214, Final Batch Loss: 0.429\n",
      "Epoch 16438, Loss: 15.973, Final Batch Loss: 0.435\n",
      "Epoch 16439, Loss: 15.922, Final Batch Loss: 0.418\n",
      "Epoch 16440, Loss: 16.100, Final Batch Loss: 0.362\n",
      "Epoch 16441, Loss: 16.338, Final Batch Loss: 0.443\n",
      "Epoch 16442, Loss: 15.960, Final Batch Loss: 0.387\n",
      "Epoch 16443, Loss: 15.998, Final Batch Loss: 0.495\n",
      "Epoch 16444, Loss: 16.327, Final Batch Loss: 0.619\n",
      "Epoch 16445, Loss: 16.095, Final Batch Loss: 0.525\n",
      "Epoch 16446, Loss: 16.187, Final Batch Loss: 0.381\n",
      "Epoch 16447, Loss: 15.999, Final Batch Loss: 0.421\n",
      "Epoch 16448, Loss: 15.965, Final Batch Loss: 0.352\n",
      "Epoch 16449, Loss: 16.033, Final Batch Loss: 0.447\n",
      "Epoch 16450, Loss: 16.038, Final Batch Loss: 0.447\n",
      "Epoch 16451, Loss: 16.147, Final Batch Loss: 0.415\n",
      "Epoch 16452, Loss: 15.931, Final Batch Loss: 0.485\n",
      "Epoch 16453, Loss: 15.732, Final Batch Loss: 0.348\n",
      "Epoch 16454, Loss: 16.074, Final Batch Loss: 0.423\n",
      "Epoch 16455, Loss: 16.007, Final Batch Loss: 0.410\n",
      "Epoch 16456, Loss: 16.113, Final Batch Loss: 0.577\n",
      "Epoch 16457, Loss: 16.200, Final Batch Loss: 0.415\n",
      "Epoch 16458, Loss: 15.923, Final Batch Loss: 0.425\n",
      "Epoch 16459, Loss: 16.145, Final Batch Loss: 0.518\n",
      "Epoch 16460, Loss: 15.999, Final Batch Loss: 0.437\n",
      "Epoch 16461, Loss: 16.131, Final Batch Loss: 0.438\n",
      "Epoch 16462, Loss: 16.098, Final Batch Loss: 0.440\n",
      "Epoch 16463, Loss: 16.290, Final Batch Loss: 0.461\n",
      "Epoch 16464, Loss: 16.320, Final Batch Loss: 0.407\n",
      "Epoch 16465, Loss: 16.151, Final Batch Loss: 0.397\n",
      "Epoch 16466, Loss: 16.056, Final Batch Loss: 0.459\n",
      "Epoch 16467, Loss: 16.004, Final Batch Loss: 0.430\n",
      "Epoch 16468, Loss: 16.195, Final Batch Loss: 0.443\n",
      "Epoch 16469, Loss: 16.110, Final Batch Loss: 0.405\n",
      "Epoch 16470, Loss: 16.172, Final Batch Loss: 0.474\n",
      "Epoch 16471, Loss: 16.067, Final Batch Loss: 0.511\n",
      "Epoch 16472, Loss: 16.246, Final Batch Loss: 0.464\n",
      "Epoch 16473, Loss: 16.086, Final Batch Loss: 0.433\n",
      "Epoch 16474, Loss: 16.310, Final Batch Loss: 0.520\n",
      "Epoch 16475, Loss: 16.157, Final Batch Loss: 0.404\n",
      "Epoch 16476, Loss: 16.205, Final Batch Loss: 0.460\n",
      "Epoch 16477, Loss: 16.086, Final Batch Loss: 0.492\n",
      "Epoch 16478, Loss: 15.972, Final Batch Loss: 0.378\n",
      "Epoch 16479, Loss: 15.967, Final Batch Loss: 0.560\n",
      "Epoch 16480, Loss: 16.134, Final Batch Loss: 0.442\n",
      "Epoch 16481, Loss: 16.431, Final Batch Loss: 0.540\n",
      "Epoch 16482, Loss: 16.271, Final Batch Loss: 0.410\n",
      "Epoch 16483, Loss: 16.404, Final Batch Loss: 0.431\n",
      "Epoch 16484, Loss: 16.086, Final Batch Loss: 0.437\n",
      "Epoch 16485, Loss: 16.179, Final Batch Loss: 0.360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16486, Loss: 16.188, Final Batch Loss: 0.518\n",
      "Epoch 16487, Loss: 16.095, Final Batch Loss: 0.482\n",
      "Epoch 16488, Loss: 16.154, Final Batch Loss: 0.554\n",
      "Epoch 16489, Loss: 16.135, Final Batch Loss: 0.420\n",
      "Epoch 16490, Loss: 16.134, Final Batch Loss: 0.430\n",
      "Epoch 16491, Loss: 16.174, Final Batch Loss: 0.384\n",
      "Epoch 16492, Loss: 15.873, Final Batch Loss: 0.452\n",
      "Epoch 16493, Loss: 16.063, Final Batch Loss: 0.454\n",
      "Epoch 16494, Loss: 16.239, Final Batch Loss: 0.525\n",
      "Epoch 16495, Loss: 16.351, Final Batch Loss: 0.442\n",
      "Epoch 16496, Loss: 16.295, Final Batch Loss: 0.488\n",
      "Epoch 16497, Loss: 16.227, Final Batch Loss: 0.571\n",
      "Epoch 16498, Loss: 16.204, Final Batch Loss: 0.463\n",
      "Epoch 16499, Loss: 16.388, Final Batch Loss: 0.388\n",
      "Epoch 16500, Loss: 15.960, Final Batch Loss: 0.398\n",
      "Epoch 16501, Loss: 16.070, Final Batch Loss: 0.401\n",
      "Epoch 16502, Loss: 15.775, Final Batch Loss: 0.405\n",
      "Epoch 16503, Loss: 16.100, Final Batch Loss: 0.438\n",
      "Epoch 16504, Loss: 16.243, Final Batch Loss: 0.422\n",
      "Epoch 16505, Loss: 15.878, Final Batch Loss: 0.379\n",
      "Epoch 16506, Loss: 16.190, Final Batch Loss: 0.450\n",
      "Epoch 16507, Loss: 16.226, Final Batch Loss: 0.426\n",
      "Epoch 16508, Loss: 16.021, Final Batch Loss: 0.425\n",
      "Epoch 16509, Loss: 16.274, Final Batch Loss: 0.427\n",
      "Epoch 16510, Loss: 16.057, Final Batch Loss: 0.454\n",
      "Epoch 16511, Loss: 16.064, Final Batch Loss: 0.365\n",
      "Epoch 16512, Loss: 16.044, Final Batch Loss: 0.509\n",
      "Epoch 16513, Loss: 16.042, Final Batch Loss: 0.498\n",
      "Epoch 16514, Loss: 16.244, Final Batch Loss: 0.460\n",
      "Epoch 16515, Loss: 16.243, Final Batch Loss: 0.508\n",
      "Epoch 16516, Loss: 16.095, Final Batch Loss: 0.416\n",
      "Epoch 16517, Loss: 15.907, Final Batch Loss: 0.550\n",
      "Epoch 16518, Loss: 16.312, Final Batch Loss: 0.441\n",
      "Epoch 16519, Loss: 16.330, Final Batch Loss: 0.386\n",
      "Epoch 16520, Loss: 16.062, Final Batch Loss: 0.474\n",
      "Epoch 16521, Loss: 16.092, Final Batch Loss: 0.494\n",
      "Epoch 16522, Loss: 16.030, Final Batch Loss: 0.401\n",
      "Epoch 16523, Loss: 16.195, Final Batch Loss: 0.511\n",
      "Epoch 16524, Loss: 16.009, Final Batch Loss: 0.363\n",
      "Epoch 16525, Loss: 16.103, Final Batch Loss: 0.406\n",
      "Epoch 16526, Loss: 16.218, Final Batch Loss: 0.429\n",
      "Epoch 16527, Loss: 16.298, Final Batch Loss: 0.561\n",
      "Epoch 16528, Loss: 16.245, Final Batch Loss: 0.480\n",
      "Epoch 16529, Loss: 16.011, Final Batch Loss: 0.519\n",
      "Epoch 16530, Loss: 15.826, Final Batch Loss: 0.414\n",
      "Epoch 16531, Loss: 15.994, Final Batch Loss: 0.470\n",
      "Epoch 16532, Loss: 16.015, Final Batch Loss: 0.364\n",
      "Epoch 16533, Loss: 16.138, Final Batch Loss: 0.473\n",
      "Epoch 16534, Loss: 16.320, Final Batch Loss: 0.490\n",
      "Epoch 16535, Loss: 15.754, Final Batch Loss: 0.401\n",
      "Epoch 16536, Loss: 16.137, Final Batch Loss: 0.384\n",
      "Epoch 16537, Loss: 16.088, Final Batch Loss: 0.462\n",
      "Epoch 16538, Loss: 16.274, Final Batch Loss: 0.552\n",
      "Epoch 16539, Loss: 16.000, Final Batch Loss: 0.447\n",
      "Epoch 16540, Loss: 16.006, Final Batch Loss: 0.547\n",
      "Epoch 16541, Loss: 16.142, Final Batch Loss: 0.439\n",
      "Epoch 16542, Loss: 16.207, Final Batch Loss: 0.360\n",
      "Epoch 16543, Loss: 16.191, Final Batch Loss: 0.489\n",
      "Epoch 16544, Loss: 15.910, Final Batch Loss: 0.393\n",
      "Epoch 16545, Loss: 16.025, Final Batch Loss: 0.371\n",
      "Epoch 16546, Loss: 16.093, Final Batch Loss: 0.434\n",
      "Epoch 16547, Loss: 16.177, Final Batch Loss: 0.488\n",
      "Epoch 16548, Loss: 16.094, Final Batch Loss: 0.505\n",
      "Epoch 16549, Loss: 16.616, Final Batch Loss: 0.560\n",
      "Epoch 16550, Loss: 16.186, Final Batch Loss: 0.457\n",
      "Epoch 16551, Loss: 15.981, Final Batch Loss: 0.397\n",
      "Epoch 16552, Loss: 15.887, Final Batch Loss: 0.418\n",
      "Epoch 16553, Loss: 16.241, Final Batch Loss: 0.581\n",
      "Epoch 16554, Loss: 15.858, Final Batch Loss: 0.399\n",
      "Epoch 16555, Loss: 15.943, Final Batch Loss: 0.459\n",
      "Epoch 16556, Loss: 16.007, Final Batch Loss: 0.523\n",
      "Epoch 16557, Loss: 16.216, Final Batch Loss: 0.503\n",
      "Epoch 16558, Loss: 16.083, Final Batch Loss: 0.464\n",
      "Epoch 16559, Loss: 16.198, Final Batch Loss: 0.379\n",
      "Epoch 16560, Loss: 16.267, Final Batch Loss: 0.426\n",
      "Epoch 16561, Loss: 16.263, Final Batch Loss: 0.536\n",
      "Epoch 16562, Loss: 16.123, Final Batch Loss: 0.471\n",
      "Epoch 16563, Loss: 15.999, Final Batch Loss: 0.524\n",
      "Epoch 16564, Loss: 15.984, Final Batch Loss: 0.458\n",
      "Epoch 16565, Loss: 16.128, Final Batch Loss: 0.474\n",
      "Epoch 16566, Loss: 16.300, Final Batch Loss: 0.408\n",
      "Epoch 16567, Loss: 16.207, Final Batch Loss: 0.441\n",
      "Epoch 16568, Loss: 16.254, Final Batch Loss: 0.532\n",
      "Epoch 16569, Loss: 16.138, Final Batch Loss: 0.468\n",
      "Epoch 16570, Loss: 16.200, Final Batch Loss: 0.438\n",
      "Epoch 16571, Loss: 16.175, Final Batch Loss: 0.352\n",
      "Epoch 16572, Loss: 16.081, Final Batch Loss: 0.503\n",
      "Epoch 16573, Loss: 15.926, Final Batch Loss: 0.287\n",
      "Epoch 16574, Loss: 16.175, Final Batch Loss: 0.479\n",
      "Epoch 16575, Loss: 16.244, Final Batch Loss: 0.426\n",
      "Epoch 16576, Loss: 16.030, Final Batch Loss: 0.388\n",
      "Epoch 16577, Loss: 15.981, Final Batch Loss: 0.394\n",
      "Epoch 16578, Loss: 16.261, Final Batch Loss: 0.530\n",
      "Epoch 16579, Loss: 16.281, Final Batch Loss: 0.336\n",
      "Epoch 16580, Loss: 16.309, Final Batch Loss: 0.446\n",
      "Epoch 16581, Loss: 16.198, Final Batch Loss: 0.471\n",
      "Epoch 16582, Loss: 15.984, Final Batch Loss: 0.432\n",
      "Epoch 16583, Loss: 16.059, Final Batch Loss: 0.440\n",
      "Epoch 16584, Loss: 16.289, Final Batch Loss: 0.438\n",
      "Epoch 16585, Loss: 16.449, Final Batch Loss: 0.449\n",
      "Epoch 16586, Loss: 16.175, Final Batch Loss: 0.469\n",
      "Epoch 16587, Loss: 16.139, Final Batch Loss: 0.471\n",
      "Epoch 16588, Loss: 16.308, Final Batch Loss: 0.521\n",
      "Epoch 16589, Loss: 16.103, Final Batch Loss: 0.450\n",
      "Epoch 16590, Loss: 15.946, Final Batch Loss: 0.372\n",
      "Epoch 16591, Loss: 16.251, Final Batch Loss: 0.529\n",
      "Epoch 16592, Loss: 16.181, Final Batch Loss: 0.494\n",
      "Epoch 16593, Loss: 16.302, Final Batch Loss: 0.413\n",
      "Epoch 16594, Loss: 16.134, Final Batch Loss: 0.385\n",
      "Epoch 16595, Loss: 16.313, Final Batch Loss: 0.436\n",
      "Epoch 16596, Loss: 15.994, Final Batch Loss: 0.424\n",
      "Epoch 16597, Loss: 16.365, Final Batch Loss: 0.357\n",
      "Epoch 16598, Loss: 16.093, Final Batch Loss: 0.412\n",
      "Epoch 16599, Loss: 16.128, Final Batch Loss: 0.407\n",
      "Epoch 16600, Loss: 16.339, Final Batch Loss: 0.504\n",
      "Epoch 16601, Loss: 16.323, Final Batch Loss: 0.535\n",
      "Epoch 16602, Loss: 16.349, Final Batch Loss: 0.506\n",
      "Epoch 16603, Loss: 16.052, Final Batch Loss: 0.346\n",
      "Epoch 16604, Loss: 16.188, Final Batch Loss: 0.405\n",
      "Epoch 16605, Loss: 16.209, Final Batch Loss: 0.425\n",
      "Epoch 16606, Loss: 16.134, Final Batch Loss: 0.443\n",
      "Epoch 16607, Loss: 16.001, Final Batch Loss: 0.381\n",
      "Epoch 16608, Loss: 16.044, Final Batch Loss: 0.455\n",
      "Epoch 16609, Loss: 16.063, Final Batch Loss: 0.437\n",
      "Epoch 16610, Loss: 16.256, Final Batch Loss: 0.408\n",
      "Epoch 16611, Loss: 16.049, Final Batch Loss: 0.348\n",
      "Epoch 16612, Loss: 16.203, Final Batch Loss: 0.373\n",
      "Epoch 16613, Loss: 16.025, Final Batch Loss: 0.429\n",
      "Epoch 16614, Loss: 16.179, Final Batch Loss: 0.452\n",
      "Epoch 16615, Loss: 15.960, Final Batch Loss: 0.312\n",
      "Epoch 16616, Loss: 16.053, Final Batch Loss: 0.560\n",
      "Epoch 16617, Loss: 15.815, Final Batch Loss: 0.342\n",
      "Epoch 16618, Loss: 16.159, Final Batch Loss: 0.555\n",
      "Epoch 16619, Loss: 16.334, Final Batch Loss: 0.513\n",
      "Epoch 16620, Loss: 16.068, Final Batch Loss: 0.467\n",
      "Epoch 16621, Loss: 16.166, Final Batch Loss: 0.381\n",
      "Epoch 16622, Loss: 16.084, Final Batch Loss: 0.473\n",
      "Epoch 16623, Loss: 16.277, Final Batch Loss: 0.431\n",
      "Epoch 16624, Loss: 16.280, Final Batch Loss: 0.438\n",
      "Epoch 16625, Loss: 16.354, Final Batch Loss: 0.467\n",
      "Epoch 16626, Loss: 16.208, Final Batch Loss: 0.394\n",
      "Epoch 16627, Loss: 16.159, Final Batch Loss: 0.444\n",
      "Epoch 16628, Loss: 15.955, Final Batch Loss: 0.417\n",
      "Epoch 16629, Loss: 16.174, Final Batch Loss: 0.495\n",
      "Epoch 16630, Loss: 16.251, Final Batch Loss: 0.446\n",
      "Epoch 16631, Loss: 16.054, Final Batch Loss: 0.446\n",
      "Epoch 16632, Loss: 15.958, Final Batch Loss: 0.368\n",
      "Epoch 16633, Loss: 16.275, Final Batch Loss: 0.460\n",
      "Epoch 16634, Loss: 16.070, Final Batch Loss: 0.507\n",
      "Epoch 16635, Loss: 16.162, Final Batch Loss: 0.420\n",
      "Epoch 16636, Loss: 16.063, Final Batch Loss: 0.368\n",
      "Epoch 16637, Loss: 16.347, Final Batch Loss: 0.444\n",
      "Epoch 16638, Loss: 16.129, Final Batch Loss: 0.469\n",
      "Epoch 16639, Loss: 16.224, Final Batch Loss: 0.392\n",
      "Epoch 16640, Loss: 15.718, Final Batch Loss: 0.425\n",
      "Epoch 16641, Loss: 16.084, Final Batch Loss: 0.436\n",
      "Epoch 16642, Loss: 16.190, Final Batch Loss: 0.426\n",
      "Epoch 16643, Loss: 16.000, Final Batch Loss: 0.412\n",
      "Epoch 16644, Loss: 16.012, Final Batch Loss: 0.410\n",
      "Epoch 16645, Loss: 16.296, Final Batch Loss: 0.485\n",
      "Epoch 16646, Loss: 16.157, Final Batch Loss: 0.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16647, Loss: 15.964, Final Batch Loss: 0.476\n",
      "Epoch 16648, Loss: 16.016, Final Batch Loss: 0.381\n",
      "Epoch 16649, Loss: 16.480, Final Batch Loss: 0.428\n",
      "Epoch 16650, Loss: 16.232, Final Batch Loss: 0.447\n",
      "Epoch 16651, Loss: 15.984, Final Batch Loss: 0.390\n",
      "Epoch 16652, Loss: 16.286, Final Batch Loss: 0.523\n",
      "Epoch 16653, Loss: 15.859, Final Batch Loss: 0.398\n",
      "Epoch 16654, Loss: 15.953, Final Batch Loss: 0.500\n",
      "Epoch 16655, Loss: 16.178, Final Batch Loss: 0.440\n",
      "Epoch 16656, Loss: 16.003, Final Batch Loss: 0.427\n",
      "Epoch 16657, Loss: 16.132, Final Batch Loss: 0.318\n",
      "Epoch 16658, Loss: 15.980, Final Batch Loss: 0.452\n",
      "Epoch 16659, Loss: 15.889, Final Batch Loss: 0.418\n",
      "Epoch 16660, Loss: 16.035, Final Batch Loss: 0.395\n",
      "Epoch 16661, Loss: 16.249, Final Batch Loss: 0.394\n",
      "Epoch 16662, Loss: 16.075, Final Batch Loss: 0.381\n",
      "Epoch 16663, Loss: 15.860, Final Batch Loss: 0.373\n",
      "Epoch 16664, Loss: 16.052, Final Batch Loss: 0.497\n",
      "Epoch 16665, Loss: 15.983, Final Batch Loss: 0.332\n",
      "Epoch 16666, Loss: 16.348, Final Batch Loss: 0.472\n",
      "Epoch 16667, Loss: 16.130, Final Batch Loss: 0.524\n",
      "Epoch 16668, Loss: 16.077, Final Batch Loss: 0.436\n",
      "Epoch 16669, Loss: 15.808, Final Batch Loss: 0.390\n",
      "Epoch 16670, Loss: 16.219, Final Batch Loss: 0.453\n",
      "Epoch 16671, Loss: 15.961, Final Batch Loss: 0.452\n",
      "Epoch 16672, Loss: 16.158, Final Batch Loss: 0.529\n",
      "Epoch 16673, Loss: 16.078, Final Batch Loss: 0.398\n",
      "Epoch 16674, Loss: 15.885, Final Batch Loss: 0.440\n",
      "Epoch 16675, Loss: 16.032, Final Batch Loss: 0.383\n",
      "Epoch 16676, Loss: 16.161, Final Batch Loss: 0.395\n",
      "Epoch 16677, Loss: 15.949, Final Batch Loss: 0.506\n",
      "Epoch 16678, Loss: 16.118, Final Batch Loss: 0.475\n",
      "Epoch 16679, Loss: 16.050, Final Batch Loss: 0.350\n",
      "Epoch 16680, Loss: 16.038, Final Batch Loss: 0.474\n",
      "Epoch 16681, Loss: 15.980, Final Batch Loss: 0.445\n",
      "Epoch 16682, Loss: 15.982, Final Batch Loss: 0.444\n",
      "Epoch 16683, Loss: 15.926, Final Batch Loss: 0.470\n",
      "Epoch 16684, Loss: 16.074, Final Batch Loss: 0.357\n",
      "Epoch 16685, Loss: 16.176, Final Batch Loss: 0.380\n",
      "Epoch 16686, Loss: 16.006, Final Batch Loss: 0.446\n",
      "Epoch 16687, Loss: 16.370, Final Batch Loss: 0.426\n",
      "Epoch 16688, Loss: 16.214, Final Batch Loss: 0.529\n",
      "Epoch 16689, Loss: 16.090, Final Batch Loss: 0.457\n",
      "Epoch 16690, Loss: 16.304, Final Batch Loss: 0.597\n",
      "Epoch 16691, Loss: 15.838, Final Batch Loss: 0.529\n",
      "Epoch 16692, Loss: 16.252, Final Batch Loss: 0.447\n",
      "Epoch 16693, Loss: 16.707, Final Batch Loss: 0.479\n",
      "Epoch 16694, Loss: 15.962, Final Batch Loss: 0.559\n",
      "Epoch 16695, Loss: 16.053, Final Batch Loss: 0.403\n",
      "Epoch 16696, Loss: 15.935, Final Batch Loss: 0.419\n",
      "Epoch 16697, Loss: 16.105, Final Batch Loss: 0.444\n",
      "Epoch 16698, Loss: 15.993, Final Batch Loss: 0.377\n",
      "Epoch 16699, Loss: 16.041, Final Batch Loss: 0.363\n",
      "Epoch 16700, Loss: 16.279, Final Batch Loss: 0.412\n",
      "Epoch 16701, Loss: 16.082, Final Batch Loss: 0.463\n",
      "Epoch 16702, Loss: 16.003, Final Batch Loss: 0.561\n",
      "Epoch 16703, Loss: 15.862, Final Batch Loss: 0.455\n",
      "Epoch 16704, Loss: 16.132, Final Batch Loss: 0.404\n",
      "Epoch 16705, Loss: 16.019, Final Batch Loss: 0.467\n",
      "Epoch 16706, Loss: 16.100, Final Batch Loss: 0.508\n",
      "Epoch 16707, Loss: 16.028, Final Batch Loss: 0.380\n",
      "Epoch 16708, Loss: 16.221, Final Batch Loss: 0.432\n",
      "Epoch 16709, Loss: 15.959, Final Batch Loss: 0.421\n",
      "Epoch 16710, Loss: 16.125, Final Batch Loss: 0.512\n",
      "Epoch 16711, Loss: 16.320, Final Batch Loss: 0.378\n",
      "Epoch 16712, Loss: 15.989, Final Batch Loss: 0.470\n",
      "Epoch 16713, Loss: 16.358, Final Batch Loss: 0.510\n",
      "Epoch 16714, Loss: 15.957, Final Batch Loss: 0.412\n",
      "Epoch 16715, Loss: 15.904, Final Batch Loss: 0.443\n",
      "Epoch 16716, Loss: 16.247, Final Batch Loss: 0.386\n",
      "Epoch 16717, Loss: 16.141, Final Batch Loss: 0.443\n",
      "Epoch 16718, Loss: 16.262, Final Batch Loss: 0.398\n",
      "Epoch 16719, Loss: 16.101, Final Batch Loss: 0.446\n",
      "Epoch 16720, Loss: 15.931, Final Batch Loss: 0.465\n",
      "Epoch 16721, Loss: 16.051, Final Batch Loss: 0.452\n",
      "Epoch 16722, Loss: 16.086, Final Batch Loss: 0.563\n",
      "Epoch 16723, Loss: 16.101, Final Batch Loss: 0.356\n",
      "Epoch 16724, Loss: 16.075, Final Batch Loss: 0.519\n",
      "Epoch 16725, Loss: 16.175, Final Batch Loss: 0.386\n",
      "Epoch 16726, Loss: 16.162, Final Batch Loss: 0.491\n",
      "Epoch 16727, Loss: 16.222, Final Batch Loss: 0.568\n",
      "Epoch 16728, Loss: 15.956, Final Batch Loss: 0.478\n",
      "Epoch 16729, Loss: 16.124, Final Batch Loss: 0.410\n",
      "Epoch 16730, Loss: 16.123, Final Batch Loss: 0.485\n",
      "Epoch 16731, Loss: 16.002, Final Batch Loss: 0.429\n",
      "Epoch 16732, Loss: 15.911, Final Batch Loss: 0.404\n",
      "Epoch 16733, Loss: 16.435, Final Batch Loss: 0.562\n",
      "Epoch 16734, Loss: 15.859, Final Batch Loss: 0.360\n",
      "Epoch 16735, Loss: 16.013, Final Batch Loss: 0.442\n",
      "Epoch 16736, Loss: 16.153, Final Batch Loss: 0.459\n",
      "Epoch 16737, Loss: 16.046, Final Batch Loss: 0.550\n",
      "Epoch 16738, Loss: 15.789, Final Batch Loss: 0.410\n",
      "Epoch 16739, Loss: 15.842, Final Batch Loss: 0.417\n",
      "Epoch 16740, Loss: 16.389, Final Batch Loss: 0.526\n",
      "Epoch 16741, Loss: 16.058, Final Batch Loss: 0.500\n",
      "Epoch 16742, Loss: 16.120, Final Batch Loss: 0.512\n",
      "Epoch 16743, Loss: 15.699, Final Batch Loss: 0.389\n",
      "Epoch 16744, Loss: 16.226, Final Batch Loss: 0.338\n",
      "Epoch 16745, Loss: 15.990, Final Batch Loss: 0.433\n",
      "Epoch 16746, Loss: 16.116, Final Batch Loss: 0.459\n",
      "Epoch 16747, Loss: 15.936, Final Batch Loss: 0.391\n",
      "Epoch 16748, Loss: 15.863, Final Batch Loss: 0.460\n",
      "Epoch 16749, Loss: 16.191, Final Batch Loss: 0.363\n",
      "Epoch 16750, Loss: 16.080, Final Batch Loss: 0.558\n",
      "Epoch 16751, Loss: 16.406, Final Batch Loss: 0.485\n",
      "Epoch 16752, Loss: 16.010, Final Batch Loss: 0.410\n",
      "Epoch 16753, Loss: 15.966, Final Batch Loss: 0.360\n",
      "Epoch 16754, Loss: 16.090, Final Batch Loss: 0.418\n",
      "Epoch 16755, Loss: 16.226, Final Batch Loss: 0.549\n",
      "Epoch 16756, Loss: 16.344, Final Batch Loss: 0.492\n",
      "Epoch 16757, Loss: 16.011, Final Batch Loss: 0.424\n",
      "Epoch 16758, Loss: 16.080, Final Batch Loss: 0.470\n",
      "Epoch 16759, Loss: 16.109, Final Batch Loss: 0.402\n",
      "Epoch 16760, Loss: 16.381, Final Batch Loss: 0.549\n",
      "Epoch 16761, Loss: 16.118, Final Batch Loss: 0.447\n",
      "Epoch 16762, Loss: 15.728, Final Batch Loss: 0.395\n",
      "Epoch 16763, Loss: 16.073, Final Batch Loss: 0.466\n",
      "Epoch 16764, Loss: 16.229, Final Batch Loss: 0.466\n",
      "Epoch 16765, Loss: 15.978, Final Batch Loss: 0.430\n",
      "Epoch 16766, Loss: 15.998, Final Batch Loss: 0.429\n",
      "Epoch 16767, Loss: 16.102, Final Batch Loss: 0.454\n",
      "Epoch 16768, Loss: 16.223, Final Batch Loss: 0.432\n",
      "Epoch 16769, Loss: 16.319, Final Batch Loss: 0.549\n",
      "Epoch 16770, Loss: 16.153, Final Batch Loss: 0.478\n",
      "Epoch 16771, Loss: 16.039, Final Batch Loss: 0.388\n",
      "Epoch 16772, Loss: 16.101, Final Batch Loss: 0.399\n",
      "Epoch 16773, Loss: 15.941, Final Batch Loss: 0.376\n",
      "Epoch 16774, Loss: 16.043, Final Batch Loss: 0.584\n",
      "Epoch 16775, Loss: 15.987, Final Batch Loss: 0.454\n",
      "Epoch 16776, Loss: 16.207, Final Batch Loss: 0.527\n",
      "Epoch 16777, Loss: 15.960, Final Batch Loss: 0.397\n",
      "Epoch 16778, Loss: 16.152, Final Batch Loss: 0.436\n",
      "Epoch 16779, Loss: 15.801, Final Batch Loss: 0.476\n",
      "Epoch 16780, Loss: 16.349, Final Batch Loss: 0.621\n",
      "Epoch 16781, Loss: 16.045, Final Batch Loss: 0.390\n",
      "Epoch 16782, Loss: 16.224, Final Batch Loss: 0.373\n",
      "Epoch 16783, Loss: 15.906, Final Batch Loss: 0.421\n",
      "Epoch 16784, Loss: 16.013, Final Batch Loss: 0.439\n",
      "Epoch 16785, Loss: 16.019, Final Batch Loss: 0.430\n",
      "Epoch 16786, Loss: 16.277, Final Batch Loss: 0.436\n",
      "Epoch 16787, Loss: 16.043, Final Batch Loss: 0.451\n",
      "Epoch 16788, Loss: 16.129, Final Batch Loss: 0.417\n",
      "Epoch 16789, Loss: 16.051, Final Batch Loss: 0.454\n",
      "Epoch 16790, Loss: 15.963, Final Batch Loss: 0.371\n",
      "Epoch 16791, Loss: 16.360, Final Batch Loss: 0.450\n",
      "Epoch 16792, Loss: 16.200, Final Batch Loss: 0.429\n",
      "Epoch 16793, Loss: 16.180, Final Batch Loss: 0.489\n",
      "Epoch 16794, Loss: 16.074, Final Batch Loss: 0.551\n",
      "Epoch 16795, Loss: 16.106, Final Batch Loss: 0.422\n",
      "Epoch 16796, Loss: 16.085, Final Batch Loss: 0.493\n",
      "Epoch 16797, Loss: 16.396, Final Batch Loss: 0.453\n",
      "Epoch 16798, Loss: 15.973, Final Batch Loss: 0.363\n",
      "Epoch 16799, Loss: 16.088, Final Batch Loss: 0.439\n",
      "Epoch 16800, Loss: 16.090, Final Batch Loss: 0.363\n",
      "Epoch 16801, Loss: 16.021, Final Batch Loss: 0.457\n",
      "Epoch 16802, Loss: 16.108, Final Batch Loss: 0.404\n",
      "Epoch 16803, Loss: 16.128, Final Batch Loss: 0.439\n",
      "Epoch 16804, Loss: 16.200, Final Batch Loss: 0.439\n",
      "Epoch 16805, Loss: 16.010, Final Batch Loss: 0.409\n",
      "Epoch 16806, Loss: 15.848, Final Batch Loss: 0.473\n",
      "Epoch 16807, Loss: 16.037, Final Batch Loss: 0.411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16808, Loss: 16.064, Final Batch Loss: 0.547\n",
      "Epoch 16809, Loss: 15.970, Final Batch Loss: 0.463\n",
      "Epoch 16810, Loss: 16.180, Final Batch Loss: 0.396\n",
      "Epoch 16811, Loss: 16.035, Final Batch Loss: 0.463\n",
      "Epoch 16812, Loss: 16.321, Final Batch Loss: 0.517\n",
      "Epoch 16813, Loss: 16.127, Final Batch Loss: 0.548\n",
      "Epoch 16814, Loss: 16.360, Final Batch Loss: 0.384\n",
      "Epoch 16815, Loss: 15.887, Final Batch Loss: 0.395\n",
      "Epoch 16816, Loss: 15.917, Final Batch Loss: 0.446\n",
      "Epoch 16817, Loss: 16.030, Final Batch Loss: 0.440\n",
      "Epoch 16818, Loss: 16.268, Final Batch Loss: 0.531\n",
      "Epoch 16819, Loss: 16.116, Final Batch Loss: 0.383\n",
      "Epoch 16820, Loss: 16.032, Final Batch Loss: 0.384\n",
      "Epoch 16821, Loss: 16.035, Final Batch Loss: 0.522\n",
      "Epoch 16822, Loss: 16.200, Final Batch Loss: 0.325\n",
      "Epoch 16823, Loss: 16.106, Final Batch Loss: 0.467\n",
      "Epoch 16824, Loss: 16.300, Final Batch Loss: 0.539\n",
      "Epoch 16825, Loss: 15.970, Final Batch Loss: 0.495\n",
      "Epoch 16826, Loss: 16.210, Final Batch Loss: 0.414\n",
      "Epoch 16827, Loss: 16.294, Final Batch Loss: 0.647\n",
      "Epoch 16828, Loss: 16.077, Final Batch Loss: 0.557\n",
      "Epoch 16829, Loss: 15.933, Final Batch Loss: 0.390\n",
      "Epoch 16830, Loss: 15.980, Final Batch Loss: 0.377\n",
      "Epoch 16831, Loss: 15.950, Final Batch Loss: 0.406\n",
      "Epoch 16832, Loss: 15.956, Final Batch Loss: 0.362\n",
      "Epoch 16833, Loss: 16.225, Final Batch Loss: 0.407\n",
      "Epoch 16834, Loss: 16.270, Final Batch Loss: 0.473\n",
      "Epoch 16835, Loss: 16.114, Final Batch Loss: 0.483\n",
      "Epoch 16836, Loss: 16.014, Final Batch Loss: 0.386\n",
      "Epoch 16837, Loss: 16.309, Final Batch Loss: 0.527\n",
      "Epoch 16838, Loss: 16.177, Final Batch Loss: 0.421\n",
      "Epoch 16839, Loss: 15.800, Final Batch Loss: 0.532\n",
      "Epoch 16840, Loss: 16.254, Final Batch Loss: 0.450\n",
      "Epoch 16841, Loss: 16.119, Final Batch Loss: 0.436\n",
      "Epoch 16842, Loss: 16.224, Final Batch Loss: 0.420\n",
      "Epoch 16843, Loss: 16.011, Final Batch Loss: 0.409\n",
      "Epoch 16844, Loss: 16.021, Final Batch Loss: 0.466\n",
      "Epoch 16845, Loss: 15.968, Final Batch Loss: 0.420\n",
      "Epoch 16846, Loss: 15.921, Final Batch Loss: 0.510\n",
      "Epoch 16847, Loss: 16.063, Final Batch Loss: 0.471\n",
      "Epoch 16848, Loss: 16.178, Final Batch Loss: 0.440\n",
      "Epoch 16849, Loss: 16.361, Final Batch Loss: 0.445\n",
      "Epoch 16850, Loss: 16.010, Final Batch Loss: 0.445\n",
      "Epoch 16851, Loss: 16.273, Final Batch Loss: 0.449\n",
      "Epoch 16852, Loss: 16.148, Final Batch Loss: 0.440\n",
      "Epoch 16853, Loss: 16.024, Final Batch Loss: 0.505\n",
      "Epoch 16854, Loss: 15.897, Final Batch Loss: 0.486\n",
      "Epoch 16855, Loss: 16.144, Final Batch Loss: 0.409\n",
      "Epoch 16856, Loss: 16.002, Final Batch Loss: 0.410\n",
      "Epoch 16857, Loss: 15.928, Final Batch Loss: 0.393\n",
      "Epoch 16858, Loss: 16.057, Final Batch Loss: 0.372\n",
      "Epoch 16859, Loss: 15.912, Final Batch Loss: 0.437\n",
      "Epoch 16860, Loss: 16.120, Final Batch Loss: 0.567\n",
      "Epoch 16861, Loss: 16.074, Final Batch Loss: 0.595\n",
      "Epoch 16862, Loss: 16.119, Final Batch Loss: 0.455\n",
      "Epoch 16863, Loss: 16.192, Final Batch Loss: 0.378\n",
      "Epoch 16864, Loss: 16.083, Final Batch Loss: 0.451\n",
      "Epoch 16865, Loss: 16.285, Final Batch Loss: 0.473\n",
      "Epoch 16866, Loss: 16.030, Final Batch Loss: 0.432\n",
      "Epoch 16867, Loss: 16.064, Final Batch Loss: 0.400\n",
      "Epoch 16868, Loss: 16.119, Final Batch Loss: 0.311\n",
      "Epoch 16869, Loss: 16.137, Final Batch Loss: 0.527\n",
      "Epoch 16870, Loss: 16.032, Final Batch Loss: 0.439\n",
      "Epoch 16871, Loss: 15.940, Final Batch Loss: 0.433\n",
      "Epoch 16872, Loss: 15.836, Final Batch Loss: 0.587\n",
      "Epoch 16873, Loss: 15.892, Final Batch Loss: 0.406\n",
      "Epoch 16874, Loss: 16.160, Final Batch Loss: 0.428\n",
      "Epoch 16875, Loss: 16.183, Final Batch Loss: 0.502\n",
      "Epoch 16876, Loss: 16.336, Final Batch Loss: 0.444\n",
      "Epoch 16877, Loss: 15.794, Final Batch Loss: 0.371\n",
      "Epoch 16878, Loss: 16.050, Final Batch Loss: 0.418\n",
      "Epoch 16879, Loss: 16.320, Final Batch Loss: 0.455\n",
      "Epoch 16880, Loss: 16.032, Final Batch Loss: 0.372\n",
      "Epoch 16881, Loss: 16.196, Final Batch Loss: 0.418\n",
      "Epoch 16882, Loss: 16.158, Final Batch Loss: 0.496\n",
      "Epoch 16883, Loss: 16.016, Final Batch Loss: 0.465\n",
      "Epoch 16884, Loss: 16.058, Final Batch Loss: 0.499\n",
      "Epoch 16885, Loss: 16.216, Final Batch Loss: 0.484\n",
      "Epoch 16886, Loss: 15.973, Final Batch Loss: 0.376\n",
      "Epoch 16887, Loss: 15.966, Final Batch Loss: 0.456\n",
      "Epoch 16888, Loss: 16.103, Final Batch Loss: 0.425\n",
      "Epoch 16889, Loss: 15.788, Final Batch Loss: 0.328\n",
      "Epoch 16890, Loss: 16.067, Final Batch Loss: 0.477\n",
      "Epoch 16891, Loss: 16.136, Final Batch Loss: 0.420\n",
      "Epoch 16892, Loss: 16.068, Final Batch Loss: 0.403\n",
      "Epoch 16893, Loss: 15.813, Final Batch Loss: 0.397\n",
      "Epoch 16894, Loss: 16.099, Final Batch Loss: 0.407\n",
      "Epoch 16895, Loss: 16.147, Final Batch Loss: 0.480\n",
      "Epoch 16896, Loss: 16.303, Final Batch Loss: 0.557\n",
      "Epoch 16897, Loss: 15.942, Final Batch Loss: 0.442\n",
      "Epoch 16898, Loss: 16.176, Final Batch Loss: 0.364\n",
      "Epoch 16899, Loss: 15.956, Final Batch Loss: 0.463\n",
      "Epoch 16900, Loss: 16.122, Final Batch Loss: 0.427\n",
      "Epoch 16901, Loss: 15.918, Final Batch Loss: 0.492\n",
      "Epoch 16902, Loss: 16.161, Final Batch Loss: 0.454\n",
      "Epoch 16903, Loss: 15.940, Final Batch Loss: 0.522\n",
      "Epoch 16904, Loss: 15.938, Final Batch Loss: 0.336\n",
      "Epoch 16905, Loss: 16.003, Final Batch Loss: 0.434\n",
      "Epoch 16906, Loss: 15.924, Final Batch Loss: 0.493\n",
      "Epoch 16907, Loss: 16.196, Final Batch Loss: 0.531\n",
      "Epoch 16908, Loss: 16.051, Final Batch Loss: 0.400\n",
      "Epoch 16909, Loss: 16.007, Final Batch Loss: 0.347\n",
      "Epoch 16910, Loss: 16.040, Final Batch Loss: 0.449\n",
      "Epoch 16911, Loss: 16.256, Final Batch Loss: 0.565\n",
      "Epoch 16912, Loss: 16.315, Final Batch Loss: 0.499\n",
      "Epoch 16913, Loss: 16.019, Final Batch Loss: 0.472\n",
      "Epoch 16914, Loss: 16.036, Final Batch Loss: 0.386\n",
      "Epoch 16915, Loss: 15.995, Final Batch Loss: 0.513\n",
      "Epoch 16916, Loss: 16.253, Final Batch Loss: 0.499\n",
      "Epoch 16917, Loss: 15.987, Final Batch Loss: 0.350\n",
      "Epoch 16918, Loss: 16.033, Final Batch Loss: 0.343\n",
      "Epoch 16919, Loss: 16.002, Final Batch Loss: 0.429\n",
      "Epoch 16920, Loss: 15.798, Final Batch Loss: 0.430\n",
      "Epoch 16921, Loss: 16.024, Final Batch Loss: 0.469\n",
      "Epoch 16922, Loss: 16.061, Final Batch Loss: 0.477\n",
      "Epoch 16923, Loss: 16.470, Final Batch Loss: 0.480\n",
      "Epoch 16924, Loss: 16.242, Final Batch Loss: 0.398\n",
      "Epoch 16925, Loss: 16.024, Final Batch Loss: 0.447\n",
      "Epoch 16926, Loss: 16.275, Final Batch Loss: 0.502\n",
      "Epoch 16927, Loss: 16.383, Final Batch Loss: 0.374\n",
      "Epoch 16928, Loss: 15.967, Final Batch Loss: 0.440\n",
      "Epoch 16929, Loss: 15.786, Final Batch Loss: 0.412\n",
      "Epoch 16930, Loss: 15.915, Final Batch Loss: 0.430\n",
      "Epoch 16931, Loss: 15.994, Final Batch Loss: 0.438\n",
      "Epoch 16932, Loss: 16.179, Final Batch Loss: 0.445\n",
      "Epoch 16933, Loss: 16.220, Final Batch Loss: 0.516\n",
      "Epoch 16934, Loss: 16.018, Final Batch Loss: 0.465\n",
      "Epoch 16935, Loss: 16.093, Final Batch Loss: 0.485\n",
      "Epoch 16936, Loss: 16.307, Final Batch Loss: 0.388\n",
      "Epoch 16937, Loss: 15.794, Final Batch Loss: 0.366\n",
      "Epoch 16938, Loss: 16.043, Final Batch Loss: 0.584\n",
      "Epoch 16939, Loss: 16.177, Final Batch Loss: 0.601\n",
      "Epoch 16940, Loss: 16.169, Final Batch Loss: 0.523\n",
      "Epoch 16941, Loss: 16.200, Final Batch Loss: 0.508\n",
      "Epoch 16942, Loss: 16.137, Final Batch Loss: 0.433\n",
      "Epoch 16943, Loss: 15.952, Final Batch Loss: 0.457\n",
      "Epoch 16944, Loss: 16.209, Final Batch Loss: 0.424\n",
      "Epoch 16945, Loss: 15.900, Final Batch Loss: 0.486\n",
      "Epoch 16946, Loss: 15.854, Final Batch Loss: 0.437\n",
      "Epoch 16947, Loss: 16.107, Final Batch Loss: 0.459\n",
      "Epoch 16948, Loss: 16.135, Final Batch Loss: 0.470\n",
      "Epoch 16949, Loss: 16.115, Final Batch Loss: 0.417\n",
      "Epoch 16950, Loss: 16.068, Final Batch Loss: 0.436\n",
      "Epoch 16951, Loss: 16.371, Final Batch Loss: 0.503\n",
      "Epoch 16952, Loss: 16.273, Final Batch Loss: 0.393\n",
      "Epoch 16953, Loss: 15.772, Final Batch Loss: 0.433\n",
      "Epoch 16954, Loss: 16.152, Final Batch Loss: 0.406\n",
      "Epoch 16955, Loss: 16.161, Final Batch Loss: 0.421\n",
      "Epoch 16956, Loss: 16.271, Final Batch Loss: 0.469\n",
      "Epoch 16957, Loss: 16.291, Final Batch Loss: 0.410\n",
      "Epoch 16958, Loss: 16.126, Final Batch Loss: 0.325\n",
      "Epoch 16959, Loss: 16.018, Final Batch Loss: 0.320\n",
      "Epoch 16960, Loss: 16.068, Final Batch Loss: 0.471\n",
      "Epoch 16961, Loss: 16.434, Final Batch Loss: 0.539\n",
      "Epoch 16962, Loss: 16.166, Final Batch Loss: 0.520\n",
      "Epoch 16963, Loss: 16.130, Final Batch Loss: 0.400\n",
      "Epoch 16964, Loss: 16.054, Final Batch Loss: 0.386\n",
      "Epoch 16965, Loss: 15.999, Final Batch Loss: 0.458\n",
      "Epoch 16966, Loss: 16.104, Final Batch Loss: 0.533\n",
      "Epoch 16967, Loss: 15.917, Final Batch Loss: 0.424\n",
      "Epoch 16968, Loss: 16.143, Final Batch Loss: 0.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16969, Loss: 16.134, Final Batch Loss: 0.371\n",
      "Epoch 16970, Loss: 16.075, Final Batch Loss: 0.344\n",
      "Epoch 16971, Loss: 16.177, Final Batch Loss: 0.562\n",
      "Epoch 16972, Loss: 16.282, Final Batch Loss: 0.454\n",
      "Epoch 16973, Loss: 16.304, Final Batch Loss: 0.495\n",
      "Epoch 16974, Loss: 16.134, Final Batch Loss: 0.409\n",
      "Epoch 16975, Loss: 16.058, Final Batch Loss: 0.410\n",
      "Epoch 16976, Loss: 16.114, Final Batch Loss: 0.450\n",
      "Epoch 16977, Loss: 16.160, Final Batch Loss: 0.492\n",
      "Epoch 16978, Loss: 16.210, Final Batch Loss: 0.520\n",
      "Epoch 16979, Loss: 16.114, Final Batch Loss: 0.387\n",
      "Epoch 16980, Loss: 16.366, Final Batch Loss: 0.511\n",
      "Epoch 16981, Loss: 15.919, Final Batch Loss: 0.458\n",
      "Epoch 16982, Loss: 16.353, Final Batch Loss: 0.322\n",
      "Epoch 16983, Loss: 16.058, Final Batch Loss: 0.434\n",
      "Epoch 16984, Loss: 16.083, Final Batch Loss: 0.471\n",
      "Epoch 16985, Loss: 15.987, Final Batch Loss: 0.427\n",
      "Epoch 16986, Loss: 16.189, Final Batch Loss: 0.499\n",
      "Epoch 16987, Loss: 15.986, Final Batch Loss: 0.504\n",
      "Epoch 16988, Loss: 15.859, Final Batch Loss: 0.375\n",
      "Epoch 16989, Loss: 16.365, Final Batch Loss: 0.508\n",
      "Epoch 16990, Loss: 16.061, Final Batch Loss: 0.439\n",
      "Epoch 16991, Loss: 16.163, Final Batch Loss: 0.404\n",
      "Epoch 16992, Loss: 15.901, Final Batch Loss: 0.469\n",
      "Epoch 16993, Loss: 16.050, Final Batch Loss: 0.532\n",
      "Epoch 16994, Loss: 16.137, Final Batch Loss: 0.448\n",
      "Epoch 16995, Loss: 16.078, Final Batch Loss: 0.511\n",
      "Epoch 16996, Loss: 16.435, Final Batch Loss: 0.484\n",
      "Epoch 16997, Loss: 15.959, Final Batch Loss: 0.456\n",
      "Epoch 16998, Loss: 16.090, Final Batch Loss: 0.418\n",
      "Epoch 16999, Loss: 16.346, Final Batch Loss: 0.422\n",
      "Epoch 17000, Loss: 16.157, Final Batch Loss: 0.441\n",
      "Epoch 17001, Loss: 16.247, Final Batch Loss: 0.477\n",
      "Epoch 17002, Loss: 16.054, Final Batch Loss: 0.502\n",
      "Epoch 17003, Loss: 15.881, Final Batch Loss: 0.504\n",
      "Epoch 17004, Loss: 16.212, Final Batch Loss: 0.450\n",
      "Epoch 17005, Loss: 16.027, Final Batch Loss: 0.409\n",
      "Epoch 17006, Loss: 16.219, Final Batch Loss: 0.547\n",
      "Epoch 17007, Loss: 16.001, Final Batch Loss: 0.331\n",
      "Epoch 17008, Loss: 16.164, Final Batch Loss: 0.475\n",
      "Epoch 17009, Loss: 16.053, Final Batch Loss: 0.464\n",
      "Epoch 17010, Loss: 16.136, Final Batch Loss: 0.482\n",
      "Epoch 17011, Loss: 15.945, Final Batch Loss: 0.354\n",
      "Epoch 17012, Loss: 15.934, Final Batch Loss: 0.372\n",
      "Epoch 17013, Loss: 16.329, Final Batch Loss: 0.482\n",
      "Epoch 17014, Loss: 15.972, Final Batch Loss: 0.496\n",
      "Epoch 17015, Loss: 16.045, Final Batch Loss: 0.382\n",
      "Epoch 17016, Loss: 16.185, Final Batch Loss: 0.668\n",
      "Epoch 17017, Loss: 16.058, Final Batch Loss: 0.503\n",
      "Epoch 17018, Loss: 15.920, Final Batch Loss: 0.592\n",
      "Epoch 17019, Loss: 16.151, Final Batch Loss: 0.432\n",
      "Epoch 17020, Loss: 15.792, Final Batch Loss: 0.553\n",
      "Epoch 17021, Loss: 16.328, Final Batch Loss: 0.556\n",
      "Epoch 17022, Loss: 16.307, Final Batch Loss: 0.470\n",
      "Epoch 17023, Loss: 16.211, Final Batch Loss: 0.531\n",
      "Epoch 17024, Loss: 16.257, Final Batch Loss: 0.452\n",
      "Epoch 17025, Loss: 15.869, Final Batch Loss: 0.495\n",
      "Epoch 17026, Loss: 16.012, Final Batch Loss: 0.464\n",
      "Epoch 17027, Loss: 16.026, Final Batch Loss: 0.370\n",
      "Epoch 17028, Loss: 16.153, Final Batch Loss: 0.497\n",
      "Epoch 17029, Loss: 16.077, Final Batch Loss: 0.457\n",
      "Epoch 17030, Loss: 16.019, Final Batch Loss: 0.420\n",
      "Epoch 17031, Loss: 16.032, Final Batch Loss: 0.379\n",
      "Epoch 17032, Loss: 16.180, Final Batch Loss: 0.401\n",
      "Epoch 17033, Loss: 16.199, Final Batch Loss: 0.517\n",
      "Epoch 17034, Loss: 15.912, Final Batch Loss: 0.392\n",
      "Epoch 17035, Loss: 16.456, Final Batch Loss: 0.555\n",
      "Epoch 17036, Loss: 16.186, Final Batch Loss: 0.575\n",
      "Epoch 17037, Loss: 16.129, Final Batch Loss: 0.446\n",
      "Epoch 17038, Loss: 16.022, Final Batch Loss: 0.452\n",
      "Epoch 17039, Loss: 16.028, Final Batch Loss: 0.426\n",
      "Epoch 17040, Loss: 16.050, Final Batch Loss: 0.437\n",
      "Epoch 17041, Loss: 16.283, Final Batch Loss: 0.534\n",
      "Epoch 17042, Loss: 16.007, Final Batch Loss: 0.374\n",
      "Epoch 17043, Loss: 16.146, Final Batch Loss: 0.448\n",
      "Epoch 17044, Loss: 16.133, Final Batch Loss: 0.413\n",
      "Epoch 17045, Loss: 16.137, Final Batch Loss: 0.470\n",
      "Epoch 17046, Loss: 16.138, Final Batch Loss: 0.447\n",
      "Epoch 17047, Loss: 16.245, Final Batch Loss: 0.327\n",
      "Epoch 17048, Loss: 16.122, Final Batch Loss: 0.471\n",
      "Epoch 17049, Loss: 15.948, Final Batch Loss: 0.558\n",
      "Epoch 17050, Loss: 16.014, Final Batch Loss: 0.450\n",
      "Epoch 17051, Loss: 16.020, Final Batch Loss: 0.380\n",
      "Epoch 17052, Loss: 16.246, Final Batch Loss: 0.527\n",
      "Epoch 17053, Loss: 15.835, Final Batch Loss: 0.359\n",
      "Epoch 17054, Loss: 15.979, Final Batch Loss: 0.506\n",
      "Epoch 17055, Loss: 15.838, Final Batch Loss: 0.402\n",
      "Epoch 17056, Loss: 15.920, Final Batch Loss: 0.484\n",
      "Epoch 17057, Loss: 16.334, Final Batch Loss: 0.472\n",
      "Epoch 17058, Loss: 16.178, Final Batch Loss: 0.444\n",
      "Epoch 17059, Loss: 16.298, Final Batch Loss: 0.360\n",
      "Epoch 17060, Loss: 16.175, Final Batch Loss: 0.459\n",
      "Epoch 17061, Loss: 16.271, Final Batch Loss: 0.442\n",
      "Epoch 17062, Loss: 15.860, Final Batch Loss: 0.466\n",
      "Epoch 17063, Loss: 16.234, Final Batch Loss: 0.440\n",
      "Epoch 17064, Loss: 16.088, Final Batch Loss: 0.448\n",
      "Epoch 17065, Loss: 15.974, Final Batch Loss: 0.481\n",
      "Epoch 17066, Loss: 16.015, Final Batch Loss: 0.451\n",
      "Epoch 17067, Loss: 15.953, Final Batch Loss: 0.315\n",
      "Epoch 17068, Loss: 16.072, Final Batch Loss: 0.360\n",
      "Epoch 17069, Loss: 16.228, Final Batch Loss: 0.527\n",
      "Epoch 17070, Loss: 15.992, Final Batch Loss: 0.441\n",
      "Epoch 17071, Loss: 16.085, Final Batch Loss: 0.368\n",
      "Epoch 17072, Loss: 15.968, Final Batch Loss: 0.437\n",
      "Epoch 17073, Loss: 16.050, Final Batch Loss: 0.437\n",
      "Epoch 17074, Loss: 16.205, Final Batch Loss: 0.390\n",
      "Epoch 17075, Loss: 16.028, Final Batch Loss: 0.448\n",
      "Epoch 17076, Loss: 16.168, Final Batch Loss: 0.572\n",
      "Epoch 17077, Loss: 16.101, Final Batch Loss: 0.432\n",
      "Epoch 17078, Loss: 16.051, Final Batch Loss: 0.445\n",
      "Epoch 17079, Loss: 16.008, Final Batch Loss: 0.455\n",
      "Epoch 17080, Loss: 16.098, Final Batch Loss: 0.404\n",
      "Epoch 17081, Loss: 16.147, Final Batch Loss: 0.451\n",
      "Epoch 17082, Loss: 16.154, Final Batch Loss: 0.518\n",
      "Epoch 17083, Loss: 15.995, Final Batch Loss: 0.427\n",
      "Epoch 17084, Loss: 16.193, Final Batch Loss: 0.397\n",
      "Epoch 17085, Loss: 15.954, Final Batch Loss: 0.436\n",
      "Epoch 17086, Loss: 16.144, Final Batch Loss: 0.490\n",
      "Epoch 17087, Loss: 16.093, Final Batch Loss: 0.461\n",
      "Epoch 17088, Loss: 16.102, Final Batch Loss: 0.393\n",
      "Epoch 17089, Loss: 16.171, Final Batch Loss: 0.441\n",
      "Epoch 17090, Loss: 16.267, Final Batch Loss: 0.446\n",
      "Epoch 17091, Loss: 16.132, Final Batch Loss: 0.414\n",
      "Epoch 17092, Loss: 16.145, Final Batch Loss: 0.471\n",
      "Epoch 17093, Loss: 16.115, Final Batch Loss: 0.430\n",
      "Epoch 17094, Loss: 16.130, Final Batch Loss: 0.446\n",
      "Epoch 17095, Loss: 16.052, Final Batch Loss: 0.478\n",
      "Epoch 17096, Loss: 16.147, Final Batch Loss: 0.561\n",
      "Epoch 17097, Loss: 16.280, Final Batch Loss: 0.541\n",
      "Epoch 17098, Loss: 15.908, Final Batch Loss: 0.363\n",
      "Epoch 17099, Loss: 15.981, Final Batch Loss: 0.385\n",
      "Epoch 17100, Loss: 16.291, Final Batch Loss: 0.614\n",
      "Epoch 17101, Loss: 15.892, Final Batch Loss: 0.384\n",
      "Epoch 17102, Loss: 16.113, Final Batch Loss: 0.445\n",
      "Epoch 17103, Loss: 15.902, Final Batch Loss: 0.370\n",
      "Epoch 17104, Loss: 15.999, Final Batch Loss: 0.415\n",
      "Epoch 17105, Loss: 16.119, Final Batch Loss: 0.349\n",
      "Epoch 17106, Loss: 16.017, Final Batch Loss: 0.429\n",
      "Epoch 17107, Loss: 16.185, Final Batch Loss: 0.409\n",
      "Epoch 17108, Loss: 15.846, Final Batch Loss: 0.406\n",
      "Epoch 17109, Loss: 16.280, Final Batch Loss: 0.452\n",
      "Epoch 17110, Loss: 16.045, Final Batch Loss: 0.498\n",
      "Epoch 17111, Loss: 16.167, Final Batch Loss: 0.295\n",
      "Epoch 17112, Loss: 16.399, Final Batch Loss: 0.598\n",
      "Epoch 17113, Loss: 16.033, Final Batch Loss: 0.337\n",
      "Epoch 17114, Loss: 16.339, Final Batch Loss: 0.388\n",
      "Epoch 17115, Loss: 16.053, Final Batch Loss: 0.399\n",
      "Epoch 17116, Loss: 16.081, Final Batch Loss: 0.496\n",
      "Epoch 17117, Loss: 16.138, Final Batch Loss: 0.603\n",
      "Epoch 17118, Loss: 16.116, Final Batch Loss: 0.457\n",
      "Epoch 17119, Loss: 15.976, Final Batch Loss: 0.399\n",
      "Epoch 17120, Loss: 16.090, Final Batch Loss: 0.365\n",
      "Epoch 17121, Loss: 16.297, Final Batch Loss: 0.564\n",
      "Epoch 17122, Loss: 15.958, Final Batch Loss: 0.458\n",
      "Epoch 17123, Loss: 15.986, Final Batch Loss: 0.455\n",
      "Epoch 17124, Loss: 16.254, Final Batch Loss: 0.482\n",
      "Epoch 17125, Loss: 16.161, Final Batch Loss: 0.605\n",
      "Epoch 17126, Loss: 16.454, Final Batch Loss: 0.428\n",
      "Epoch 17127, Loss: 16.323, Final Batch Loss: 0.443\n",
      "Epoch 17128, Loss: 15.767, Final Batch Loss: 0.408\n",
      "Epoch 17129, Loss: 16.012, Final Batch Loss: 0.393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17130, Loss: 15.945, Final Batch Loss: 0.333\n",
      "Epoch 17131, Loss: 15.923, Final Batch Loss: 0.500\n",
      "Epoch 17132, Loss: 16.075, Final Batch Loss: 0.423\n",
      "Epoch 17133, Loss: 16.310, Final Batch Loss: 0.423\n",
      "Epoch 17134, Loss: 16.179, Final Batch Loss: 0.439\n",
      "Epoch 17135, Loss: 15.906, Final Batch Loss: 0.527\n",
      "Epoch 17136, Loss: 15.993, Final Batch Loss: 0.429\n",
      "Epoch 17137, Loss: 16.109, Final Batch Loss: 0.429\n",
      "Epoch 17138, Loss: 15.972, Final Batch Loss: 0.401\n",
      "Epoch 17139, Loss: 15.992, Final Batch Loss: 0.468\n",
      "Epoch 17140, Loss: 16.105, Final Batch Loss: 0.491\n",
      "Epoch 17141, Loss: 15.858, Final Batch Loss: 0.446\n",
      "Epoch 17142, Loss: 16.110, Final Batch Loss: 0.405\n",
      "Epoch 17143, Loss: 16.113, Final Batch Loss: 0.419\n",
      "Epoch 17144, Loss: 15.931, Final Batch Loss: 0.409\n",
      "Epoch 17145, Loss: 16.105, Final Batch Loss: 0.431\n",
      "Epoch 17146, Loss: 15.999, Final Batch Loss: 0.396\n",
      "Epoch 17147, Loss: 15.961, Final Batch Loss: 0.384\n",
      "Epoch 17148, Loss: 15.911, Final Batch Loss: 0.395\n",
      "Epoch 17149, Loss: 15.563, Final Batch Loss: 0.410\n",
      "Epoch 17150, Loss: 16.295, Final Batch Loss: 0.424\n",
      "Epoch 17151, Loss: 16.254, Final Batch Loss: 0.383\n",
      "Epoch 17152, Loss: 15.952, Final Batch Loss: 0.462\n",
      "Epoch 17153, Loss: 15.918, Final Batch Loss: 0.476\n",
      "Epoch 17154, Loss: 15.876, Final Batch Loss: 0.347\n",
      "Epoch 17155, Loss: 16.192, Final Batch Loss: 0.498\n",
      "Epoch 17156, Loss: 15.936, Final Batch Loss: 0.425\n",
      "Epoch 17157, Loss: 16.155, Final Batch Loss: 0.473\n",
      "Epoch 17158, Loss: 16.186, Final Batch Loss: 0.437\n",
      "Epoch 17159, Loss: 16.099, Final Batch Loss: 0.428\n",
      "Epoch 17160, Loss: 15.804, Final Batch Loss: 0.405\n",
      "Epoch 17161, Loss: 16.221, Final Batch Loss: 0.420\n",
      "Epoch 17162, Loss: 16.173, Final Batch Loss: 0.499\n",
      "Epoch 17163, Loss: 16.183, Final Batch Loss: 0.590\n",
      "Epoch 17164, Loss: 16.057, Final Batch Loss: 0.421\n",
      "Epoch 17165, Loss: 16.043, Final Batch Loss: 0.491\n",
      "Epoch 17166, Loss: 15.973, Final Batch Loss: 0.393\n",
      "Epoch 17167, Loss: 16.110, Final Batch Loss: 0.443\n",
      "Epoch 17168, Loss: 16.105, Final Batch Loss: 0.511\n",
      "Epoch 17169, Loss: 16.247, Final Batch Loss: 0.379\n",
      "Epoch 17170, Loss: 16.201, Final Batch Loss: 0.417\n",
      "Epoch 17171, Loss: 16.132, Final Batch Loss: 0.400\n",
      "Epoch 17172, Loss: 16.229, Final Batch Loss: 0.509\n",
      "Epoch 17173, Loss: 15.921, Final Batch Loss: 0.469\n",
      "Epoch 17174, Loss: 16.214, Final Batch Loss: 0.415\n",
      "Epoch 17175, Loss: 15.822, Final Batch Loss: 0.363\n",
      "Epoch 17176, Loss: 16.059, Final Batch Loss: 0.595\n",
      "Epoch 17177, Loss: 15.800, Final Batch Loss: 0.461\n",
      "Epoch 17178, Loss: 15.753, Final Batch Loss: 0.389\n",
      "Epoch 17179, Loss: 15.909, Final Batch Loss: 0.408\n",
      "Epoch 17180, Loss: 16.080, Final Batch Loss: 0.509\n",
      "Epoch 17181, Loss: 16.144, Final Batch Loss: 0.501\n",
      "Epoch 17182, Loss: 16.155, Final Batch Loss: 0.473\n",
      "Epoch 17183, Loss: 15.978, Final Batch Loss: 0.478\n",
      "Epoch 17184, Loss: 16.017, Final Batch Loss: 0.473\n",
      "Epoch 17185, Loss: 16.415, Final Batch Loss: 0.507\n",
      "Epoch 17186, Loss: 16.024, Final Batch Loss: 0.422\n",
      "Epoch 17187, Loss: 16.040, Final Batch Loss: 0.343\n",
      "Epoch 17188, Loss: 16.115, Final Batch Loss: 0.451\n",
      "Epoch 17189, Loss: 16.074, Final Batch Loss: 0.436\n",
      "Epoch 17190, Loss: 15.993, Final Batch Loss: 0.376\n",
      "Epoch 17191, Loss: 15.957, Final Batch Loss: 0.497\n",
      "Epoch 17192, Loss: 15.831, Final Batch Loss: 0.496\n",
      "Epoch 17193, Loss: 16.081, Final Batch Loss: 0.405\n",
      "Epoch 17194, Loss: 16.029, Final Batch Loss: 0.488\n",
      "Epoch 17195, Loss: 16.174, Final Batch Loss: 0.432\n",
      "Epoch 17196, Loss: 16.040, Final Batch Loss: 0.378\n",
      "Epoch 17197, Loss: 16.207, Final Batch Loss: 0.550\n",
      "Epoch 17198, Loss: 16.026, Final Batch Loss: 0.436\n",
      "Epoch 17199, Loss: 15.704, Final Batch Loss: 0.487\n",
      "Epoch 17200, Loss: 15.925, Final Batch Loss: 0.484\n",
      "Epoch 17201, Loss: 15.904, Final Batch Loss: 0.475\n",
      "Epoch 17202, Loss: 16.068, Final Batch Loss: 0.422\n",
      "Epoch 17203, Loss: 16.039, Final Batch Loss: 0.430\n",
      "Epoch 17204, Loss: 16.074, Final Batch Loss: 0.484\n",
      "Epoch 17205, Loss: 16.117, Final Batch Loss: 0.486\n",
      "Epoch 17206, Loss: 15.816, Final Batch Loss: 0.397\n",
      "Epoch 17207, Loss: 16.173, Final Batch Loss: 0.547\n",
      "Epoch 17208, Loss: 16.183, Final Batch Loss: 0.457\n",
      "Epoch 17209, Loss: 16.304, Final Batch Loss: 0.538\n",
      "Epoch 17210, Loss: 16.123, Final Batch Loss: 0.478\n",
      "Epoch 17211, Loss: 15.980, Final Batch Loss: 0.394\n",
      "Epoch 17212, Loss: 16.136, Final Batch Loss: 0.378\n",
      "Epoch 17213, Loss: 16.326, Final Batch Loss: 0.425\n",
      "Epoch 17214, Loss: 16.075, Final Batch Loss: 0.565\n",
      "Epoch 17215, Loss: 15.975, Final Batch Loss: 0.475\n",
      "Epoch 17216, Loss: 15.983, Final Batch Loss: 0.434\n",
      "Epoch 17217, Loss: 16.087, Final Batch Loss: 0.504\n",
      "Epoch 17218, Loss: 15.921, Final Batch Loss: 0.378\n",
      "Epoch 17219, Loss: 16.164, Final Batch Loss: 0.448\n",
      "Epoch 17220, Loss: 16.070, Final Batch Loss: 0.375\n",
      "Epoch 17221, Loss: 15.978, Final Batch Loss: 0.464\n",
      "Epoch 17222, Loss: 16.156, Final Batch Loss: 0.482\n",
      "Epoch 17223, Loss: 16.161, Final Batch Loss: 0.483\n",
      "Epoch 17224, Loss: 15.932, Final Batch Loss: 0.428\n",
      "Epoch 17225, Loss: 15.921, Final Batch Loss: 0.389\n",
      "Epoch 17226, Loss: 15.965, Final Batch Loss: 0.594\n",
      "Epoch 17227, Loss: 16.052, Final Batch Loss: 0.389\n",
      "Epoch 17228, Loss: 16.216, Final Batch Loss: 0.366\n",
      "Epoch 17229, Loss: 15.716, Final Batch Loss: 0.408\n",
      "Epoch 17230, Loss: 15.949, Final Batch Loss: 0.510\n",
      "Epoch 17231, Loss: 16.003, Final Batch Loss: 0.385\n",
      "Epoch 17232, Loss: 16.262, Final Batch Loss: 0.445\n",
      "Epoch 17233, Loss: 15.923, Final Batch Loss: 0.482\n",
      "Epoch 17234, Loss: 15.908, Final Batch Loss: 0.460\n",
      "Epoch 17235, Loss: 16.015, Final Batch Loss: 0.439\n",
      "Epoch 17236, Loss: 16.081, Final Batch Loss: 0.526\n",
      "Epoch 17237, Loss: 16.093, Final Batch Loss: 0.405\n",
      "Epoch 17238, Loss: 16.240, Final Batch Loss: 0.540\n",
      "Epoch 17239, Loss: 16.041, Final Batch Loss: 0.467\n",
      "Epoch 17240, Loss: 16.179, Final Batch Loss: 0.436\n",
      "Epoch 17241, Loss: 16.449, Final Batch Loss: 0.461\n",
      "Epoch 17242, Loss: 16.168, Final Batch Loss: 0.411\n",
      "Epoch 17243, Loss: 16.019, Final Batch Loss: 0.432\n",
      "Epoch 17244, Loss: 16.133, Final Batch Loss: 0.528\n",
      "Epoch 17245, Loss: 16.122, Final Batch Loss: 0.352\n",
      "Epoch 17246, Loss: 16.342, Final Batch Loss: 0.467\n",
      "Epoch 17247, Loss: 16.109, Final Batch Loss: 0.431\n",
      "Epoch 17248, Loss: 16.208, Final Batch Loss: 0.536\n",
      "Epoch 17249, Loss: 16.063, Final Batch Loss: 0.576\n",
      "Epoch 17250, Loss: 16.445, Final Batch Loss: 0.417\n",
      "Epoch 17251, Loss: 16.183, Final Batch Loss: 0.391\n",
      "Epoch 17252, Loss: 15.945, Final Batch Loss: 0.420\n",
      "Epoch 17253, Loss: 16.060, Final Batch Loss: 0.541\n",
      "Epoch 17254, Loss: 16.088, Final Batch Loss: 0.408\n",
      "Epoch 17255, Loss: 16.368, Final Batch Loss: 0.548\n",
      "Epoch 17256, Loss: 16.025, Final Batch Loss: 0.372\n",
      "Epoch 17257, Loss: 16.095, Final Batch Loss: 0.549\n",
      "Epoch 17258, Loss: 16.045, Final Batch Loss: 0.430\n",
      "Epoch 17259, Loss: 15.876, Final Batch Loss: 0.415\n",
      "Epoch 17260, Loss: 16.038, Final Batch Loss: 0.502\n",
      "Epoch 17261, Loss: 16.230, Final Batch Loss: 0.429\n",
      "Epoch 17262, Loss: 15.882, Final Batch Loss: 0.449\n",
      "Epoch 17263, Loss: 16.045, Final Batch Loss: 0.461\n",
      "Epoch 17264, Loss: 16.283, Final Batch Loss: 0.479\n",
      "Epoch 17265, Loss: 15.954, Final Batch Loss: 0.417\n",
      "Epoch 17266, Loss: 15.890, Final Batch Loss: 0.443\n",
      "Epoch 17267, Loss: 16.128, Final Batch Loss: 0.443\n",
      "Epoch 17268, Loss: 16.004, Final Batch Loss: 0.485\n",
      "Epoch 17269, Loss: 16.138, Final Batch Loss: 0.437\n",
      "Epoch 17270, Loss: 16.219, Final Batch Loss: 0.398\n",
      "Epoch 17271, Loss: 16.032, Final Batch Loss: 0.395\n",
      "Epoch 17272, Loss: 16.212, Final Batch Loss: 0.420\n",
      "Epoch 17273, Loss: 15.956, Final Batch Loss: 0.511\n",
      "Epoch 17274, Loss: 15.881, Final Batch Loss: 0.509\n",
      "Epoch 17275, Loss: 16.003, Final Batch Loss: 0.364\n",
      "Epoch 17276, Loss: 15.941, Final Batch Loss: 0.480\n",
      "Epoch 17277, Loss: 16.194, Final Batch Loss: 0.366\n",
      "Epoch 17278, Loss: 16.089, Final Batch Loss: 0.478\n",
      "Epoch 17279, Loss: 16.166, Final Batch Loss: 0.507\n",
      "Epoch 17280, Loss: 16.001, Final Batch Loss: 0.454\n",
      "Epoch 17281, Loss: 16.194, Final Batch Loss: 0.454\n",
      "Epoch 17282, Loss: 16.117, Final Batch Loss: 0.411\n",
      "Epoch 17283, Loss: 15.794, Final Batch Loss: 0.398\n",
      "Epoch 17284, Loss: 16.357, Final Batch Loss: 0.458\n",
      "Epoch 17285, Loss: 16.098, Final Batch Loss: 0.466\n",
      "Epoch 17286, Loss: 15.987, Final Batch Loss: 0.475\n",
      "Epoch 17287, Loss: 16.093, Final Batch Loss: 0.428\n",
      "Epoch 17288, Loss: 16.184, Final Batch Loss: 0.481\n",
      "Epoch 17289, Loss: 16.024, Final Batch Loss: 0.582\n",
      "Epoch 17290, Loss: 16.106, Final Batch Loss: 0.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17291, Loss: 16.330, Final Batch Loss: 0.329\n",
      "Epoch 17292, Loss: 16.002, Final Batch Loss: 0.524\n",
      "Epoch 17293, Loss: 16.143, Final Batch Loss: 0.440\n",
      "Epoch 17294, Loss: 15.991, Final Batch Loss: 0.500\n",
      "Epoch 17295, Loss: 16.340, Final Batch Loss: 0.412\n",
      "Epoch 17296, Loss: 16.404, Final Batch Loss: 0.364\n",
      "Epoch 17297, Loss: 16.155, Final Batch Loss: 0.453\n",
      "Epoch 17298, Loss: 15.904, Final Batch Loss: 0.389\n",
      "Epoch 17299, Loss: 15.841, Final Batch Loss: 0.403\n",
      "Epoch 17300, Loss: 15.921, Final Batch Loss: 0.425\n",
      "Epoch 17301, Loss: 16.174, Final Batch Loss: 0.377\n",
      "Epoch 17302, Loss: 16.194, Final Batch Loss: 0.455\n",
      "Epoch 17303, Loss: 16.130, Final Batch Loss: 0.417\n",
      "Epoch 17304, Loss: 16.221, Final Batch Loss: 0.428\n",
      "Epoch 17305, Loss: 16.084, Final Batch Loss: 0.413\n",
      "Epoch 17306, Loss: 16.039, Final Batch Loss: 0.516\n",
      "Epoch 17307, Loss: 16.252, Final Batch Loss: 0.379\n",
      "Epoch 17308, Loss: 16.156, Final Batch Loss: 0.452\n",
      "Epoch 17309, Loss: 16.107, Final Batch Loss: 0.515\n",
      "Epoch 17310, Loss: 15.877, Final Batch Loss: 0.454\n",
      "Epoch 17311, Loss: 16.138, Final Batch Loss: 0.367\n",
      "Epoch 17312, Loss: 15.900, Final Batch Loss: 0.467\n",
      "Epoch 17313, Loss: 16.034, Final Batch Loss: 0.414\n",
      "Epoch 17314, Loss: 15.992, Final Batch Loss: 0.418\n",
      "Epoch 17315, Loss: 16.076, Final Batch Loss: 0.360\n",
      "Epoch 17316, Loss: 16.143, Final Batch Loss: 0.466\n",
      "Epoch 17317, Loss: 16.072, Final Batch Loss: 0.512\n",
      "Epoch 17318, Loss: 16.201, Final Batch Loss: 0.459\n",
      "Epoch 17319, Loss: 16.079, Final Batch Loss: 0.424\n",
      "Epoch 17320, Loss: 16.221, Final Batch Loss: 0.397\n",
      "Epoch 17321, Loss: 16.049, Final Batch Loss: 0.480\n",
      "Epoch 17322, Loss: 16.010, Final Batch Loss: 0.398\n",
      "Epoch 17323, Loss: 15.982, Final Batch Loss: 0.424\n",
      "Epoch 17324, Loss: 16.091, Final Batch Loss: 0.432\n",
      "Epoch 17325, Loss: 16.312, Final Batch Loss: 0.524\n",
      "Epoch 17326, Loss: 15.975, Final Batch Loss: 0.371\n",
      "Epoch 17327, Loss: 16.203, Final Batch Loss: 0.395\n",
      "Epoch 17328, Loss: 16.097, Final Batch Loss: 0.467\n",
      "Epoch 17329, Loss: 16.229, Final Batch Loss: 0.468\n",
      "Epoch 17330, Loss: 16.131, Final Batch Loss: 0.521\n",
      "Epoch 17331, Loss: 16.341, Final Batch Loss: 0.499\n",
      "Epoch 17332, Loss: 15.939, Final Batch Loss: 0.398\n",
      "Epoch 17333, Loss: 15.764, Final Batch Loss: 0.468\n",
      "Epoch 17334, Loss: 16.097, Final Batch Loss: 0.462\n",
      "Epoch 17335, Loss: 15.925, Final Batch Loss: 0.374\n",
      "Epoch 17336, Loss: 15.930, Final Batch Loss: 0.548\n",
      "Epoch 17337, Loss: 16.081, Final Batch Loss: 0.510\n",
      "Epoch 17338, Loss: 16.031, Final Batch Loss: 0.473\n",
      "Epoch 17339, Loss: 16.054, Final Batch Loss: 0.382\n",
      "Epoch 17340, Loss: 15.974, Final Batch Loss: 0.467\n",
      "Epoch 17341, Loss: 16.004, Final Batch Loss: 0.346\n",
      "Epoch 17342, Loss: 16.018, Final Batch Loss: 0.427\n",
      "Epoch 17343, Loss: 16.129, Final Batch Loss: 0.381\n",
      "Epoch 17344, Loss: 16.334, Final Batch Loss: 0.496\n",
      "Epoch 17345, Loss: 16.047, Final Batch Loss: 0.390\n",
      "Epoch 17346, Loss: 16.088, Final Batch Loss: 0.555\n",
      "Epoch 17347, Loss: 16.070, Final Batch Loss: 0.490\n",
      "Epoch 17348, Loss: 16.131, Final Batch Loss: 0.508\n",
      "Epoch 17349, Loss: 15.935, Final Batch Loss: 0.494\n",
      "Epoch 17350, Loss: 16.069, Final Batch Loss: 0.541\n",
      "Epoch 17351, Loss: 16.014, Final Batch Loss: 0.458\n",
      "Epoch 17352, Loss: 16.110, Final Batch Loss: 0.460\n",
      "Epoch 17353, Loss: 16.045, Final Batch Loss: 0.441\n",
      "Epoch 17354, Loss: 15.975, Final Batch Loss: 0.326\n",
      "Epoch 17355, Loss: 16.323, Final Batch Loss: 0.417\n",
      "Epoch 17356, Loss: 16.332, Final Batch Loss: 0.491\n",
      "Epoch 17357, Loss: 15.678, Final Batch Loss: 0.477\n",
      "Epoch 17358, Loss: 15.998, Final Batch Loss: 0.439\n",
      "Epoch 17359, Loss: 15.910, Final Batch Loss: 0.417\n",
      "Epoch 17360, Loss: 15.965, Final Batch Loss: 0.483\n",
      "Epoch 17361, Loss: 16.217, Final Batch Loss: 0.470\n",
      "Epoch 17362, Loss: 16.017, Final Batch Loss: 0.459\n",
      "Epoch 17363, Loss: 16.041, Final Batch Loss: 0.458\n",
      "Epoch 17364, Loss: 15.952, Final Batch Loss: 0.440\n",
      "Epoch 17365, Loss: 16.095, Final Batch Loss: 0.380\n",
      "Epoch 17366, Loss: 15.983, Final Batch Loss: 0.347\n",
      "Epoch 17367, Loss: 16.168, Final Batch Loss: 0.427\n",
      "Epoch 17368, Loss: 16.589, Final Batch Loss: 0.439\n",
      "Epoch 17369, Loss: 16.065, Final Batch Loss: 0.412\n",
      "Epoch 17370, Loss: 16.129, Final Batch Loss: 0.443\n",
      "Epoch 17371, Loss: 16.228, Final Batch Loss: 0.392\n",
      "Epoch 17372, Loss: 16.189, Final Batch Loss: 0.418\n",
      "Epoch 17373, Loss: 16.045, Final Batch Loss: 0.347\n",
      "Epoch 17374, Loss: 16.115, Final Batch Loss: 0.473\n",
      "Epoch 17375, Loss: 16.239, Final Batch Loss: 0.356\n",
      "Epoch 17376, Loss: 16.206, Final Batch Loss: 0.464\n",
      "Epoch 17377, Loss: 15.947, Final Batch Loss: 0.439\n",
      "Epoch 17378, Loss: 16.091, Final Batch Loss: 0.358\n",
      "Epoch 17379, Loss: 16.015, Final Batch Loss: 0.419\n",
      "Epoch 17380, Loss: 15.988, Final Batch Loss: 0.505\n",
      "Epoch 17381, Loss: 16.199, Final Batch Loss: 0.482\n",
      "Epoch 17382, Loss: 16.121, Final Batch Loss: 0.414\n",
      "Epoch 17383, Loss: 16.028, Final Batch Loss: 0.451\n",
      "Epoch 17384, Loss: 15.980, Final Batch Loss: 0.445\n",
      "Epoch 17385, Loss: 16.118, Final Batch Loss: 0.466\n",
      "Epoch 17386, Loss: 16.080, Final Batch Loss: 0.459\n",
      "Epoch 17387, Loss: 16.220, Final Batch Loss: 0.524\n",
      "Epoch 17388, Loss: 16.290, Final Batch Loss: 0.395\n",
      "Epoch 17389, Loss: 15.873, Final Batch Loss: 0.417\n",
      "Epoch 17390, Loss: 16.086, Final Batch Loss: 0.387\n",
      "Epoch 17391, Loss: 15.998, Final Batch Loss: 0.486\n",
      "Epoch 17392, Loss: 16.170, Final Batch Loss: 0.528\n",
      "Epoch 17393, Loss: 16.348, Final Batch Loss: 0.534\n",
      "Epoch 17394, Loss: 15.920, Final Batch Loss: 0.379\n",
      "Epoch 17395, Loss: 16.172, Final Batch Loss: 0.499\n",
      "Epoch 17396, Loss: 16.162, Final Batch Loss: 0.492\n",
      "Epoch 17397, Loss: 16.043, Final Batch Loss: 0.441\n",
      "Epoch 17398, Loss: 16.120, Final Batch Loss: 0.365\n",
      "Epoch 17399, Loss: 15.944, Final Batch Loss: 0.448\n",
      "Epoch 17400, Loss: 16.073, Final Batch Loss: 0.460\n",
      "Epoch 17401, Loss: 16.130, Final Batch Loss: 0.540\n",
      "Epoch 17402, Loss: 15.898, Final Batch Loss: 0.347\n",
      "Epoch 17403, Loss: 16.113, Final Batch Loss: 0.497\n",
      "Epoch 17404, Loss: 16.435, Final Batch Loss: 0.443\n",
      "Epoch 17405, Loss: 16.205, Final Batch Loss: 0.363\n",
      "Epoch 17406, Loss: 16.458, Final Batch Loss: 0.487\n",
      "Epoch 17407, Loss: 16.082, Final Batch Loss: 0.471\n",
      "Epoch 17408, Loss: 15.827, Final Batch Loss: 0.389\n",
      "Epoch 17409, Loss: 16.138, Final Batch Loss: 0.537\n",
      "Epoch 17410, Loss: 15.925, Final Batch Loss: 0.441\n",
      "Epoch 17411, Loss: 15.921, Final Batch Loss: 0.473\n",
      "Epoch 17412, Loss: 16.344, Final Batch Loss: 0.516\n",
      "Epoch 17413, Loss: 16.111, Final Batch Loss: 0.480\n",
      "Epoch 17414, Loss: 16.206, Final Batch Loss: 0.508\n",
      "Epoch 17415, Loss: 16.291, Final Batch Loss: 0.600\n",
      "Epoch 17416, Loss: 16.274, Final Batch Loss: 0.353\n",
      "Epoch 17417, Loss: 16.021, Final Batch Loss: 0.391\n",
      "Epoch 17418, Loss: 16.204, Final Batch Loss: 0.530\n",
      "Epoch 17419, Loss: 15.877, Final Batch Loss: 0.348\n",
      "Epoch 17420, Loss: 16.100, Final Batch Loss: 0.420\n",
      "Epoch 17421, Loss: 16.128, Final Batch Loss: 0.549\n",
      "Epoch 17422, Loss: 15.957, Final Batch Loss: 0.431\n",
      "Epoch 17423, Loss: 15.736, Final Batch Loss: 0.429\n",
      "Epoch 17424, Loss: 16.087, Final Batch Loss: 0.603\n",
      "Epoch 17425, Loss: 16.013, Final Batch Loss: 0.372\n",
      "Epoch 17426, Loss: 16.185, Final Batch Loss: 0.501\n",
      "Epoch 17427, Loss: 16.043, Final Batch Loss: 0.448\n",
      "Epoch 17428, Loss: 16.042, Final Batch Loss: 0.400\n",
      "Epoch 17429, Loss: 16.025, Final Batch Loss: 0.427\n",
      "Epoch 17430, Loss: 16.104, Final Batch Loss: 0.546\n",
      "Epoch 17431, Loss: 16.130, Final Batch Loss: 0.553\n",
      "Epoch 17432, Loss: 16.257, Final Batch Loss: 0.541\n",
      "Epoch 17433, Loss: 16.019, Final Batch Loss: 0.435\n",
      "Epoch 17434, Loss: 16.056, Final Batch Loss: 0.385\n",
      "Epoch 17435, Loss: 16.215, Final Batch Loss: 0.344\n",
      "Epoch 17436, Loss: 16.329, Final Batch Loss: 0.431\n",
      "Epoch 17437, Loss: 16.172, Final Batch Loss: 0.533\n",
      "Epoch 17438, Loss: 16.377, Final Batch Loss: 0.384\n",
      "Epoch 17439, Loss: 16.190, Final Batch Loss: 0.468\n",
      "Epoch 17440, Loss: 15.898, Final Batch Loss: 0.545\n",
      "Epoch 17441, Loss: 15.916, Final Batch Loss: 0.501\n",
      "Epoch 17442, Loss: 16.572, Final Batch Loss: 0.495\n",
      "Epoch 17443, Loss: 16.182, Final Batch Loss: 0.432\n",
      "Epoch 17444, Loss: 16.159, Final Batch Loss: 0.370\n",
      "Epoch 17445, Loss: 15.889, Final Batch Loss: 0.411\n",
      "Epoch 17446, Loss: 16.132, Final Batch Loss: 0.507\n",
      "Epoch 17447, Loss: 16.189, Final Batch Loss: 0.463\n",
      "Epoch 17448, Loss: 16.167, Final Batch Loss: 0.476\n",
      "Epoch 17449, Loss: 15.823, Final Batch Loss: 0.428\n",
      "Epoch 17450, Loss: 15.698, Final Batch Loss: 0.431\n",
      "Epoch 17451, Loss: 16.069, Final Batch Loss: 0.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17452, Loss: 15.945, Final Batch Loss: 0.474\n",
      "Epoch 17453, Loss: 16.094, Final Batch Loss: 0.427\n",
      "Epoch 17454, Loss: 16.059, Final Batch Loss: 0.497\n",
      "Epoch 17455, Loss: 16.370, Final Batch Loss: 0.391\n",
      "Epoch 17456, Loss: 16.016, Final Batch Loss: 0.402\n",
      "Epoch 17457, Loss: 16.161, Final Batch Loss: 0.466\n",
      "Epoch 17458, Loss: 15.902, Final Batch Loss: 0.415\n",
      "Epoch 17459, Loss: 16.072, Final Batch Loss: 0.468\n",
      "Epoch 17460, Loss: 16.119, Final Batch Loss: 0.439\n",
      "Epoch 17461, Loss: 15.931, Final Batch Loss: 0.469\n",
      "Epoch 17462, Loss: 16.159, Final Batch Loss: 0.416\n",
      "Epoch 17463, Loss: 15.687, Final Batch Loss: 0.417\n",
      "Epoch 17464, Loss: 16.181, Final Batch Loss: 0.328\n",
      "Epoch 17465, Loss: 15.926, Final Batch Loss: 0.458\n",
      "Epoch 17466, Loss: 16.328, Final Batch Loss: 0.421\n",
      "Epoch 17467, Loss: 16.152, Final Batch Loss: 0.497\n",
      "Epoch 17468, Loss: 15.888, Final Batch Loss: 0.369\n",
      "Epoch 17469, Loss: 16.234, Final Batch Loss: 0.528\n",
      "Epoch 17470, Loss: 15.983, Final Batch Loss: 0.479\n",
      "Epoch 17471, Loss: 16.051, Final Batch Loss: 0.544\n",
      "Epoch 17472, Loss: 16.253, Final Batch Loss: 0.402\n",
      "Epoch 17473, Loss: 15.920, Final Batch Loss: 0.367\n",
      "Epoch 17474, Loss: 16.014, Final Batch Loss: 0.421\n",
      "Epoch 17475, Loss: 16.225, Final Batch Loss: 0.450\n",
      "Epoch 17476, Loss: 16.043, Final Batch Loss: 0.496\n",
      "Epoch 17477, Loss: 16.157, Final Batch Loss: 0.523\n",
      "Epoch 17478, Loss: 15.991, Final Batch Loss: 0.445\n",
      "Epoch 17479, Loss: 16.079, Final Batch Loss: 0.449\n",
      "Epoch 17480, Loss: 15.955, Final Batch Loss: 0.469\n",
      "Epoch 17481, Loss: 15.815, Final Batch Loss: 0.445\n",
      "Epoch 17482, Loss: 16.008, Final Batch Loss: 0.520\n",
      "Epoch 17483, Loss: 15.969, Final Batch Loss: 0.447\n",
      "Epoch 17484, Loss: 15.908, Final Batch Loss: 0.418\n",
      "Epoch 17485, Loss: 15.947, Final Batch Loss: 0.484\n",
      "Epoch 17486, Loss: 15.977, Final Batch Loss: 0.490\n",
      "Epoch 17487, Loss: 16.115, Final Batch Loss: 0.506\n",
      "Epoch 17488, Loss: 16.081, Final Batch Loss: 0.458\n",
      "Epoch 17489, Loss: 15.923, Final Batch Loss: 0.364\n",
      "Epoch 17490, Loss: 16.086, Final Batch Loss: 0.456\n",
      "Epoch 17491, Loss: 16.107, Final Batch Loss: 0.482\n",
      "Epoch 17492, Loss: 16.421, Final Batch Loss: 0.525\n",
      "Epoch 17493, Loss: 15.819, Final Batch Loss: 0.396\n",
      "Epoch 17494, Loss: 16.054, Final Batch Loss: 0.442\n",
      "Epoch 17495, Loss: 15.922, Final Batch Loss: 0.465\n",
      "Epoch 17496, Loss: 16.003, Final Batch Loss: 0.474\n",
      "Epoch 17497, Loss: 16.257, Final Batch Loss: 0.464\n",
      "Epoch 17498, Loss: 16.068, Final Batch Loss: 0.465\n",
      "Epoch 17499, Loss: 16.008, Final Batch Loss: 0.395\n",
      "Epoch 17500, Loss: 16.155, Final Batch Loss: 0.518\n",
      "Epoch 17501, Loss: 16.127, Final Batch Loss: 0.523\n",
      "Epoch 17502, Loss: 15.945, Final Batch Loss: 0.404\n",
      "Epoch 17503, Loss: 15.921, Final Batch Loss: 0.426\n",
      "Epoch 17504, Loss: 16.032, Final Batch Loss: 0.386\n",
      "Epoch 17505, Loss: 15.986, Final Batch Loss: 0.409\n",
      "Epoch 17506, Loss: 15.884, Final Batch Loss: 0.368\n",
      "Epoch 17507, Loss: 16.365, Final Batch Loss: 0.471\n",
      "Epoch 17508, Loss: 15.897, Final Batch Loss: 0.398\n",
      "Epoch 17509, Loss: 15.991, Final Batch Loss: 0.411\n",
      "Epoch 17510, Loss: 15.942, Final Batch Loss: 0.359\n",
      "Epoch 17511, Loss: 15.885, Final Batch Loss: 0.457\n",
      "Epoch 17512, Loss: 16.076, Final Batch Loss: 0.450\n",
      "Epoch 17513, Loss: 15.815, Final Batch Loss: 0.433\n",
      "Epoch 17514, Loss: 16.309, Final Batch Loss: 0.480\n",
      "Epoch 17515, Loss: 15.695, Final Batch Loss: 0.554\n",
      "Epoch 17516, Loss: 16.066, Final Batch Loss: 0.483\n",
      "Epoch 17517, Loss: 15.977, Final Batch Loss: 0.432\n",
      "Epoch 17518, Loss: 16.249, Final Batch Loss: 0.436\n",
      "Epoch 17519, Loss: 16.301, Final Batch Loss: 0.381\n",
      "Epoch 17520, Loss: 15.861, Final Batch Loss: 0.441\n",
      "Epoch 17521, Loss: 16.083, Final Batch Loss: 0.497\n",
      "Epoch 17522, Loss: 16.116, Final Batch Loss: 0.391\n",
      "Epoch 17523, Loss: 16.069, Final Batch Loss: 0.394\n",
      "Epoch 17524, Loss: 15.989, Final Batch Loss: 0.465\n",
      "Epoch 17525, Loss: 16.333, Final Batch Loss: 0.449\n",
      "Epoch 17526, Loss: 16.068, Final Batch Loss: 0.411\n",
      "Epoch 17527, Loss: 16.037, Final Batch Loss: 0.406\n",
      "Epoch 17528, Loss: 16.008, Final Batch Loss: 0.452\n",
      "Epoch 17529, Loss: 16.127, Final Batch Loss: 0.495\n",
      "Epoch 17530, Loss: 16.094, Final Batch Loss: 0.491\n",
      "Epoch 17531, Loss: 16.297, Final Batch Loss: 0.632\n",
      "Epoch 17532, Loss: 16.280, Final Batch Loss: 0.561\n",
      "Epoch 17533, Loss: 15.817, Final Batch Loss: 0.513\n",
      "Epoch 17534, Loss: 16.067, Final Batch Loss: 0.418\n",
      "Epoch 17535, Loss: 16.302, Final Batch Loss: 0.536\n",
      "Epoch 17536, Loss: 15.897, Final Batch Loss: 0.415\n",
      "Epoch 17537, Loss: 16.013, Final Batch Loss: 0.453\n",
      "Epoch 17538, Loss: 15.834, Final Batch Loss: 0.478\n",
      "Epoch 17539, Loss: 16.226, Final Batch Loss: 0.486\n",
      "Epoch 17540, Loss: 16.018, Final Batch Loss: 0.507\n",
      "Epoch 17541, Loss: 16.071, Final Batch Loss: 0.444\n",
      "Epoch 17542, Loss: 15.864, Final Batch Loss: 0.511\n",
      "Epoch 17543, Loss: 16.021, Final Batch Loss: 0.480\n",
      "Epoch 17544, Loss: 16.133, Final Batch Loss: 0.388\n",
      "Epoch 17545, Loss: 16.549, Final Batch Loss: 0.477\n",
      "Epoch 17546, Loss: 16.148, Final Batch Loss: 0.439\n",
      "Epoch 17547, Loss: 16.088, Final Batch Loss: 0.446\n",
      "Epoch 17548, Loss: 16.179, Final Batch Loss: 0.396\n",
      "Epoch 17549, Loss: 16.325, Final Batch Loss: 0.528\n",
      "Epoch 17550, Loss: 16.047, Final Batch Loss: 0.414\n",
      "Epoch 17551, Loss: 16.123, Final Batch Loss: 0.483\n",
      "Epoch 17552, Loss: 15.903, Final Batch Loss: 0.365\n",
      "Epoch 17553, Loss: 16.172, Final Batch Loss: 0.447\n",
      "Epoch 17554, Loss: 16.219, Final Batch Loss: 0.413\n",
      "Epoch 17555, Loss: 15.920, Final Batch Loss: 0.540\n",
      "Epoch 17556, Loss: 15.899, Final Batch Loss: 0.537\n",
      "Epoch 17557, Loss: 15.914, Final Batch Loss: 0.375\n",
      "Epoch 17558, Loss: 16.332, Final Batch Loss: 0.482\n",
      "Epoch 17559, Loss: 16.056, Final Batch Loss: 0.369\n",
      "Epoch 17560, Loss: 15.846, Final Batch Loss: 0.412\n",
      "Epoch 17561, Loss: 16.079, Final Batch Loss: 0.558\n",
      "Epoch 17562, Loss: 16.073, Final Batch Loss: 0.480\n",
      "Epoch 17563, Loss: 16.376, Final Batch Loss: 0.482\n",
      "Epoch 17564, Loss: 15.769, Final Batch Loss: 0.424\n",
      "Epoch 17565, Loss: 15.919, Final Batch Loss: 0.398\n",
      "Epoch 17566, Loss: 16.185, Final Batch Loss: 0.415\n",
      "Epoch 17567, Loss: 15.988, Final Batch Loss: 0.475\n",
      "Epoch 17568, Loss: 15.907, Final Batch Loss: 0.470\n",
      "Epoch 17569, Loss: 16.258, Final Batch Loss: 0.314\n",
      "Epoch 17570, Loss: 16.135, Final Batch Loss: 0.434\n",
      "Epoch 17571, Loss: 16.013, Final Batch Loss: 0.504\n",
      "Epoch 17572, Loss: 16.049, Final Batch Loss: 0.400\n",
      "Epoch 17573, Loss: 16.051, Final Batch Loss: 0.346\n",
      "Epoch 17574, Loss: 15.860, Final Batch Loss: 0.371\n",
      "Epoch 17575, Loss: 16.554, Final Batch Loss: 0.446\n",
      "Epoch 17576, Loss: 16.149, Final Batch Loss: 0.434\n",
      "Epoch 17577, Loss: 15.816, Final Batch Loss: 0.442\n",
      "Epoch 17578, Loss: 16.149, Final Batch Loss: 0.462\n",
      "Epoch 17579, Loss: 15.940, Final Batch Loss: 0.424\n",
      "Epoch 17580, Loss: 15.991, Final Batch Loss: 0.371\n",
      "Epoch 17581, Loss: 16.189, Final Batch Loss: 0.467\n",
      "Epoch 17582, Loss: 16.067, Final Batch Loss: 0.430\n",
      "Epoch 17583, Loss: 16.348, Final Batch Loss: 0.441\n",
      "Epoch 17584, Loss: 15.909, Final Batch Loss: 0.436\n",
      "Epoch 17585, Loss: 16.249, Final Batch Loss: 0.408\n",
      "Epoch 17586, Loss: 16.352, Final Batch Loss: 0.391\n",
      "Epoch 17587, Loss: 16.026, Final Batch Loss: 0.458\n",
      "Epoch 17588, Loss: 15.980, Final Batch Loss: 0.429\n",
      "Epoch 17589, Loss: 16.567, Final Batch Loss: 0.534\n",
      "Epoch 17590, Loss: 16.267, Final Batch Loss: 0.592\n",
      "Epoch 17591, Loss: 16.031, Final Batch Loss: 0.427\n",
      "Epoch 17592, Loss: 15.979, Final Batch Loss: 0.375\n",
      "Epoch 17593, Loss: 16.189, Final Batch Loss: 0.694\n",
      "Epoch 17594, Loss: 16.035, Final Batch Loss: 0.520\n",
      "Epoch 17595, Loss: 16.102, Final Batch Loss: 0.349\n",
      "Epoch 17596, Loss: 15.776, Final Batch Loss: 0.384\n",
      "Epoch 17597, Loss: 15.868, Final Batch Loss: 0.408\n",
      "Epoch 17598, Loss: 16.177, Final Batch Loss: 0.463\n",
      "Epoch 17599, Loss: 16.044, Final Batch Loss: 0.584\n",
      "Epoch 17600, Loss: 16.185, Final Batch Loss: 0.473\n",
      "Epoch 17601, Loss: 15.865, Final Batch Loss: 0.371\n",
      "Epoch 17602, Loss: 15.996, Final Batch Loss: 0.450\n",
      "Epoch 17603, Loss: 16.275, Final Batch Loss: 0.330\n",
      "Epoch 17604, Loss: 15.904, Final Batch Loss: 0.448\n",
      "Epoch 17605, Loss: 16.233, Final Batch Loss: 0.455\n",
      "Epoch 17606, Loss: 15.943, Final Batch Loss: 0.399\n",
      "Epoch 17607, Loss: 15.915, Final Batch Loss: 0.444\n",
      "Epoch 17608, Loss: 16.198, Final Batch Loss: 0.456\n",
      "Epoch 17609, Loss: 15.876, Final Batch Loss: 0.563\n",
      "Epoch 17610, Loss: 15.896, Final Batch Loss: 0.391\n",
      "Epoch 17611, Loss: 16.200, Final Batch Loss: 0.546\n",
      "Epoch 17612, Loss: 16.018, Final Batch Loss: 0.418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17613, Loss: 16.420, Final Batch Loss: 0.570\n",
      "Epoch 17614, Loss: 16.168, Final Batch Loss: 0.596\n",
      "Epoch 17615, Loss: 16.353, Final Batch Loss: 0.314\n",
      "Epoch 17616, Loss: 16.083, Final Batch Loss: 0.421\n",
      "Epoch 17617, Loss: 16.212, Final Batch Loss: 0.497\n",
      "Epoch 17618, Loss: 16.146, Final Batch Loss: 0.473\n",
      "Epoch 17619, Loss: 16.104, Final Batch Loss: 0.402\n",
      "Epoch 17620, Loss: 15.978, Final Batch Loss: 0.389\n",
      "Epoch 17621, Loss: 15.858, Final Batch Loss: 0.488\n",
      "Epoch 17622, Loss: 15.849, Final Batch Loss: 0.378\n",
      "Epoch 17623, Loss: 15.757, Final Batch Loss: 0.384\n",
      "Epoch 17624, Loss: 16.096, Final Batch Loss: 0.491\n",
      "Epoch 17625, Loss: 15.863, Final Batch Loss: 0.532\n",
      "Epoch 17626, Loss: 16.146, Final Batch Loss: 0.508\n",
      "Epoch 17627, Loss: 16.283, Final Batch Loss: 0.488\n",
      "Epoch 17628, Loss: 16.012, Final Batch Loss: 0.506\n",
      "Epoch 17629, Loss: 16.100, Final Batch Loss: 0.399\n",
      "Epoch 17630, Loss: 15.994, Final Batch Loss: 0.401\n",
      "Epoch 17631, Loss: 15.955, Final Batch Loss: 0.393\n",
      "Epoch 17632, Loss: 16.310, Final Batch Loss: 0.413\n",
      "Epoch 17633, Loss: 15.668, Final Batch Loss: 0.461\n",
      "Epoch 17634, Loss: 16.083, Final Batch Loss: 0.484\n",
      "Epoch 17635, Loss: 15.982, Final Batch Loss: 0.497\n",
      "Epoch 17636, Loss: 16.130, Final Batch Loss: 0.456\n",
      "Epoch 17637, Loss: 15.903, Final Batch Loss: 0.423\n",
      "Epoch 17638, Loss: 15.862, Final Batch Loss: 0.448\n",
      "Epoch 17639, Loss: 15.856, Final Batch Loss: 0.421\n",
      "Epoch 17640, Loss: 16.029, Final Batch Loss: 0.446\n",
      "Epoch 17641, Loss: 16.378, Final Batch Loss: 0.378\n",
      "Epoch 17642, Loss: 16.216, Final Batch Loss: 0.316\n",
      "Epoch 17643, Loss: 16.213, Final Batch Loss: 0.438\n",
      "Epoch 17644, Loss: 16.082, Final Batch Loss: 0.332\n",
      "Epoch 17645, Loss: 16.019, Final Batch Loss: 0.426\n",
      "Epoch 17646, Loss: 16.099, Final Batch Loss: 0.414\n",
      "Epoch 17647, Loss: 16.021, Final Batch Loss: 0.531\n",
      "Epoch 17648, Loss: 16.087, Final Batch Loss: 0.450\n",
      "Epoch 17649, Loss: 16.121, Final Batch Loss: 0.442\n",
      "Epoch 17650, Loss: 16.038, Final Batch Loss: 0.513\n",
      "Epoch 17651, Loss: 16.004, Final Batch Loss: 0.516\n",
      "Epoch 17652, Loss: 16.014, Final Batch Loss: 0.545\n",
      "Epoch 17653, Loss: 16.096, Final Batch Loss: 0.413\n",
      "Epoch 17654, Loss: 16.097, Final Batch Loss: 0.485\n",
      "Epoch 17655, Loss: 16.054, Final Batch Loss: 0.508\n",
      "Epoch 17656, Loss: 15.945, Final Batch Loss: 0.434\n",
      "Epoch 17657, Loss: 16.042, Final Batch Loss: 0.387\n",
      "Epoch 17658, Loss: 16.060, Final Batch Loss: 0.476\n",
      "Epoch 17659, Loss: 15.796, Final Batch Loss: 0.344\n",
      "Epoch 17660, Loss: 15.834, Final Batch Loss: 0.390\n",
      "Epoch 17661, Loss: 16.288, Final Batch Loss: 0.543\n",
      "Epoch 17662, Loss: 16.392, Final Batch Loss: 0.493\n",
      "Epoch 17663, Loss: 16.220, Final Batch Loss: 0.469\n",
      "Epoch 17664, Loss: 16.214, Final Batch Loss: 0.440\n",
      "Epoch 17665, Loss: 16.169, Final Batch Loss: 0.449\n",
      "Epoch 17666, Loss: 16.211, Final Batch Loss: 0.480\n",
      "Epoch 17667, Loss: 16.451, Final Batch Loss: 0.427\n",
      "Epoch 17668, Loss: 15.987, Final Batch Loss: 0.392\n",
      "Epoch 17669, Loss: 16.006, Final Batch Loss: 0.359\n",
      "Epoch 17670, Loss: 16.153, Final Batch Loss: 0.432\n",
      "Epoch 17671, Loss: 16.275, Final Batch Loss: 0.395\n",
      "Epoch 17672, Loss: 16.174, Final Batch Loss: 0.452\n",
      "Epoch 17673, Loss: 16.063, Final Batch Loss: 0.551\n",
      "Epoch 17674, Loss: 16.462, Final Batch Loss: 0.354\n",
      "Epoch 17675, Loss: 15.863, Final Batch Loss: 0.483\n",
      "Epoch 17676, Loss: 16.053, Final Batch Loss: 0.514\n",
      "Epoch 17677, Loss: 16.026, Final Batch Loss: 0.455\n",
      "Epoch 17678, Loss: 16.185, Final Batch Loss: 0.447\n",
      "Epoch 17679, Loss: 15.991, Final Batch Loss: 0.385\n",
      "Epoch 17680, Loss: 16.152, Final Batch Loss: 0.483\n",
      "Epoch 17681, Loss: 16.129, Final Batch Loss: 0.476\n",
      "Epoch 17682, Loss: 15.931, Final Batch Loss: 0.424\n",
      "Epoch 17683, Loss: 16.057, Final Batch Loss: 0.457\n",
      "Epoch 17684, Loss: 16.084, Final Batch Loss: 0.444\n",
      "Epoch 17685, Loss: 16.194, Final Batch Loss: 0.449\n",
      "Epoch 17686, Loss: 16.101, Final Batch Loss: 0.572\n",
      "Epoch 17687, Loss: 16.074, Final Batch Loss: 0.470\n",
      "Epoch 17688, Loss: 16.283, Final Batch Loss: 0.453\n",
      "Epoch 17689, Loss: 15.980, Final Batch Loss: 0.504\n",
      "Epoch 17690, Loss: 15.939, Final Batch Loss: 0.387\n",
      "Epoch 17691, Loss: 15.972, Final Batch Loss: 0.596\n",
      "Epoch 17692, Loss: 16.100, Final Batch Loss: 0.376\n",
      "Epoch 17693, Loss: 16.113, Final Batch Loss: 0.516\n",
      "Epoch 17694, Loss: 16.134, Final Batch Loss: 0.505\n",
      "Epoch 17695, Loss: 16.004, Final Batch Loss: 0.485\n",
      "Epoch 17696, Loss: 16.266, Final Batch Loss: 0.418\n",
      "Epoch 17697, Loss: 16.151, Final Batch Loss: 0.483\n",
      "Epoch 17698, Loss: 16.225, Final Batch Loss: 0.530\n",
      "Epoch 17699, Loss: 16.054, Final Batch Loss: 0.487\n",
      "Epoch 17700, Loss: 15.946, Final Batch Loss: 0.403\n",
      "Epoch 17701, Loss: 15.858, Final Batch Loss: 0.420\n",
      "Epoch 17702, Loss: 15.933, Final Batch Loss: 0.467\n",
      "Epoch 17703, Loss: 16.329, Final Batch Loss: 0.509\n",
      "Epoch 17704, Loss: 16.073, Final Batch Loss: 0.428\n",
      "Epoch 17705, Loss: 16.002, Final Batch Loss: 0.433\n",
      "Epoch 17706, Loss: 16.045, Final Batch Loss: 0.428\n",
      "Epoch 17707, Loss: 15.941, Final Batch Loss: 0.381\n",
      "Epoch 17708, Loss: 16.162, Final Batch Loss: 0.427\n",
      "Epoch 17709, Loss: 16.042, Final Batch Loss: 0.363\n",
      "Epoch 17710, Loss: 16.344, Final Batch Loss: 0.474\n",
      "Epoch 17711, Loss: 16.002, Final Batch Loss: 0.458\n",
      "Epoch 17712, Loss: 16.099, Final Batch Loss: 0.491\n",
      "Epoch 17713, Loss: 15.949, Final Batch Loss: 0.471\n",
      "Epoch 17714, Loss: 15.973, Final Batch Loss: 0.445\n",
      "Epoch 17715, Loss: 16.252, Final Batch Loss: 0.492\n",
      "Epoch 17716, Loss: 15.922, Final Batch Loss: 0.385\n",
      "Epoch 17717, Loss: 15.782, Final Batch Loss: 0.486\n",
      "Epoch 17718, Loss: 16.402, Final Batch Loss: 0.517\n",
      "Epoch 17719, Loss: 16.099, Final Batch Loss: 0.417\n",
      "Epoch 17720, Loss: 15.979, Final Batch Loss: 0.412\n",
      "Epoch 17721, Loss: 16.006, Final Batch Loss: 0.471\n",
      "Epoch 17722, Loss: 16.256, Final Batch Loss: 0.564\n",
      "Epoch 17723, Loss: 16.124, Final Batch Loss: 0.455\n",
      "Epoch 17724, Loss: 16.323, Final Batch Loss: 0.372\n",
      "Epoch 17725, Loss: 16.112, Final Batch Loss: 0.409\n",
      "Epoch 17726, Loss: 15.908, Final Batch Loss: 0.474\n",
      "Epoch 17727, Loss: 16.132, Final Batch Loss: 0.406\n",
      "Epoch 17728, Loss: 16.059, Final Batch Loss: 0.384\n",
      "Epoch 17729, Loss: 16.102, Final Batch Loss: 0.477\n",
      "Epoch 17730, Loss: 16.123, Final Batch Loss: 0.384\n",
      "Epoch 17731, Loss: 16.365, Final Batch Loss: 0.437\n",
      "Epoch 17732, Loss: 15.974, Final Batch Loss: 0.465\n",
      "Epoch 17733, Loss: 15.913, Final Batch Loss: 0.506\n",
      "Epoch 17734, Loss: 16.052, Final Batch Loss: 0.444\n",
      "Epoch 17735, Loss: 16.196, Final Batch Loss: 0.478\n",
      "Epoch 17736, Loss: 16.136, Final Batch Loss: 0.435\n",
      "Epoch 17737, Loss: 16.266, Final Batch Loss: 0.476\n",
      "Epoch 17738, Loss: 15.830, Final Batch Loss: 0.403\n",
      "Epoch 17739, Loss: 16.131, Final Batch Loss: 0.451\n",
      "Epoch 17740, Loss: 16.376, Final Batch Loss: 0.505\n",
      "Epoch 17741, Loss: 15.936, Final Batch Loss: 0.428\n",
      "Epoch 17742, Loss: 15.936, Final Batch Loss: 0.354\n",
      "Epoch 17743, Loss: 16.173, Final Batch Loss: 0.543\n",
      "Epoch 17744, Loss: 16.026, Final Batch Loss: 0.449\n",
      "Epoch 17745, Loss: 15.997, Final Batch Loss: 0.409\n",
      "Epoch 17746, Loss: 16.158, Final Batch Loss: 0.404\n",
      "Epoch 17747, Loss: 16.093, Final Batch Loss: 0.405\n",
      "Epoch 17748, Loss: 16.143, Final Batch Loss: 0.438\n",
      "Epoch 17749, Loss: 16.071, Final Batch Loss: 0.426\n",
      "Epoch 17750, Loss: 16.022, Final Batch Loss: 0.500\n",
      "Epoch 17751, Loss: 16.133, Final Batch Loss: 0.427\n",
      "Epoch 17752, Loss: 15.965, Final Batch Loss: 0.463\n",
      "Epoch 17753, Loss: 16.249, Final Batch Loss: 0.407\n",
      "Epoch 17754, Loss: 16.317, Final Batch Loss: 0.410\n",
      "Epoch 17755, Loss: 16.002, Final Batch Loss: 0.461\n",
      "Epoch 17756, Loss: 16.274, Final Batch Loss: 0.510\n",
      "Epoch 17757, Loss: 15.975, Final Batch Loss: 0.408\n",
      "Epoch 17758, Loss: 16.306, Final Batch Loss: 0.441\n",
      "Epoch 17759, Loss: 16.066, Final Batch Loss: 0.366\n",
      "Epoch 17760, Loss: 16.018, Final Batch Loss: 0.401\n",
      "Epoch 17761, Loss: 15.937, Final Batch Loss: 0.422\n",
      "Epoch 17762, Loss: 16.195, Final Batch Loss: 0.431\n",
      "Epoch 17763, Loss: 15.623, Final Batch Loss: 0.436\n",
      "Epoch 17764, Loss: 15.859, Final Batch Loss: 0.488\n",
      "Epoch 17765, Loss: 16.174, Final Batch Loss: 0.418\n",
      "Epoch 17766, Loss: 15.939, Final Batch Loss: 0.494\n",
      "Epoch 17767, Loss: 16.055, Final Batch Loss: 0.473\n",
      "Epoch 17768, Loss: 16.057, Final Batch Loss: 0.406\n",
      "Epoch 17769, Loss: 16.169, Final Batch Loss: 0.434\n",
      "Epoch 17770, Loss: 16.013, Final Batch Loss: 0.427\n",
      "Epoch 17771, Loss: 16.079, Final Batch Loss: 0.432\n",
      "Epoch 17772, Loss: 16.072, Final Batch Loss: 0.404\n",
      "Epoch 17773, Loss: 16.106, Final Batch Loss: 0.459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17774, Loss: 15.862, Final Batch Loss: 0.473\n",
      "Epoch 17775, Loss: 16.188, Final Batch Loss: 0.414\n",
      "Epoch 17776, Loss: 16.079, Final Batch Loss: 0.438\n",
      "Epoch 17777, Loss: 15.938, Final Batch Loss: 0.501\n",
      "Epoch 17778, Loss: 15.973, Final Batch Loss: 0.417\n",
      "Epoch 17779, Loss: 16.139, Final Batch Loss: 0.486\n",
      "Epoch 17780, Loss: 16.075, Final Batch Loss: 0.366\n",
      "Epoch 17781, Loss: 16.240, Final Batch Loss: 0.542\n",
      "Epoch 17782, Loss: 15.893, Final Batch Loss: 0.437\n",
      "Epoch 17783, Loss: 16.411, Final Batch Loss: 0.377\n",
      "Epoch 17784, Loss: 16.291, Final Batch Loss: 0.511\n",
      "Epoch 17785, Loss: 16.154, Final Batch Loss: 0.424\n",
      "Epoch 17786, Loss: 15.969, Final Batch Loss: 0.444\n",
      "Epoch 17787, Loss: 16.322, Final Batch Loss: 0.442\n",
      "Epoch 17788, Loss: 16.138, Final Batch Loss: 0.417\n",
      "Epoch 17789, Loss: 16.089, Final Batch Loss: 0.416\n",
      "Epoch 17790, Loss: 16.072, Final Batch Loss: 0.407\n",
      "Epoch 17791, Loss: 16.132, Final Batch Loss: 0.328\n",
      "Epoch 17792, Loss: 15.973, Final Batch Loss: 0.568\n",
      "Epoch 17793, Loss: 15.836, Final Batch Loss: 0.467\n",
      "Epoch 17794, Loss: 16.007, Final Batch Loss: 0.514\n",
      "Epoch 17795, Loss: 15.957, Final Batch Loss: 0.368\n",
      "Epoch 17796, Loss: 16.335, Final Batch Loss: 0.384\n",
      "Epoch 17797, Loss: 15.820, Final Batch Loss: 0.409\n",
      "Epoch 17798, Loss: 16.071, Final Batch Loss: 0.374\n",
      "Epoch 17799, Loss: 16.046, Final Batch Loss: 0.429\n",
      "Epoch 17800, Loss: 16.226, Final Batch Loss: 0.420\n",
      "Epoch 17801, Loss: 16.010, Final Batch Loss: 0.406\n",
      "Epoch 17802, Loss: 15.878, Final Batch Loss: 0.481\n",
      "Epoch 17803, Loss: 16.351, Final Batch Loss: 0.486\n",
      "Epoch 17804, Loss: 16.135, Final Batch Loss: 0.401\n",
      "Epoch 17805, Loss: 16.037, Final Batch Loss: 0.390\n",
      "Epoch 17806, Loss: 16.302, Final Batch Loss: 0.418\n",
      "Epoch 17807, Loss: 16.043, Final Batch Loss: 0.553\n",
      "Epoch 17808, Loss: 15.872, Final Batch Loss: 0.365\n",
      "Epoch 17809, Loss: 16.010, Final Batch Loss: 0.387\n",
      "Epoch 17810, Loss: 16.316, Final Batch Loss: 0.427\n",
      "Epoch 17811, Loss: 15.900, Final Batch Loss: 0.499\n",
      "Epoch 17812, Loss: 16.239, Final Batch Loss: 0.516\n",
      "Epoch 17813, Loss: 15.955, Final Batch Loss: 0.436\n",
      "Epoch 17814, Loss: 15.946, Final Batch Loss: 0.412\n",
      "Epoch 17815, Loss: 15.828, Final Batch Loss: 0.447\n",
      "Epoch 17816, Loss: 16.244, Final Batch Loss: 0.538\n",
      "Epoch 17817, Loss: 15.912, Final Batch Loss: 0.440\n",
      "Epoch 17818, Loss: 16.044, Final Batch Loss: 0.387\n",
      "Epoch 17819, Loss: 16.164, Final Batch Loss: 0.580\n",
      "Epoch 17820, Loss: 16.076, Final Batch Loss: 0.514\n",
      "Epoch 17821, Loss: 16.162, Final Batch Loss: 0.383\n",
      "Epoch 17822, Loss: 16.091, Final Batch Loss: 0.435\n",
      "Epoch 17823, Loss: 16.002, Final Batch Loss: 0.515\n",
      "Epoch 17824, Loss: 16.115, Final Batch Loss: 0.356\n",
      "Epoch 17825, Loss: 16.177, Final Batch Loss: 0.463\n",
      "Epoch 17826, Loss: 16.258, Final Batch Loss: 0.423\n",
      "Epoch 17827, Loss: 16.017, Final Batch Loss: 0.587\n",
      "Epoch 17828, Loss: 16.043, Final Batch Loss: 0.325\n",
      "Epoch 17829, Loss: 16.040, Final Batch Loss: 0.393\n",
      "Epoch 17830, Loss: 16.291, Final Batch Loss: 0.464\n",
      "Epoch 17831, Loss: 16.090, Final Batch Loss: 0.405\n",
      "Epoch 17832, Loss: 16.095, Final Batch Loss: 0.450\n",
      "Epoch 17833, Loss: 16.024, Final Batch Loss: 0.445\n",
      "Epoch 17834, Loss: 16.010, Final Batch Loss: 0.474\n",
      "Epoch 17835, Loss: 16.047, Final Batch Loss: 0.360\n",
      "Epoch 17836, Loss: 15.796, Final Batch Loss: 0.417\n",
      "Epoch 17837, Loss: 16.008, Final Batch Loss: 0.431\n",
      "Epoch 17838, Loss: 16.045, Final Batch Loss: 0.391\n",
      "Epoch 17839, Loss: 15.911, Final Batch Loss: 0.467\n",
      "Epoch 17840, Loss: 16.035, Final Batch Loss: 0.470\n",
      "Epoch 17841, Loss: 15.981, Final Batch Loss: 0.430\n",
      "Epoch 17842, Loss: 15.919, Final Batch Loss: 0.476\n",
      "Epoch 17843, Loss: 16.038, Final Batch Loss: 0.363\n",
      "Epoch 17844, Loss: 16.160, Final Batch Loss: 0.539\n",
      "Epoch 17845, Loss: 16.165, Final Batch Loss: 0.401\n",
      "Epoch 17846, Loss: 16.206, Final Batch Loss: 0.388\n",
      "Epoch 17847, Loss: 16.283, Final Batch Loss: 0.460\n",
      "Epoch 17848, Loss: 16.130, Final Batch Loss: 0.459\n",
      "Epoch 17849, Loss: 15.871, Final Batch Loss: 0.416\n",
      "Epoch 17850, Loss: 16.109, Final Batch Loss: 0.404\n",
      "Epoch 17851, Loss: 16.085, Final Batch Loss: 0.372\n",
      "Epoch 17852, Loss: 16.197, Final Batch Loss: 0.467\n",
      "Epoch 17853, Loss: 16.166, Final Batch Loss: 0.434\n",
      "Epoch 17854, Loss: 16.034, Final Batch Loss: 0.369\n",
      "Epoch 17855, Loss: 16.212, Final Batch Loss: 0.453\n",
      "Epoch 17856, Loss: 15.798, Final Batch Loss: 0.422\n",
      "Epoch 17857, Loss: 15.932, Final Batch Loss: 0.502\n",
      "Epoch 17858, Loss: 16.036, Final Batch Loss: 0.444\n",
      "Epoch 17859, Loss: 15.847, Final Batch Loss: 0.452\n",
      "Epoch 17860, Loss: 16.248, Final Batch Loss: 0.589\n",
      "Epoch 17861, Loss: 15.831, Final Batch Loss: 0.458\n",
      "Epoch 17862, Loss: 16.293, Final Batch Loss: 0.368\n",
      "Epoch 17863, Loss: 15.770, Final Batch Loss: 0.365\n",
      "Epoch 17864, Loss: 16.040, Final Batch Loss: 0.506\n",
      "Epoch 17865, Loss: 15.745, Final Batch Loss: 0.338\n",
      "Epoch 17866, Loss: 16.112, Final Batch Loss: 0.400\n",
      "Epoch 17867, Loss: 16.190, Final Batch Loss: 0.412\n",
      "Epoch 17868, Loss: 16.148, Final Batch Loss: 0.471\n",
      "Epoch 17869, Loss: 15.942, Final Batch Loss: 0.576\n",
      "Epoch 17870, Loss: 16.314, Final Batch Loss: 0.420\n",
      "Epoch 17871, Loss: 16.077, Final Batch Loss: 0.308\n",
      "Epoch 17872, Loss: 16.270, Final Batch Loss: 0.500\n",
      "Epoch 17873, Loss: 15.943, Final Batch Loss: 0.458\n",
      "Epoch 17874, Loss: 15.841, Final Batch Loss: 0.471\n",
      "Epoch 17875, Loss: 15.815, Final Batch Loss: 0.382\n",
      "Epoch 17876, Loss: 16.117, Final Batch Loss: 0.405\n",
      "Epoch 17877, Loss: 15.995, Final Batch Loss: 0.413\n",
      "Epoch 17878, Loss: 16.092, Final Batch Loss: 0.422\n",
      "Epoch 17879, Loss: 16.086, Final Batch Loss: 0.585\n",
      "Epoch 17880, Loss: 16.111, Final Batch Loss: 0.518\n",
      "Epoch 17881, Loss: 16.315, Final Batch Loss: 0.634\n",
      "Epoch 17882, Loss: 16.077, Final Batch Loss: 0.550\n",
      "Epoch 17883, Loss: 16.193, Final Batch Loss: 0.353\n",
      "Epoch 17884, Loss: 16.019, Final Batch Loss: 0.433\n",
      "Epoch 17885, Loss: 16.025, Final Batch Loss: 0.441\n",
      "Epoch 17886, Loss: 15.882, Final Batch Loss: 0.497\n",
      "Epoch 17887, Loss: 16.060, Final Batch Loss: 0.446\n",
      "Epoch 17888, Loss: 15.897, Final Batch Loss: 0.417\n",
      "Epoch 17889, Loss: 15.884, Final Batch Loss: 0.536\n",
      "Epoch 17890, Loss: 15.849, Final Batch Loss: 0.451\n",
      "Epoch 17891, Loss: 15.959, Final Batch Loss: 0.492\n",
      "Epoch 17892, Loss: 16.046, Final Batch Loss: 0.460\n",
      "Epoch 17893, Loss: 16.134, Final Batch Loss: 0.467\n",
      "Epoch 17894, Loss: 16.121, Final Batch Loss: 0.417\n",
      "Epoch 17895, Loss: 16.041, Final Batch Loss: 0.441\n",
      "Epoch 17896, Loss: 16.059, Final Batch Loss: 0.492\n",
      "Epoch 17897, Loss: 16.043, Final Batch Loss: 0.435\n",
      "Epoch 17898, Loss: 15.747, Final Batch Loss: 0.385\n",
      "Epoch 17899, Loss: 16.125, Final Batch Loss: 0.453\n",
      "Epoch 17900, Loss: 16.151, Final Batch Loss: 0.469\n",
      "Epoch 17901, Loss: 16.129, Final Batch Loss: 0.502\n",
      "Epoch 17902, Loss: 16.313, Final Batch Loss: 0.446\n",
      "Epoch 17903, Loss: 15.926, Final Batch Loss: 0.497\n",
      "Epoch 17904, Loss: 16.170, Final Batch Loss: 0.411\n",
      "Epoch 17905, Loss: 16.160, Final Batch Loss: 0.538\n",
      "Epoch 17906, Loss: 16.086, Final Batch Loss: 0.501\n",
      "Epoch 17907, Loss: 16.006, Final Batch Loss: 0.429\n",
      "Epoch 17908, Loss: 15.785, Final Batch Loss: 0.408\n",
      "Epoch 17909, Loss: 16.238, Final Batch Loss: 0.500\n",
      "Epoch 17910, Loss: 16.211, Final Batch Loss: 0.555\n",
      "Epoch 17911, Loss: 16.395, Final Batch Loss: 0.429\n",
      "Epoch 17912, Loss: 16.100, Final Batch Loss: 0.527\n",
      "Epoch 17913, Loss: 16.111, Final Batch Loss: 0.506\n",
      "Epoch 17914, Loss: 16.055, Final Batch Loss: 0.530\n",
      "Epoch 17915, Loss: 16.155, Final Batch Loss: 0.486\n",
      "Epoch 17916, Loss: 15.887, Final Batch Loss: 0.431\n",
      "Epoch 17917, Loss: 16.177, Final Batch Loss: 0.637\n",
      "Epoch 17918, Loss: 16.200, Final Batch Loss: 0.447\n",
      "Epoch 17919, Loss: 16.091, Final Batch Loss: 0.423\n",
      "Epoch 17920, Loss: 16.008, Final Batch Loss: 0.424\n",
      "Epoch 17921, Loss: 15.996, Final Batch Loss: 0.407\n",
      "Epoch 17922, Loss: 16.183, Final Batch Loss: 0.474\n",
      "Epoch 17923, Loss: 16.169, Final Batch Loss: 0.494\n",
      "Epoch 17924, Loss: 16.052, Final Batch Loss: 0.453\n",
      "Epoch 17925, Loss: 16.055, Final Batch Loss: 0.437\n",
      "Epoch 17926, Loss: 15.786, Final Batch Loss: 0.409\n",
      "Epoch 17927, Loss: 16.151, Final Batch Loss: 0.573\n",
      "Epoch 17928, Loss: 15.952, Final Batch Loss: 0.479\n",
      "Epoch 17929, Loss: 15.818, Final Batch Loss: 0.344\n",
      "Epoch 17930, Loss: 15.949, Final Batch Loss: 0.395\n",
      "Epoch 17931, Loss: 16.079, Final Batch Loss: 0.462\n",
      "Epoch 17932, Loss: 15.967, Final Batch Loss: 0.365\n",
      "Epoch 17933, Loss: 15.778, Final Batch Loss: 0.440\n",
      "Epoch 17934, Loss: 16.341, Final Batch Loss: 0.492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17935, Loss: 16.259, Final Batch Loss: 0.388\n",
      "Epoch 17936, Loss: 16.108, Final Batch Loss: 0.510\n",
      "Epoch 17937, Loss: 16.055, Final Batch Loss: 0.338\n",
      "Epoch 17938, Loss: 16.131, Final Batch Loss: 0.431\n",
      "Epoch 17939, Loss: 15.971, Final Batch Loss: 0.441\n",
      "Epoch 17940, Loss: 15.943, Final Batch Loss: 0.484\n",
      "Epoch 17941, Loss: 16.333, Final Batch Loss: 0.494\n",
      "Epoch 17942, Loss: 16.077, Final Batch Loss: 0.431\n",
      "Epoch 17943, Loss: 15.813, Final Batch Loss: 0.348\n",
      "Epoch 17944, Loss: 16.298, Final Batch Loss: 0.416\n",
      "Epoch 17945, Loss: 15.784, Final Batch Loss: 0.424\n",
      "Epoch 17946, Loss: 15.839, Final Batch Loss: 0.370\n",
      "Epoch 17947, Loss: 15.838, Final Batch Loss: 0.389\n",
      "Epoch 17948, Loss: 16.169, Final Batch Loss: 0.504\n",
      "Epoch 17949, Loss: 16.140, Final Batch Loss: 0.496\n",
      "Epoch 17950, Loss: 16.013, Final Batch Loss: 0.331\n",
      "Epoch 17951, Loss: 16.064, Final Batch Loss: 0.546\n",
      "Epoch 17952, Loss: 16.208, Final Batch Loss: 0.481\n",
      "Epoch 17953, Loss: 15.960, Final Batch Loss: 0.446\n",
      "Epoch 17954, Loss: 16.212, Final Batch Loss: 0.390\n",
      "Epoch 17955, Loss: 15.910, Final Batch Loss: 0.389\n",
      "Epoch 17956, Loss: 16.050, Final Batch Loss: 0.401\n",
      "Epoch 17957, Loss: 15.844, Final Batch Loss: 0.397\n",
      "Epoch 17958, Loss: 15.914, Final Batch Loss: 0.376\n",
      "Epoch 17959, Loss: 16.199, Final Batch Loss: 0.456\n",
      "Epoch 17960, Loss: 16.304, Final Batch Loss: 0.514\n",
      "Epoch 17961, Loss: 16.078, Final Batch Loss: 0.474\n",
      "Epoch 17962, Loss: 15.983, Final Batch Loss: 0.393\n",
      "Epoch 17963, Loss: 16.049, Final Batch Loss: 0.471\n",
      "Epoch 17964, Loss: 15.933, Final Batch Loss: 0.453\n",
      "Epoch 17965, Loss: 16.053, Final Batch Loss: 0.599\n",
      "Epoch 17966, Loss: 16.027, Final Batch Loss: 0.434\n",
      "Epoch 17967, Loss: 15.969, Final Batch Loss: 0.554\n",
      "Epoch 17968, Loss: 16.213, Final Batch Loss: 0.490\n",
      "Epoch 17969, Loss: 16.073, Final Batch Loss: 0.410\n",
      "Epoch 17970, Loss: 16.137, Final Batch Loss: 0.433\n",
      "Epoch 17971, Loss: 15.941, Final Batch Loss: 0.569\n",
      "Epoch 17972, Loss: 15.745, Final Batch Loss: 0.480\n",
      "Epoch 17973, Loss: 16.107, Final Batch Loss: 0.326\n",
      "Epoch 17974, Loss: 16.273, Final Batch Loss: 0.447\n",
      "Epoch 17975, Loss: 15.960, Final Batch Loss: 0.526\n",
      "Epoch 17976, Loss: 16.021, Final Batch Loss: 0.449\n",
      "Epoch 17977, Loss: 16.146, Final Batch Loss: 0.473\n",
      "Epoch 17978, Loss: 16.118, Final Batch Loss: 0.481\n",
      "Epoch 17979, Loss: 15.949, Final Batch Loss: 0.397\n",
      "Epoch 17980, Loss: 16.067, Final Batch Loss: 0.429\n",
      "Epoch 17981, Loss: 16.217, Final Batch Loss: 0.419\n",
      "Epoch 17982, Loss: 15.863, Final Batch Loss: 0.466\n",
      "Epoch 17983, Loss: 16.053, Final Batch Loss: 0.443\n",
      "Epoch 17984, Loss: 16.196, Final Batch Loss: 0.527\n",
      "Epoch 17985, Loss: 16.050, Final Batch Loss: 0.478\n",
      "Epoch 17986, Loss: 15.851, Final Batch Loss: 0.371\n",
      "Epoch 17987, Loss: 15.975, Final Batch Loss: 0.468\n",
      "Epoch 17988, Loss: 16.060, Final Batch Loss: 0.410\n",
      "Epoch 17989, Loss: 15.993, Final Batch Loss: 0.427\n",
      "Epoch 17990, Loss: 16.023, Final Batch Loss: 0.440\n",
      "Epoch 17991, Loss: 15.994, Final Batch Loss: 0.467\n",
      "Epoch 17992, Loss: 16.184, Final Batch Loss: 0.443\n",
      "Epoch 17993, Loss: 15.782, Final Batch Loss: 0.403\n",
      "Epoch 17994, Loss: 15.949, Final Batch Loss: 0.461\n",
      "Epoch 17995, Loss: 16.000, Final Batch Loss: 0.410\n",
      "Epoch 17996, Loss: 15.974, Final Batch Loss: 0.378\n",
      "Epoch 17997, Loss: 16.025, Final Batch Loss: 0.505\n",
      "Epoch 17998, Loss: 16.133, Final Batch Loss: 0.459\n",
      "Epoch 17999, Loss: 15.932, Final Batch Loss: 0.311\n",
      "Epoch 18000, Loss: 15.975, Final Batch Loss: 0.342\n",
      "Epoch 18001, Loss: 16.133, Final Batch Loss: 0.454\n",
      "Epoch 18002, Loss: 16.162, Final Batch Loss: 0.390\n",
      "Epoch 18003, Loss: 15.934, Final Batch Loss: 0.327\n",
      "Epoch 18004, Loss: 15.912, Final Batch Loss: 0.434\n",
      "Epoch 18005, Loss: 16.259, Final Batch Loss: 0.450\n",
      "Epoch 18006, Loss: 16.074, Final Batch Loss: 0.410\n",
      "Epoch 18007, Loss: 15.999, Final Batch Loss: 0.439\n",
      "Epoch 18008, Loss: 15.872, Final Batch Loss: 0.424\n",
      "Epoch 18009, Loss: 16.029, Final Batch Loss: 0.404\n",
      "Epoch 18010, Loss: 16.241, Final Batch Loss: 0.388\n",
      "Epoch 18011, Loss: 16.276, Final Batch Loss: 0.508\n",
      "Epoch 18012, Loss: 16.220, Final Batch Loss: 0.447\n",
      "Epoch 18013, Loss: 15.938, Final Batch Loss: 0.323\n",
      "Epoch 18014, Loss: 16.074, Final Batch Loss: 0.360\n",
      "Epoch 18015, Loss: 15.872, Final Batch Loss: 0.438\n",
      "Epoch 18016, Loss: 15.927, Final Batch Loss: 0.442\n",
      "Epoch 18017, Loss: 15.936, Final Batch Loss: 0.468\n",
      "Epoch 18018, Loss: 16.187, Final Batch Loss: 0.498\n",
      "Epoch 18019, Loss: 15.916, Final Batch Loss: 0.502\n",
      "Epoch 18020, Loss: 16.114, Final Batch Loss: 0.422\n",
      "Epoch 18021, Loss: 16.016, Final Batch Loss: 0.429\n",
      "Epoch 18022, Loss: 16.252, Final Batch Loss: 0.475\n",
      "Epoch 18023, Loss: 16.071, Final Batch Loss: 0.471\n",
      "Epoch 18024, Loss: 16.001, Final Batch Loss: 0.357\n",
      "Epoch 18025, Loss: 16.013, Final Batch Loss: 0.437\n",
      "Epoch 18026, Loss: 16.143, Final Batch Loss: 0.385\n",
      "Epoch 18027, Loss: 16.319, Final Batch Loss: 0.482\n",
      "Epoch 18028, Loss: 15.802, Final Batch Loss: 0.376\n",
      "Epoch 18029, Loss: 16.004, Final Batch Loss: 0.456\n",
      "Epoch 18030, Loss: 16.056, Final Batch Loss: 0.501\n",
      "Epoch 18031, Loss: 16.330, Final Batch Loss: 0.404\n",
      "Epoch 18032, Loss: 15.847, Final Batch Loss: 0.524\n",
      "Epoch 18033, Loss: 16.161, Final Batch Loss: 0.455\n",
      "Epoch 18034, Loss: 15.856, Final Batch Loss: 0.473\n",
      "Epoch 18035, Loss: 16.130, Final Batch Loss: 0.434\n",
      "Epoch 18036, Loss: 15.923, Final Batch Loss: 0.418\n",
      "Epoch 18037, Loss: 16.053, Final Batch Loss: 0.499\n",
      "Epoch 18038, Loss: 15.870, Final Batch Loss: 0.460\n",
      "Epoch 18039, Loss: 16.172, Final Batch Loss: 0.583\n",
      "Epoch 18040, Loss: 15.838, Final Batch Loss: 0.473\n",
      "Epoch 18041, Loss: 15.969, Final Batch Loss: 0.432\n",
      "Epoch 18042, Loss: 16.107, Final Batch Loss: 0.401\n",
      "Epoch 18043, Loss: 15.894, Final Batch Loss: 0.451\n",
      "Epoch 18044, Loss: 16.092, Final Batch Loss: 0.424\n",
      "Epoch 18045, Loss: 15.918, Final Batch Loss: 0.409\n",
      "Epoch 18046, Loss: 15.968, Final Batch Loss: 0.410\n",
      "Epoch 18047, Loss: 16.141, Final Batch Loss: 0.472\n",
      "Epoch 18048, Loss: 15.817, Final Batch Loss: 0.491\n",
      "Epoch 18049, Loss: 16.446, Final Batch Loss: 0.467\n",
      "Epoch 18050, Loss: 15.922, Final Batch Loss: 0.488\n",
      "Epoch 18051, Loss: 15.865, Final Batch Loss: 0.389\n",
      "Epoch 18052, Loss: 15.988, Final Batch Loss: 0.431\n",
      "Epoch 18053, Loss: 15.675, Final Batch Loss: 0.341\n",
      "Epoch 18054, Loss: 15.906, Final Batch Loss: 0.407\n",
      "Epoch 18055, Loss: 16.246, Final Batch Loss: 0.451\n",
      "Epoch 18056, Loss: 15.951, Final Batch Loss: 0.424\n",
      "Epoch 18057, Loss: 16.062, Final Batch Loss: 0.461\n",
      "Epoch 18058, Loss: 15.876, Final Batch Loss: 0.394\n",
      "Epoch 18059, Loss: 15.889, Final Batch Loss: 0.460\n",
      "Epoch 18060, Loss: 16.007, Final Batch Loss: 0.427\n",
      "Epoch 18061, Loss: 16.035, Final Batch Loss: 0.506\n",
      "Epoch 18062, Loss: 16.087, Final Batch Loss: 0.382\n",
      "Epoch 18063, Loss: 16.157, Final Batch Loss: 0.447\n",
      "Epoch 18064, Loss: 16.033, Final Batch Loss: 0.499\n",
      "Epoch 18065, Loss: 16.332, Final Batch Loss: 0.360\n",
      "Epoch 18066, Loss: 16.099, Final Batch Loss: 0.444\n",
      "Epoch 18067, Loss: 16.154, Final Batch Loss: 0.416\n",
      "Epoch 18068, Loss: 16.258, Final Batch Loss: 0.556\n",
      "Epoch 18069, Loss: 16.067, Final Batch Loss: 0.464\n",
      "Epoch 18070, Loss: 15.885, Final Batch Loss: 0.462\n",
      "Epoch 18071, Loss: 16.194, Final Batch Loss: 0.381\n",
      "Epoch 18072, Loss: 15.796, Final Batch Loss: 0.417\n",
      "Epoch 18073, Loss: 16.203, Final Batch Loss: 0.466\n",
      "Epoch 18074, Loss: 15.961, Final Batch Loss: 0.406\n",
      "Epoch 18075, Loss: 16.080, Final Batch Loss: 0.386\n",
      "Epoch 18076, Loss: 15.971, Final Batch Loss: 0.549\n",
      "Epoch 18077, Loss: 15.901, Final Batch Loss: 0.369\n",
      "Epoch 18078, Loss: 16.150, Final Batch Loss: 0.475\n",
      "Epoch 18079, Loss: 16.073, Final Batch Loss: 0.433\n",
      "Epoch 18080, Loss: 15.968, Final Batch Loss: 0.421\n",
      "Epoch 18081, Loss: 15.686, Final Batch Loss: 0.378\n",
      "Epoch 18082, Loss: 16.002, Final Batch Loss: 0.434\n",
      "Epoch 18083, Loss: 16.091, Final Batch Loss: 0.506\n",
      "Epoch 18084, Loss: 16.172, Final Batch Loss: 0.379\n",
      "Epoch 18085, Loss: 16.118, Final Batch Loss: 0.353\n",
      "Epoch 18086, Loss: 16.165, Final Batch Loss: 0.410\n",
      "Epoch 18087, Loss: 15.785, Final Batch Loss: 0.429\n",
      "Epoch 18088, Loss: 15.990, Final Batch Loss: 0.472\n",
      "Epoch 18089, Loss: 15.786, Final Batch Loss: 0.441\n",
      "Epoch 18090, Loss: 16.154, Final Batch Loss: 0.467\n",
      "Epoch 18091, Loss: 15.953, Final Batch Loss: 0.405\n",
      "Epoch 18092, Loss: 16.101, Final Batch Loss: 0.569\n",
      "Epoch 18093, Loss: 15.918, Final Batch Loss: 0.423\n",
      "Epoch 18094, Loss: 15.865, Final Batch Loss: 0.466\n",
      "Epoch 18095, Loss: 15.978, Final Batch Loss: 0.532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18096, Loss: 16.197, Final Batch Loss: 0.539\n",
      "Epoch 18097, Loss: 16.182, Final Batch Loss: 0.439\n",
      "Epoch 18098, Loss: 16.209, Final Batch Loss: 0.525\n",
      "Epoch 18099, Loss: 15.950, Final Batch Loss: 0.435\n",
      "Epoch 18100, Loss: 16.136, Final Batch Loss: 0.360\n",
      "Epoch 18101, Loss: 15.951, Final Batch Loss: 0.457\n",
      "Epoch 18102, Loss: 16.382, Final Batch Loss: 0.515\n",
      "Epoch 18103, Loss: 16.327, Final Batch Loss: 0.547\n",
      "Epoch 18104, Loss: 16.016, Final Batch Loss: 0.390\n",
      "Epoch 18105, Loss: 16.309, Final Batch Loss: 0.586\n",
      "Epoch 18106, Loss: 16.229, Final Batch Loss: 0.485\n",
      "Epoch 18107, Loss: 16.065, Final Batch Loss: 0.437\n",
      "Epoch 18108, Loss: 16.122, Final Batch Loss: 0.408\n",
      "Epoch 18109, Loss: 16.077, Final Batch Loss: 0.451\n",
      "Epoch 18110, Loss: 15.950, Final Batch Loss: 0.497\n",
      "Epoch 18111, Loss: 16.107, Final Batch Loss: 0.497\n",
      "Epoch 18112, Loss: 16.229, Final Batch Loss: 0.539\n",
      "Epoch 18113, Loss: 15.860, Final Batch Loss: 0.589\n",
      "Epoch 18114, Loss: 15.824, Final Batch Loss: 0.412\n",
      "Epoch 18115, Loss: 16.195, Final Batch Loss: 0.478\n",
      "Epoch 18116, Loss: 16.271, Final Batch Loss: 0.468\n",
      "Epoch 18117, Loss: 15.914, Final Batch Loss: 0.528\n",
      "Epoch 18118, Loss: 16.033, Final Batch Loss: 0.472\n",
      "Epoch 18119, Loss: 16.305, Final Batch Loss: 0.442\n",
      "Epoch 18120, Loss: 15.984, Final Batch Loss: 0.355\n",
      "Epoch 18121, Loss: 16.162, Final Batch Loss: 0.434\n",
      "Epoch 18122, Loss: 15.984, Final Batch Loss: 0.413\n",
      "Epoch 18123, Loss: 16.055, Final Batch Loss: 0.483\n",
      "Epoch 18124, Loss: 16.189, Final Batch Loss: 0.456\n",
      "Epoch 18125, Loss: 16.360, Final Batch Loss: 0.532\n",
      "Epoch 18126, Loss: 15.973, Final Batch Loss: 0.478\n",
      "Epoch 18127, Loss: 15.933, Final Batch Loss: 0.355\n",
      "Epoch 18128, Loss: 16.090, Final Batch Loss: 0.378\n",
      "Epoch 18129, Loss: 16.152, Final Batch Loss: 0.352\n",
      "Epoch 18130, Loss: 16.458, Final Batch Loss: 0.486\n",
      "Epoch 18131, Loss: 16.151, Final Batch Loss: 0.424\n",
      "Epoch 18132, Loss: 16.015, Final Batch Loss: 0.368\n",
      "Epoch 18133, Loss: 16.191, Final Batch Loss: 0.362\n",
      "Epoch 18134, Loss: 15.923, Final Batch Loss: 0.402\n",
      "Epoch 18135, Loss: 15.941, Final Batch Loss: 0.382\n",
      "Epoch 18136, Loss: 16.114, Final Batch Loss: 0.401\n",
      "Epoch 18137, Loss: 15.995, Final Batch Loss: 0.436\n",
      "Epoch 18138, Loss: 15.946, Final Batch Loss: 0.564\n",
      "Epoch 18139, Loss: 15.991, Final Batch Loss: 0.465\n",
      "Epoch 18140, Loss: 16.182, Final Batch Loss: 0.602\n",
      "Epoch 18141, Loss: 15.989, Final Batch Loss: 0.377\n",
      "Epoch 18142, Loss: 16.102, Final Batch Loss: 0.426\n",
      "Epoch 18143, Loss: 16.058, Final Batch Loss: 0.446\n",
      "Epoch 18144, Loss: 16.159, Final Batch Loss: 0.521\n",
      "Epoch 18145, Loss: 15.855, Final Batch Loss: 0.409\n",
      "Epoch 18146, Loss: 16.097, Final Batch Loss: 0.445\n",
      "Epoch 18147, Loss: 16.157, Final Batch Loss: 0.468\n",
      "Epoch 18148, Loss: 16.051, Final Batch Loss: 0.562\n",
      "Epoch 18149, Loss: 15.994, Final Batch Loss: 0.466\n",
      "Epoch 18150, Loss: 16.220, Final Batch Loss: 0.408\n",
      "Epoch 18151, Loss: 16.150, Final Batch Loss: 0.430\n",
      "Epoch 18152, Loss: 15.786, Final Batch Loss: 0.348\n",
      "Epoch 18153, Loss: 16.130, Final Batch Loss: 0.476\n",
      "Epoch 18154, Loss: 16.231, Final Batch Loss: 0.536\n",
      "Epoch 18155, Loss: 16.111, Final Batch Loss: 0.480\n",
      "Epoch 18156, Loss: 16.144, Final Batch Loss: 0.429\n",
      "Epoch 18157, Loss: 15.998, Final Batch Loss: 0.457\n",
      "Epoch 18158, Loss: 16.099, Final Batch Loss: 0.421\n",
      "Epoch 18159, Loss: 16.164, Final Batch Loss: 0.485\n",
      "Epoch 18160, Loss: 15.947, Final Batch Loss: 0.399\n",
      "Epoch 18161, Loss: 16.303, Final Batch Loss: 0.492\n",
      "Epoch 18162, Loss: 15.823, Final Batch Loss: 0.431\n",
      "Epoch 18163, Loss: 16.356, Final Batch Loss: 0.455\n",
      "Epoch 18164, Loss: 15.872, Final Batch Loss: 0.594\n",
      "Epoch 18165, Loss: 16.111, Final Batch Loss: 0.442\n",
      "Epoch 18166, Loss: 16.014, Final Batch Loss: 0.504\n",
      "Epoch 18167, Loss: 16.226, Final Batch Loss: 0.400\n",
      "Epoch 18168, Loss: 15.993, Final Batch Loss: 0.440\n",
      "Epoch 18169, Loss: 16.093, Final Batch Loss: 0.526\n",
      "Epoch 18170, Loss: 15.937, Final Batch Loss: 0.396\n",
      "Epoch 18171, Loss: 15.944, Final Batch Loss: 0.424\n",
      "Epoch 18172, Loss: 16.085, Final Batch Loss: 0.477\n",
      "Epoch 18173, Loss: 16.009, Final Batch Loss: 0.479\n",
      "Epoch 18174, Loss: 16.165, Final Batch Loss: 0.451\n",
      "Epoch 18175, Loss: 16.163, Final Batch Loss: 0.401\n",
      "Epoch 18176, Loss: 16.206, Final Batch Loss: 0.599\n",
      "Epoch 18177, Loss: 16.138, Final Batch Loss: 0.456\n",
      "Epoch 18178, Loss: 16.151, Final Batch Loss: 0.389\n",
      "Epoch 18179, Loss: 16.111, Final Batch Loss: 0.405\n",
      "Epoch 18180, Loss: 16.238, Final Batch Loss: 0.502\n",
      "Epoch 18181, Loss: 16.147, Final Batch Loss: 0.408\n",
      "Epoch 18182, Loss: 15.943, Final Batch Loss: 0.425\n",
      "Epoch 18183, Loss: 16.065, Final Batch Loss: 0.343\n",
      "Epoch 18184, Loss: 16.030, Final Batch Loss: 0.378\n",
      "Epoch 18185, Loss: 16.252, Final Batch Loss: 0.630\n",
      "Epoch 18186, Loss: 15.957, Final Batch Loss: 0.518\n",
      "Epoch 18187, Loss: 15.942, Final Batch Loss: 0.417\n",
      "Epoch 18188, Loss: 16.272, Final Batch Loss: 0.421\n",
      "Epoch 18189, Loss: 16.123, Final Batch Loss: 0.504\n",
      "Epoch 18190, Loss: 16.294, Final Batch Loss: 0.402\n",
      "Epoch 18191, Loss: 15.841, Final Batch Loss: 0.429\n",
      "Epoch 18192, Loss: 16.093, Final Batch Loss: 0.411\n",
      "Epoch 18193, Loss: 15.770, Final Batch Loss: 0.396\n",
      "Epoch 18194, Loss: 16.093, Final Batch Loss: 0.425\n",
      "Epoch 18195, Loss: 15.981, Final Batch Loss: 0.407\n",
      "Epoch 18196, Loss: 15.940, Final Batch Loss: 0.390\n",
      "Epoch 18197, Loss: 16.026, Final Batch Loss: 0.444\n",
      "Epoch 18198, Loss: 16.077, Final Batch Loss: 0.466\n",
      "Epoch 18199, Loss: 15.856, Final Batch Loss: 0.374\n",
      "Epoch 18200, Loss: 15.937, Final Batch Loss: 0.370\n",
      "Epoch 18201, Loss: 16.114, Final Batch Loss: 0.411\n",
      "Epoch 18202, Loss: 16.055, Final Batch Loss: 0.384\n",
      "Epoch 18203, Loss: 16.223, Final Batch Loss: 0.543\n",
      "Epoch 18204, Loss: 15.660, Final Batch Loss: 0.344\n",
      "Epoch 18205, Loss: 15.879, Final Batch Loss: 0.290\n",
      "Epoch 18206, Loss: 16.269, Final Batch Loss: 0.529\n",
      "Epoch 18207, Loss: 16.386, Final Batch Loss: 0.428\n",
      "Epoch 18208, Loss: 15.886, Final Batch Loss: 0.354\n",
      "Epoch 18209, Loss: 16.142, Final Batch Loss: 0.432\n",
      "Epoch 18210, Loss: 16.138, Final Batch Loss: 0.454\n",
      "Epoch 18211, Loss: 16.076, Final Batch Loss: 0.482\n",
      "Epoch 18212, Loss: 16.068, Final Batch Loss: 0.428\n",
      "Epoch 18213, Loss: 16.090, Final Batch Loss: 0.455\n",
      "Epoch 18214, Loss: 16.153, Final Batch Loss: 0.489\n",
      "Epoch 18215, Loss: 16.218, Final Batch Loss: 0.428\n",
      "Epoch 18216, Loss: 16.108, Final Batch Loss: 0.533\n",
      "Epoch 18217, Loss: 16.319, Final Batch Loss: 0.393\n",
      "Epoch 18218, Loss: 16.120, Final Batch Loss: 0.378\n",
      "Epoch 18219, Loss: 16.011, Final Batch Loss: 0.484\n",
      "Epoch 18220, Loss: 15.787, Final Batch Loss: 0.395\n",
      "Epoch 18221, Loss: 15.989, Final Batch Loss: 0.433\n",
      "Epoch 18222, Loss: 15.985, Final Batch Loss: 0.410\n",
      "Epoch 18223, Loss: 16.141, Final Batch Loss: 0.459\n",
      "Epoch 18224, Loss: 16.143, Final Batch Loss: 0.439\n",
      "Epoch 18225, Loss: 16.172, Final Batch Loss: 0.531\n",
      "Epoch 18226, Loss: 16.033, Final Batch Loss: 0.510\n",
      "Epoch 18227, Loss: 15.805, Final Batch Loss: 0.400\n",
      "Epoch 18228, Loss: 16.201, Final Batch Loss: 0.408\n",
      "Epoch 18229, Loss: 16.283, Final Batch Loss: 0.469\n",
      "Epoch 18230, Loss: 15.955, Final Batch Loss: 0.515\n",
      "Epoch 18231, Loss: 16.114, Final Batch Loss: 0.466\n",
      "Epoch 18232, Loss: 16.162, Final Batch Loss: 0.481\n",
      "Epoch 18233, Loss: 16.162, Final Batch Loss: 0.486\n",
      "Epoch 18234, Loss: 16.075, Final Batch Loss: 0.505\n",
      "Epoch 18235, Loss: 16.088, Final Batch Loss: 0.381\n",
      "Epoch 18236, Loss: 16.241, Final Batch Loss: 0.459\n",
      "Epoch 18237, Loss: 16.113, Final Batch Loss: 0.397\n",
      "Epoch 18238, Loss: 16.019, Final Batch Loss: 0.513\n",
      "Epoch 18239, Loss: 16.057, Final Batch Loss: 0.399\n",
      "Epoch 18240, Loss: 15.950, Final Batch Loss: 0.496\n",
      "Epoch 18241, Loss: 16.139, Final Batch Loss: 0.375\n",
      "Epoch 18242, Loss: 15.769, Final Batch Loss: 0.370\n",
      "Epoch 18243, Loss: 16.133, Final Batch Loss: 0.481\n",
      "Epoch 18244, Loss: 15.912, Final Batch Loss: 0.528\n",
      "Epoch 18245, Loss: 15.905, Final Batch Loss: 0.356\n",
      "Epoch 18246, Loss: 15.847, Final Batch Loss: 0.383\n",
      "Epoch 18247, Loss: 15.844, Final Batch Loss: 0.390\n",
      "Epoch 18248, Loss: 16.376, Final Batch Loss: 0.555\n",
      "Epoch 18249, Loss: 15.957, Final Batch Loss: 0.476\n",
      "Epoch 18250, Loss: 15.761, Final Batch Loss: 0.344\n",
      "Epoch 18251, Loss: 16.519, Final Batch Loss: 0.577\n",
      "Epoch 18252, Loss: 15.912, Final Batch Loss: 0.379\n",
      "Epoch 18253, Loss: 16.154, Final Batch Loss: 0.443\n",
      "Epoch 18254, Loss: 16.046, Final Batch Loss: 0.387\n",
      "Epoch 18255, Loss: 15.868, Final Batch Loss: 0.384\n",
      "Epoch 18256, Loss: 16.143, Final Batch Loss: 0.472\n",
      "Epoch 18257, Loss: 16.025, Final Batch Loss: 0.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18258, Loss: 16.056, Final Batch Loss: 0.460\n",
      "Epoch 18259, Loss: 16.145, Final Batch Loss: 0.459\n",
      "Epoch 18260, Loss: 15.948, Final Batch Loss: 0.372\n",
      "Epoch 18261, Loss: 15.970, Final Batch Loss: 0.462\n",
      "Epoch 18262, Loss: 16.083, Final Batch Loss: 0.490\n",
      "Epoch 18263, Loss: 15.887, Final Batch Loss: 0.416\n",
      "Epoch 18264, Loss: 16.376, Final Batch Loss: 0.449\n",
      "Epoch 18265, Loss: 15.881, Final Batch Loss: 0.483\n",
      "Epoch 18266, Loss: 15.786, Final Batch Loss: 0.376\n",
      "Epoch 18267, Loss: 15.811, Final Batch Loss: 0.366\n",
      "Epoch 18268, Loss: 15.844, Final Batch Loss: 0.481\n",
      "Epoch 18269, Loss: 15.998, Final Batch Loss: 0.399\n",
      "Epoch 18270, Loss: 16.142, Final Batch Loss: 0.387\n",
      "Epoch 18271, Loss: 16.061, Final Batch Loss: 0.422\n",
      "Epoch 18272, Loss: 15.987, Final Batch Loss: 0.415\n",
      "Epoch 18273, Loss: 15.829, Final Batch Loss: 0.393\n",
      "Epoch 18274, Loss: 15.805, Final Batch Loss: 0.423\n",
      "Epoch 18275, Loss: 16.008, Final Batch Loss: 0.511\n",
      "Epoch 18276, Loss: 16.268, Final Batch Loss: 0.543\n",
      "Epoch 18277, Loss: 15.946, Final Batch Loss: 0.494\n",
      "Epoch 18278, Loss: 16.332, Final Batch Loss: 0.379\n",
      "Epoch 18279, Loss: 15.882, Final Batch Loss: 0.411\n",
      "Epoch 18280, Loss: 16.034, Final Batch Loss: 0.354\n",
      "Epoch 18281, Loss: 16.114, Final Batch Loss: 0.503\n",
      "Epoch 18282, Loss: 16.009, Final Batch Loss: 0.380\n",
      "Epoch 18283, Loss: 16.158, Final Batch Loss: 0.566\n",
      "Epoch 18284, Loss: 15.983, Final Batch Loss: 0.456\n",
      "Epoch 18285, Loss: 15.779, Final Batch Loss: 0.510\n",
      "Epoch 18286, Loss: 15.897, Final Batch Loss: 0.445\n",
      "Epoch 18287, Loss: 16.046, Final Batch Loss: 0.374\n",
      "Epoch 18288, Loss: 15.962, Final Batch Loss: 0.336\n",
      "Epoch 18289, Loss: 15.936, Final Batch Loss: 0.364\n",
      "Epoch 18290, Loss: 16.029, Final Batch Loss: 0.511\n",
      "Epoch 18291, Loss: 16.068, Final Batch Loss: 0.478\n",
      "Epoch 18292, Loss: 16.211, Final Batch Loss: 0.501\n",
      "Epoch 18293, Loss: 16.127, Final Batch Loss: 0.385\n",
      "Epoch 18294, Loss: 15.996, Final Batch Loss: 0.412\n",
      "Epoch 18295, Loss: 16.215, Final Batch Loss: 0.489\n",
      "Epoch 18296, Loss: 16.180, Final Batch Loss: 0.535\n",
      "Epoch 18297, Loss: 16.054, Final Batch Loss: 0.489\n",
      "Epoch 18298, Loss: 16.146, Final Batch Loss: 0.459\n",
      "Epoch 18299, Loss: 16.305, Final Batch Loss: 0.589\n",
      "Epoch 18300, Loss: 15.944, Final Batch Loss: 0.368\n",
      "Epoch 18301, Loss: 16.121, Final Batch Loss: 0.461\n",
      "Epoch 18302, Loss: 16.103, Final Batch Loss: 0.517\n",
      "Epoch 18303, Loss: 15.940, Final Batch Loss: 0.400\n",
      "Epoch 18304, Loss: 16.161, Final Batch Loss: 0.547\n",
      "Epoch 18305, Loss: 16.024, Final Batch Loss: 0.510\n",
      "Epoch 18306, Loss: 15.928, Final Batch Loss: 0.516\n",
      "Epoch 18307, Loss: 16.175, Final Batch Loss: 0.509\n",
      "Epoch 18308, Loss: 15.954, Final Batch Loss: 0.389\n",
      "Epoch 18309, Loss: 16.211, Final Batch Loss: 0.475\n",
      "Epoch 18310, Loss: 16.189, Final Batch Loss: 0.552\n",
      "Epoch 18311, Loss: 15.969, Final Batch Loss: 0.490\n",
      "Epoch 18312, Loss: 16.177, Final Batch Loss: 0.466\n",
      "Epoch 18313, Loss: 15.772, Final Batch Loss: 0.460\n",
      "Epoch 18314, Loss: 16.062, Final Batch Loss: 0.475\n",
      "Epoch 18315, Loss: 15.930, Final Batch Loss: 0.477\n",
      "Epoch 18316, Loss: 16.136, Final Batch Loss: 0.526\n",
      "Epoch 18317, Loss: 16.218, Final Batch Loss: 0.472\n",
      "Epoch 18318, Loss: 16.354, Final Batch Loss: 0.570\n",
      "Epoch 18319, Loss: 16.008, Final Batch Loss: 0.418\n",
      "Epoch 18320, Loss: 15.913, Final Batch Loss: 0.433\n",
      "Epoch 18321, Loss: 16.328, Final Batch Loss: 0.377\n",
      "Epoch 18322, Loss: 15.854, Final Batch Loss: 0.384\n",
      "Epoch 18323, Loss: 16.373, Final Batch Loss: 0.526\n",
      "Epoch 18324, Loss: 16.089, Final Batch Loss: 0.479\n",
      "Epoch 18325, Loss: 15.630, Final Batch Loss: 0.365\n",
      "Epoch 18326, Loss: 15.928, Final Batch Loss: 0.488\n",
      "Epoch 18327, Loss: 15.832, Final Batch Loss: 0.478\n",
      "Epoch 18328, Loss: 16.035, Final Batch Loss: 0.446\n",
      "Epoch 18329, Loss: 16.214, Final Batch Loss: 0.456\n",
      "Epoch 18330, Loss: 16.038, Final Batch Loss: 0.474\n",
      "Epoch 18331, Loss: 15.905, Final Batch Loss: 0.399\n",
      "Epoch 18332, Loss: 16.131, Final Batch Loss: 0.506\n",
      "Epoch 18333, Loss: 16.026, Final Batch Loss: 0.447\n",
      "Epoch 18334, Loss: 16.156, Final Batch Loss: 0.460\n",
      "Epoch 18335, Loss: 16.151, Final Batch Loss: 0.470\n",
      "Epoch 18336, Loss: 16.107, Final Batch Loss: 0.324\n",
      "Epoch 18337, Loss: 16.014, Final Batch Loss: 0.382\n",
      "Epoch 18338, Loss: 16.049, Final Batch Loss: 0.550\n",
      "Epoch 18339, Loss: 16.107, Final Batch Loss: 0.416\n",
      "Epoch 18340, Loss: 16.179, Final Batch Loss: 0.354\n",
      "Epoch 18341, Loss: 16.065, Final Batch Loss: 0.408\n",
      "Epoch 18342, Loss: 15.962, Final Batch Loss: 0.398\n",
      "Epoch 18343, Loss: 16.236, Final Batch Loss: 0.512\n",
      "Epoch 18344, Loss: 16.061, Final Batch Loss: 0.479\n",
      "Epoch 18345, Loss: 15.790, Final Batch Loss: 0.427\n",
      "Epoch 18346, Loss: 16.156, Final Batch Loss: 0.579\n",
      "Epoch 18347, Loss: 16.059, Final Batch Loss: 0.448\n",
      "Epoch 18348, Loss: 15.800, Final Batch Loss: 0.441\n",
      "Epoch 18349, Loss: 16.127, Final Batch Loss: 0.413\n",
      "Epoch 18350, Loss: 16.176, Final Batch Loss: 0.502\n",
      "Epoch 18351, Loss: 16.075, Final Batch Loss: 0.484\n",
      "Epoch 18352, Loss: 16.084, Final Batch Loss: 0.451\n",
      "Epoch 18353, Loss: 15.961, Final Batch Loss: 0.437\n",
      "Epoch 18354, Loss: 16.334, Final Batch Loss: 0.499\n",
      "Epoch 18355, Loss: 16.306, Final Batch Loss: 0.376\n",
      "Epoch 18356, Loss: 15.926, Final Batch Loss: 0.362\n",
      "Epoch 18357, Loss: 16.018, Final Batch Loss: 0.436\n",
      "Epoch 18358, Loss: 16.000, Final Batch Loss: 0.469\n",
      "Epoch 18359, Loss: 16.203, Final Batch Loss: 0.521\n",
      "Epoch 18360, Loss: 16.335, Final Batch Loss: 0.481\n",
      "Epoch 18361, Loss: 15.964, Final Batch Loss: 0.459\n",
      "Epoch 18362, Loss: 15.967, Final Batch Loss: 0.466\n",
      "Epoch 18363, Loss: 15.977, Final Batch Loss: 0.430\n",
      "Epoch 18364, Loss: 16.138, Final Batch Loss: 0.427\n",
      "Epoch 18365, Loss: 16.105, Final Batch Loss: 0.369\n",
      "Epoch 18366, Loss: 16.073, Final Batch Loss: 0.427\n",
      "Epoch 18367, Loss: 15.860, Final Batch Loss: 0.380\n",
      "Epoch 18368, Loss: 15.991, Final Batch Loss: 0.424\n",
      "Epoch 18369, Loss: 15.823, Final Batch Loss: 0.392\n",
      "Epoch 18370, Loss: 15.819, Final Batch Loss: 0.353\n",
      "Epoch 18371, Loss: 16.010, Final Batch Loss: 0.482\n",
      "Epoch 18372, Loss: 15.987, Final Batch Loss: 0.427\n",
      "Epoch 18373, Loss: 15.782, Final Batch Loss: 0.308\n",
      "Epoch 18374, Loss: 16.227, Final Batch Loss: 0.471\n",
      "Epoch 18375, Loss: 15.845, Final Batch Loss: 0.450\n",
      "Epoch 18376, Loss: 15.932, Final Batch Loss: 0.489\n",
      "Epoch 18377, Loss: 16.257, Final Batch Loss: 0.438\n",
      "Epoch 18378, Loss: 16.031, Final Batch Loss: 0.438\n",
      "Epoch 18379, Loss: 16.125, Final Batch Loss: 0.438\n",
      "Epoch 18380, Loss: 16.075, Final Batch Loss: 0.489\n",
      "Epoch 18381, Loss: 15.907, Final Batch Loss: 0.373\n",
      "Epoch 18382, Loss: 16.112, Final Batch Loss: 0.503\n",
      "Epoch 18383, Loss: 16.096, Final Batch Loss: 0.490\n",
      "Epoch 18384, Loss: 15.838, Final Batch Loss: 0.391\n",
      "Epoch 18385, Loss: 16.192, Final Batch Loss: 0.497\n",
      "Epoch 18386, Loss: 15.915, Final Batch Loss: 0.487\n",
      "Epoch 18387, Loss: 16.158, Final Batch Loss: 0.341\n",
      "Epoch 18388, Loss: 15.904, Final Batch Loss: 0.394\n",
      "Epoch 18389, Loss: 16.224, Final Batch Loss: 0.532\n",
      "Epoch 18390, Loss: 16.147, Final Batch Loss: 0.484\n",
      "Epoch 18391, Loss: 16.009, Final Batch Loss: 0.427\n",
      "Epoch 18392, Loss: 15.996, Final Batch Loss: 0.535\n",
      "Epoch 18393, Loss: 15.845, Final Batch Loss: 0.457\n",
      "Epoch 18394, Loss: 16.124, Final Batch Loss: 0.580\n",
      "Epoch 18395, Loss: 15.993, Final Batch Loss: 0.442\n",
      "Epoch 18396, Loss: 16.061, Final Batch Loss: 0.562\n",
      "Epoch 18397, Loss: 16.346, Final Batch Loss: 0.440\n",
      "Epoch 18398, Loss: 16.041, Final Batch Loss: 0.387\n",
      "Epoch 18399, Loss: 16.000, Final Batch Loss: 0.330\n",
      "Epoch 18400, Loss: 15.758, Final Batch Loss: 0.396\n",
      "Epoch 18401, Loss: 16.002, Final Batch Loss: 0.477\n",
      "Epoch 18402, Loss: 16.098, Final Batch Loss: 0.436\n",
      "Epoch 18403, Loss: 16.000, Final Batch Loss: 0.543\n",
      "Epoch 18404, Loss: 16.004, Final Batch Loss: 0.416\n",
      "Epoch 18405, Loss: 15.969, Final Batch Loss: 0.384\n",
      "Epoch 18406, Loss: 15.988, Final Batch Loss: 0.399\n",
      "Epoch 18407, Loss: 16.189, Final Batch Loss: 0.432\n",
      "Epoch 18408, Loss: 15.831, Final Batch Loss: 0.300\n",
      "Epoch 18409, Loss: 16.016, Final Batch Loss: 0.395\n",
      "Epoch 18410, Loss: 16.071, Final Batch Loss: 0.465\n",
      "Epoch 18411, Loss: 16.062, Final Batch Loss: 0.518\n",
      "Epoch 18412, Loss: 15.806, Final Batch Loss: 0.401\n",
      "Epoch 18413, Loss: 16.128, Final Batch Loss: 0.410\n",
      "Epoch 18414, Loss: 15.850, Final Batch Loss: 0.523\n",
      "Epoch 18415, Loss: 16.003, Final Batch Loss: 0.570\n",
      "Epoch 18416, Loss: 15.867, Final Batch Loss: 0.405\n",
      "Epoch 18417, Loss: 16.125, Final Batch Loss: 0.444\n",
      "Epoch 18418, Loss: 16.097, Final Batch Loss: 0.645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18419, Loss: 15.904, Final Batch Loss: 0.441\n",
      "Epoch 18420, Loss: 16.090, Final Batch Loss: 0.351\n",
      "Epoch 18421, Loss: 16.085, Final Batch Loss: 0.297\n",
      "Epoch 18422, Loss: 16.170, Final Batch Loss: 0.408\n",
      "Epoch 18423, Loss: 16.040, Final Batch Loss: 0.532\n",
      "Epoch 18424, Loss: 15.881, Final Batch Loss: 0.356\n",
      "Epoch 18425, Loss: 16.048, Final Batch Loss: 0.373\n",
      "Epoch 18426, Loss: 16.040, Final Batch Loss: 0.356\n",
      "Epoch 18427, Loss: 15.933, Final Batch Loss: 0.415\n",
      "Epoch 18428, Loss: 15.955, Final Batch Loss: 0.446\n",
      "Epoch 18429, Loss: 16.084, Final Batch Loss: 0.455\n",
      "Epoch 18430, Loss: 16.109, Final Batch Loss: 0.449\n",
      "Epoch 18431, Loss: 15.987, Final Batch Loss: 0.434\n",
      "Epoch 18432, Loss: 15.975, Final Batch Loss: 0.424\n",
      "Epoch 18433, Loss: 15.735, Final Batch Loss: 0.411\n",
      "Epoch 18434, Loss: 16.137, Final Batch Loss: 0.376\n",
      "Epoch 18435, Loss: 16.020, Final Batch Loss: 0.409\n",
      "Epoch 18436, Loss: 15.913, Final Batch Loss: 0.416\n",
      "Epoch 18437, Loss: 15.726, Final Batch Loss: 0.355\n",
      "Epoch 18438, Loss: 15.918, Final Batch Loss: 0.423\n",
      "Epoch 18439, Loss: 15.981, Final Batch Loss: 0.434\n",
      "Epoch 18440, Loss: 15.893, Final Batch Loss: 0.442\n",
      "Epoch 18441, Loss: 15.826, Final Batch Loss: 0.428\n",
      "Epoch 18442, Loss: 15.949, Final Batch Loss: 0.512\n",
      "Epoch 18443, Loss: 15.960, Final Batch Loss: 0.437\n",
      "Epoch 18444, Loss: 15.765, Final Batch Loss: 0.344\n",
      "Epoch 18445, Loss: 15.989, Final Batch Loss: 0.467\n",
      "Epoch 18446, Loss: 15.879, Final Batch Loss: 0.511\n",
      "Epoch 18447, Loss: 16.201, Final Batch Loss: 0.526\n",
      "Epoch 18448, Loss: 15.820, Final Batch Loss: 0.406\n",
      "Epoch 18449, Loss: 16.259, Final Batch Loss: 0.499\n",
      "Epoch 18450, Loss: 16.091, Final Batch Loss: 0.346\n",
      "Epoch 18451, Loss: 16.327, Final Batch Loss: 0.575\n",
      "Epoch 18452, Loss: 15.600, Final Batch Loss: 0.409\n",
      "Epoch 18453, Loss: 15.843, Final Batch Loss: 0.504\n",
      "Epoch 18454, Loss: 15.856, Final Batch Loss: 0.562\n",
      "Epoch 18455, Loss: 15.819, Final Batch Loss: 0.479\n",
      "Epoch 18456, Loss: 15.948, Final Batch Loss: 0.438\n",
      "Epoch 18457, Loss: 16.271, Final Batch Loss: 0.435\n",
      "Epoch 18458, Loss: 15.897, Final Batch Loss: 0.389\n",
      "Epoch 18459, Loss: 15.915, Final Batch Loss: 0.435\n",
      "Epoch 18460, Loss: 16.150, Final Batch Loss: 0.372\n",
      "Epoch 18461, Loss: 15.996, Final Batch Loss: 0.505\n",
      "Epoch 18462, Loss: 15.946, Final Batch Loss: 0.425\n",
      "Epoch 18463, Loss: 16.152, Final Batch Loss: 0.438\n",
      "Epoch 18464, Loss: 15.981, Final Batch Loss: 0.385\n",
      "Epoch 18465, Loss: 16.003, Final Batch Loss: 0.339\n",
      "Epoch 18466, Loss: 15.952, Final Batch Loss: 0.501\n",
      "Epoch 18467, Loss: 15.949, Final Batch Loss: 0.480\n",
      "Epoch 18468, Loss: 16.013, Final Batch Loss: 0.438\n",
      "Epoch 18469, Loss: 15.941, Final Batch Loss: 0.390\n",
      "Epoch 18470, Loss: 16.056, Final Batch Loss: 0.394\n",
      "Epoch 18471, Loss: 15.806, Final Batch Loss: 0.412\n",
      "Epoch 18472, Loss: 15.930, Final Batch Loss: 0.366\n",
      "Epoch 18473, Loss: 15.983, Final Batch Loss: 0.432\n",
      "Epoch 18474, Loss: 16.193, Final Batch Loss: 0.509\n",
      "Epoch 18475, Loss: 15.787, Final Batch Loss: 0.440\n",
      "Epoch 18476, Loss: 15.917, Final Batch Loss: 0.501\n",
      "Epoch 18477, Loss: 16.260, Final Batch Loss: 0.401\n",
      "Epoch 18478, Loss: 16.089, Final Batch Loss: 0.375\n",
      "Epoch 18479, Loss: 15.628, Final Batch Loss: 0.390\n",
      "Epoch 18480, Loss: 16.138, Final Batch Loss: 0.454\n",
      "Epoch 18481, Loss: 16.055, Final Batch Loss: 0.438\n",
      "Epoch 18482, Loss: 16.055, Final Batch Loss: 0.444\n",
      "Epoch 18483, Loss: 16.324, Final Batch Loss: 0.401\n",
      "Epoch 18484, Loss: 16.163, Final Batch Loss: 0.444\n",
      "Epoch 18485, Loss: 15.955, Final Batch Loss: 0.416\n",
      "Epoch 18486, Loss: 16.258, Final Batch Loss: 0.429\n",
      "Epoch 18487, Loss: 15.905, Final Batch Loss: 0.419\n",
      "Epoch 18488, Loss: 16.056, Final Batch Loss: 0.499\n",
      "Epoch 18489, Loss: 15.985, Final Batch Loss: 0.464\n",
      "Epoch 18490, Loss: 16.100, Final Batch Loss: 0.363\n",
      "Epoch 18491, Loss: 15.981, Final Batch Loss: 0.458\n",
      "Epoch 18492, Loss: 15.872, Final Batch Loss: 0.477\n",
      "Epoch 18493, Loss: 15.883, Final Batch Loss: 0.517\n",
      "Epoch 18494, Loss: 15.829, Final Batch Loss: 0.433\n",
      "Epoch 18495, Loss: 15.741, Final Batch Loss: 0.375\n",
      "Epoch 18496, Loss: 15.948, Final Batch Loss: 0.460\n",
      "Epoch 18497, Loss: 16.114, Final Batch Loss: 0.477\n",
      "Epoch 18498, Loss: 16.009, Final Batch Loss: 0.410\n",
      "Epoch 18499, Loss: 16.140, Final Batch Loss: 0.543\n",
      "Epoch 18500, Loss: 15.986, Final Batch Loss: 0.400\n",
      "Epoch 18501, Loss: 16.309, Final Batch Loss: 0.544\n",
      "Epoch 18502, Loss: 16.172, Final Batch Loss: 0.498\n",
      "Epoch 18503, Loss: 15.862, Final Batch Loss: 0.324\n",
      "Epoch 18504, Loss: 16.185, Final Batch Loss: 0.458\n",
      "Epoch 18505, Loss: 15.904, Final Batch Loss: 0.462\n",
      "Epoch 18506, Loss: 15.923, Final Batch Loss: 0.443\n",
      "Epoch 18507, Loss: 15.988, Final Batch Loss: 0.539\n",
      "Epoch 18508, Loss: 15.985, Final Batch Loss: 0.539\n",
      "Epoch 18509, Loss: 15.970, Final Batch Loss: 0.448\n",
      "Epoch 18510, Loss: 15.757, Final Batch Loss: 0.437\n",
      "Epoch 18511, Loss: 15.968, Final Batch Loss: 0.427\n",
      "Epoch 18512, Loss: 16.078, Final Batch Loss: 0.513\n",
      "Epoch 18513, Loss: 16.243, Final Batch Loss: 0.509\n",
      "Epoch 18514, Loss: 16.131, Final Batch Loss: 0.549\n",
      "Epoch 18515, Loss: 16.072, Final Batch Loss: 0.487\n",
      "Epoch 18516, Loss: 15.911, Final Batch Loss: 0.429\n",
      "Epoch 18517, Loss: 16.052, Final Batch Loss: 0.444\n",
      "Epoch 18518, Loss: 16.033, Final Batch Loss: 0.500\n",
      "Epoch 18519, Loss: 15.860, Final Batch Loss: 0.369\n",
      "Epoch 18520, Loss: 15.903, Final Batch Loss: 0.408\n",
      "Epoch 18521, Loss: 15.989, Final Batch Loss: 0.478\n",
      "Epoch 18522, Loss: 15.811, Final Batch Loss: 0.449\n",
      "Epoch 18523, Loss: 16.068, Final Batch Loss: 0.406\n",
      "Epoch 18524, Loss: 16.100, Final Batch Loss: 0.468\n",
      "Epoch 18525, Loss: 16.225, Final Batch Loss: 0.435\n",
      "Epoch 18526, Loss: 16.249, Final Batch Loss: 0.491\n",
      "Epoch 18527, Loss: 15.928, Final Batch Loss: 0.440\n",
      "Epoch 18528, Loss: 16.083, Final Batch Loss: 0.449\n",
      "Epoch 18529, Loss: 16.184, Final Batch Loss: 0.419\n",
      "Epoch 18530, Loss: 16.183, Final Batch Loss: 0.463\n",
      "Epoch 18531, Loss: 16.183, Final Batch Loss: 0.392\n",
      "Epoch 18532, Loss: 15.831, Final Batch Loss: 0.484\n",
      "Epoch 18533, Loss: 16.091, Final Batch Loss: 0.520\n",
      "Epoch 18534, Loss: 16.010, Final Batch Loss: 0.417\n",
      "Epoch 18535, Loss: 15.710, Final Batch Loss: 0.400\n",
      "Epoch 18536, Loss: 16.124, Final Batch Loss: 0.478\n",
      "Epoch 18537, Loss: 16.335, Final Batch Loss: 0.599\n",
      "Epoch 18538, Loss: 15.953, Final Batch Loss: 0.484\n",
      "Epoch 18539, Loss: 16.051, Final Batch Loss: 0.477\n",
      "Epoch 18540, Loss: 15.851, Final Batch Loss: 0.458\n",
      "Epoch 18541, Loss: 16.197, Final Batch Loss: 0.392\n",
      "Epoch 18542, Loss: 15.882, Final Batch Loss: 0.467\n",
      "Epoch 18543, Loss: 15.930, Final Batch Loss: 0.452\n",
      "Epoch 18544, Loss: 15.955, Final Batch Loss: 0.416\n",
      "Epoch 18545, Loss: 16.045, Final Batch Loss: 0.407\n",
      "Epoch 18546, Loss: 16.588, Final Batch Loss: 0.419\n",
      "Epoch 18547, Loss: 15.944, Final Batch Loss: 0.399\n",
      "Epoch 18548, Loss: 16.121, Final Batch Loss: 0.470\n",
      "Epoch 18549, Loss: 15.880, Final Batch Loss: 0.424\n",
      "Epoch 18550, Loss: 16.087, Final Batch Loss: 0.413\n",
      "Epoch 18551, Loss: 15.992, Final Batch Loss: 0.538\n",
      "Epoch 18552, Loss: 16.034, Final Batch Loss: 0.405\n",
      "Epoch 18553, Loss: 16.181, Final Batch Loss: 0.472\n",
      "Epoch 18554, Loss: 16.082, Final Batch Loss: 0.496\n",
      "Epoch 18555, Loss: 15.894, Final Batch Loss: 0.386\n",
      "Epoch 18556, Loss: 15.982, Final Batch Loss: 0.414\n",
      "Epoch 18557, Loss: 16.149, Final Batch Loss: 0.396\n",
      "Epoch 18558, Loss: 16.278, Final Batch Loss: 0.463\n",
      "Epoch 18559, Loss: 15.891, Final Batch Loss: 0.330\n",
      "Epoch 18560, Loss: 15.718, Final Batch Loss: 0.411\n",
      "Epoch 18561, Loss: 15.974, Final Batch Loss: 0.485\n",
      "Epoch 18562, Loss: 16.118, Final Batch Loss: 0.466\n",
      "Epoch 18563, Loss: 16.048, Final Batch Loss: 0.354\n",
      "Epoch 18564, Loss: 16.097, Final Batch Loss: 0.377\n",
      "Epoch 18565, Loss: 16.062, Final Batch Loss: 0.386\n",
      "Epoch 18566, Loss: 16.148, Final Batch Loss: 0.486\n",
      "Epoch 18567, Loss: 16.177, Final Batch Loss: 0.443\n",
      "Epoch 18568, Loss: 15.897, Final Batch Loss: 0.419\n",
      "Epoch 18569, Loss: 16.140, Final Batch Loss: 0.504\n",
      "Epoch 18570, Loss: 16.114, Final Batch Loss: 0.488\n",
      "Epoch 18571, Loss: 15.867, Final Batch Loss: 0.422\n",
      "Epoch 18572, Loss: 16.055, Final Batch Loss: 0.480\n",
      "Epoch 18573, Loss: 16.250, Final Batch Loss: 0.461\n",
      "Epoch 18574, Loss: 15.759, Final Batch Loss: 0.374\n",
      "Epoch 18575, Loss: 16.120, Final Batch Loss: 0.458\n",
      "Epoch 18576, Loss: 15.942, Final Batch Loss: 0.398\n",
      "Epoch 18577, Loss: 16.269, Final Batch Loss: 0.456\n",
      "Epoch 18578, Loss: 16.094, Final Batch Loss: 0.383\n",
      "Epoch 18579, Loss: 16.064, Final Batch Loss: 0.459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18580, Loss: 16.001, Final Batch Loss: 0.438\n",
      "Epoch 18581, Loss: 16.402, Final Batch Loss: 0.619\n",
      "Epoch 18582, Loss: 15.939, Final Batch Loss: 0.519\n",
      "Epoch 18583, Loss: 16.209, Final Batch Loss: 0.518\n",
      "Epoch 18584, Loss: 16.064, Final Batch Loss: 0.393\n",
      "Epoch 18585, Loss: 16.048, Final Batch Loss: 0.478\n",
      "Epoch 18586, Loss: 16.092, Final Batch Loss: 0.435\n",
      "Epoch 18587, Loss: 15.925, Final Batch Loss: 0.470\n",
      "Epoch 18588, Loss: 16.089, Final Batch Loss: 0.471\n",
      "Epoch 18589, Loss: 16.204, Final Batch Loss: 0.431\n",
      "Epoch 18590, Loss: 15.960, Final Batch Loss: 0.296\n",
      "Epoch 18591, Loss: 16.199, Final Batch Loss: 0.454\n",
      "Epoch 18592, Loss: 16.179, Final Batch Loss: 0.491\n",
      "Epoch 18593, Loss: 16.100, Final Batch Loss: 0.489\n",
      "Epoch 18594, Loss: 15.952, Final Batch Loss: 0.417\n",
      "Epoch 18595, Loss: 15.943, Final Batch Loss: 0.581\n",
      "Epoch 18596, Loss: 15.751, Final Batch Loss: 0.423\n",
      "Epoch 18597, Loss: 15.761, Final Batch Loss: 0.448\n",
      "Epoch 18598, Loss: 15.897, Final Batch Loss: 0.462\n",
      "Epoch 18599, Loss: 16.178, Final Batch Loss: 0.481\n",
      "Epoch 18600, Loss: 15.998, Final Batch Loss: 0.426\n",
      "Epoch 18601, Loss: 16.068, Final Batch Loss: 0.413\n",
      "Epoch 18602, Loss: 16.148, Final Batch Loss: 0.475\n",
      "Epoch 18603, Loss: 15.956, Final Batch Loss: 0.434\n",
      "Epoch 18604, Loss: 16.174, Final Batch Loss: 0.468\n",
      "Epoch 18605, Loss: 15.875, Final Batch Loss: 0.456\n",
      "Epoch 18606, Loss: 16.108, Final Batch Loss: 0.443\n",
      "Epoch 18607, Loss: 15.914, Final Batch Loss: 0.403\n",
      "Epoch 18608, Loss: 16.109, Final Batch Loss: 0.573\n",
      "Epoch 18609, Loss: 15.684, Final Batch Loss: 0.361\n",
      "Epoch 18610, Loss: 16.030, Final Batch Loss: 0.587\n",
      "Epoch 18611, Loss: 16.041, Final Batch Loss: 0.493\n",
      "Epoch 18612, Loss: 15.868, Final Batch Loss: 0.529\n",
      "Epoch 18613, Loss: 16.023, Final Batch Loss: 0.447\n",
      "Epoch 18614, Loss: 16.246, Final Batch Loss: 0.483\n",
      "Epoch 18615, Loss: 16.047, Final Batch Loss: 0.417\n",
      "Epoch 18616, Loss: 16.162, Final Batch Loss: 0.381\n",
      "Epoch 18617, Loss: 15.926, Final Batch Loss: 0.450\n",
      "Epoch 18618, Loss: 16.092, Final Batch Loss: 0.497\n",
      "Epoch 18619, Loss: 16.015, Final Batch Loss: 0.435\n",
      "Epoch 18620, Loss: 15.710, Final Batch Loss: 0.357\n",
      "Epoch 18621, Loss: 16.058, Final Batch Loss: 0.480\n",
      "Epoch 18622, Loss: 16.049, Final Batch Loss: 0.446\n",
      "Epoch 18623, Loss: 15.992, Final Batch Loss: 0.494\n",
      "Epoch 18624, Loss: 15.801, Final Batch Loss: 0.420\n",
      "Epoch 18625, Loss: 16.142, Final Batch Loss: 0.387\n",
      "Epoch 18626, Loss: 16.093, Final Batch Loss: 0.453\n",
      "Epoch 18627, Loss: 15.785, Final Batch Loss: 0.468\n",
      "Epoch 18628, Loss: 15.989, Final Batch Loss: 0.525\n",
      "Epoch 18629, Loss: 16.118, Final Batch Loss: 0.507\n",
      "Epoch 18630, Loss: 16.087, Final Batch Loss: 0.550\n",
      "Epoch 18631, Loss: 15.977, Final Batch Loss: 0.426\n",
      "Epoch 18632, Loss: 16.127, Final Batch Loss: 0.427\n",
      "Epoch 18633, Loss: 16.206, Final Batch Loss: 0.530\n",
      "Epoch 18634, Loss: 15.912, Final Batch Loss: 0.386\n",
      "Epoch 18635, Loss: 15.918, Final Batch Loss: 0.443\n",
      "Epoch 18636, Loss: 15.911, Final Batch Loss: 0.458\n",
      "Epoch 18637, Loss: 16.231, Final Batch Loss: 0.427\n",
      "Epoch 18638, Loss: 16.105, Final Batch Loss: 0.510\n",
      "Epoch 18639, Loss: 15.904, Final Batch Loss: 0.416\n",
      "Epoch 18640, Loss: 16.348, Final Batch Loss: 0.488\n",
      "Epoch 18641, Loss: 16.007, Final Batch Loss: 0.581\n",
      "Epoch 18642, Loss: 16.127, Final Batch Loss: 0.440\n",
      "Epoch 18643, Loss: 16.176, Final Batch Loss: 0.416\n",
      "Epoch 18644, Loss: 15.891, Final Batch Loss: 0.518\n",
      "Epoch 18645, Loss: 16.123, Final Batch Loss: 0.401\n",
      "Epoch 18646, Loss: 16.088, Final Batch Loss: 0.420\n",
      "Epoch 18647, Loss: 15.983, Final Batch Loss: 0.379\n",
      "Epoch 18648, Loss: 16.384, Final Batch Loss: 0.409\n",
      "Epoch 18649, Loss: 15.762, Final Batch Loss: 0.410\n",
      "Epoch 18650, Loss: 15.947, Final Batch Loss: 0.518\n",
      "Epoch 18651, Loss: 15.963, Final Batch Loss: 0.400\n",
      "Epoch 18652, Loss: 16.002, Final Batch Loss: 0.408\n",
      "Epoch 18653, Loss: 15.967, Final Batch Loss: 0.458\n",
      "Epoch 18654, Loss: 16.212, Final Batch Loss: 0.340\n",
      "Epoch 18655, Loss: 15.961, Final Batch Loss: 0.338\n",
      "Epoch 18656, Loss: 15.941, Final Batch Loss: 0.493\n",
      "Epoch 18657, Loss: 16.264, Final Batch Loss: 0.403\n",
      "Epoch 18658, Loss: 16.043, Final Batch Loss: 0.512\n",
      "Epoch 18659, Loss: 15.691, Final Batch Loss: 0.437\n",
      "Epoch 18660, Loss: 16.113, Final Batch Loss: 0.366\n",
      "Epoch 18661, Loss: 16.373, Final Batch Loss: 0.471\n",
      "Epoch 18662, Loss: 15.894, Final Batch Loss: 0.443\n",
      "Epoch 18663, Loss: 16.192, Final Batch Loss: 0.446\n",
      "Epoch 18664, Loss: 16.177, Final Batch Loss: 0.430\n",
      "Epoch 18665, Loss: 16.085, Final Batch Loss: 0.399\n",
      "Epoch 18666, Loss: 15.793, Final Batch Loss: 0.490\n",
      "Epoch 18667, Loss: 15.866, Final Batch Loss: 0.344\n",
      "Epoch 18668, Loss: 16.261, Final Batch Loss: 0.527\n",
      "Epoch 18669, Loss: 16.131, Final Batch Loss: 0.562\n",
      "Epoch 18670, Loss: 15.995, Final Batch Loss: 0.496\n",
      "Epoch 18671, Loss: 16.287, Final Batch Loss: 0.375\n",
      "Epoch 18672, Loss: 15.900, Final Batch Loss: 0.472\n",
      "Epoch 18673, Loss: 15.776, Final Batch Loss: 0.422\n",
      "Epoch 18674, Loss: 16.047, Final Batch Loss: 0.389\n",
      "Epoch 18675, Loss: 15.903, Final Batch Loss: 0.457\n",
      "Epoch 18676, Loss: 16.139, Final Batch Loss: 0.465\n",
      "Epoch 18677, Loss: 15.865, Final Batch Loss: 0.417\n",
      "Epoch 18678, Loss: 16.218, Final Batch Loss: 0.483\n",
      "Epoch 18679, Loss: 16.048, Final Batch Loss: 0.506\n",
      "Epoch 18680, Loss: 16.021, Final Batch Loss: 0.381\n",
      "Epoch 18681, Loss: 15.985, Final Batch Loss: 0.457\n",
      "Epoch 18682, Loss: 16.053, Final Batch Loss: 0.444\n",
      "Epoch 18683, Loss: 15.805, Final Batch Loss: 0.450\n",
      "Epoch 18684, Loss: 16.154, Final Batch Loss: 0.540\n",
      "Epoch 18685, Loss: 16.311, Final Batch Loss: 0.559\n",
      "Epoch 18686, Loss: 15.836, Final Batch Loss: 0.412\n",
      "Epoch 18687, Loss: 16.072, Final Batch Loss: 0.568\n",
      "Epoch 18688, Loss: 16.049, Final Batch Loss: 0.480\n",
      "Epoch 18689, Loss: 15.935, Final Batch Loss: 0.532\n",
      "Epoch 18690, Loss: 15.844, Final Batch Loss: 0.426\n",
      "Epoch 18691, Loss: 15.978, Final Batch Loss: 0.461\n",
      "Epoch 18692, Loss: 16.059, Final Batch Loss: 0.437\n",
      "Epoch 18693, Loss: 16.103, Final Batch Loss: 0.410\n",
      "Epoch 18694, Loss: 16.128, Final Batch Loss: 0.369\n",
      "Epoch 18695, Loss: 15.993, Final Batch Loss: 0.385\n",
      "Epoch 18696, Loss: 16.008, Final Batch Loss: 0.404\n",
      "Epoch 18697, Loss: 15.948, Final Batch Loss: 0.377\n",
      "Epoch 18698, Loss: 16.017, Final Batch Loss: 0.374\n",
      "Epoch 18699, Loss: 15.853, Final Batch Loss: 0.539\n",
      "Epoch 18700, Loss: 15.789, Final Batch Loss: 0.432\n",
      "Epoch 18701, Loss: 16.279, Final Batch Loss: 0.550\n",
      "Epoch 18702, Loss: 16.234, Final Batch Loss: 0.349\n",
      "Epoch 18703, Loss: 15.943, Final Batch Loss: 0.406\n",
      "Epoch 18704, Loss: 15.664, Final Batch Loss: 0.446\n",
      "Epoch 18705, Loss: 15.836, Final Batch Loss: 0.489\n",
      "Epoch 18706, Loss: 15.617, Final Batch Loss: 0.314\n",
      "Epoch 18707, Loss: 16.262, Final Batch Loss: 0.481\n",
      "Epoch 18708, Loss: 16.124, Final Batch Loss: 0.443\n",
      "Epoch 18709, Loss: 16.283, Final Batch Loss: 0.525\n",
      "Epoch 18710, Loss: 16.354, Final Batch Loss: 0.496\n",
      "Epoch 18711, Loss: 15.894, Final Batch Loss: 0.444\n",
      "Epoch 18712, Loss: 16.129, Final Batch Loss: 0.631\n",
      "Epoch 18713, Loss: 15.962, Final Batch Loss: 0.460\n",
      "Epoch 18714, Loss: 16.095, Final Batch Loss: 0.513\n",
      "Epoch 18715, Loss: 15.814, Final Batch Loss: 0.391\n",
      "Epoch 18716, Loss: 16.091, Final Batch Loss: 0.515\n",
      "Epoch 18717, Loss: 16.334, Final Batch Loss: 0.490\n",
      "Epoch 18718, Loss: 16.046, Final Batch Loss: 0.447\n",
      "Epoch 18719, Loss: 16.086, Final Batch Loss: 0.435\n",
      "Epoch 18720, Loss: 16.037, Final Batch Loss: 0.508\n",
      "Epoch 18721, Loss: 16.023, Final Batch Loss: 0.399\n",
      "Epoch 18722, Loss: 15.955, Final Batch Loss: 0.408\n",
      "Epoch 18723, Loss: 15.994, Final Batch Loss: 0.408\n",
      "Epoch 18724, Loss: 16.206, Final Batch Loss: 0.515\n",
      "Epoch 18725, Loss: 15.901, Final Batch Loss: 0.322\n",
      "Epoch 18726, Loss: 16.103, Final Batch Loss: 0.518\n",
      "Epoch 18727, Loss: 15.956, Final Batch Loss: 0.439\n",
      "Epoch 18728, Loss: 16.107, Final Batch Loss: 0.496\n",
      "Epoch 18729, Loss: 16.021, Final Batch Loss: 0.483\n",
      "Epoch 18730, Loss: 15.853, Final Batch Loss: 0.362\n",
      "Epoch 18731, Loss: 15.713, Final Batch Loss: 0.420\n",
      "Epoch 18732, Loss: 16.115, Final Batch Loss: 0.426\n",
      "Epoch 18733, Loss: 16.211, Final Batch Loss: 0.393\n",
      "Epoch 18734, Loss: 16.057, Final Batch Loss: 0.509\n",
      "Epoch 18735, Loss: 16.150, Final Batch Loss: 0.550\n",
      "Epoch 18736, Loss: 16.277, Final Batch Loss: 0.542\n",
      "Epoch 18737, Loss: 16.051, Final Batch Loss: 0.482\n",
      "Epoch 18738, Loss: 15.948, Final Batch Loss: 0.400\n",
      "Epoch 18739, Loss: 15.946, Final Batch Loss: 0.463\n",
      "Epoch 18740, Loss: 15.907, Final Batch Loss: 0.408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18741, Loss: 16.148, Final Batch Loss: 0.397\n",
      "Epoch 18742, Loss: 16.191, Final Batch Loss: 0.438\n",
      "Epoch 18743, Loss: 16.215, Final Batch Loss: 0.431\n",
      "Epoch 18744, Loss: 16.105, Final Batch Loss: 0.469\n",
      "Epoch 18745, Loss: 16.109, Final Batch Loss: 0.429\n",
      "Epoch 18746, Loss: 16.022, Final Batch Loss: 0.418\n",
      "Epoch 18747, Loss: 15.951, Final Batch Loss: 0.476\n",
      "Epoch 18748, Loss: 15.995, Final Batch Loss: 0.464\n",
      "Epoch 18749, Loss: 16.161, Final Batch Loss: 0.450\n",
      "Epoch 18750, Loss: 16.184, Final Batch Loss: 0.484\n",
      "Epoch 18751, Loss: 15.957, Final Batch Loss: 0.458\n",
      "Epoch 18752, Loss: 16.072, Final Batch Loss: 0.356\n",
      "Epoch 18753, Loss: 15.758, Final Batch Loss: 0.408\n",
      "Epoch 18754, Loss: 15.991, Final Batch Loss: 0.374\n",
      "Epoch 18755, Loss: 15.878, Final Batch Loss: 0.422\n",
      "Epoch 18756, Loss: 15.800, Final Batch Loss: 0.355\n",
      "Epoch 18757, Loss: 16.239, Final Batch Loss: 0.475\n",
      "Epoch 18758, Loss: 16.055, Final Batch Loss: 0.405\n",
      "Epoch 18759, Loss: 16.278, Final Batch Loss: 0.345\n",
      "Epoch 18760, Loss: 16.057, Final Batch Loss: 0.367\n",
      "Epoch 18761, Loss: 16.350, Final Batch Loss: 0.529\n",
      "Epoch 18762, Loss: 15.949, Final Batch Loss: 0.443\n",
      "Epoch 18763, Loss: 16.005, Final Batch Loss: 0.433\n",
      "Epoch 18764, Loss: 16.253, Final Batch Loss: 0.517\n",
      "Epoch 18765, Loss: 16.076, Final Batch Loss: 0.480\n",
      "Epoch 18766, Loss: 16.058, Final Batch Loss: 0.430\n",
      "Epoch 18767, Loss: 15.692, Final Batch Loss: 0.428\n",
      "Epoch 18768, Loss: 15.809, Final Batch Loss: 0.536\n",
      "Epoch 18769, Loss: 16.012, Final Batch Loss: 0.508\n",
      "Epoch 18770, Loss: 16.273, Final Batch Loss: 0.389\n",
      "Epoch 18771, Loss: 15.962, Final Batch Loss: 0.419\n",
      "Epoch 18772, Loss: 15.984, Final Batch Loss: 0.324\n",
      "Epoch 18773, Loss: 16.293, Final Batch Loss: 0.625\n",
      "Epoch 18774, Loss: 15.987, Final Batch Loss: 0.376\n",
      "Epoch 18775, Loss: 15.718, Final Batch Loss: 0.411\n",
      "Epoch 18776, Loss: 15.943, Final Batch Loss: 0.524\n",
      "Epoch 18777, Loss: 16.212, Final Batch Loss: 0.333\n",
      "Epoch 18778, Loss: 15.906, Final Batch Loss: 0.380\n",
      "Epoch 18779, Loss: 15.745, Final Batch Loss: 0.390\n",
      "Epoch 18780, Loss: 16.128, Final Batch Loss: 0.477\n",
      "Epoch 18781, Loss: 15.755, Final Batch Loss: 0.371\n",
      "Epoch 18782, Loss: 16.142, Final Batch Loss: 0.406\n",
      "Epoch 18783, Loss: 15.911, Final Batch Loss: 0.512\n",
      "Epoch 18784, Loss: 15.602, Final Batch Loss: 0.349\n",
      "Epoch 18785, Loss: 16.006, Final Batch Loss: 0.483\n",
      "Epoch 18786, Loss: 15.896, Final Batch Loss: 0.477\n",
      "Epoch 18787, Loss: 16.344, Final Batch Loss: 0.443\n",
      "Epoch 18788, Loss: 16.214, Final Batch Loss: 0.568\n",
      "Epoch 18789, Loss: 16.041, Final Batch Loss: 0.586\n",
      "Epoch 18790, Loss: 15.911, Final Batch Loss: 0.434\n",
      "Epoch 18791, Loss: 16.023, Final Batch Loss: 0.399\n",
      "Epoch 18792, Loss: 15.925, Final Batch Loss: 0.417\n",
      "Epoch 18793, Loss: 16.007, Final Batch Loss: 0.434\n",
      "Epoch 18794, Loss: 15.889, Final Batch Loss: 0.409\n",
      "Epoch 18795, Loss: 16.031, Final Batch Loss: 0.326\n",
      "Epoch 18796, Loss: 16.074, Final Batch Loss: 0.410\n",
      "Epoch 18797, Loss: 16.127, Final Batch Loss: 0.444\n",
      "Epoch 18798, Loss: 16.224, Final Batch Loss: 0.511\n",
      "Epoch 18799, Loss: 16.127, Final Batch Loss: 0.524\n",
      "Epoch 18800, Loss: 16.061, Final Batch Loss: 0.384\n",
      "Epoch 18801, Loss: 15.891, Final Batch Loss: 0.411\n",
      "Epoch 18802, Loss: 15.895, Final Batch Loss: 0.366\n",
      "Epoch 18803, Loss: 15.934, Final Batch Loss: 0.490\n",
      "Epoch 18804, Loss: 16.210, Final Batch Loss: 0.504\n",
      "Epoch 18805, Loss: 15.826, Final Batch Loss: 0.456\n",
      "Epoch 18806, Loss: 16.223, Final Batch Loss: 0.463\n",
      "Epoch 18807, Loss: 16.065, Final Batch Loss: 0.347\n",
      "Epoch 18808, Loss: 15.882, Final Batch Loss: 0.375\n",
      "Epoch 18809, Loss: 16.181, Final Batch Loss: 0.374\n",
      "Epoch 18810, Loss: 15.931, Final Batch Loss: 0.431\n",
      "Epoch 18811, Loss: 16.109, Final Batch Loss: 0.470\n",
      "Epoch 18812, Loss: 16.083, Final Batch Loss: 0.422\n",
      "Epoch 18813, Loss: 15.976, Final Batch Loss: 0.410\n",
      "Epoch 18814, Loss: 16.140, Final Batch Loss: 0.405\n",
      "Epoch 18815, Loss: 16.408, Final Batch Loss: 0.503\n",
      "Epoch 18816, Loss: 16.110, Final Batch Loss: 0.561\n",
      "Epoch 18817, Loss: 16.135, Final Batch Loss: 0.556\n",
      "Epoch 18818, Loss: 15.985, Final Batch Loss: 0.374\n",
      "Epoch 18819, Loss: 16.312, Final Batch Loss: 0.550\n",
      "Epoch 18820, Loss: 15.648, Final Batch Loss: 0.343\n",
      "Epoch 18821, Loss: 15.850, Final Batch Loss: 0.509\n",
      "Epoch 18822, Loss: 15.981, Final Batch Loss: 0.403\n",
      "Epoch 18823, Loss: 16.081, Final Batch Loss: 0.355\n",
      "Epoch 18824, Loss: 16.111, Final Batch Loss: 0.503\n",
      "Epoch 18825, Loss: 16.088, Final Batch Loss: 0.502\n",
      "Epoch 18826, Loss: 15.685, Final Batch Loss: 0.460\n",
      "Epoch 18827, Loss: 16.130, Final Batch Loss: 0.559\n",
      "Epoch 18828, Loss: 16.189, Final Batch Loss: 0.426\n",
      "Epoch 18829, Loss: 16.241, Final Batch Loss: 0.492\n",
      "Epoch 18830, Loss: 15.736, Final Batch Loss: 0.441\n",
      "Epoch 18831, Loss: 15.761, Final Batch Loss: 0.458\n",
      "Epoch 18832, Loss: 15.890, Final Batch Loss: 0.499\n",
      "Epoch 18833, Loss: 15.911, Final Batch Loss: 0.384\n",
      "Epoch 18834, Loss: 15.949, Final Batch Loss: 0.600\n",
      "Epoch 18835, Loss: 15.865, Final Batch Loss: 0.405\n",
      "Epoch 18836, Loss: 15.826, Final Batch Loss: 0.452\n",
      "Epoch 18837, Loss: 15.934, Final Batch Loss: 0.429\n",
      "Epoch 18838, Loss: 15.888, Final Batch Loss: 0.358\n",
      "Epoch 18839, Loss: 16.106, Final Batch Loss: 0.352\n",
      "Epoch 18840, Loss: 16.116, Final Batch Loss: 0.499\n",
      "Epoch 18841, Loss: 15.908, Final Batch Loss: 0.365\n",
      "Epoch 18842, Loss: 16.140, Final Batch Loss: 0.512\n",
      "Epoch 18843, Loss: 15.984, Final Batch Loss: 0.410\n",
      "Epoch 18844, Loss: 16.255, Final Batch Loss: 0.548\n",
      "Epoch 18845, Loss: 16.250, Final Batch Loss: 0.549\n",
      "Epoch 18846, Loss: 15.983, Final Batch Loss: 0.485\n",
      "Epoch 18847, Loss: 15.997, Final Batch Loss: 0.378\n",
      "Epoch 18848, Loss: 15.969, Final Batch Loss: 0.449\n",
      "Epoch 18849, Loss: 16.090, Final Batch Loss: 0.382\n",
      "Epoch 18850, Loss: 16.107, Final Batch Loss: 0.414\n",
      "Epoch 18851, Loss: 15.901, Final Batch Loss: 0.425\n",
      "Epoch 18852, Loss: 15.854, Final Batch Loss: 0.447\n",
      "Epoch 18853, Loss: 16.088, Final Batch Loss: 0.570\n",
      "Epoch 18854, Loss: 16.210, Final Batch Loss: 0.502\n",
      "Epoch 18855, Loss: 15.902, Final Batch Loss: 0.395\n",
      "Epoch 18856, Loss: 15.905, Final Batch Loss: 0.522\n",
      "Epoch 18857, Loss: 16.113, Final Batch Loss: 0.433\n",
      "Epoch 18858, Loss: 15.937, Final Batch Loss: 0.482\n",
      "Epoch 18859, Loss: 16.068, Final Batch Loss: 0.368\n",
      "Epoch 18860, Loss: 16.128, Final Batch Loss: 0.487\n",
      "Epoch 18861, Loss: 15.948, Final Batch Loss: 0.394\n",
      "Epoch 18862, Loss: 15.736, Final Batch Loss: 0.400\n",
      "Epoch 18863, Loss: 16.076, Final Batch Loss: 0.508\n",
      "Epoch 18864, Loss: 16.304, Final Batch Loss: 0.448\n",
      "Epoch 18865, Loss: 16.268, Final Batch Loss: 0.522\n",
      "Epoch 18866, Loss: 15.736, Final Batch Loss: 0.534\n",
      "Epoch 18867, Loss: 16.123, Final Batch Loss: 0.482\n",
      "Epoch 18868, Loss: 16.110, Final Batch Loss: 0.474\n",
      "Epoch 18869, Loss: 15.859, Final Batch Loss: 0.380\n",
      "Epoch 18870, Loss: 15.886, Final Batch Loss: 0.461\n",
      "Epoch 18871, Loss: 16.247, Final Batch Loss: 0.419\n",
      "Epoch 18872, Loss: 16.157, Final Batch Loss: 0.466\n",
      "Epoch 18873, Loss: 15.864, Final Batch Loss: 0.466\n",
      "Epoch 18874, Loss: 15.861, Final Batch Loss: 0.366\n",
      "Epoch 18875, Loss: 16.006, Final Batch Loss: 0.499\n",
      "Epoch 18876, Loss: 15.930, Final Batch Loss: 0.536\n",
      "Epoch 18877, Loss: 16.071, Final Batch Loss: 0.432\n",
      "Epoch 18878, Loss: 16.049, Final Batch Loss: 0.383\n",
      "Epoch 18879, Loss: 16.028, Final Batch Loss: 0.372\n",
      "Epoch 18880, Loss: 15.797, Final Batch Loss: 0.421\n",
      "Epoch 18881, Loss: 15.797, Final Batch Loss: 0.465\n",
      "Epoch 18882, Loss: 16.290, Final Batch Loss: 0.584\n",
      "Epoch 18883, Loss: 16.007, Final Batch Loss: 0.460\n",
      "Epoch 18884, Loss: 16.062, Final Batch Loss: 0.503\n",
      "Epoch 18885, Loss: 15.836, Final Batch Loss: 0.358\n",
      "Epoch 18886, Loss: 15.777, Final Batch Loss: 0.508\n",
      "Epoch 18887, Loss: 15.818, Final Batch Loss: 0.489\n",
      "Epoch 18888, Loss: 15.955, Final Batch Loss: 0.516\n",
      "Epoch 18889, Loss: 15.961, Final Batch Loss: 0.395\n",
      "Epoch 18890, Loss: 16.266, Final Batch Loss: 0.420\n",
      "Epoch 18891, Loss: 15.997, Final Batch Loss: 0.576\n",
      "Epoch 18892, Loss: 16.183, Final Batch Loss: 0.565\n",
      "Epoch 18893, Loss: 16.136, Final Batch Loss: 0.339\n",
      "Epoch 18894, Loss: 15.768, Final Batch Loss: 0.441\n",
      "Epoch 18895, Loss: 16.026, Final Batch Loss: 0.449\n",
      "Epoch 18896, Loss: 15.811, Final Batch Loss: 0.478\n",
      "Epoch 18897, Loss: 15.876, Final Batch Loss: 0.533\n",
      "Epoch 18898, Loss: 16.105, Final Batch Loss: 0.462\n",
      "Epoch 18899, Loss: 15.837, Final Batch Loss: 0.409\n",
      "Epoch 18900, Loss: 16.352, Final Batch Loss: 0.480\n",
      "Epoch 18901, Loss: 16.236, Final Batch Loss: 0.429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18902, Loss: 16.159, Final Batch Loss: 0.479\n",
      "Epoch 18903, Loss: 15.998, Final Batch Loss: 0.505\n",
      "Epoch 18904, Loss: 16.049, Final Batch Loss: 0.398\n",
      "Epoch 18905, Loss: 16.036, Final Batch Loss: 0.416\n",
      "Epoch 18906, Loss: 15.830, Final Batch Loss: 0.444\n",
      "Epoch 18907, Loss: 16.285, Final Batch Loss: 0.316\n",
      "Epoch 18908, Loss: 16.225, Final Batch Loss: 0.385\n",
      "Epoch 18909, Loss: 15.863, Final Batch Loss: 0.423\n",
      "Epoch 18910, Loss: 15.923, Final Batch Loss: 0.375\n",
      "Epoch 18911, Loss: 16.171, Final Batch Loss: 0.585\n",
      "Epoch 18912, Loss: 16.038, Final Batch Loss: 0.380\n",
      "Epoch 18913, Loss: 15.956, Final Batch Loss: 0.378\n",
      "Epoch 18914, Loss: 16.255, Final Batch Loss: 0.454\n",
      "Epoch 18915, Loss: 15.830, Final Batch Loss: 0.496\n",
      "Epoch 18916, Loss: 15.796, Final Batch Loss: 0.406\n",
      "Epoch 18917, Loss: 15.878, Final Batch Loss: 0.470\n",
      "Epoch 18918, Loss: 16.087, Final Batch Loss: 0.521\n",
      "Epoch 18919, Loss: 15.747, Final Batch Loss: 0.501\n",
      "Epoch 18920, Loss: 15.976, Final Batch Loss: 0.568\n",
      "Epoch 18921, Loss: 16.048, Final Batch Loss: 0.402\n",
      "Epoch 18922, Loss: 16.253, Final Batch Loss: 0.447\n",
      "Epoch 18923, Loss: 16.055, Final Batch Loss: 0.459\n",
      "Epoch 18924, Loss: 16.077, Final Batch Loss: 0.383\n",
      "Epoch 18925, Loss: 16.022, Final Batch Loss: 0.446\n",
      "Epoch 18926, Loss: 16.247, Final Batch Loss: 0.626\n",
      "Epoch 18927, Loss: 16.102, Final Batch Loss: 0.448\n",
      "Epoch 18928, Loss: 16.053, Final Batch Loss: 0.464\n",
      "Epoch 18929, Loss: 15.722, Final Batch Loss: 0.442\n",
      "Epoch 18930, Loss: 15.975, Final Batch Loss: 0.416\n",
      "Epoch 18931, Loss: 15.814, Final Batch Loss: 0.445\n",
      "Epoch 18932, Loss: 15.830, Final Batch Loss: 0.414\n",
      "Epoch 18933, Loss: 15.947, Final Batch Loss: 0.470\n",
      "Epoch 18934, Loss: 15.731, Final Batch Loss: 0.401\n",
      "Epoch 18935, Loss: 15.966, Final Batch Loss: 0.436\n",
      "Epoch 18936, Loss: 16.019, Final Batch Loss: 0.470\n",
      "Epoch 18937, Loss: 15.963, Final Batch Loss: 0.454\n",
      "Epoch 18938, Loss: 15.953, Final Batch Loss: 0.452\n",
      "Epoch 18939, Loss: 16.042, Final Batch Loss: 0.477\n",
      "Epoch 18940, Loss: 15.735, Final Batch Loss: 0.426\n",
      "Epoch 18941, Loss: 16.099, Final Batch Loss: 0.381\n",
      "Epoch 18942, Loss: 15.896, Final Batch Loss: 0.445\n",
      "Epoch 18943, Loss: 16.054, Final Batch Loss: 0.371\n",
      "Epoch 18944, Loss: 16.124, Final Batch Loss: 0.361\n",
      "Epoch 18945, Loss: 15.950, Final Batch Loss: 0.422\n",
      "Epoch 18946, Loss: 15.844, Final Batch Loss: 0.455\n",
      "Epoch 18947, Loss: 16.121, Final Batch Loss: 0.500\n",
      "Epoch 18948, Loss: 15.967, Final Batch Loss: 0.459\n",
      "Epoch 18949, Loss: 15.908, Final Batch Loss: 0.526\n",
      "Epoch 18950, Loss: 16.003, Final Batch Loss: 0.397\n",
      "Epoch 18951, Loss: 15.949, Final Batch Loss: 0.526\n",
      "Epoch 18952, Loss: 16.119, Final Batch Loss: 0.476\n",
      "Epoch 18953, Loss: 15.921, Final Batch Loss: 0.497\n",
      "Epoch 18954, Loss: 16.076, Final Batch Loss: 0.437\n",
      "Epoch 18955, Loss: 16.254, Final Batch Loss: 0.429\n",
      "Epoch 18956, Loss: 15.850, Final Batch Loss: 0.547\n",
      "Epoch 18957, Loss: 15.994, Final Batch Loss: 0.421\n",
      "Epoch 18958, Loss: 16.122, Final Batch Loss: 0.429\n",
      "Epoch 18959, Loss: 16.092, Final Batch Loss: 0.435\n",
      "Epoch 18960, Loss: 16.055, Final Batch Loss: 0.466\n",
      "Epoch 18961, Loss: 16.293, Final Batch Loss: 0.518\n",
      "Epoch 18962, Loss: 16.003, Final Batch Loss: 0.424\n",
      "Epoch 18963, Loss: 15.910, Final Batch Loss: 0.413\n",
      "Epoch 18964, Loss: 15.933, Final Batch Loss: 0.446\n",
      "Epoch 18965, Loss: 15.857, Final Batch Loss: 0.420\n",
      "Epoch 18966, Loss: 16.031, Final Batch Loss: 0.518\n",
      "Epoch 18967, Loss: 15.920, Final Batch Loss: 0.460\n",
      "Epoch 18968, Loss: 15.806, Final Batch Loss: 0.347\n",
      "Epoch 18969, Loss: 16.067, Final Batch Loss: 0.388\n",
      "Epoch 18970, Loss: 16.053, Final Batch Loss: 0.442\n",
      "Epoch 18971, Loss: 15.937, Final Batch Loss: 0.450\n",
      "Epoch 18972, Loss: 15.920, Final Batch Loss: 0.380\n",
      "Epoch 18973, Loss: 16.297, Final Batch Loss: 0.505\n",
      "Epoch 18974, Loss: 16.053, Final Batch Loss: 0.453\n",
      "Epoch 18975, Loss: 16.313, Final Batch Loss: 0.452\n",
      "Epoch 18976, Loss: 16.196, Final Batch Loss: 0.603\n",
      "Epoch 18977, Loss: 16.054, Final Batch Loss: 0.568\n",
      "Epoch 18978, Loss: 16.234, Final Batch Loss: 0.512\n",
      "Epoch 18979, Loss: 15.918, Final Batch Loss: 0.448\n",
      "Epoch 18980, Loss: 15.978, Final Batch Loss: 0.480\n",
      "Epoch 18981, Loss: 16.087, Final Batch Loss: 0.421\n",
      "Epoch 18982, Loss: 15.887, Final Batch Loss: 0.469\n",
      "Epoch 18983, Loss: 16.125, Final Batch Loss: 0.462\n",
      "Epoch 18984, Loss: 16.243, Final Batch Loss: 0.387\n",
      "Epoch 18985, Loss: 16.087, Final Batch Loss: 0.577\n",
      "Epoch 18986, Loss: 15.839, Final Batch Loss: 0.412\n",
      "Epoch 18987, Loss: 16.067, Final Batch Loss: 0.494\n",
      "Epoch 18988, Loss: 16.034, Final Batch Loss: 0.332\n",
      "Epoch 18989, Loss: 15.944, Final Batch Loss: 0.468\n",
      "Epoch 18990, Loss: 16.061, Final Batch Loss: 0.445\n",
      "Epoch 18991, Loss: 15.916, Final Batch Loss: 0.434\n",
      "Epoch 18992, Loss: 16.081, Final Batch Loss: 0.453\n",
      "Epoch 18993, Loss: 16.134, Final Batch Loss: 0.473\n",
      "Epoch 18994, Loss: 16.016, Final Batch Loss: 0.382\n",
      "Epoch 18995, Loss: 16.043, Final Batch Loss: 0.469\n",
      "Epoch 18996, Loss: 16.306, Final Batch Loss: 0.431\n",
      "Epoch 18997, Loss: 16.048, Final Batch Loss: 0.470\n",
      "Epoch 18998, Loss: 16.339, Final Batch Loss: 0.452\n",
      "Epoch 18999, Loss: 15.992, Final Batch Loss: 0.387\n",
      "Epoch 19000, Loss: 16.032, Final Batch Loss: 0.364\n",
      "Epoch 19001, Loss: 16.062, Final Batch Loss: 0.491\n",
      "Epoch 19002, Loss: 16.056, Final Batch Loss: 0.461\n",
      "Epoch 19003, Loss: 16.429, Final Batch Loss: 0.427\n",
      "Epoch 19004, Loss: 15.910, Final Batch Loss: 0.492\n",
      "Epoch 19005, Loss: 15.941, Final Batch Loss: 0.448\n",
      "Epoch 19006, Loss: 15.935, Final Batch Loss: 0.437\n",
      "Epoch 19007, Loss: 16.328, Final Batch Loss: 0.555\n",
      "Epoch 19008, Loss: 15.693, Final Batch Loss: 0.354\n",
      "Epoch 19009, Loss: 15.877, Final Batch Loss: 0.417\n",
      "Epoch 19010, Loss: 16.043, Final Batch Loss: 0.380\n",
      "Epoch 19011, Loss: 16.130, Final Batch Loss: 0.440\n",
      "Epoch 19012, Loss: 15.782, Final Batch Loss: 0.414\n",
      "Epoch 19013, Loss: 16.042, Final Batch Loss: 0.471\n",
      "Epoch 19014, Loss: 16.105, Final Batch Loss: 0.471\n",
      "Epoch 19015, Loss: 15.898, Final Batch Loss: 0.459\n",
      "Epoch 19016, Loss: 16.047, Final Batch Loss: 0.509\n",
      "Epoch 19017, Loss: 15.842, Final Batch Loss: 0.517\n",
      "Epoch 19018, Loss: 15.860, Final Batch Loss: 0.436\n",
      "Epoch 19019, Loss: 16.204, Final Batch Loss: 0.488\n",
      "Epoch 19020, Loss: 15.874, Final Batch Loss: 0.475\n",
      "Epoch 19021, Loss: 16.101, Final Batch Loss: 0.459\n",
      "Epoch 19022, Loss: 15.895, Final Batch Loss: 0.383\n",
      "Epoch 19023, Loss: 15.811, Final Batch Loss: 0.443\n",
      "Epoch 19024, Loss: 15.961, Final Batch Loss: 0.415\n",
      "Epoch 19025, Loss: 16.038, Final Batch Loss: 0.493\n",
      "Epoch 19026, Loss: 16.288, Final Batch Loss: 0.377\n",
      "Epoch 19027, Loss: 16.008, Final Batch Loss: 0.429\n",
      "Epoch 19028, Loss: 16.102, Final Batch Loss: 0.525\n",
      "Epoch 19029, Loss: 16.157, Final Batch Loss: 0.450\n",
      "Epoch 19030, Loss: 15.894, Final Batch Loss: 0.435\n",
      "Epoch 19031, Loss: 16.274, Final Batch Loss: 0.465\n",
      "Epoch 19032, Loss: 15.961, Final Batch Loss: 0.323\n",
      "Epoch 19033, Loss: 16.044, Final Batch Loss: 0.456\n",
      "Epoch 19034, Loss: 15.935, Final Batch Loss: 0.341\n",
      "Epoch 19035, Loss: 16.111, Final Batch Loss: 0.481\n",
      "Epoch 19036, Loss: 16.147, Final Batch Loss: 0.472\n",
      "Epoch 19037, Loss: 15.938, Final Batch Loss: 0.426\n",
      "Epoch 19038, Loss: 16.142, Final Batch Loss: 0.350\n",
      "Epoch 19039, Loss: 16.384, Final Batch Loss: 0.593\n",
      "Epoch 19040, Loss: 15.789, Final Batch Loss: 0.403\n",
      "Epoch 19041, Loss: 16.151, Final Batch Loss: 0.524\n",
      "Epoch 19042, Loss: 16.224, Final Batch Loss: 0.440\n",
      "Epoch 19043, Loss: 15.985, Final Batch Loss: 0.512\n",
      "Epoch 19044, Loss: 16.023, Final Batch Loss: 0.509\n",
      "Epoch 19045, Loss: 16.067, Final Batch Loss: 0.490\n",
      "Epoch 19046, Loss: 16.188, Final Batch Loss: 0.366\n",
      "Epoch 19047, Loss: 15.968, Final Batch Loss: 0.488\n",
      "Epoch 19048, Loss: 16.023, Final Batch Loss: 0.589\n",
      "Epoch 19049, Loss: 15.758, Final Batch Loss: 0.391\n",
      "Epoch 19050, Loss: 15.697, Final Batch Loss: 0.490\n",
      "Epoch 19051, Loss: 15.923, Final Batch Loss: 0.430\n",
      "Epoch 19052, Loss: 15.972, Final Batch Loss: 0.423\n",
      "Epoch 19053, Loss: 15.845, Final Batch Loss: 0.365\n",
      "Epoch 19054, Loss: 16.041, Final Batch Loss: 0.429\n",
      "Epoch 19055, Loss: 15.955, Final Batch Loss: 0.425\n",
      "Epoch 19056, Loss: 16.013, Final Batch Loss: 0.496\n",
      "Epoch 19057, Loss: 16.155, Final Batch Loss: 0.506\n",
      "Epoch 19058, Loss: 16.166, Final Batch Loss: 0.405\n",
      "Epoch 19059, Loss: 16.151, Final Batch Loss: 0.498\n",
      "Epoch 19060, Loss: 16.011, Final Batch Loss: 0.450\n",
      "Epoch 19061, Loss: 16.139, Final Batch Loss: 0.451\n",
      "Epoch 19062, Loss: 16.071, Final Batch Loss: 0.454\n",
      "Epoch 19063, Loss: 15.760, Final Batch Loss: 0.464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19064, Loss: 15.912, Final Batch Loss: 0.490\n",
      "Epoch 19065, Loss: 16.224, Final Batch Loss: 0.442\n",
      "Epoch 19066, Loss: 15.645, Final Batch Loss: 0.472\n",
      "Epoch 19067, Loss: 16.156, Final Batch Loss: 0.567\n",
      "Epoch 19068, Loss: 16.004, Final Batch Loss: 0.424\n",
      "Epoch 19069, Loss: 16.234, Final Batch Loss: 0.586\n",
      "Epoch 19070, Loss: 15.740, Final Batch Loss: 0.422\n",
      "Epoch 19071, Loss: 15.971, Final Batch Loss: 0.516\n",
      "Epoch 19072, Loss: 15.895, Final Batch Loss: 0.478\n",
      "Epoch 19073, Loss: 16.274, Final Batch Loss: 0.462\n",
      "Epoch 19074, Loss: 16.162, Final Batch Loss: 0.493\n",
      "Epoch 19075, Loss: 16.013, Final Batch Loss: 0.472\n",
      "Epoch 19076, Loss: 16.030, Final Batch Loss: 0.482\n",
      "Epoch 19077, Loss: 16.139, Final Batch Loss: 0.392\n",
      "Epoch 19078, Loss: 16.103, Final Batch Loss: 0.463\n",
      "Epoch 19079, Loss: 15.903, Final Batch Loss: 0.491\n",
      "Epoch 19080, Loss: 16.030, Final Batch Loss: 0.446\n",
      "Epoch 19081, Loss: 16.073, Final Batch Loss: 0.451\n",
      "Epoch 19082, Loss: 15.874, Final Batch Loss: 0.401\n",
      "Epoch 19083, Loss: 16.263, Final Batch Loss: 0.605\n",
      "Epoch 19084, Loss: 15.934, Final Batch Loss: 0.527\n",
      "Epoch 19085, Loss: 16.222, Final Batch Loss: 0.401\n",
      "Epoch 19086, Loss: 15.920, Final Batch Loss: 0.399\n",
      "Epoch 19087, Loss: 16.103, Final Batch Loss: 0.349\n",
      "Epoch 19088, Loss: 16.115, Final Batch Loss: 0.472\n",
      "Epoch 19089, Loss: 15.941, Final Batch Loss: 0.428\n",
      "Epoch 19090, Loss: 15.999, Final Batch Loss: 0.439\n",
      "Epoch 19091, Loss: 15.871, Final Batch Loss: 0.446\n",
      "Epoch 19092, Loss: 16.021, Final Batch Loss: 0.333\n",
      "Epoch 19093, Loss: 16.028, Final Batch Loss: 0.401\n",
      "Epoch 19094, Loss: 15.948, Final Batch Loss: 0.393\n",
      "Epoch 19095, Loss: 15.972, Final Batch Loss: 0.454\n",
      "Epoch 19096, Loss: 15.913, Final Batch Loss: 0.447\n",
      "Epoch 19097, Loss: 16.099, Final Batch Loss: 0.421\n",
      "Epoch 19098, Loss: 16.008, Final Batch Loss: 0.386\n",
      "Epoch 19099, Loss: 16.083, Final Batch Loss: 0.399\n",
      "Epoch 19100, Loss: 16.177, Final Batch Loss: 0.483\n",
      "Epoch 19101, Loss: 16.096, Final Batch Loss: 0.461\n",
      "Epoch 19102, Loss: 15.740, Final Batch Loss: 0.378\n",
      "Epoch 19103, Loss: 15.972, Final Batch Loss: 0.390\n",
      "Epoch 19104, Loss: 15.976, Final Batch Loss: 0.509\n",
      "Epoch 19105, Loss: 16.141, Final Batch Loss: 0.493\n",
      "Epoch 19106, Loss: 15.792, Final Batch Loss: 0.327\n",
      "Epoch 19107, Loss: 15.925, Final Batch Loss: 0.392\n",
      "Epoch 19108, Loss: 15.927, Final Batch Loss: 0.518\n",
      "Epoch 19109, Loss: 16.210, Final Batch Loss: 0.530\n",
      "Epoch 19110, Loss: 15.897, Final Batch Loss: 0.532\n",
      "Epoch 19111, Loss: 16.146, Final Batch Loss: 0.623\n",
      "Epoch 19112, Loss: 16.028, Final Batch Loss: 0.476\n",
      "Epoch 19113, Loss: 15.810, Final Batch Loss: 0.558\n",
      "Epoch 19114, Loss: 15.960, Final Batch Loss: 0.518\n",
      "Epoch 19115, Loss: 16.146, Final Batch Loss: 0.432\n",
      "Epoch 19116, Loss: 16.010, Final Batch Loss: 0.442\n",
      "Epoch 19117, Loss: 15.884, Final Batch Loss: 0.483\n",
      "Epoch 19118, Loss: 15.836, Final Batch Loss: 0.433\n",
      "Epoch 19119, Loss: 16.026, Final Batch Loss: 0.478\n",
      "Epoch 19120, Loss: 16.025, Final Batch Loss: 0.503\n",
      "Epoch 19121, Loss: 16.096, Final Batch Loss: 0.357\n",
      "Epoch 19122, Loss: 15.859, Final Batch Loss: 0.382\n",
      "Epoch 19123, Loss: 15.802, Final Batch Loss: 0.415\n",
      "Epoch 19124, Loss: 15.888, Final Batch Loss: 0.417\n",
      "Epoch 19125, Loss: 16.089, Final Batch Loss: 0.443\n",
      "Epoch 19126, Loss: 16.016, Final Batch Loss: 0.444\n",
      "Epoch 19127, Loss: 15.699, Final Batch Loss: 0.445\n",
      "Epoch 19128, Loss: 16.004, Final Batch Loss: 0.433\n",
      "Epoch 19129, Loss: 16.098, Final Batch Loss: 0.524\n",
      "Epoch 19130, Loss: 15.836, Final Batch Loss: 0.421\n",
      "Epoch 19131, Loss: 16.037, Final Batch Loss: 0.499\n",
      "Epoch 19132, Loss: 15.989, Final Batch Loss: 0.356\n",
      "Epoch 19133, Loss: 15.847, Final Batch Loss: 0.456\n",
      "Epoch 19134, Loss: 15.851, Final Batch Loss: 0.429\n",
      "Epoch 19135, Loss: 15.780, Final Batch Loss: 0.435\n",
      "Epoch 19136, Loss: 16.017, Final Batch Loss: 0.462\n",
      "Epoch 19137, Loss: 16.221, Final Batch Loss: 0.387\n",
      "Epoch 19138, Loss: 15.999, Final Batch Loss: 0.461\n",
      "Epoch 19139, Loss: 16.093, Final Batch Loss: 0.419\n",
      "Epoch 19140, Loss: 16.032, Final Batch Loss: 0.456\n",
      "Epoch 19141, Loss: 15.797, Final Batch Loss: 0.425\n",
      "Epoch 19142, Loss: 16.189, Final Batch Loss: 0.381\n",
      "Epoch 19143, Loss: 16.227, Final Batch Loss: 0.538\n",
      "Epoch 19144, Loss: 16.427, Final Batch Loss: 0.525\n",
      "Epoch 19145, Loss: 15.999, Final Batch Loss: 0.424\n",
      "Epoch 19146, Loss: 16.385, Final Batch Loss: 0.490\n",
      "Epoch 19147, Loss: 15.948, Final Batch Loss: 0.566\n",
      "Epoch 19148, Loss: 15.840, Final Batch Loss: 0.392\n",
      "Epoch 19149, Loss: 16.031, Final Batch Loss: 0.372\n",
      "Epoch 19150, Loss: 16.070, Final Batch Loss: 0.498\n",
      "Epoch 19151, Loss: 15.781, Final Batch Loss: 0.475\n",
      "Epoch 19152, Loss: 16.252, Final Batch Loss: 0.480\n",
      "Epoch 19153, Loss: 15.867, Final Batch Loss: 0.417\n",
      "Epoch 19154, Loss: 15.968, Final Batch Loss: 0.327\n",
      "Epoch 19155, Loss: 16.275, Final Batch Loss: 0.524\n",
      "Epoch 19156, Loss: 15.860, Final Batch Loss: 0.395\n",
      "Epoch 19157, Loss: 16.058, Final Batch Loss: 0.398\n",
      "Epoch 19158, Loss: 16.195, Final Batch Loss: 0.483\n",
      "Epoch 19159, Loss: 15.899, Final Batch Loss: 0.414\n",
      "Epoch 19160, Loss: 16.021, Final Batch Loss: 0.418\n",
      "Epoch 19161, Loss: 16.025, Final Batch Loss: 0.501\n",
      "Epoch 19162, Loss: 15.979, Final Batch Loss: 0.343\n",
      "Epoch 19163, Loss: 15.996, Final Batch Loss: 0.457\n",
      "Epoch 19164, Loss: 15.869, Final Batch Loss: 0.385\n",
      "Epoch 19165, Loss: 16.084, Final Batch Loss: 0.400\n",
      "Epoch 19166, Loss: 16.022, Final Batch Loss: 0.478\n",
      "Epoch 19167, Loss: 16.117, Final Batch Loss: 0.459\n",
      "Epoch 19168, Loss: 16.077, Final Batch Loss: 0.431\n",
      "Epoch 19169, Loss: 15.950, Final Batch Loss: 0.438\n",
      "Epoch 19170, Loss: 15.943, Final Batch Loss: 0.423\n",
      "Epoch 19171, Loss: 16.024, Final Batch Loss: 0.411\n",
      "Epoch 19172, Loss: 15.874, Final Batch Loss: 0.434\n",
      "Epoch 19173, Loss: 15.699, Final Batch Loss: 0.444\n",
      "Epoch 19174, Loss: 15.808, Final Batch Loss: 0.505\n",
      "Epoch 19175, Loss: 16.060, Final Batch Loss: 0.486\n",
      "Epoch 19176, Loss: 15.991, Final Batch Loss: 0.540\n",
      "Epoch 19177, Loss: 15.961, Final Batch Loss: 0.485\n",
      "Epoch 19178, Loss: 16.013, Final Batch Loss: 0.421\n",
      "Epoch 19179, Loss: 15.891, Final Batch Loss: 0.583\n",
      "Epoch 19180, Loss: 16.093, Final Batch Loss: 0.428\n",
      "Epoch 19181, Loss: 16.274, Final Batch Loss: 0.407\n",
      "Epoch 19182, Loss: 15.943, Final Batch Loss: 0.403\n",
      "Epoch 19183, Loss: 15.909, Final Batch Loss: 0.349\n",
      "Epoch 19184, Loss: 16.046, Final Batch Loss: 0.368\n",
      "Epoch 19185, Loss: 15.944, Final Batch Loss: 0.555\n",
      "Epoch 19186, Loss: 15.728, Final Batch Loss: 0.405\n",
      "Epoch 19187, Loss: 15.972, Final Batch Loss: 0.442\n",
      "Epoch 19188, Loss: 15.993, Final Batch Loss: 0.465\n",
      "Epoch 19189, Loss: 16.028, Final Batch Loss: 0.443\n",
      "Epoch 19190, Loss: 16.005, Final Batch Loss: 0.411\n",
      "Epoch 19191, Loss: 16.382, Final Batch Loss: 0.594\n",
      "Epoch 19192, Loss: 15.957, Final Batch Loss: 0.354\n",
      "Epoch 19193, Loss: 16.140, Final Batch Loss: 0.501\n",
      "Epoch 19194, Loss: 16.257, Final Batch Loss: 0.402\n",
      "Epoch 19195, Loss: 16.029, Final Batch Loss: 0.401\n",
      "Epoch 19196, Loss: 15.594, Final Batch Loss: 0.452\n",
      "Epoch 19197, Loss: 16.251, Final Batch Loss: 0.399\n",
      "Epoch 19198, Loss: 16.049, Final Batch Loss: 0.458\n",
      "Epoch 19199, Loss: 15.742, Final Batch Loss: 0.378\n",
      "Epoch 19200, Loss: 15.877, Final Batch Loss: 0.376\n",
      "Epoch 19201, Loss: 16.011, Final Batch Loss: 0.541\n",
      "Epoch 19202, Loss: 15.770, Final Batch Loss: 0.527\n",
      "Epoch 19203, Loss: 16.057, Final Batch Loss: 0.451\n",
      "Epoch 19204, Loss: 15.751, Final Batch Loss: 0.325\n",
      "Epoch 19205, Loss: 16.049, Final Batch Loss: 0.475\n",
      "Epoch 19206, Loss: 15.951, Final Batch Loss: 0.468\n",
      "Epoch 19207, Loss: 15.935, Final Batch Loss: 0.457\n",
      "Epoch 19208, Loss: 16.242, Final Batch Loss: 0.419\n",
      "Epoch 19209, Loss: 16.289, Final Batch Loss: 0.459\n",
      "Epoch 19210, Loss: 15.882, Final Batch Loss: 0.384\n",
      "Epoch 19211, Loss: 15.846, Final Batch Loss: 0.398\n",
      "Epoch 19212, Loss: 16.193, Final Batch Loss: 0.432\n",
      "Epoch 19213, Loss: 16.089, Final Batch Loss: 0.347\n",
      "Epoch 19214, Loss: 15.963, Final Batch Loss: 0.316\n",
      "Epoch 19215, Loss: 16.014, Final Batch Loss: 0.382\n",
      "Epoch 19216, Loss: 15.778, Final Batch Loss: 0.450\n",
      "Epoch 19217, Loss: 16.035, Final Batch Loss: 0.417\n",
      "Epoch 19218, Loss: 16.250, Final Batch Loss: 0.474\n",
      "Epoch 19219, Loss: 16.184, Final Batch Loss: 0.432\n",
      "Epoch 19220, Loss: 16.313, Final Batch Loss: 0.519\n",
      "Epoch 19221, Loss: 15.830, Final Batch Loss: 0.390\n",
      "Epoch 19222, Loss: 15.600, Final Batch Loss: 0.338\n",
      "Epoch 19223, Loss: 16.196, Final Batch Loss: 0.612\n",
      "Epoch 19224, Loss: 16.040, Final Batch Loss: 0.578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19225, Loss: 16.055, Final Batch Loss: 0.495\n",
      "Epoch 19226, Loss: 15.862, Final Batch Loss: 0.375\n",
      "Epoch 19227, Loss: 16.285, Final Batch Loss: 0.454\n",
      "Epoch 19228, Loss: 16.113, Final Batch Loss: 0.389\n",
      "Epoch 19229, Loss: 16.086, Final Batch Loss: 0.330\n",
      "Epoch 19230, Loss: 16.028, Final Batch Loss: 0.379\n",
      "Epoch 19231, Loss: 15.969, Final Batch Loss: 0.525\n",
      "Epoch 19232, Loss: 15.891, Final Batch Loss: 0.469\n",
      "Epoch 19233, Loss: 15.869, Final Batch Loss: 0.463\n",
      "Epoch 19234, Loss: 16.152, Final Batch Loss: 0.442\n",
      "Epoch 19235, Loss: 15.950, Final Batch Loss: 0.453\n",
      "Epoch 19236, Loss: 15.843, Final Batch Loss: 0.475\n",
      "Epoch 19237, Loss: 16.022, Final Batch Loss: 0.490\n",
      "Epoch 19238, Loss: 16.143, Final Batch Loss: 0.471\n",
      "Epoch 19239, Loss: 15.925, Final Batch Loss: 0.410\n",
      "Epoch 19240, Loss: 16.048, Final Batch Loss: 0.436\n",
      "Epoch 19241, Loss: 15.902, Final Batch Loss: 0.382\n",
      "Epoch 19242, Loss: 15.922, Final Batch Loss: 0.415\n",
      "Epoch 19243, Loss: 15.754, Final Batch Loss: 0.325\n",
      "Epoch 19244, Loss: 16.208, Final Batch Loss: 0.598\n",
      "Epoch 19245, Loss: 16.189, Final Batch Loss: 0.466\n",
      "Epoch 19246, Loss: 16.243, Final Batch Loss: 0.456\n",
      "Epoch 19247, Loss: 15.979, Final Batch Loss: 0.498\n",
      "Epoch 19248, Loss: 16.149, Final Batch Loss: 0.523\n",
      "Epoch 19249, Loss: 16.209, Final Batch Loss: 0.474\n",
      "Epoch 19250, Loss: 15.888, Final Batch Loss: 0.314\n",
      "Epoch 19251, Loss: 15.968, Final Batch Loss: 0.489\n",
      "Epoch 19252, Loss: 16.077, Final Batch Loss: 0.472\n",
      "Epoch 19253, Loss: 15.892, Final Batch Loss: 0.432\n",
      "Epoch 19254, Loss: 15.993, Final Batch Loss: 0.491\n",
      "Epoch 19255, Loss: 15.853, Final Batch Loss: 0.506\n",
      "Epoch 19256, Loss: 15.924, Final Batch Loss: 0.404\n",
      "Epoch 19257, Loss: 15.817, Final Batch Loss: 0.448\n",
      "Epoch 19258, Loss: 16.171, Final Batch Loss: 0.474\n",
      "Epoch 19259, Loss: 16.039, Final Batch Loss: 0.470\n",
      "Epoch 19260, Loss: 16.150, Final Batch Loss: 0.417\n",
      "Epoch 19261, Loss: 16.162, Final Batch Loss: 0.430\n",
      "Epoch 19262, Loss: 15.922, Final Batch Loss: 0.453\n",
      "Epoch 19263, Loss: 16.017, Final Batch Loss: 0.434\n",
      "Epoch 19264, Loss: 16.089, Final Batch Loss: 0.526\n",
      "Epoch 19265, Loss: 15.732, Final Batch Loss: 0.496\n",
      "Epoch 19266, Loss: 16.286, Final Batch Loss: 0.507\n",
      "Epoch 19267, Loss: 15.686, Final Batch Loss: 0.377\n",
      "Epoch 19268, Loss: 15.846, Final Batch Loss: 0.393\n",
      "Epoch 19269, Loss: 15.987, Final Batch Loss: 0.444\n",
      "Epoch 19270, Loss: 16.132, Final Batch Loss: 0.396\n",
      "Epoch 19271, Loss: 15.954, Final Batch Loss: 0.382\n",
      "Epoch 19272, Loss: 15.945, Final Batch Loss: 0.316\n",
      "Epoch 19273, Loss: 15.723, Final Batch Loss: 0.370\n",
      "Epoch 19274, Loss: 15.866, Final Batch Loss: 0.352\n",
      "Epoch 19275, Loss: 16.043, Final Batch Loss: 0.418\n",
      "Epoch 19276, Loss: 15.793, Final Batch Loss: 0.332\n",
      "Epoch 19277, Loss: 15.939, Final Batch Loss: 0.430\n",
      "Epoch 19278, Loss: 16.203, Final Batch Loss: 0.594\n",
      "Epoch 19279, Loss: 15.650, Final Batch Loss: 0.388\n",
      "Epoch 19280, Loss: 16.181, Final Batch Loss: 0.429\n",
      "Epoch 19281, Loss: 16.122, Final Batch Loss: 0.524\n",
      "Epoch 19282, Loss: 16.062, Final Batch Loss: 0.434\n",
      "Epoch 19283, Loss: 16.307, Final Batch Loss: 0.453\n",
      "Epoch 19284, Loss: 16.060, Final Batch Loss: 0.373\n",
      "Epoch 19285, Loss: 15.846, Final Batch Loss: 0.543\n",
      "Epoch 19286, Loss: 16.119, Final Batch Loss: 0.430\n",
      "Epoch 19287, Loss: 16.120, Final Batch Loss: 0.389\n",
      "Epoch 19288, Loss: 15.849, Final Batch Loss: 0.407\n",
      "Epoch 19289, Loss: 15.996, Final Batch Loss: 0.399\n",
      "Epoch 19290, Loss: 15.864, Final Batch Loss: 0.408\n",
      "Epoch 19291, Loss: 16.071, Final Batch Loss: 0.418\n",
      "Epoch 19292, Loss: 15.867, Final Batch Loss: 0.400\n",
      "Epoch 19293, Loss: 15.915, Final Batch Loss: 0.497\n",
      "Epoch 19294, Loss: 15.866, Final Batch Loss: 0.362\n",
      "Epoch 19295, Loss: 16.105, Final Batch Loss: 0.443\n",
      "Epoch 19296, Loss: 15.927, Final Batch Loss: 0.372\n",
      "Epoch 19297, Loss: 15.957, Final Batch Loss: 0.471\n",
      "Epoch 19298, Loss: 16.045, Final Batch Loss: 0.412\n",
      "Epoch 19299, Loss: 16.092, Final Batch Loss: 0.376\n",
      "Epoch 19300, Loss: 16.066, Final Batch Loss: 0.415\n",
      "Epoch 19301, Loss: 16.178, Final Batch Loss: 0.545\n",
      "Epoch 19302, Loss: 16.213, Final Batch Loss: 0.478\n",
      "Epoch 19303, Loss: 15.886, Final Batch Loss: 0.389\n",
      "Epoch 19304, Loss: 16.095, Final Batch Loss: 0.491\n",
      "Epoch 19305, Loss: 15.770, Final Batch Loss: 0.428\n",
      "Epoch 19306, Loss: 15.984, Final Batch Loss: 0.389\n",
      "Epoch 19307, Loss: 15.611, Final Batch Loss: 0.394\n",
      "Epoch 19308, Loss: 16.086, Final Batch Loss: 0.459\n",
      "Epoch 19309, Loss: 15.951, Final Batch Loss: 0.453\n",
      "Epoch 19310, Loss: 16.008, Final Batch Loss: 0.372\n",
      "Epoch 19311, Loss: 15.940, Final Batch Loss: 0.438\n",
      "Epoch 19312, Loss: 15.764, Final Batch Loss: 0.429\n",
      "Epoch 19313, Loss: 15.826, Final Batch Loss: 0.412\n",
      "Epoch 19314, Loss: 15.889, Final Batch Loss: 0.428\n",
      "Epoch 19315, Loss: 15.973, Final Batch Loss: 0.395\n",
      "Epoch 19316, Loss: 15.904, Final Batch Loss: 0.399\n",
      "Epoch 19317, Loss: 16.209, Final Batch Loss: 0.349\n",
      "Epoch 19318, Loss: 15.866, Final Batch Loss: 0.317\n",
      "Epoch 19319, Loss: 16.120, Final Batch Loss: 0.423\n",
      "Epoch 19320, Loss: 16.014, Final Batch Loss: 0.415\n",
      "Epoch 19321, Loss: 15.774, Final Batch Loss: 0.380\n",
      "Epoch 19322, Loss: 16.044, Final Batch Loss: 0.442\n",
      "Epoch 19323, Loss: 15.895, Final Batch Loss: 0.385\n",
      "Epoch 19324, Loss: 15.734, Final Batch Loss: 0.371\n",
      "Epoch 19325, Loss: 16.056, Final Batch Loss: 0.499\n",
      "Epoch 19326, Loss: 16.002, Final Batch Loss: 0.371\n",
      "Epoch 19327, Loss: 16.013, Final Batch Loss: 0.521\n",
      "Epoch 19328, Loss: 15.950, Final Batch Loss: 0.329\n",
      "Epoch 19329, Loss: 16.009, Final Batch Loss: 0.414\n",
      "Epoch 19330, Loss: 15.987, Final Batch Loss: 0.468\n",
      "Epoch 19331, Loss: 15.750, Final Batch Loss: 0.365\n",
      "Epoch 19332, Loss: 16.011, Final Batch Loss: 0.462\n",
      "Epoch 19333, Loss: 16.269, Final Batch Loss: 0.516\n",
      "Epoch 19334, Loss: 15.983, Final Batch Loss: 0.447\n",
      "Epoch 19335, Loss: 15.997, Final Batch Loss: 0.402\n",
      "Epoch 19336, Loss: 16.029, Final Batch Loss: 0.530\n",
      "Epoch 19337, Loss: 15.842, Final Batch Loss: 0.445\n",
      "Epoch 19338, Loss: 16.018, Final Batch Loss: 0.464\n",
      "Epoch 19339, Loss: 16.076, Final Batch Loss: 0.470\n",
      "Epoch 19340, Loss: 15.952, Final Batch Loss: 0.412\n",
      "Epoch 19341, Loss: 15.757, Final Batch Loss: 0.375\n",
      "Epoch 19342, Loss: 15.936, Final Batch Loss: 0.484\n",
      "Epoch 19343, Loss: 16.180, Final Batch Loss: 0.481\n",
      "Epoch 19344, Loss: 16.009, Final Batch Loss: 0.525\n",
      "Epoch 19345, Loss: 16.145, Final Batch Loss: 0.405\n",
      "Epoch 19346, Loss: 16.080, Final Batch Loss: 0.449\n",
      "Epoch 19347, Loss: 16.089, Final Batch Loss: 0.539\n",
      "Epoch 19348, Loss: 15.970, Final Batch Loss: 0.490\n",
      "Epoch 19349, Loss: 16.072, Final Batch Loss: 0.482\n",
      "Epoch 19350, Loss: 15.801, Final Batch Loss: 0.420\n",
      "Epoch 19351, Loss: 16.037, Final Batch Loss: 0.476\n",
      "Epoch 19352, Loss: 15.906, Final Batch Loss: 0.416\n",
      "Epoch 19353, Loss: 16.069, Final Batch Loss: 0.510\n",
      "Epoch 19354, Loss: 16.404, Final Batch Loss: 0.492\n",
      "Epoch 19355, Loss: 16.124, Final Batch Loss: 0.530\n",
      "Epoch 19356, Loss: 15.840, Final Batch Loss: 0.365\n",
      "Epoch 19357, Loss: 16.074, Final Batch Loss: 0.329\n",
      "Epoch 19358, Loss: 16.065, Final Batch Loss: 0.444\n",
      "Epoch 19359, Loss: 15.912, Final Batch Loss: 0.415\n",
      "Epoch 19360, Loss: 15.936, Final Batch Loss: 0.446\n",
      "Epoch 19361, Loss: 15.810, Final Batch Loss: 0.450\n",
      "Epoch 19362, Loss: 16.105, Final Batch Loss: 0.504\n",
      "Epoch 19363, Loss: 15.893, Final Batch Loss: 0.456\n",
      "Epoch 19364, Loss: 15.711, Final Batch Loss: 0.393\n",
      "Epoch 19365, Loss: 16.033, Final Batch Loss: 0.549\n",
      "Epoch 19366, Loss: 16.045, Final Batch Loss: 0.461\n",
      "Epoch 19367, Loss: 15.935, Final Batch Loss: 0.395\n",
      "Epoch 19368, Loss: 15.951, Final Batch Loss: 0.422\n",
      "Epoch 19369, Loss: 15.543, Final Batch Loss: 0.380\n",
      "Epoch 19370, Loss: 15.755, Final Batch Loss: 0.414\n",
      "Epoch 19371, Loss: 15.977, Final Batch Loss: 0.396\n",
      "Epoch 19372, Loss: 15.904, Final Batch Loss: 0.491\n",
      "Epoch 19373, Loss: 16.191, Final Batch Loss: 0.443\n",
      "Epoch 19374, Loss: 15.851, Final Batch Loss: 0.472\n",
      "Epoch 19375, Loss: 15.904, Final Batch Loss: 0.413\n",
      "Epoch 19376, Loss: 16.198, Final Batch Loss: 0.418\n",
      "Epoch 19377, Loss: 15.916, Final Batch Loss: 0.431\n",
      "Epoch 19378, Loss: 15.991, Final Batch Loss: 0.363\n",
      "Epoch 19379, Loss: 15.792, Final Batch Loss: 0.454\n",
      "Epoch 19380, Loss: 15.912, Final Batch Loss: 0.347\n",
      "Epoch 19381, Loss: 15.912, Final Batch Loss: 0.403\n",
      "Epoch 19382, Loss: 16.144, Final Batch Loss: 0.435\n",
      "Epoch 19383, Loss: 15.980, Final Batch Loss: 0.417\n",
      "Epoch 19384, Loss: 15.983, Final Batch Loss: 0.370\n",
      "Epoch 19385, Loss: 15.791, Final Batch Loss: 0.460\n",
      "Epoch 19386, Loss: 16.138, Final Batch Loss: 0.536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19387, Loss: 15.871, Final Batch Loss: 0.452\n",
      "Epoch 19388, Loss: 16.211, Final Batch Loss: 0.458\n",
      "Epoch 19389, Loss: 16.129, Final Batch Loss: 0.448\n",
      "Epoch 19390, Loss: 15.671, Final Batch Loss: 0.389\n",
      "Epoch 19391, Loss: 15.922, Final Batch Loss: 0.507\n",
      "Epoch 19392, Loss: 16.315, Final Batch Loss: 0.527\n",
      "Epoch 19393, Loss: 15.904, Final Batch Loss: 0.494\n",
      "Epoch 19394, Loss: 15.703, Final Batch Loss: 0.439\n",
      "Epoch 19395, Loss: 16.018, Final Batch Loss: 0.504\n",
      "Epoch 19396, Loss: 15.872, Final Batch Loss: 0.379\n",
      "Epoch 19397, Loss: 16.207, Final Batch Loss: 0.347\n",
      "Epoch 19398, Loss: 16.057, Final Batch Loss: 0.547\n",
      "Epoch 19399, Loss: 15.970, Final Batch Loss: 0.516\n",
      "Epoch 19400, Loss: 15.895, Final Batch Loss: 0.379\n",
      "Epoch 19401, Loss: 15.879, Final Batch Loss: 0.417\n",
      "Epoch 19402, Loss: 15.978, Final Batch Loss: 0.422\n",
      "Epoch 19403, Loss: 16.077, Final Batch Loss: 0.442\n",
      "Epoch 19404, Loss: 15.627, Final Batch Loss: 0.332\n",
      "Epoch 19405, Loss: 16.294, Final Batch Loss: 0.396\n",
      "Epoch 19406, Loss: 16.137, Final Batch Loss: 0.473\n",
      "Epoch 19407, Loss: 15.835, Final Batch Loss: 0.484\n",
      "Epoch 19408, Loss: 15.802, Final Batch Loss: 0.441\n",
      "Epoch 19409, Loss: 16.200, Final Batch Loss: 0.522\n",
      "Epoch 19410, Loss: 16.077, Final Batch Loss: 0.401\n",
      "Epoch 19411, Loss: 16.032, Final Batch Loss: 0.466\n",
      "Epoch 19412, Loss: 15.911, Final Batch Loss: 0.356\n",
      "Epoch 19413, Loss: 15.989, Final Batch Loss: 0.475\n",
      "Epoch 19414, Loss: 15.944, Final Batch Loss: 0.362\n",
      "Epoch 19415, Loss: 16.344, Final Batch Loss: 0.500\n",
      "Epoch 19416, Loss: 16.032, Final Batch Loss: 0.418\n",
      "Epoch 19417, Loss: 15.591, Final Batch Loss: 0.446\n",
      "Epoch 19418, Loss: 15.935, Final Batch Loss: 0.386\n",
      "Epoch 19419, Loss: 15.868, Final Batch Loss: 0.455\n",
      "Epoch 19420, Loss: 15.692, Final Batch Loss: 0.609\n",
      "Epoch 19421, Loss: 15.833, Final Batch Loss: 0.372\n",
      "Epoch 19422, Loss: 16.046, Final Batch Loss: 0.474\n",
      "Epoch 19423, Loss: 15.862, Final Batch Loss: 0.428\n",
      "Epoch 19424, Loss: 16.042, Final Batch Loss: 0.406\n",
      "Epoch 19425, Loss: 15.957, Final Batch Loss: 0.398\n",
      "Epoch 19426, Loss: 16.133, Final Batch Loss: 0.513\n",
      "Epoch 19427, Loss: 16.159, Final Batch Loss: 0.450\n",
      "Epoch 19428, Loss: 15.953, Final Batch Loss: 0.507\n",
      "Epoch 19429, Loss: 16.086, Final Batch Loss: 0.503\n",
      "Epoch 19430, Loss: 16.346, Final Batch Loss: 0.608\n",
      "Epoch 19431, Loss: 15.763, Final Batch Loss: 0.344\n",
      "Epoch 19432, Loss: 15.947, Final Batch Loss: 0.453\n",
      "Epoch 19433, Loss: 15.663, Final Batch Loss: 0.362\n",
      "Epoch 19434, Loss: 15.890, Final Batch Loss: 0.368\n",
      "Epoch 19435, Loss: 15.859, Final Batch Loss: 0.499\n",
      "Epoch 19436, Loss: 15.990, Final Batch Loss: 0.432\n",
      "Epoch 19437, Loss: 16.238, Final Batch Loss: 0.403\n",
      "Epoch 19438, Loss: 16.164, Final Batch Loss: 0.364\n",
      "Epoch 19439, Loss: 15.907, Final Batch Loss: 0.443\n",
      "Epoch 19440, Loss: 16.202, Final Batch Loss: 0.472\n",
      "Epoch 19441, Loss: 15.799, Final Batch Loss: 0.365\n",
      "Epoch 19442, Loss: 15.904, Final Batch Loss: 0.480\n",
      "Epoch 19443, Loss: 16.096, Final Batch Loss: 0.545\n",
      "Epoch 19444, Loss: 15.811, Final Batch Loss: 0.454\n",
      "Epoch 19445, Loss: 16.134, Final Batch Loss: 0.377\n",
      "Epoch 19446, Loss: 16.104, Final Batch Loss: 0.649\n",
      "Epoch 19447, Loss: 15.992, Final Batch Loss: 0.425\n",
      "Epoch 19448, Loss: 15.594, Final Batch Loss: 0.402\n",
      "Epoch 19449, Loss: 15.973, Final Batch Loss: 0.445\n",
      "Epoch 19450, Loss: 15.847, Final Batch Loss: 0.354\n",
      "Epoch 19451, Loss: 16.244, Final Batch Loss: 0.469\n",
      "Epoch 19452, Loss: 16.060, Final Batch Loss: 0.409\n",
      "Epoch 19453, Loss: 16.109, Final Batch Loss: 0.548\n",
      "Epoch 19454, Loss: 15.935, Final Batch Loss: 0.399\n",
      "Epoch 19455, Loss: 16.085, Final Batch Loss: 0.410\n",
      "Epoch 19456, Loss: 16.053, Final Batch Loss: 0.475\n",
      "Epoch 19457, Loss: 16.022, Final Batch Loss: 0.462\n",
      "Epoch 19458, Loss: 15.944, Final Batch Loss: 0.409\n",
      "Epoch 19459, Loss: 15.713, Final Batch Loss: 0.546\n",
      "Epoch 19460, Loss: 16.133, Final Batch Loss: 0.532\n",
      "Epoch 19461, Loss: 16.147, Final Batch Loss: 0.396\n",
      "Epoch 19462, Loss: 15.954, Final Batch Loss: 0.455\n",
      "Epoch 19463, Loss: 15.731, Final Batch Loss: 0.313\n",
      "Epoch 19464, Loss: 15.907, Final Batch Loss: 0.484\n",
      "Epoch 19465, Loss: 15.919, Final Batch Loss: 0.537\n",
      "Epoch 19466, Loss: 16.249, Final Batch Loss: 0.450\n",
      "Epoch 19467, Loss: 15.924, Final Batch Loss: 0.446\n",
      "Epoch 19468, Loss: 15.951, Final Batch Loss: 0.399\n",
      "Epoch 19469, Loss: 16.024, Final Batch Loss: 0.521\n",
      "Epoch 19470, Loss: 16.083, Final Batch Loss: 0.435\n",
      "Epoch 19471, Loss: 16.009, Final Batch Loss: 0.417\n",
      "Epoch 19472, Loss: 15.929, Final Batch Loss: 0.445\n",
      "Epoch 19473, Loss: 15.784, Final Batch Loss: 0.385\n",
      "Epoch 19474, Loss: 16.060, Final Batch Loss: 0.460\n",
      "Epoch 19475, Loss: 16.135, Final Batch Loss: 0.401\n",
      "Epoch 19476, Loss: 16.123, Final Batch Loss: 0.488\n",
      "Epoch 19477, Loss: 16.230, Final Batch Loss: 0.474\n",
      "Epoch 19478, Loss: 15.862, Final Batch Loss: 0.473\n",
      "Epoch 19479, Loss: 15.901, Final Batch Loss: 0.482\n",
      "Epoch 19480, Loss: 15.859, Final Batch Loss: 0.424\n",
      "Epoch 19481, Loss: 15.839, Final Batch Loss: 0.404\n",
      "Epoch 19482, Loss: 15.796, Final Batch Loss: 0.558\n",
      "Epoch 19483, Loss: 15.910, Final Batch Loss: 0.467\n",
      "Epoch 19484, Loss: 16.040, Final Batch Loss: 0.400\n",
      "Epoch 19485, Loss: 15.936, Final Batch Loss: 0.423\n",
      "Epoch 19486, Loss: 15.860, Final Batch Loss: 0.541\n",
      "Epoch 19487, Loss: 15.964, Final Batch Loss: 0.529\n",
      "Epoch 19488, Loss: 15.853, Final Batch Loss: 0.438\n",
      "Epoch 19489, Loss: 16.103, Final Batch Loss: 0.401\n",
      "Epoch 19490, Loss: 15.632, Final Batch Loss: 0.458\n",
      "Epoch 19491, Loss: 15.891, Final Batch Loss: 0.364\n",
      "Epoch 19492, Loss: 15.881, Final Batch Loss: 0.404\n",
      "Epoch 19493, Loss: 16.095, Final Batch Loss: 0.344\n",
      "Epoch 19494, Loss: 15.887, Final Batch Loss: 0.370\n",
      "Epoch 19495, Loss: 16.074, Final Batch Loss: 0.360\n",
      "Epoch 19496, Loss: 16.364, Final Batch Loss: 0.744\n",
      "Epoch 19497, Loss: 16.200, Final Batch Loss: 0.499\n",
      "Epoch 19498, Loss: 15.752, Final Batch Loss: 0.477\n",
      "Epoch 19499, Loss: 16.094, Final Batch Loss: 0.415\n",
      "Epoch 19500, Loss: 15.974, Final Batch Loss: 0.464\n",
      "Epoch 19501, Loss: 15.933, Final Batch Loss: 0.512\n",
      "Epoch 19502, Loss: 15.990, Final Batch Loss: 0.398\n",
      "Epoch 19503, Loss: 15.997, Final Batch Loss: 0.513\n",
      "Epoch 19504, Loss: 15.891, Final Batch Loss: 0.374\n",
      "Epoch 19505, Loss: 16.006, Final Batch Loss: 0.487\n",
      "Epoch 19506, Loss: 16.005, Final Batch Loss: 0.494\n",
      "Epoch 19507, Loss: 15.987, Final Batch Loss: 0.412\n",
      "Epoch 19508, Loss: 15.858, Final Batch Loss: 0.436\n",
      "Epoch 19509, Loss: 16.186, Final Batch Loss: 0.465\n",
      "Epoch 19510, Loss: 15.928, Final Batch Loss: 0.392\n",
      "Epoch 19511, Loss: 15.854, Final Batch Loss: 0.442\n",
      "Epoch 19512, Loss: 15.897, Final Batch Loss: 0.513\n",
      "Epoch 19513, Loss: 15.855, Final Batch Loss: 0.435\n",
      "Epoch 19514, Loss: 15.852, Final Batch Loss: 0.407\n",
      "Epoch 19515, Loss: 16.048, Final Batch Loss: 0.357\n",
      "Epoch 19516, Loss: 16.018, Final Batch Loss: 0.507\n",
      "Epoch 19517, Loss: 15.878, Final Batch Loss: 0.437\n",
      "Epoch 19518, Loss: 15.993, Final Batch Loss: 0.609\n",
      "Epoch 19519, Loss: 15.958, Final Batch Loss: 0.396\n",
      "Epoch 19520, Loss: 15.998, Final Batch Loss: 0.399\n",
      "Epoch 19521, Loss: 15.524, Final Batch Loss: 0.359\n",
      "Epoch 19522, Loss: 15.989, Final Batch Loss: 0.456\n",
      "Epoch 19523, Loss: 15.979, Final Batch Loss: 0.500\n",
      "Epoch 19524, Loss: 15.932, Final Batch Loss: 0.462\n",
      "Epoch 19525, Loss: 16.201, Final Batch Loss: 0.446\n",
      "Epoch 19526, Loss: 15.665, Final Batch Loss: 0.442\n",
      "Epoch 19527, Loss: 15.926, Final Batch Loss: 0.427\n",
      "Epoch 19528, Loss: 15.965, Final Batch Loss: 0.505\n",
      "Epoch 19529, Loss: 15.925, Final Batch Loss: 0.473\n",
      "Epoch 19530, Loss: 15.785, Final Batch Loss: 0.456\n",
      "Epoch 19531, Loss: 16.022, Final Batch Loss: 0.480\n",
      "Epoch 19532, Loss: 15.795, Final Batch Loss: 0.399\n",
      "Epoch 19533, Loss: 16.089, Final Batch Loss: 0.417\n",
      "Epoch 19534, Loss: 15.864, Final Batch Loss: 0.504\n",
      "Epoch 19535, Loss: 15.909, Final Batch Loss: 0.454\n",
      "Epoch 19536, Loss: 15.800, Final Batch Loss: 0.333\n",
      "Epoch 19537, Loss: 15.729, Final Batch Loss: 0.384\n",
      "Epoch 19538, Loss: 16.432, Final Batch Loss: 0.411\n",
      "Epoch 19539, Loss: 15.931, Final Batch Loss: 0.400\n",
      "Epoch 19540, Loss: 15.900, Final Batch Loss: 0.465\n",
      "Epoch 19541, Loss: 16.480, Final Batch Loss: 0.474\n",
      "Epoch 19542, Loss: 16.036, Final Batch Loss: 0.494\n",
      "Epoch 19543, Loss: 16.049, Final Batch Loss: 0.409\n",
      "Epoch 19544, Loss: 16.134, Final Batch Loss: 0.358\n",
      "Epoch 19545, Loss: 15.868, Final Batch Loss: 0.362\n",
      "Epoch 19546, Loss: 16.009, Final Batch Loss: 0.536\n",
      "Epoch 19547, Loss: 16.134, Final Batch Loss: 0.714\n",
      "Epoch 19548, Loss: 15.860, Final Batch Loss: 0.454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19549, Loss: 16.187, Final Batch Loss: 0.501\n",
      "Epoch 19550, Loss: 15.916, Final Batch Loss: 0.473\n",
      "Epoch 19551, Loss: 16.138, Final Batch Loss: 0.495\n",
      "Epoch 19552, Loss: 16.009, Final Batch Loss: 0.531\n",
      "Epoch 19553, Loss: 16.126, Final Batch Loss: 0.378\n",
      "Epoch 19554, Loss: 15.901, Final Batch Loss: 0.536\n",
      "Epoch 19555, Loss: 15.914, Final Batch Loss: 0.512\n",
      "Epoch 19556, Loss: 16.091, Final Batch Loss: 0.577\n",
      "Epoch 19557, Loss: 15.979, Final Batch Loss: 0.400\n",
      "Epoch 19558, Loss: 15.930, Final Batch Loss: 0.435\n",
      "Epoch 19559, Loss: 16.345, Final Batch Loss: 0.414\n",
      "Epoch 19560, Loss: 16.183, Final Batch Loss: 0.470\n",
      "Epoch 19561, Loss: 16.063, Final Batch Loss: 0.414\n",
      "Epoch 19562, Loss: 16.094, Final Batch Loss: 0.430\n",
      "Epoch 19563, Loss: 16.099, Final Batch Loss: 0.356\n",
      "Epoch 19564, Loss: 15.952, Final Batch Loss: 0.464\n",
      "Epoch 19565, Loss: 15.858, Final Batch Loss: 0.366\n",
      "Epoch 19566, Loss: 16.060, Final Batch Loss: 0.455\n",
      "Epoch 19567, Loss: 16.227, Final Batch Loss: 0.527\n",
      "Epoch 19568, Loss: 15.932, Final Batch Loss: 0.323\n",
      "Epoch 19569, Loss: 15.945, Final Batch Loss: 0.449\n",
      "Epoch 19570, Loss: 16.102, Final Batch Loss: 0.461\n",
      "Epoch 19571, Loss: 15.859, Final Batch Loss: 0.465\n",
      "Epoch 19572, Loss: 15.814, Final Batch Loss: 0.459\n",
      "Epoch 19573, Loss: 16.256, Final Batch Loss: 0.454\n",
      "Epoch 19574, Loss: 15.967, Final Batch Loss: 0.440\n",
      "Epoch 19575, Loss: 16.103, Final Batch Loss: 0.442\n",
      "Epoch 19576, Loss: 16.058, Final Batch Loss: 0.624\n",
      "Epoch 19577, Loss: 16.232, Final Batch Loss: 0.456\n",
      "Epoch 19578, Loss: 15.949, Final Batch Loss: 0.412\n",
      "Epoch 19579, Loss: 16.401, Final Batch Loss: 0.463\n",
      "Epoch 19580, Loss: 15.972, Final Batch Loss: 0.419\n",
      "Epoch 19581, Loss: 16.327, Final Batch Loss: 0.514\n",
      "Epoch 19582, Loss: 15.780, Final Batch Loss: 0.537\n",
      "Epoch 19583, Loss: 15.825, Final Batch Loss: 0.397\n",
      "Epoch 19584, Loss: 16.138, Final Batch Loss: 0.473\n",
      "Epoch 19585, Loss: 15.826, Final Batch Loss: 0.457\n",
      "Epoch 19586, Loss: 16.285, Final Batch Loss: 0.425\n",
      "Epoch 19587, Loss: 15.698, Final Batch Loss: 0.413\n",
      "Epoch 19588, Loss: 15.914, Final Batch Loss: 0.393\n",
      "Epoch 19589, Loss: 15.741, Final Batch Loss: 0.406\n",
      "Epoch 19590, Loss: 16.169, Final Batch Loss: 0.512\n",
      "Epoch 19591, Loss: 16.068, Final Batch Loss: 0.381\n",
      "Epoch 19592, Loss: 16.245, Final Batch Loss: 0.503\n",
      "Epoch 19593, Loss: 16.093, Final Batch Loss: 0.661\n",
      "Epoch 19594, Loss: 16.298, Final Batch Loss: 0.505\n",
      "Epoch 19595, Loss: 16.015, Final Batch Loss: 0.423\n",
      "Epoch 19596, Loss: 15.988, Final Batch Loss: 0.401\n",
      "Epoch 19597, Loss: 15.769, Final Batch Loss: 0.459\n",
      "Epoch 19598, Loss: 16.176, Final Batch Loss: 0.366\n",
      "Epoch 19599, Loss: 15.723, Final Batch Loss: 0.456\n",
      "Epoch 19600, Loss: 16.079, Final Batch Loss: 0.427\n",
      "Epoch 19601, Loss: 15.939, Final Batch Loss: 0.482\n",
      "Epoch 19602, Loss: 15.815, Final Batch Loss: 0.427\n",
      "Epoch 19603, Loss: 15.872, Final Batch Loss: 0.377\n",
      "Epoch 19604, Loss: 15.674, Final Batch Loss: 0.345\n",
      "Epoch 19605, Loss: 16.159, Final Batch Loss: 0.485\n",
      "Epoch 19606, Loss: 16.221, Final Batch Loss: 0.508\n",
      "Epoch 19607, Loss: 15.904, Final Batch Loss: 0.373\n",
      "Epoch 19608, Loss: 15.917, Final Batch Loss: 0.410\n",
      "Epoch 19609, Loss: 15.786, Final Batch Loss: 0.328\n",
      "Epoch 19610, Loss: 16.447, Final Batch Loss: 0.463\n",
      "Epoch 19611, Loss: 15.863, Final Batch Loss: 0.378\n",
      "Epoch 19612, Loss: 16.134, Final Batch Loss: 0.487\n",
      "Epoch 19613, Loss: 16.067, Final Batch Loss: 0.443\n",
      "Epoch 19614, Loss: 16.293, Final Batch Loss: 0.413\n",
      "Epoch 19615, Loss: 16.046, Final Batch Loss: 0.552\n",
      "Epoch 19616, Loss: 15.850, Final Batch Loss: 0.368\n",
      "Epoch 19617, Loss: 16.165, Final Batch Loss: 0.479\n",
      "Epoch 19618, Loss: 16.163, Final Batch Loss: 0.438\n",
      "Epoch 19619, Loss: 15.959, Final Batch Loss: 0.468\n",
      "Epoch 19620, Loss: 15.893, Final Batch Loss: 0.426\n",
      "Epoch 19621, Loss: 16.018, Final Batch Loss: 0.524\n",
      "Epoch 19622, Loss: 16.016, Final Batch Loss: 0.443\n",
      "Epoch 19623, Loss: 15.937, Final Batch Loss: 0.391\n",
      "Epoch 19624, Loss: 16.223, Final Batch Loss: 0.428\n",
      "Epoch 19625, Loss: 15.969, Final Batch Loss: 0.471\n",
      "Epoch 19626, Loss: 16.228, Final Batch Loss: 0.531\n",
      "Epoch 19627, Loss: 16.046, Final Batch Loss: 0.393\n",
      "Epoch 19628, Loss: 15.563, Final Batch Loss: 0.433\n",
      "Epoch 19629, Loss: 15.865, Final Batch Loss: 0.439\n",
      "Epoch 19630, Loss: 15.995, Final Batch Loss: 0.440\n",
      "Epoch 19631, Loss: 15.925, Final Batch Loss: 0.433\n",
      "Epoch 19632, Loss: 16.057, Final Batch Loss: 0.448\n",
      "Epoch 19633, Loss: 16.049, Final Batch Loss: 0.461\n",
      "Epoch 19634, Loss: 15.979, Final Batch Loss: 0.340\n",
      "Epoch 19635, Loss: 15.940, Final Batch Loss: 0.490\n",
      "Epoch 19636, Loss: 15.658, Final Batch Loss: 0.405\n",
      "Epoch 19637, Loss: 16.192, Final Batch Loss: 0.476\n",
      "Epoch 19638, Loss: 15.945, Final Batch Loss: 0.417\n",
      "Epoch 19639, Loss: 15.953, Final Batch Loss: 0.385\n",
      "Epoch 19640, Loss: 15.916, Final Batch Loss: 0.428\n",
      "Epoch 19641, Loss: 16.024, Final Batch Loss: 0.447\n",
      "Epoch 19642, Loss: 15.786, Final Batch Loss: 0.471\n",
      "Epoch 19643, Loss: 15.965, Final Batch Loss: 0.417\n",
      "Epoch 19644, Loss: 15.968, Final Batch Loss: 0.404\n",
      "Epoch 19645, Loss: 16.241, Final Batch Loss: 0.383\n",
      "Epoch 19646, Loss: 15.956, Final Batch Loss: 0.504\n",
      "Epoch 19647, Loss: 16.295, Final Batch Loss: 0.377\n",
      "Epoch 19648, Loss: 15.724, Final Batch Loss: 0.461\n",
      "Epoch 19649, Loss: 16.103, Final Batch Loss: 0.501\n",
      "Epoch 19650, Loss: 16.004, Final Batch Loss: 0.382\n",
      "Epoch 19651, Loss: 15.857, Final Batch Loss: 0.353\n",
      "Epoch 19652, Loss: 16.112, Final Batch Loss: 0.465\n",
      "Epoch 19653, Loss: 15.944, Final Batch Loss: 0.458\n",
      "Epoch 19654, Loss: 15.768, Final Batch Loss: 0.352\n",
      "Epoch 19655, Loss: 15.962, Final Batch Loss: 0.457\n",
      "Epoch 19656, Loss: 16.092, Final Batch Loss: 0.443\n",
      "Epoch 19657, Loss: 16.118, Final Batch Loss: 0.465\n",
      "Epoch 19658, Loss: 16.201, Final Batch Loss: 0.519\n",
      "Epoch 19659, Loss: 16.069, Final Batch Loss: 0.418\n",
      "Epoch 19660, Loss: 15.856, Final Batch Loss: 0.459\n",
      "Epoch 19661, Loss: 16.154, Final Batch Loss: 0.376\n",
      "Epoch 19662, Loss: 16.148, Final Batch Loss: 0.483\n",
      "Epoch 19663, Loss: 15.920, Final Batch Loss: 0.397\n",
      "Epoch 19664, Loss: 16.045, Final Batch Loss: 0.371\n",
      "Epoch 19665, Loss: 15.817, Final Batch Loss: 0.368\n",
      "Epoch 19666, Loss: 15.775, Final Batch Loss: 0.412\n",
      "Epoch 19667, Loss: 15.922, Final Batch Loss: 0.365\n",
      "Epoch 19668, Loss: 15.803, Final Batch Loss: 0.486\n",
      "Epoch 19669, Loss: 16.055, Final Batch Loss: 0.358\n",
      "Epoch 19670, Loss: 15.969, Final Batch Loss: 0.500\n",
      "Epoch 19671, Loss: 16.098, Final Batch Loss: 0.530\n",
      "Epoch 19672, Loss: 15.839, Final Batch Loss: 0.411\n",
      "Epoch 19673, Loss: 16.050, Final Batch Loss: 0.396\n",
      "Epoch 19674, Loss: 15.932, Final Batch Loss: 0.373\n",
      "Epoch 19675, Loss: 16.142, Final Batch Loss: 0.525\n",
      "Epoch 19676, Loss: 16.109, Final Batch Loss: 0.451\n",
      "Epoch 19677, Loss: 15.812, Final Batch Loss: 0.452\n",
      "Epoch 19678, Loss: 16.009, Final Batch Loss: 0.500\n",
      "Epoch 19679, Loss: 16.115, Final Batch Loss: 0.486\n",
      "Epoch 19680, Loss: 15.910, Final Batch Loss: 0.362\n",
      "Epoch 19681, Loss: 16.175, Final Batch Loss: 0.516\n",
      "Epoch 19682, Loss: 15.711, Final Batch Loss: 0.393\n",
      "Epoch 19683, Loss: 15.962, Final Batch Loss: 0.441\n",
      "Epoch 19684, Loss: 15.929, Final Batch Loss: 0.496\n",
      "Epoch 19685, Loss: 15.998, Final Batch Loss: 0.414\n",
      "Epoch 19686, Loss: 16.096, Final Batch Loss: 0.476\n",
      "Epoch 19687, Loss: 15.886, Final Batch Loss: 0.304\n",
      "Epoch 19688, Loss: 16.037, Final Batch Loss: 0.387\n",
      "Epoch 19689, Loss: 15.949, Final Batch Loss: 0.441\n",
      "Epoch 19690, Loss: 15.863, Final Batch Loss: 0.342\n",
      "Epoch 19691, Loss: 16.139, Final Batch Loss: 0.530\n",
      "Epoch 19692, Loss: 15.799, Final Batch Loss: 0.347\n",
      "Epoch 19693, Loss: 15.795, Final Batch Loss: 0.473\n",
      "Epoch 19694, Loss: 16.091, Final Batch Loss: 0.451\n",
      "Epoch 19695, Loss: 15.904, Final Batch Loss: 0.323\n",
      "Epoch 19696, Loss: 16.213, Final Batch Loss: 0.507\n",
      "Epoch 19697, Loss: 15.868, Final Batch Loss: 0.440\n",
      "Epoch 19698, Loss: 15.634, Final Batch Loss: 0.434\n",
      "Epoch 19699, Loss: 15.744, Final Batch Loss: 0.416\n",
      "Epoch 19700, Loss: 16.047, Final Batch Loss: 0.489\n",
      "Epoch 19701, Loss: 16.130, Final Batch Loss: 0.332\n",
      "Epoch 19702, Loss: 15.909, Final Batch Loss: 0.418\n",
      "Epoch 19703, Loss: 16.131, Final Batch Loss: 0.413\n",
      "Epoch 19704, Loss: 15.908, Final Batch Loss: 0.460\n",
      "Epoch 19705, Loss: 15.928, Final Batch Loss: 0.385\n",
      "Epoch 19706, Loss: 15.697, Final Batch Loss: 0.308\n",
      "Epoch 19707, Loss: 16.011, Final Batch Loss: 0.412\n",
      "Epoch 19708, Loss: 16.302, Final Batch Loss: 0.480\n",
      "Epoch 19709, Loss: 15.645, Final Batch Loss: 0.358\n",
      "Epoch 19710, Loss: 15.862, Final Batch Loss: 0.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19711, Loss: 15.788, Final Batch Loss: 0.426\n",
      "Epoch 19712, Loss: 16.033, Final Batch Loss: 0.499\n",
      "Epoch 19713, Loss: 16.036, Final Batch Loss: 0.421\n",
      "Epoch 19714, Loss: 15.844, Final Batch Loss: 0.469\n",
      "Epoch 19715, Loss: 16.000, Final Batch Loss: 0.441\n",
      "Epoch 19716, Loss: 16.011, Final Batch Loss: 0.390\n",
      "Epoch 19717, Loss: 15.801, Final Batch Loss: 0.470\n",
      "Epoch 19718, Loss: 16.029, Final Batch Loss: 0.479\n",
      "Epoch 19719, Loss: 15.606, Final Batch Loss: 0.455\n",
      "Epoch 19720, Loss: 15.888, Final Batch Loss: 0.436\n",
      "Epoch 19721, Loss: 15.771, Final Batch Loss: 0.483\n",
      "Epoch 19722, Loss: 15.941, Final Batch Loss: 0.411\n",
      "Epoch 19723, Loss: 15.968, Final Batch Loss: 0.454\n",
      "Epoch 19724, Loss: 16.045, Final Batch Loss: 0.371\n",
      "Epoch 19725, Loss: 15.986, Final Batch Loss: 0.409\n",
      "Epoch 19726, Loss: 16.267, Final Batch Loss: 0.420\n",
      "Epoch 19727, Loss: 16.208, Final Batch Loss: 0.566\n",
      "Epoch 19728, Loss: 15.642, Final Batch Loss: 0.437\n",
      "Epoch 19729, Loss: 15.540, Final Batch Loss: 0.380\n",
      "Epoch 19730, Loss: 15.830, Final Batch Loss: 0.412\n",
      "Epoch 19731, Loss: 15.879, Final Batch Loss: 0.487\n",
      "Epoch 19732, Loss: 15.835, Final Batch Loss: 0.415\n",
      "Epoch 19733, Loss: 15.923, Final Batch Loss: 0.369\n",
      "Epoch 19734, Loss: 15.841, Final Batch Loss: 0.430\n",
      "Epoch 19735, Loss: 16.172, Final Batch Loss: 0.528\n",
      "Epoch 19736, Loss: 16.023, Final Batch Loss: 0.430\n",
      "Epoch 19737, Loss: 15.960, Final Batch Loss: 0.397\n",
      "Epoch 19738, Loss: 16.207, Final Batch Loss: 0.606\n",
      "Epoch 19739, Loss: 16.035, Final Batch Loss: 0.483\n",
      "Epoch 19740, Loss: 15.796, Final Batch Loss: 0.437\n",
      "Epoch 19741, Loss: 16.311, Final Batch Loss: 0.526\n",
      "Epoch 19742, Loss: 16.171, Final Batch Loss: 0.427\n",
      "Epoch 19743, Loss: 15.690, Final Batch Loss: 0.421\n",
      "Epoch 19744, Loss: 15.861, Final Batch Loss: 0.375\n",
      "Epoch 19745, Loss: 15.915, Final Batch Loss: 0.409\n",
      "Epoch 19746, Loss: 15.896, Final Batch Loss: 0.418\n",
      "Epoch 19747, Loss: 15.818, Final Batch Loss: 0.422\n",
      "Epoch 19748, Loss: 15.794, Final Batch Loss: 0.413\n",
      "Epoch 19749, Loss: 15.775, Final Batch Loss: 0.428\n",
      "Epoch 19750, Loss: 15.746, Final Batch Loss: 0.346\n",
      "Epoch 19751, Loss: 16.050, Final Batch Loss: 0.419\n",
      "Epoch 19752, Loss: 16.054, Final Batch Loss: 0.513\n",
      "Epoch 19753, Loss: 16.050, Final Batch Loss: 0.556\n",
      "Epoch 19754, Loss: 15.886, Final Batch Loss: 0.443\n",
      "Epoch 19755, Loss: 16.022, Final Batch Loss: 0.332\n",
      "Epoch 19756, Loss: 15.940, Final Batch Loss: 0.422\n",
      "Epoch 19757, Loss: 15.870, Final Batch Loss: 0.571\n",
      "Epoch 19758, Loss: 15.908, Final Batch Loss: 0.469\n",
      "Epoch 19759, Loss: 15.853, Final Batch Loss: 0.495\n",
      "Epoch 19760, Loss: 16.219, Final Batch Loss: 0.557\n",
      "Epoch 19761, Loss: 15.976, Final Batch Loss: 0.520\n",
      "Epoch 19762, Loss: 15.793, Final Batch Loss: 0.366\n",
      "Epoch 19763, Loss: 16.026, Final Batch Loss: 0.545\n",
      "Epoch 19764, Loss: 16.016, Final Batch Loss: 0.396\n",
      "Epoch 19765, Loss: 16.192, Final Batch Loss: 0.398\n",
      "Epoch 19766, Loss: 16.071, Final Batch Loss: 0.403\n",
      "Epoch 19767, Loss: 16.421, Final Batch Loss: 0.382\n",
      "Epoch 19768, Loss: 16.183, Final Batch Loss: 0.477\n",
      "Epoch 19769, Loss: 15.979, Final Batch Loss: 0.522\n",
      "Epoch 19770, Loss: 15.913, Final Batch Loss: 0.471\n",
      "Epoch 19771, Loss: 15.982, Final Batch Loss: 0.483\n",
      "Epoch 19772, Loss: 15.814, Final Batch Loss: 0.428\n",
      "Epoch 19773, Loss: 16.078, Final Batch Loss: 0.403\n",
      "Epoch 19774, Loss: 15.983, Final Batch Loss: 0.440\n",
      "Epoch 19775, Loss: 16.213, Final Batch Loss: 0.512\n",
      "Epoch 19776, Loss: 15.915, Final Batch Loss: 0.399\n",
      "Epoch 19777, Loss: 15.886, Final Batch Loss: 0.414\n",
      "Epoch 19778, Loss: 15.899, Final Batch Loss: 0.406\n",
      "Epoch 19779, Loss: 15.770, Final Batch Loss: 0.366\n",
      "Epoch 19780, Loss: 16.212, Final Batch Loss: 0.395\n",
      "Epoch 19781, Loss: 15.860, Final Batch Loss: 0.424\n",
      "Epoch 19782, Loss: 16.090, Final Batch Loss: 0.432\n",
      "Epoch 19783, Loss: 16.066, Final Batch Loss: 0.471\n",
      "Epoch 19784, Loss: 15.987, Final Batch Loss: 0.604\n",
      "Epoch 19785, Loss: 16.134, Final Batch Loss: 0.463\n",
      "Epoch 19786, Loss: 16.309, Final Batch Loss: 0.522\n",
      "Epoch 19787, Loss: 15.953, Final Batch Loss: 0.448\n",
      "Epoch 19788, Loss: 16.127, Final Batch Loss: 0.467\n",
      "Epoch 19789, Loss: 15.937, Final Batch Loss: 0.420\n",
      "Epoch 19790, Loss: 15.741, Final Batch Loss: 0.356\n",
      "Epoch 19791, Loss: 16.074, Final Batch Loss: 0.400\n",
      "Epoch 19792, Loss: 15.742, Final Batch Loss: 0.380\n",
      "Epoch 19793, Loss: 15.993, Final Batch Loss: 0.419\n",
      "Epoch 19794, Loss: 16.182, Final Batch Loss: 0.418\n",
      "Epoch 19795, Loss: 16.148, Final Batch Loss: 0.435\n",
      "Epoch 19796, Loss: 16.004, Final Batch Loss: 0.545\n",
      "Epoch 19797, Loss: 16.055, Final Batch Loss: 0.426\n",
      "Epoch 19798, Loss: 16.080, Final Batch Loss: 0.464\n",
      "Epoch 19799, Loss: 15.986, Final Batch Loss: 0.438\n",
      "Epoch 19800, Loss: 15.861, Final Batch Loss: 0.520\n",
      "Epoch 19801, Loss: 15.922, Final Batch Loss: 0.457\n",
      "Epoch 19802, Loss: 15.939, Final Batch Loss: 0.442\n",
      "Epoch 19803, Loss: 15.888, Final Batch Loss: 0.404\n",
      "Epoch 19804, Loss: 15.798, Final Batch Loss: 0.416\n",
      "Epoch 19805, Loss: 16.079, Final Batch Loss: 0.485\n",
      "Epoch 19806, Loss: 15.923, Final Batch Loss: 0.378\n",
      "Epoch 19807, Loss: 16.016, Final Batch Loss: 0.424\n",
      "Epoch 19808, Loss: 15.912, Final Batch Loss: 0.414\n",
      "Epoch 19809, Loss: 16.096, Final Batch Loss: 0.460\n",
      "Epoch 19810, Loss: 15.851, Final Batch Loss: 0.385\n",
      "Epoch 19811, Loss: 15.948, Final Batch Loss: 0.436\n",
      "Epoch 19812, Loss: 15.759, Final Batch Loss: 0.658\n",
      "Epoch 19813, Loss: 16.179, Final Batch Loss: 0.392\n",
      "Epoch 19814, Loss: 15.776, Final Batch Loss: 0.507\n",
      "Epoch 19815, Loss: 16.025, Final Batch Loss: 0.587\n",
      "Epoch 19816, Loss: 16.120, Final Batch Loss: 0.497\n",
      "Epoch 19817, Loss: 16.188, Final Batch Loss: 0.488\n",
      "Epoch 19818, Loss: 16.186, Final Batch Loss: 0.476\n",
      "Epoch 19819, Loss: 15.879, Final Batch Loss: 0.487\n",
      "Epoch 19820, Loss: 15.997, Final Batch Loss: 0.465\n",
      "Epoch 19821, Loss: 15.726, Final Batch Loss: 0.415\n",
      "Epoch 19822, Loss: 15.739, Final Batch Loss: 0.379\n",
      "Epoch 19823, Loss: 15.826, Final Batch Loss: 0.415\n",
      "Epoch 19824, Loss: 16.251, Final Batch Loss: 0.471\n",
      "Epoch 19825, Loss: 15.693, Final Batch Loss: 0.397\n",
      "Epoch 19826, Loss: 15.642, Final Batch Loss: 0.362\n",
      "Epoch 19827, Loss: 15.921, Final Batch Loss: 0.388\n",
      "Epoch 19828, Loss: 16.449, Final Batch Loss: 0.454\n",
      "Epoch 19829, Loss: 16.011, Final Batch Loss: 0.430\n",
      "Epoch 19830, Loss: 15.974, Final Batch Loss: 0.534\n",
      "Epoch 19831, Loss: 16.116, Final Batch Loss: 0.479\n",
      "Epoch 19832, Loss: 16.021, Final Batch Loss: 0.474\n",
      "Epoch 19833, Loss: 15.798, Final Batch Loss: 0.457\n",
      "Epoch 19834, Loss: 16.053, Final Batch Loss: 0.404\n",
      "Epoch 19835, Loss: 15.952, Final Batch Loss: 0.316\n",
      "Epoch 19836, Loss: 15.954, Final Batch Loss: 0.375\n",
      "Epoch 19837, Loss: 15.787, Final Batch Loss: 0.440\n",
      "Epoch 19838, Loss: 16.129, Final Batch Loss: 0.433\n",
      "Epoch 19839, Loss: 16.210, Final Batch Loss: 0.441\n",
      "Epoch 19840, Loss: 15.893, Final Batch Loss: 0.468\n",
      "Epoch 19841, Loss: 16.233, Final Batch Loss: 0.424\n",
      "Epoch 19842, Loss: 15.960, Final Batch Loss: 0.463\n",
      "Epoch 19843, Loss: 15.833, Final Batch Loss: 0.362\n",
      "Epoch 19844, Loss: 16.150, Final Batch Loss: 0.445\n",
      "Epoch 19845, Loss: 16.014, Final Batch Loss: 0.474\n",
      "Epoch 19846, Loss: 16.140, Final Batch Loss: 0.421\n",
      "Epoch 19847, Loss: 16.112, Final Batch Loss: 0.426\n",
      "Epoch 19848, Loss: 16.025, Final Batch Loss: 0.492\n",
      "Epoch 19849, Loss: 15.720, Final Batch Loss: 0.423\n",
      "Epoch 19850, Loss: 16.013, Final Batch Loss: 0.359\n",
      "Epoch 19851, Loss: 16.125, Final Batch Loss: 0.496\n",
      "Epoch 19852, Loss: 15.934, Final Batch Loss: 0.424\n",
      "Epoch 19853, Loss: 15.934, Final Batch Loss: 0.410\n",
      "Epoch 19854, Loss: 16.129, Final Batch Loss: 0.460\n",
      "Epoch 19855, Loss: 15.904, Final Batch Loss: 0.489\n",
      "Epoch 19856, Loss: 16.000, Final Batch Loss: 0.455\n",
      "Epoch 19857, Loss: 15.925, Final Batch Loss: 0.421\n",
      "Epoch 19858, Loss: 15.957, Final Batch Loss: 0.380\n",
      "Epoch 19859, Loss: 15.914, Final Batch Loss: 0.586\n",
      "Epoch 19860, Loss: 16.136, Final Batch Loss: 0.484\n",
      "Epoch 19861, Loss: 16.245, Final Batch Loss: 0.437\n",
      "Epoch 19862, Loss: 16.251, Final Batch Loss: 0.457\n",
      "Epoch 19863, Loss: 15.932, Final Batch Loss: 0.391\n",
      "Epoch 19864, Loss: 16.075, Final Batch Loss: 0.419\n",
      "Epoch 19865, Loss: 16.089, Final Batch Loss: 0.438\n",
      "Epoch 19866, Loss: 15.836, Final Batch Loss: 0.469\n",
      "Epoch 19867, Loss: 15.755, Final Batch Loss: 0.352\n",
      "Epoch 19868, Loss: 15.947, Final Batch Loss: 0.389\n",
      "Epoch 19869, Loss: 15.985, Final Batch Loss: 0.421\n",
      "Epoch 19870, Loss: 15.598, Final Batch Loss: 0.388\n",
      "Epoch 19871, Loss: 15.779, Final Batch Loss: 0.430\n",
      "Epoch 19872, Loss: 16.020, Final Batch Loss: 0.467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19873, Loss: 16.017, Final Batch Loss: 0.382\n",
      "Epoch 19874, Loss: 16.204, Final Batch Loss: 0.453\n",
      "Epoch 19875, Loss: 16.059, Final Batch Loss: 0.376\n",
      "Epoch 19876, Loss: 15.970, Final Batch Loss: 0.438\n",
      "Epoch 19877, Loss: 15.881, Final Batch Loss: 0.383\n",
      "Epoch 19878, Loss: 16.069, Final Batch Loss: 0.451\n",
      "Epoch 19879, Loss: 15.888, Final Batch Loss: 0.362\n",
      "Epoch 19880, Loss: 15.851, Final Batch Loss: 0.463\n",
      "Epoch 19881, Loss: 15.998, Final Batch Loss: 0.490\n",
      "Epoch 19882, Loss: 16.355, Final Batch Loss: 0.486\n",
      "Epoch 19883, Loss: 15.938, Final Batch Loss: 0.512\n",
      "Epoch 19884, Loss: 16.139, Final Batch Loss: 0.456\n",
      "Epoch 19885, Loss: 15.946, Final Batch Loss: 0.447\n",
      "Epoch 19886, Loss: 16.133, Final Batch Loss: 0.436\n",
      "Epoch 19887, Loss: 15.960, Final Batch Loss: 0.386\n",
      "Epoch 19888, Loss: 16.260, Final Batch Loss: 0.517\n",
      "Epoch 19889, Loss: 15.784, Final Batch Loss: 0.532\n",
      "Epoch 19890, Loss: 15.756, Final Batch Loss: 0.475\n",
      "Epoch 19891, Loss: 16.225, Final Batch Loss: 0.540\n",
      "Epoch 19892, Loss: 16.058, Final Batch Loss: 0.415\n",
      "Epoch 19893, Loss: 16.080, Final Batch Loss: 0.444\n",
      "Epoch 19894, Loss: 16.066, Final Batch Loss: 0.380\n",
      "Epoch 19895, Loss: 15.966, Final Batch Loss: 0.486\n",
      "Epoch 19896, Loss: 16.189, Final Batch Loss: 0.471\n",
      "Epoch 19897, Loss: 16.222, Final Batch Loss: 0.455\n",
      "Epoch 19898, Loss: 16.034, Final Batch Loss: 0.437\n",
      "Epoch 19899, Loss: 16.082, Final Batch Loss: 0.489\n",
      "Epoch 19900, Loss: 15.947, Final Batch Loss: 0.488\n",
      "Epoch 19901, Loss: 15.905, Final Batch Loss: 0.384\n",
      "Epoch 19902, Loss: 16.227, Final Batch Loss: 0.385\n",
      "Epoch 19903, Loss: 15.807, Final Batch Loss: 0.362\n",
      "Epoch 19904, Loss: 16.082, Final Batch Loss: 0.380\n",
      "Epoch 19905, Loss: 16.010, Final Batch Loss: 0.456\n",
      "Epoch 19906, Loss: 16.121, Final Batch Loss: 0.498\n",
      "Epoch 19907, Loss: 15.968, Final Batch Loss: 0.370\n",
      "Epoch 19908, Loss: 15.857, Final Batch Loss: 0.416\n",
      "Epoch 19909, Loss: 15.693, Final Batch Loss: 0.455\n",
      "Epoch 19910, Loss: 15.951, Final Batch Loss: 0.430\n",
      "Epoch 19911, Loss: 16.135, Final Batch Loss: 0.514\n",
      "Epoch 19912, Loss: 15.911, Final Batch Loss: 0.466\n",
      "Epoch 19913, Loss: 15.656, Final Batch Loss: 0.420\n",
      "Epoch 19914, Loss: 15.908, Final Batch Loss: 0.439\n",
      "Epoch 19915, Loss: 16.128, Final Batch Loss: 0.453\n",
      "Epoch 19916, Loss: 15.918, Final Batch Loss: 0.325\n",
      "Epoch 19917, Loss: 15.908, Final Batch Loss: 0.529\n",
      "Epoch 19918, Loss: 16.049, Final Batch Loss: 0.449\n",
      "Epoch 19919, Loss: 16.220, Final Batch Loss: 0.439\n",
      "Epoch 19920, Loss: 16.146, Final Batch Loss: 0.414\n",
      "Epoch 19921, Loss: 16.239, Final Batch Loss: 0.501\n",
      "Epoch 19922, Loss: 16.039, Final Batch Loss: 0.453\n",
      "Epoch 19923, Loss: 15.742, Final Batch Loss: 0.440\n",
      "Epoch 19924, Loss: 16.070, Final Batch Loss: 0.449\n",
      "Epoch 19925, Loss: 15.895, Final Batch Loss: 0.478\n",
      "Epoch 19926, Loss: 16.090, Final Batch Loss: 0.487\n",
      "Epoch 19927, Loss: 16.072, Final Batch Loss: 0.312\n",
      "Epoch 19928, Loss: 15.810, Final Batch Loss: 0.344\n",
      "Epoch 19929, Loss: 15.983, Final Batch Loss: 0.422\n",
      "Epoch 19930, Loss: 16.068, Final Batch Loss: 0.413\n",
      "Epoch 19931, Loss: 15.484, Final Batch Loss: 0.380\n",
      "Epoch 19932, Loss: 15.960, Final Batch Loss: 0.404\n",
      "Epoch 19933, Loss: 15.967, Final Batch Loss: 0.394\n",
      "Epoch 19934, Loss: 15.946, Final Batch Loss: 0.524\n",
      "Epoch 19935, Loss: 15.741, Final Batch Loss: 0.393\n",
      "Epoch 19936, Loss: 15.751, Final Batch Loss: 0.469\n",
      "Epoch 19937, Loss: 15.912, Final Batch Loss: 0.393\n",
      "Epoch 19938, Loss: 15.797, Final Batch Loss: 0.446\n",
      "Epoch 19939, Loss: 16.080, Final Batch Loss: 0.440\n",
      "Epoch 19940, Loss: 15.899, Final Batch Loss: 0.368\n",
      "Epoch 19941, Loss: 16.126, Final Batch Loss: 0.462\n",
      "Epoch 19942, Loss: 15.929, Final Batch Loss: 0.466\n",
      "Epoch 19943, Loss: 15.994, Final Batch Loss: 0.472\n",
      "Epoch 19944, Loss: 15.851, Final Batch Loss: 0.436\n",
      "Epoch 19945, Loss: 16.357, Final Batch Loss: 0.418\n",
      "Epoch 19946, Loss: 15.926, Final Batch Loss: 0.406\n",
      "Epoch 19947, Loss: 16.112, Final Batch Loss: 0.488\n",
      "Epoch 19948, Loss: 15.965, Final Batch Loss: 0.447\n",
      "Epoch 19949, Loss: 15.918, Final Batch Loss: 0.456\n",
      "Epoch 19950, Loss: 15.903, Final Batch Loss: 0.454\n",
      "Epoch 19951, Loss: 16.185, Final Batch Loss: 0.404\n",
      "Epoch 19952, Loss: 15.772, Final Batch Loss: 0.489\n",
      "Epoch 19953, Loss: 15.840, Final Batch Loss: 0.465\n",
      "Epoch 19954, Loss: 16.004, Final Batch Loss: 0.405\n",
      "Epoch 19955, Loss: 16.011, Final Batch Loss: 0.384\n",
      "Epoch 19956, Loss: 15.986, Final Batch Loss: 0.438\n",
      "Epoch 19957, Loss: 16.053, Final Batch Loss: 0.440\n",
      "Epoch 19958, Loss: 16.084, Final Batch Loss: 0.456\n",
      "Epoch 19959, Loss: 15.836, Final Batch Loss: 0.378\n",
      "Epoch 19960, Loss: 16.037, Final Batch Loss: 0.533\n",
      "Epoch 19961, Loss: 15.892, Final Batch Loss: 0.411\n",
      "Epoch 19962, Loss: 15.881, Final Batch Loss: 0.415\n",
      "Epoch 19963, Loss: 15.809, Final Batch Loss: 0.464\n",
      "Epoch 19964, Loss: 15.901, Final Batch Loss: 0.407\n",
      "Epoch 19965, Loss: 16.079, Final Batch Loss: 0.586\n",
      "Epoch 19966, Loss: 15.958, Final Batch Loss: 0.506\n",
      "Epoch 19967, Loss: 15.884, Final Batch Loss: 0.358\n",
      "Epoch 19968, Loss: 15.828, Final Batch Loss: 0.444\n",
      "Epoch 19969, Loss: 15.966, Final Batch Loss: 0.375\n",
      "Epoch 19970, Loss: 15.779, Final Batch Loss: 0.318\n",
      "Epoch 19971, Loss: 15.851, Final Batch Loss: 0.420\n",
      "Epoch 19972, Loss: 16.172, Final Batch Loss: 0.470\n",
      "Epoch 19973, Loss: 16.202, Final Batch Loss: 0.449\n",
      "Epoch 19974, Loss: 16.272, Final Batch Loss: 0.505\n",
      "Epoch 19975, Loss: 15.951, Final Batch Loss: 0.465\n",
      "Epoch 19976, Loss: 16.115, Final Batch Loss: 0.418\n",
      "Epoch 19977, Loss: 15.745, Final Batch Loss: 0.422\n",
      "Epoch 19978, Loss: 15.999, Final Batch Loss: 0.468\n",
      "Epoch 19979, Loss: 16.031, Final Batch Loss: 0.468\n",
      "Epoch 19980, Loss: 15.847, Final Batch Loss: 0.327\n",
      "Epoch 19981, Loss: 16.002, Final Batch Loss: 0.505\n",
      "Epoch 19982, Loss: 15.830, Final Batch Loss: 0.487\n",
      "Epoch 19983, Loss: 15.767, Final Batch Loss: 0.391\n",
      "Epoch 19984, Loss: 15.965, Final Batch Loss: 0.485\n",
      "Epoch 19985, Loss: 16.217, Final Batch Loss: 0.376\n",
      "Epoch 19986, Loss: 16.280, Final Batch Loss: 0.405\n",
      "Epoch 19987, Loss: 16.046, Final Batch Loss: 0.468\n",
      "Epoch 19988, Loss: 16.247, Final Batch Loss: 0.525\n",
      "Epoch 19989, Loss: 16.055, Final Batch Loss: 0.523\n",
      "Epoch 19990, Loss: 16.174, Final Batch Loss: 0.380\n",
      "Epoch 19991, Loss: 16.115, Final Batch Loss: 0.523\n",
      "Epoch 19992, Loss: 15.955, Final Batch Loss: 0.372\n",
      "Epoch 19993, Loss: 15.968, Final Batch Loss: 0.417\n",
      "Epoch 19994, Loss: 15.948, Final Batch Loss: 0.421\n",
      "Epoch 19995, Loss: 15.836, Final Batch Loss: 0.428\n",
      "Epoch 19996, Loss: 15.894, Final Batch Loss: 0.496\n",
      "Epoch 19997, Loss: 16.021, Final Batch Loss: 0.474\n",
      "Epoch 19998, Loss: 15.917, Final Batch Loss: 0.492\n",
      "Epoch 19999, Loss: 15.951, Final Batch Loss: 0.597\n",
      "Epoch 20000, Loss: 15.876, Final Batch Loss: 0.412\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, label = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, label.long()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    #print(f'Epoch {epoch + 1}, Loss: {total_loss:.3f}, Final Batch Loss: {loss.item():.3f}, Penalty: {penalty:.3f}')\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss:.3f}, Final Batch Loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92       518\n",
      "           1       0.97      0.98      0.97       512\n",
      "           2       0.63      0.77      0.69       608\n",
      "           3       0.73      0.55      0.63       487\n",
      "           4       0.81      0.77      0.79       539\n",
      "\n",
      "    accuracy                           0.80      2664\n",
      "   macro avg       0.81      0.80      0.80      2664\n",
      "weighted avg       0.81      0.80      0.80      2664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():   \n",
    "        features, labels = batch\n",
    "        _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "        print(metrics.classification_report(labels.cpu(), preds.cpu(), zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
