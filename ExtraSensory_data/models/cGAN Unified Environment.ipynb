{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b295ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from prettytable import PrettyTable\n",
    "from pylab import *\n",
    "from scipy.stats import wasserstein_distance\n",
    "import random\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092fed2",
   "metadata": {},
   "source": [
    "### Load acceleration data for 3 users engaging in 3 activities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5f99f",
   "metadata": {},
   "source": [
    "The classifiers were trained on three specific users and with the acceleration features of sitting, sleeping, and walking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d92dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acceleration_features(filepath):\n",
    "    ### filepath should be to the aggregated data file\n",
    "    data = pd.read_csv(filepath)\n",
    "    data = data[(data['UUID'] == '0BFC35E2-4817-4865-BFA7-764742302A2D') | (data['UUID'] == '0A986513-7828-4D53-AA1F-E02D6DF9561B') | (data['UUID'] == '00EABED2-271D-49D8-B599-1D4A09240601')] \n",
    "    data.drop(columns = ['timestamp'], inplace = True)\n",
    "\n",
    "    only_walking = data[(data['label:FIX_walking'] == 1) & (data['label:SITTING'] == 0) & (data['label:SLEEPING'] == 0)]\n",
    "    only_walking = only_walking.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz'] \n",
    "    only_walking['label'] = \"WALKING\"\n",
    "\n",
    "    only_sitting = data[(data['label:FIX_walking'] == 0) & (data['label:SITTING'] == 1) & (data['label:SLEEPING'] == 0)]\n",
    "    only_sitting = only_sitting.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz'] \n",
    "    only_sitting['label'] = \"SITTING\"\n",
    "\n",
    "    only_sleeping = data[(data['label:FIX_walking'] == 0) & (data['label:SITTING'] == 0) & (data['label:SLEEPING'] == 1)]\n",
    "    only_sleeping = only_sleeping.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz'] \n",
    "    only_sleeping['label'] = \"SLEEPING\"\n",
    "\n",
    "    df = pd.concat([only_walking, only_sitting, only_sleeping], axis = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793684de",
   "metadata": {},
   "source": [
    "### Move Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8b56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaryClare's\n",
    "#os.chdir('/Users/maryclaremartin/Documents/jup/ExtraSensory')\n",
    "\n",
    "# Josh's\n",
    "os.chdir(\"D:/Research_Projects/REU2021-human-context-recognition/ExtraSensory_data\")\n",
    "\n",
    "# Harrison's\n",
    "#os.chdir(\"/Users/hkimr/Desktop/WPI Github/REU2021-human-context-recognition/ExtraSensory_data\")\n",
    "softmax = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2518e4",
   "metadata": {},
   "source": [
    "### Load & Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929125c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load and scale data\n",
    "#returns scaled data (X) and labels (Y)\n",
    "#file_name: string, file with data to be used\n",
    "#label: array, list of activities to use\n",
    "#users: array, list of users whose data is to be used\n",
    "\n",
    "def start_data(file_name, label, users):\n",
    "    #read csv into dataframe\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data[data['UUID'].isin(users)]\n",
    "\n",
    "    #seperate only acceleration data\n",
    "    X = data.loc[:,'raw_acc:magnitude_stats:mean':'raw_acc:3d:ro_yz']    \n",
    "    y = data[label]\n",
    "\n",
    "    #seperate only \"on\" labels\n",
    "    X = X[(y!=0).any(axis=1)]\n",
    "    y = y[(y!=0).any(axis=1)]\n",
    "    \n",
    "    #interpolate averages per column\n",
    "    X = interpolation(X).values\n",
    "    y = interpolation(y).values\n",
    "    \n",
    "    #scale the data\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X = sc.fit_transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f925998",
   "metadata": {},
   "source": [
    "### The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2620d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    #torch.manual_seed(0)\n",
    "    return torch.randn(n_samples, z_dim).to(device)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, int(hidden_dim/2)),\n",
    "            generator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            generator_block(int(hidden_dim/4), 30),\n",
    "            generator_block(30, 28),\n",
    "            nn.Linear(28, feature_dim)\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "##calculates generator loss\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#criterion1: loss function1\n",
    "#criterion2: loss function2\n",
    "#batch_size: batch size\n",
    "#z_dim: number of dimensions in the latent space\n",
    "def get_gen_loss(gen, disc, act, usr, criterion1, criterion2, batch_size, z_dim, activities, users):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, activities)\n",
    "    usr_vectors = get_usr_matrix(batch_size, users)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "    fake_features = gen(to_gen)\n",
    "    disc.eval()\n",
    "    pred_disc = disc(fake_features.to(device))\n",
    "    disc.train()\n",
    "    pred_act = act(fake_features.to(device)) ### CrossEntropyLoss Criterion automatically applies softmax and torch.max\n",
    "    pred_usr = usr(fake_features.to(device))\n",
    "    \n",
    "    d_loss = criterion1(pred_disc, torch.ones_like(pred_disc))\n",
    "    act_loss = criterion2(pred_act, act_vectors[0].to(device))\n",
    "    usr_loss = criterion2(pred_usr, usr_vectors[0].to(device))\n",
    "    \n",
    "    gen_loss = d_loss + act_loss + usr_loss\n",
    "    return gen_loss\n",
    "    \n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    #print(indexes)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c1320",
   "metadata": {},
   "source": [
    "### Create Fake Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f8ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_samples(gen, batch_size, z_dim):\n",
    "    \"\"\"\n",
    "    Generates fake acceleration features given a batch size, latent vector dimension, and trained generator.\n",
    "    \n",
    "    \"\"\"\n",
    "    latent_vectors = get_noise(batch_size, z_dim) ### Retrieves a 2D tensor of noise\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    \n",
    "    return fake_features ### Returns a 2D tensor of fake features of size batch_size x z_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3171dc9",
   "metadata": {},
   "source": [
    "### The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480f06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "#defines discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim = 26, hidden_dim = 16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            discriminator_block(feature_dim, hidden_dim),\n",
    "            discriminator_block(hidden_dim, int(hidden_dim/2)),\n",
    "            discriminator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            nn.Linear(int(hidden_dim/4), 1),\n",
    "            nn.Sigmoid()                    \n",
    "        )\n",
    "    def forward(self, feature_vector):\n",
    "        return self.disc(feature_vector)\n",
    "    \n",
    "def get_disc_loss(gen, disc, criterion, real_features, batch_size, z_dim, a_dim, u_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, a_dim)\n",
    "    usr_vectors = get_usr_matrix(batch_size, u_dim)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors.to(device), act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "    gen.eval()\n",
    "    fake_features = gen(to_gen)\n",
    "    gen.train()\n",
    "    pred_fake = disc(fake_features.detach())\n",
    "    \n",
    "    ground_truth = torch.zeros_like(pred_fake)\n",
    "    loss_fake = criterion(pred_fake, ground_truth)\n",
    "    \n",
    "    pred_real = disc(real_features.to(device))\n",
    "    ground_truth = torch.ones_like(pred_real)\n",
    "    loss_real = criterion(pred_real, ground_truth)\n",
    "    \n",
    "    disc_loss = (loss_fake + loss_real) / 2\n",
    "    return disc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1a69b",
   "metadata": {},
   "source": [
    "### User Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a2370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class User_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(User_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bb4e3",
   "metadata": {},
   "source": [
    "### Activity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9617998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Activity_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Activity_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            classifier_block(10, 5),\n",
    "            nn.Linear(5, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #softmax = nn.Softmax(dim = 1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e674e",
   "metadata": {},
   "source": [
    "### Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29773593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replaces Nan values with average values\n",
    "#df: data frame of data to use\n",
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b0303",
   "metadata": {},
   "source": [
    "### Visualize Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d2c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##prints a plot of a generator batch\n",
    "#gen: generator\n",
    "#b_size: batch size\n",
    "#epochs: current epoch (-1)\n",
    "def visualize_gen_batch(gen, b_size, epochs = -1):\n",
    "    #print(str(b_size))\n",
    "    latent_vectors = get_noise(b_size, z_dim)\n",
    "    #print(latent_vectors.shape)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    #print(fake_features.shape)\n",
    "    \n",
    "    w_img = fake_features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Generated Batch at Epoch ' + str(epochs), fontweight =\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "##prints a plot of a batch of real data\n",
    "#features: real data\n",
    "def visualize_real_batch(features):\n",
    "    w_img = features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Real Batch of Data', fontweight =\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0526a",
   "metadata": {},
   "source": [
    "### Calculate Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c5b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculates performance statistics for each epoch of training\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#b_size: batch size\n",
    "#z_dim: number of dimensions of the latent space\n",
    "##returns accuracy, precision, recall, fpR, and f1 score\n",
    "def performance_stats(gen, disc, b_size, z_dim, a_dim, u_dim, batch = None):\n",
    "    tp = 0 #true positive\n",
    "    fp = 0 #false positive\n",
    "    tn = 0 #true negative\n",
    "    fn = 0 #false negative\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if batch is None:\n",
    "            latent_vectors = get_noise(b_size, z_dim)\n",
    "            \n",
    "            act_vectors = get_act_matrix(b_size, a_dim)\n",
    "            usr_vectors = get_usr_matrix(b_size, u_dim)\n",
    "            to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "            gen.eval()\n",
    "            fake_features = gen(to_gen)\n",
    "            gen.train()\n",
    "            y_hat = torch.round(disc(fake_features))\n",
    "            y_label = [0] * b_size\n",
    "            y_label = torch.Tensor(y_label).to(device)\n",
    "        else:\n",
    "            latent_vectors = get_noise(b_size, z_dim)\n",
    "            act_vectors = get_act_matrix(b_size, a_dim)\n",
    "            usr_vectors = get_usr_matrix(b_size, u_dim)\n",
    "            \n",
    "            to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "            gen.eval()\n",
    "            fake_features = gen(to_gen)\n",
    "            gen.train()\n",
    "            \n",
    "            disc.eval()\n",
    "            y_hat = torch.round(disc(fake_features.to(device)))\n",
    "            y_label = torch.Tensor([0] * b_size)\n",
    "            \n",
    "            real_y_hat = torch.round(disc(batch.to(device)))\n",
    "            disc.train()\n",
    "            y_add = torch.Tensor([1] * b_size)\n",
    "            y_label = torch.cat((y_label, y_add), dim = 0)\n",
    "            #for i in range(0, int(b_size/2)):\n",
    "            # y_label.append(1)\n",
    "            y_hat = torch.cat((y_hat, real_y_hat), dim = 0).to(device)\n",
    "            \n",
    "            #print(y_hat)\n",
    "            #print(y_label)\n",
    "         \n",
    "        \n",
    "        for k in range(len(y_label)):\n",
    "            if y_label[k] == 1 and y_hat[k] == 1:\n",
    "                tp += 1\n",
    "            elif y_label[k] == 1 and y_hat[k] == 0:\n",
    "                fn += 1\n",
    "            elif y_label[k] == 0 and y_hat[k] == 0:\n",
    "                tn += 1\n",
    "            elif y_label[k] == 0 and y_hat[k] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "                exit()\n",
    "            \n",
    "        class_acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"TP: \"  + str(tp))\n",
    "        print(\"FP: \"  + str(fp))\n",
    "        print(\"TN: \"  + str(tn))\n",
    "        print(\"FN: \"  + str(fn) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if tp + fp == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            \n",
    "        if tp + fn == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "            \n",
    "        if fp + tn == 0:\n",
    "            fpR = 0\n",
    "        else: \n",
    "            fpR = fp / (fp + tn)\n",
    "\n",
    "        #print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "        #print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "        #print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "        #print(f'F-1 Score: {2*(precision * recall / (precision + recall + 0.001)):.2f}')\n",
    "        return class_acc, precision, recall, fpR, 2*(precision * recall / (precision + recall + 0.001))\n",
    "\n",
    "def performance_stats_class(gen, classifier, machine, batch_size, z_dim, a_dim, u_dim):\n",
    "    tp = 0 #true positive\n",
    "    fp = 0 #false positive\n",
    "    tn = 0 #true negative\n",
    "    fn = 0 #false negative\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent_vectors = get_noise(batch_size, z_dim)\n",
    "        act_vectors = get_act_matrix(batch_size, a_dim)\n",
    "        usr_vectors = get_usr_matrix(batch_size, u_dim)\n",
    "    \n",
    "        to_gen = torch.cat((latent_vectors, act_vectors[1].to(device), usr_vectors[1].to(device)), 1)\n",
    "        gen.eval()\n",
    "        fake_features = gen(to_gen)\n",
    "        gen.train()\n",
    "        #print(fake_features)\n",
    "    \n",
    "        _, pred_class = torch.max(softmax(classifier(fake_features.to(device))), dim = 1)\n",
    "        #print(pred_class)\n",
    "        labels = []\n",
    "        if machine == \"act\":\n",
    "            labels = act_vectors\n",
    "        else:\n",
    "            labels = usr_vectors\n",
    "        \n",
    "        return torch.eq(labels[0].to(device), pred_class).sum()/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121c7d0",
   "metadata": {},
   "source": [
    "### Create Density Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9401d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and plot density curves for mean, x, y, z acceleration\n",
    "#reals: real data\n",
    "#fakes: generated data\n",
    "def density_curves(reals, fakes):\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    subplot(2, 2, 1)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,0], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,0], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 2)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,18], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,18], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean X-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 3)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,19], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,19], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Y-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 4)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,20], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,20], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Z-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574b48f",
   "metadata": {},
   "source": [
    "### Calculate Wassertein distance for each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe452c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate Waaserstein distances for each dimension\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "#feature_dim: number ofd dimensions in the feature space\n",
    "#sample: sample of data\n",
    "def all_Wasserstein_dists(gen, z_dim, feature_dim, sample):\n",
    "    wasser_dim = []\n",
    "    latent_vectors = get_noise(len(sample), z_dim)\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    for k in range(feature_dim):\n",
    "        wasser_dim.append(wasserstein_distance(fake_features[:, k].cpu().detach().numpy(), sample[:, k].cpu().detach().numpy()))\n",
    "    return torch.tensor(wasser_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d4d8f",
   "metadata": {},
   "source": [
    "### Visualizing Generation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf2423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates and prints a plot of the generated vs real data\n",
    "#data: data used\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "def visualize_gen(data, gen, z_dim, a_dim, u_dim):\n",
    "    #Number of datum to visualize\n",
    "    sample_size = len(data)\n",
    "    reals = data[0:sample_size, :]\n",
    "    fakes = get_fake_samples(gen, sample_size, z_dim).detach()\n",
    "    density_curves(reals, fakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd0ff5",
   "metadata": {},
   "source": [
    "### Initialize Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aebeaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initalize parameters that depend on training loop parameters\n",
    "#X: acceleration data\n",
    "#y: labels associated with X data (fake or real)\n",
    "#z_dim: number of dimensions to the latent space\n",
    "#disc_lr: discriminator learning rate\n",
    "#gen_lr: generator learning rate\n",
    "#DISCRIMINATOR: 1 to indicate if discriminator is training\n",
    "#batch_size: batch size\n",
    "#disc: initialized discrimiantor\n",
    "\n",
    "def initialize_params(X, y, z_dim, a_dim, u_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc):\n",
    "    #initialize generator\n",
    "    gen = Generator(z_dim + a_dim + u_dim).to(device)\n",
    "    #indicate that discriminator is training\n",
    "    to_train = DISCRIMINATOR\n",
    "    #create training features\n",
    "    train_features = torch.tensor(X)\n",
    "    #create training labels\n",
    "    train_labels = torch.tensor(y)\n",
    "    #concatenate to create training data\n",
    "    train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    #create data loader for training data\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    #initialize generator and discriminator optimizers\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr = disc_lr)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr = gen_lr)\n",
    "    \n",
    "    return gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462505c",
   "metadata": {},
   "source": [
    "# Save / Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ccf4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path and name of the Generator and Discriminator accordingly\n",
    "def save_model(gen, disc, model_name):\n",
    "    torch.save(gen.state_dict(), f\"saved_models/{model_name}_gen\")\n",
    "    torch.save(disc.state_dict(), f\"saved_models/{model_name}_disc\")\n",
    "    \n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5222799",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eddb1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Training loop to train GAN\n",
    "\n",
    "#Parameters to specifiy: \n",
    "    #X: starting accelerometer data\n",
    "    #y: starting labels for X data (fake or real)\n",
    "    \n",
    "#Set parameters (do not change)\n",
    "    #criterion: loss function (BCE)\n",
    "    #dig: number of significant digits for printing (5)\n",
    "    #feature_dim: Number of dimensions of output from generator (26)\n",
    "    #GENERATOR: set generator to zero for training\n",
    "    #DISCRIMINATOR: set discriminator to one for training\n",
    "    #train_string: starting machine to train (DISC)\n",
    "    #disc: initalize discriminator\n",
    "    #rel_epochs: Epochs passed since last switch (constant training) (0)\n",
    "    #rows: initialization of array to save data of each epoch to CSV file ([])\n",
    "    #heading: array of column headings for table ([\"Epoch\", \"Machine Training\", \"Discriminator Loss\", \n",
    "                    #\"Generator Loss\", \"FPR\", \"Recall\", \"Median Wasserstein\", \"Mean Wasserstein\"])\n",
    "    #table: intialize a table as a pretty table to save epoch data\n",
    "    #switch_count: number of switches in dynamic training (0)\n",
    "    \n",
    "#Set parameters (can change):\n",
    "    #z_dim: number of dimensions of latent vector (100)\n",
    "    #gen_lr: generator learning rate (.001)\n",
    "    #disc_lr: discriminator learning rate (.001) (shoud be equal to gen_lr)\n",
    "    #batch_size: batch size (75)\n",
    "    #print_batches: Show model performance per batch (False)\n",
    "    #n_epochs: number of epochs to train (100)\n",
    "    #constant_train_flag: (False)\n",
    "        #Set to true to train based on constant # of epochs per machine \n",
    "        #Set to false to train dynamically based on machine performance\n",
    "        \n",
    "    #Constant training approach:\n",
    "        #disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (5)\n",
    "        #gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #epoch_threshold: Epoch number to change training epoch ratio (50)\n",
    "        #disc_epochs_change: New number of consecutive epochs to train discriminator after epoch threshold is exceeded (1)\n",
    "        #gen_epochs_change: New number of consecutive epochs to train generator after epoch threshold is exceeded (50)\n",
    "    \n",
    "    #Dynamic training approach:                        \n",
    "        #static_threshold: Epoch number to change from static ratio to dynamic (18)\n",
    "        #static_disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (4)\n",
    "        #static_gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #pull_threshold: Accuracy threshold for switching machine training when the generator is no longer competitive (0.4)\n",
    "        #push_threshold: Accuracy threshold for switching machine training when the discriminator is no longer competitive (0.6)\n",
    "        #recall_threshold: threshold for recall to switch machine training when discriminator is training well\n",
    "        #switch_flag: indicates if we should switch our training machine (False)\n",
    "        \n",
    "def training_loop(X, y, act, usr, criterion1 = nn.BCELoss(), criterion2 = nn.CrossEntropyLoss(), gan_id = \"Mod Test Gan\", dig = 5, feature_dim = 26, \n",
    "                  GENERATOR = 0, DISCRIMINATOR = 1, train_string = \"DISC\", disc = Discriminator(), z_dim = 100, a_dim = 3, u_dim = 3, \n",
    "                  gen_lr =  0.001, disc_lr = 0.001, batch_size = 100, constant_train_flag = False, disc_epochs = 5,\n",
    "                  gen_epochs = 2, epoch_threshold = 50, disc_epochs_change = 5, gen_epochs_change = 2, rel_epochs = 0,\n",
    "                 static_threshold = 28, static_disc_epochs = 5, static_gen_epochs = 2, pull_threshold = 0.3,\n",
    "                 push_threshold = 0.7, recall_threshold = 0.75, print_batches = False, n_epochs = 1000, rows = [],\n",
    "                 heading = [\"Epoch\", \"Training\", \"Discriminator Loss\", \"Generator Loss\", \"D_Accuracy\", \"D_fpR\", \"D_Recall\", \"A_fpR\", \"U_fpR\"],\n",
    "                 table = PrettyTable(), switch_flag = False, switch_count = 0, last_real_features = []):\n",
    "    \n",
    "    disc.to(device)\n",
    "    #returns generator, sets discriminator training, creates training tensor, loads data, and initializes optimizers\n",
    "    gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen = initialize_params(X, y, z_dim, a_dim, u_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc)\n",
    "\n",
    "    #set pretty table field names\n",
    "    table.field_names = heading\n",
    "    \n",
    "    #visualize_gen(X, gen, z_dim, a_dim, u_dim)\n",
    "\n",
    "    gen_epochs = 0\n",
    "    \n",
    "    last_D_loss = -1.0\n",
    "    last_G_loss = -1.0\n",
    "    \n",
    "    mean_mean = []\n",
    "    mean_median = []\n",
    "    \n",
    "    for epoch in range(n_epochs):  \n",
    "        if constant_train_flag:\n",
    "            if to_train == DISCRIMINATOR and rel_epochs >= disc_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GEN\"\n",
    "\n",
    "            elif to_train == GENERATOR and rel_epochs >= gen_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = DISCRIMINATOR\n",
    "                train_string = \"DISC\"\n",
    "\n",
    "            # Change epoch ratio after intial 'leveling out'\n",
    "            if epoch == epoch_threshold:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GENERATOR\"\n",
    "\n",
    "                old_ratio = gen_epochs / disc_epochs\n",
    "                gen_epochs = gen_epochs_change\n",
    "                disc_epochs = disc_epochs_change\n",
    "                new_ratio = gen_epochs / disc_epochs\n",
    "                print(f'\\n\\nTraining ratio of G/D switched from {old_ratio:.{dig}f} to {new_ratio:.{dig}f}\\n\\n')\n",
    "        else:\n",
    "            if epoch < static_threshold:\n",
    "                if to_train == DISCRIMINATOR and rel_epochs >= static_disc_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "\n",
    "                elif to_train == GENERATOR and rel_epochs >= static_gen_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "\n",
    "            else:\n",
    "                if not switch_flag:\n",
    "                    print(\"\\nSwitching to Dynamic Training\\n\")\n",
    "                    switch_flag = True\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "                if to_train == DISCRIMINATOR and fpR <= pull_threshold and R >= recall_threshold:\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "                    print(\"\\nPull Generator\\n\")\n",
    "                    switch_count += 1\n",
    "                if to_train == GENERATOR and fpR >= push_threshold:\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "                    print(\"\\nPush Generator\\n\")\n",
    "                    switch_count += 1\n",
    "        print(f'Epoch[{epoch + 1}/{n_epochs}] Train: {train_string} ', end ='')\n",
    "        for batch_idx, (real_features, _) in enumerate(train_loader):\n",
    "            #batch_size = len(real_features)\n",
    "            #print(len(real_features))\n",
    "            \n",
    "            if print_batches:\n",
    "                    print(f'\\n\\tBatch[{batch_idx + 1}/{len(train_loader)}] |', end ='')\n",
    "\n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Training Discriminator\n",
    "                #visualize_real_batch(real_features.float())\n",
    "                opt_disc.zero_grad()\n",
    "                disc_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), len(real_features), z_dim, a_dim, u_dim)\n",
    "                disc_loss.backward(retain_graph = True)\n",
    "                #disc_loss.backward()\n",
    "                opt_disc.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), z_dim, a_dim, u_dim, batch = real_features.float())\n",
    "                A_fpR = performance_stats_class(gen, act, \"act\", batch_size, z_dim, a_dim, u_dim)\n",
    "                U_fpR = performance_stats_class(gen, usr, \"usr\", batch_size, z_dim, a_dim, u_dim)\n",
    "                #w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                #median_w_dist = torch.median(w_dist)\n",
    "                #mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                #mean_mean.append(mean_w_dist)\n",
    "                #mean_median.append(median_w_dist)\n",
    "\n",
    "                last_D_loss = disc_loss.item()\n",
    "                \n",
    "                if last_G_loss == -1.0:\n",
    "                    last_G_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, len(real_features), z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f}')\n",
    "            else:\n",
    "                ### Training Generator\n",
    "                opt_gen.zero_grad()\n",
    "                gen_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, len(real_features), z_dim, a_dim, u_dim)\n",
    "                gen_loss.backward()\n",
    "                opt_gen.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, len(real_features), z_dim, a_dim, u_dim, batch = real_features.float())\n",
    "                A_fpR = performance_stats_class(gen, act, \"act\", batch_size, z_dim, a_dim, u_dim)\n",
    "                U_fpR = performance_stats_class(gen, usr, \"usr\", batch_size, z_dim, a_dim, u_dim)\n",
    "                #w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                #median_w_dist = torch.median(w_dist)\n",
    "                #mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                #mean_mean.append(mean_w_dist)\n",
    "                #mean_median.append(median_w_dist)\n",
    "                \n",
    "                last_G_loss = gen_loss.item()\n",
    "                \n",
    "                if last_D_loss == -1.0:\n",
    "                    last_D_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), len(real_features), z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f}')\n",
    "\n",
    "        if not print_batches:\n",
    "            \n",
    "            mean_mean_w = torch.mean(torch.Tensor(mean_mean)) \n",
    "            \n",
    "            mean_median_w = torch.mean(torch.Tensor(mean_median))\n",
    "            \n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Currently doesn't print Median/Mean Wasserstein --> Change if needed\n",
    "                print(f'| LossD: {last_D_loss:.{dig}f}, LossG: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | R: {R:.{dig}f} | A_fpR: {A_fpR:.{dig}f} | U_fpR: {U_fpR:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Disc\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{A_fpR:.{dig}f}\", f\"{U_fpR:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "            else:\n",
    "                print(f'| LossD: {last_D_loss:.{dig}f}, LossG: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | R: {R:.{dig}f} | A_fpR: {A_fpR:.{dig}f} | U_fpR: {U_fpR:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Gen\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{R:.{dig}f}\", f\"{A_fpR:.{dig}f}\", f\"{U_fpR:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "                gen_epochs += 1\n",
    "        mean_mean.clear()\n",
    "        mean_median.clear()\n",
    "        rel_epochs += 1\n",
    "    print(\"\\n\\nTraining Session Finished\")\n",
    "    print(f\"Encountered {switch_count} non-trivial training swaps\")\n",
    "    percent = gen_epochs / n_epochs\n",
    "    print(f\"Trained Generator {gen_epochs} out of {n_epochs} ({percent:.3f})\")\n",
    "    f = open(\"model_outputs/\" + gan_id + \".txt\", \"w\")\n",
    "    f.write(table.get_string())\n",
    "    f.close()\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".txt\\\"\")\n",
    "\n",
    "    with open(\"model_outputs/\" + gan_id + \".csv\", \"w\") as csvfile: \n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "        # writing the fields \n",
    "        csvwriter.writerow(heading)\n",
    "        # writing the data rows \n",
    "        csvwriter.writerows(rows)\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".csv\\\"\")\n",
    "    save_model(gen, disc, gan_id)\n",
    "    model_output = pd.read_csv(\"model_outputs/\" + gan_id + \".csv\")\n",
    "    #visualize_gen(X, gen, z_dim)\n",
    "    \n",
    "    # Change path and name of the Generator and Discriminator accordingly\n",
    "    save_model(gen, disc, gan_id)\n",
    "    \n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8f1fd",
   "metadata": {},
   "source": [
    "# Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9924892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot metrics based on data (csv)\n",
    "def plot_metrics(data, vanilla = True):\n",
    "    if vanilla:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "    \n",
    "        plt.figure(figsize = (15, 15))\n",
    "        subplot(2, 2, 1)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'FPR', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 2)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'Recall', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 3)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 4)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "        plt.figure(figsize = (15, 8))\n",
    "        \n",
    "        subplot(1, 2, 1)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(1, 2, 2)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c10e2",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf46cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.72644, LossG: 10.94945 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33897 | U_fpR: 0.33563\n",
      "Epoch[2/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.72192, LossG: 10.94945 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33241 | U_fpR: 0.33690\n",
      "Epoch[3/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.71839, LossG: 10.94945 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33161 | U_fpR: 0.33264\n",
      "Epoch[4/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.71543, LossG: 10.94945 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33333 | U_fpR: 0.33563\n",
      "Epoch[5/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.71260, LossG: 10.94945 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33011 | U_fpR: 0.32562\n",
      "Epoch[6/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.71260, LossG: 10.63792 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33656 | U_fpR: 0.33391\n",
      "Epoch[7/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.71260, LossG: 7.83216 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.32654 | U_fpR: 0.33414\n",
      "Epoch[8/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.71014, LossG: 7.83216 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33667 | U_fpR: 0.33989\n",
      "Epoch[9/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.70767, LossG: 7.83216 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33218 | U_fpR: 0.32850\n",
      "Epoch[10/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.70568, LossG: 7.83216 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33506 | U_fpR: 0.34403\n",
      "Epoch[11/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.70346, LossG: 7.83216 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.32712 | U_fpR: 0.32873\n",
      "Epoch[12/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.70115, LossG: 7.83216 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33034 | U_fpR: 0.33517\n",
      "Epoch[13/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.70115, LossG: 6.83112 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.32424 | U_fpR: 0.34254\n",
      "Epoch[14/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.70115, LossG: 6.39623 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.32700 | U_fpR: 0.33195\n",
      "Epoch[15/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.69880, LossG: 6.39623 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33391 | U_fpR: 0.32505\n",
      "Epoch[16/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.69666, LossG: 6.39623 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33149 | U_fpR: 0.33805\n",
      "Epoch[17/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.69445, LossG: 6.39623 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33679 | U_fpR: 0.33517\n",
      "Epoch[18/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.69235, LossG: 6.39623 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33161 | U_fpR: 0.33414\n",
      "Epoch[19/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.69033, LossG: 6.39623 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33679 | U_fpR: 0.33011\n",
      "Epoch[20/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.69033, LossG: 5.88644 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33656 | U_fpR: 0.33103\n",
      "Epoch[21/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.69033, LossG: 5.61667 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33138 | U_fpR: 0.33149\n",
      "Epoch[22/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.68774, LossG: 5.61667 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.34070 | U_fpR: 0.33218\n",
      "Epoch[23/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.68541, LossG: 5.61667 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33702 | U_fpR: 0.33598\n",
      "Epoch[24/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.68226, LossG: 5.61667 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.32896 | U_fpR: 0.32804\n",
      "Epoch[25/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.67938, LossG: 5.61667 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.32850 | U_fpR: 0.33725\n",
      "Epoch[26/5000] Train: DISC \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.67612, LossG: 5.61667 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.34058 | U_fpR: 0.33379\n",
      "Epoch[27/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.67612, LossG: 5.35515 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33000 | U_fpR: 0.33230\n",
      "Epoch[28/5000] Train: GEN \n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8691\n",
      "\n",
      "| LossD: 0.67612, LossG: 5.10154 | Acc: 0.50000 | fpR: 0.00000 | R: 0.00000 | A_fpR: 0.33725 | U_fpR: 0.33425\n",
      "\n",
      "Switching to Dynamic Training\n",
      "\n",
      "Epoch[29/5000] Train: DISC \n",
      "\n",
      "TP: 5\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8686\n",
      "\n",
      "| LossD: 0.67177, LossG: 5.10154 | Acc: 0.50029 | fpR: 0.00000 | R: 0.00058 | A_fpR: 0.34001 | U_fpR: 0.33667\n",
      "Epoch[30/5000] Train: DISC \n",
      "\n",
      "TP: 39\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8652\n",
      "\n",
      "| LossD: 0.66764, LossG: 5.10154 | Acc: 0.50224 | fpR: 0.00000 | R: 0.00449 | A_fpR: 0.33276 | U_fpR: 0.33000\n",
      "Epoch[31/5000] Train: DISC \n",
      "\n",
      "TP: 99\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8592\n",
      "\n",
      "| LossD: 0.66325, LossG: 5.10154 | Acc: 0.50570 | fpR: 0.00000 | R: 0.01139 | A_fpR: 0.32620 | U_fpR: 0.33863\n",
      "Epoch[32/5000] Train: DISC \n",
      "\n",
      "TP: 213\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8478\n",
      "\n",
      "| LossD: 0.65824, LossG: 5.10154 | Acc: 0.51225 | fpR: 0.00000 | R: 0.02451 | A_fpR: 0.34162 | U_fpR: 0.32793\n",
      "Epoch[33/5000] Train: DISC \n",
      "\n",
      "TP: 316\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8375\n",
      "\n",
      "| LossD: 0.65312, LossG: 5.10154 | Acc: 0.51818 | fpR: 0.00000 | R: 0.03636 | A_fpR: 0.34357 | U_fpR: 0.32988\n",
      "Epoch[34/5000] Train: DISC \n",
      "\n",
      "TP: 429\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8262\n",
      "\n",
      "| LossD: 0.64695, LossG: 5.10154 | Acc: 0.52468 | fpR: 0.00000 | R: 0.04936 | A_fpR: 0.32919 | U_fpR: 0.33241\n",
      "Epoch[35/5000] Train: DISC \n",
      "\n",
      "TP: 585\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 8106\n",
      "\n",
      "| LossD: 0.64076, LossG: 5.10154 | Acc: 0.53366 | fpR: 0.00000 | R: 0.06731 | A_fpR: 0.33690 | U_fpR: 0.33276\n",
      "Epoch[36/5000] Train: DISC \n",
      "\n",
      "TP: 756\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 7935\n",
      "\n",
      "| LossD: 0.63405, LossG: 5.10154 | Acc: 0.54349 | fpR: 0.00000 | R: 0.08699 | A_fpR: 0.32954 | U_fpR: 0.33402\n",
      "Epoch[37/5000] Train: DISC \n",
      "\n",
      "TP: 926\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 7765\n",
      "\n",
      "| LossD: 0.62609, LossG: 5.10154 | Acc: 0.55327 | fpR: 0.00000 | R: 0.10655 | A_fpR: 0.33057 | U_fpR: 0.33310\n",
      "Epoch[38/5000] Train: DISC \n",
      "\n",
      "TP: 1396\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 7295\n",
      "\n",
      "| LossD: 0.61850, LossG: 5.10154 | Acc: 0.58031 | fpR: 0.00000 | R: 0.16063 | A_fpR: 0.33391 | U_fpR: 0.31895\n",
      "Epoch[39/5000] Train: DISC \n",
      "\n",
      "TP: 3539\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 5152\n",
      "\n",
      "| LossD: 0.60911, LossG: 5.10154 | Acc: 0.70360 | fpR: 0.00000 | R: 0.40720 | A_fpR: 0.33034 | U_fpR: 0.33230\n",
      "Epoch[40/5000] Train: DISC \n",
      "\n",
      "TP: 5290\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 3401\n",
      "\n",
      "| LossD: 0.60005, LossG: 5.10154 | Acc: 0.80434 | fpR: 0.00000 | R: 0.60868 | A_fpR: 0.33506 | U_fpR: 0.33517\n",
      "Epoch[41/5000] Train: DISC \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 5.10154 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33276 | U_fpR: 0.32885\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[42/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 4.95880 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.34277 | U_fpR: 0.33943\n",
      "Epoch[43/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 4.89617 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33138 | U_fpR: 0.33460\n",
      "Epoch[44/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 4.73214 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33218 | U_fpR: 0.33299\n",
      "Epoch[45/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 4.54905 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.32931 | U_fpR: 0.33517\n",
      "Epoch[46/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 4.38040 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33310 | U_fpR: 0.33391\n",
      "Epoch[47/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 4.23699 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.34288 | U_fpR: 0.33748\n",
      "Epoch[48/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 4.10481 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.32459 | U_fpR: 0.35658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[49/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.95373 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33333 | U_fpR: 0.36233\n",
      "Epoch[50/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.86177 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33644 | U_fpR: 0.38902\n",
      "Epoch[51/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.72086 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.32816 | U_fpR: 0.40490\n",
      "Epoch[52/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.66515 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33771 | U_fpR: 0.41790\n",
      "Epoch[53/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.54742 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.34070 | U_fpR: 0.42952\n",
      "Epoch[54/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.42445 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33322 | U_fpR: 0.44701\n",
      "Epoch[55/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.38881 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33115 | U_fpR: 0.46335\n",
      "Epoch[56/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.31207 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33897 | U_fpR: 0.47808\n",
      "Epoch[57/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.29376 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.32355 | U_fpR: 0.49419\n",
      "Epoch[58/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.18882 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33414 | U_fpR: 0.52986\n",
      "Epoch[59/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.11493 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33851 | U_fpR: 0.54482\n",
      "Epoch[60/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.07240 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33333 | U_fpR: 0.57393\n",
      "Epoch[61/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1\n",
      "TN: 8690\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 3.02821 | Acc: 0.87913 | fpR: 0.00012 | R: 0.75837 | A_fpR: 0.33023 | U_fpR: 0.59073\n",
      "Epoch[62/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 0\n",
      "TN: 8691\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.95542 | Acc: 0.87919 | fpR: 0.00000 | R: 0.75837 | A_fpR: 0.33023 | U_fpR: 0.62214\n",
      "Epoch[63/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3\n",
      "TN: 8688\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.91516 | Acc: 0.87901 | fpR: 0.00035 | R: 0.75837 | A_fpR: 0.33333 | U_fpR: 0.63330\n",
      "Epoch[64/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1\n",
      "TN: 8690\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.87757 | Acc: 0.87913 | fpR: 0.00012 | R: 0.75837 | A_fpR: 0.33725 | U_fpR: 0.66103\n",
      "Epoch[65/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1\n",
      "TN: 8690\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.81468 | Acc: 0.87913 | fpR: 0.00012 | R: 0.75837 | A_fpR: 0.34231 | U_fpR: 0.67173\n",
      "Epoch[66/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1\n",
      "TN: 8690\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.76560 | Acc: 0.87913 | fpR: 0.00012 | R: 0.75837 | A_fpR: 0.33782 | U_fpR: 0.69854\n",
      "Epoch[67/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1\n",
      "TN: 8690\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.72980 | Acc: 0.87913 | fpR: 0.00012 | R: 0.75837 | A_fpR: 0.33529 | U_fpR: 0.69946\n",
      "Epoch[68/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3\n",
      "TN: 8688\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.69295 | Acc: 0.87901 | fpR: 0.00035 | R: 0.75837 | A_fpR: 0.33748 | U_fpR: 0.70855\n",
      "Epoch[69/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 6\n",
      "TN: 8685\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.67045 | Acc: 0.87884 | fpR: 0.00069 | R: 0.75837 | A_fpR: 0.33759 | U_fpR: 0.71499\n",
      "Epoch[70/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 9\n",
      "TN: 8682\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.64304 | Acc: 0.87867 | fpR: 0.00104 | R: 0.75837 | A_fpR: 0.32896 | U_fpR: 0.72661\n",
      "Epoch[71/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 17\n",
      "TN: 8674\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.61939 | Acc: 0.87821 | fpR: 0.00196 | R: 0.75837 | A_fpR: 0.33609 | U_fpR: 0.73870\n",
      "Epoch[72/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 15\n",
      "TN: 8676\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.59285 | Acc: 0.87832 | fpR: 0.00173 | R: 0.75837 | A_fpR: 0.33460 | U_fpR: 0.73605\n",
      "Epoch[73/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 26\n",
      "TN: 8665\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.56601 | Acc: 0.87769 | fpR: 0.00299 | R: 0.75837 | A_fpR: 0.34185 | U_fpR: 0.74226\n",
      "Epoch[74/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 37\n",
      "TN: 8654\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.55588 | Acc: 0.87706 | fpR: 0.00426 | R: 0.75837 | A_fpR: 0.33609 | U_fpR: 0.75009\n",
      "Epoch[75/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 72\n",
      "TN: 8619\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.53551 | Acc: 0.87504 | fpR: 0.00828 | R: 0.75837 | A_fpR: 0.33840 | U_fpR: 0.75273\n",
      "Epoch[76/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 85\n",
      "TN: 8606\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.50862 | Acc: 0.87430 | fpR: 0.00978 | R: 0.75837 | A_fpR: 0.34139 | U_fpR: 0.74997\n",
      "Epoch[77/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 95\n",
      "TN: 8596\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.50027 | Acc: 0.87372 | fpR: 0.01093 | R: 0.75837 | A_fpR: 0.33115 | U_fpR: 0.75066\n",
      "Epoch[78/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 127\n",
      "TN: 8564\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.48237 | Acc: 0.87188 | fpR: 0.01461 | R: 0.75837 | A_fpR: 0.33897 | U_fpR: 0.75745\n",
      "Epoch[79/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 145\n",
      "TN: 8546\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.46769 | Acc: 0.87084 | fpR: 0.01668 | R: 0.75837 | A_fpR: 0.33483 | U_fpR: 0.75446\n",
      "Epoch[80/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 187\n",
      "TN: 8504\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.46978 | Acc: 0.86843 | fpR: 0.02152 | R: 0.75837 | A_fpR: 0.33909 | U_fpR: 0.76263\n",
      "Epoch[81/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 206\n",
      "TN: 8485\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.45183 | Acc: 0.86733 | fpR: 0.02370 | R: 0.75837 | A_fpR: 0.33632 | U_fpR: 0.75780\n",
      "Epoch[82/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 247\n",
      "TN: 8444\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.42897 | Acc: 0.86498 | fpR: 0.02842 | R: 0.75837 | A_fpR: 0.32447 | U_fpR: 0.77068\n",
      "Epoch[83/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 279\n",
      "TN: 8412\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.41057 | Acc: 0.86313 | fpR: 0.03210 | R: 0.75837 | A_fpR: 0.32747 | U_fpR: 0.76988\n",
      "Epoch[84/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 311\n",
      "TN: 8380\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.40900 | Acc: 0.86129 | fpR: 0.03578 | R: 0.75837 | A_fpR: 0.33598 | U_fpR: 0.77690\n",
      "Epoch[85/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 316\n",
      "TN: 8375\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.40879 | Acc: 0.86101 | fpR: 0.03636 | R: 0.75837 | A_fpR: 0.34449 | U_fpR: 0.77356\n",
      "Epoch[86/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 362\n",
      "TN: 8329\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.39816 | Acc: 0.85836 | fpR: 0.04165 | R: 0.75837 | A_fpR: 0.33748 | U_fpR: 0.78092\n",
      "Epoch[87/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 384\n",
      "TN: 8307\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.39597 | Acc: 0.85709 | fpR: 0.04418 | R: 0.75837 | A_fpR: 0.32724 | U_fpR: 0.78633\n",
      "Epoch[88/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 434\n",
      "TN: 8257\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.38166 | Acc: 0.85422 | fpR: 0.04994 | R: 0.75837 | A_fpR: 0.32712 | U_fpR: 0.78518\n",
      "Epoch[89/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 478\n",
      "TN: 8213\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.36753 | Acc: 0.85169 | fpR: 0.05500 | R: 0.75837 | A_fpR: 0.34864 | U_fpR: 0.80451\n",
      "Epoch[90/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 506\n",
      "TN: 8185\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.35955 | Acc: 0.85007 | fpR: 0.05822 | R: 0.75837 | A_fpR: 0.34081 | U_fpR: 0.79588\n",
      "Epoch[91/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 531\n",
      "TN: 8160\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.35531 | Acc: 0.84864 | fpR: 0.06110 | R: 0.75837 | A_fpR: 0.33851 | U_fpR: 0.80520\n",
      "Epoch[92/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 554\n",
      "TN: 8137\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.34647 | Acc: 0.84731 | fpR: 0.06374 | R: 0.75837 | A_fpR: 0.33333 | U_fpR: 0.81418\n",
      "Epoch[93/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 601\n",
      "TN: 8090\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.34196 | Acc: 0.84461 | fpR: 0.06915 | R: 0.75837 | A_fpR: 0.33955 | U_fpR: 0.82913\n",
      "Epoch[94/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 626\n",
      "TN: 8065\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.33250 | Acc: 0.84317 | fpR: 0.07203 | R: 0.75837 | A_fpR: 0.33690 | U_fpR: 0.82373\n",
      "Epoch[95/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 755\n",
      "TN: 7936\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.32447 | Acc: 0.83575 | fpR: 0.08687 | R: 0.75837 | A_fpR: 0.33782 | U_fpR: 0.83293\n",
      "Epoch[96/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 783\n",
      "TN: 7908\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.29634 | Acc: 0.83414 | fpR: 0.09009 | R: 0.75837 | A_fpR: 0.34116 | U_fpR: 0.84018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[97/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 784\n",
      "TN: 7907\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.29017 | Acc: 0.83408 | fpR: 0.09021 | R: 0.75837 | A_fpR: 0.34231 | U_fpR: 0.84133\n",
      "Epoch[98/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 826\n",
      "TN: 7865\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.29036 | Acc: 0.83166 | fpR: 0.09504 | R: 0.75837 | A_fpR: 0.33955 | U_fpR: 0.85215\n",
      "Epoch[99/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 897\n",
      "TN: 7794\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.26914 | Acc: 0.82758 | fpR: 0.10321 | R: 0.75837 | A_fpR: 0.34139 | U_fpR: 0.84973\n",
      "Epoch[100/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1037\n",
      "TN: 7654\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.26294 | Acc: 0.81953 | fpR: 0.11932 | R: 0.75837 | A_fpR: 0.33368 | U_fpR: 0.86676\n",
      "Epoch[101/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1091\n",
      "TN: 7600\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.25176 | Acc: 0.81642 | fpR: 0.12553 | R: 0.75837 | A_fpR: 0.33886 | U_fpR: 0.87930\n",
      "Epoch[102/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1120\n",
      "TN: 7571\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.22929 | Acc: 0.81475 | fpR: 0.12887 | R: 0.75837 | A_fpR: 0.34518 | U_fpR: 0.89345\n",
      "Epoch[103/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1255\n",
      "TN: 7436\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.22034 | Acc: 0.80698 | fpR: 0.14440 | R: 0.75837 | A_fpR: 0.35473 | U_fpR: 0.90024\n",
      "Epoch[104/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1330\n",
      "TN: 7361\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.21644 | Acc: 0.80267 | fpR: 0.15303 | R: 0.75837 | A_fpR: 0.35358 | U_fpR: 0.91083\n",
      "Epoch[105/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1373\n",
      "TN: 7318\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.19626 | Acc: 0.80020 | fpR: 0.15798 | R: 0.75837 | A_fpR: 0.34875 | U_fpR: 0.92429\n",
      "Epoch[106/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1440\n",
      "TN: 7251\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.19191 | Acc: 0.79634 | fpR: 0.16569 | R: 0.75837 | A_fpR: 0.34369 | U_fpR: 0.93073\n",
      "Epoch[107/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1442\n",
      "TN: 7249\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.16260 | Acc: 0.79623 | fpR: 0.16592 | R: 0.75837 | A_fpR: 0.34967 | U_fpR: 0.93844\n",
      "Epoch[108/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1551\n",
      "TN: 7140\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.15244 | Acc: 0.78996 | fpR: 0.17846 | R: 0.75837 | A_fpR: 0.35450 | U_fpR: 0.94431\n",
      "Epoch[109/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1654\n",
      "TN: 7037\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.15138 | Acc: 0.78403 | fpR: 0.19031 | R: 0.75837 | A_fpR: 0.35381 | U_fpR: 0.95340\n",
      "Epoch[110/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1688\n",
      "TN: 7003\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.12866 | Acc: 0.78207 | fpR: 0.19422 | R: 0.75837 | A_fpR: 0.35059 | U_fpR: 0.95685\n",
      "Epoch[111/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1707\n",
      "TN: 6984\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.10934 | Acc: 0.78098 | fpR: 0.19641 | R: 0.75837 | A_fpR: 0.35427 | U_fpR: 0.96410\n",
      "Epoch[112/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1814\n",
      "TN: 6877\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.09714 | Acc: 0.77482 | fpR: 0.20872 | R: 0.75837 | A_fpR: 0.34680 | U_fpR: 0.96836\n",
      "Epoch[113/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1910\n",
      "TN: 6781\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.07842 | Acc: 0.76930 | fpR: 0.21977 | R: 0.75837 | A_fpR: 0.34634 | U_fpR: 0.97123\n",
      "Epoch[114/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 1909\n",
      "TN: 6782\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.07156 | Acc: 0.76936 | fpR: 0.21965 | R: 0.75837 | A_fpR: 0.34588 | U_fpR: 0.97860\n",
      "Epoch[115/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2017\n",
      "TN: 6674\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.06090 | Acc: 0.76315 | fpR: 0.23208 | R: 0.75837 | A_fpR: 0.35163 | U_fpR: 0.98067\n",
      "Epoch[116/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2088\n",
      "TN: 6603\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.03872 | Acc: 0.75906 | fpR: 0.24025 | R: 0.75837 | A_fpR: 0.36049 | U_fpR: 0.98562\n",
      "Epoch[117/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2098\n",
      "TN: 6593\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.02990 | Acc: 0.75849 | fpR: 0.24140 | R: 0.75837 | A_fpR: 0.36440 | U_fpR: 0.98631\n",
      "Epoch[118/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2152\n",
      "TN: 6539\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 2.00883 | Acc: 0.75538 | fpR: 0.24761 | R: 0.75837 | A_fpR: 0.35784 | U_fpR: 0.98792\n",
      "Epoch[119/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2256\n",
      "TN: 6435\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.99505 | Acc: 0.74940 | fpR: 0.25958 | R: 0.75837 | A_fpR: 0.35232 | U_fpR: 0.99010\n",
      "Epoch[120/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2317\n",
      "TN: 6374\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.98103 | Acc: 0.74589 | fpR: 0.26660 | R: 0.75837 | A_fpR: 0.36014 | U_fpR: 0.99091\n",
      "Epoch[121/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2397\n",
      "TN: 6294\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.97930 | Acc: 0.74128 | fpR: 0.27580 | R: 0.75837 | A_fpR: 0.35025 | U_fpR: 0.99333\n",
      "Epoch[122/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2405\n",
      "TN: 6286\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.96013 | Acc: 0.74082 | fpR: 0.27672 | R: 0.75837 | A_fpR: 0.36037 | U_fpR: 0.99482\n",
      "Epoch[123/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2454\n",
      "TN: 6237\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.95489 | Acc: 0.73800 | fpR: 0.28236 | R: 0.75837 | A_fpR: 0.36647 | U_fpR: 0.99551\n",
      "Epoch[124/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2570\n",
      "TN: 6121\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.93629 | Acc: 0.73133 | fpR: 0.29571 | R: 0.75837 | A_fpR: 0.35278 | U_fpR: 0.99701\n",
      "Epoch[125/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2569\n",
      "TN: 6122\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.93314 | Acc: 0.73139 | fpR: 0.29559 | R: 0.75837 | A_fpR: 0.36452 | U_fpR: 0.99770\n",
      "Epoch[126/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2580\n",
      "TN: 6111\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.92571 | Acc: 0.73076 | fpR: 0.29686 | R: 0.75837 | A_fpR: 0.36866 | U_fpR: 0.99689\n",
      "Epoch[127/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2569\n",
      "TN: 6122\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.92013 | Acc: 0.73139 | fpR: 0.29559 | R: 0.75837 | A_fpR: 0.35830 | U_fpR: 0.99781\n",
      "Epoch[128/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2690\n",
      "TN: 6001\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.91325 | Acc: 0.72443 | fpR: 0.30952 | R: 0.75837 | A_fpR: 0.35312 | U_fpR: 0.99885\n",
      "Epoch[129/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2738\n",
      "TN: 5953\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.90374 | Acc: 0.72167 | fpR: 0.31504 | R: 0.75837 | A_fpR: 0.35186 | U_fpR: 0.99850\n",
      "Epoch[130/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2747\n",
      "TN: 5944\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.89008 | Acc: 0.72115 | fpR: 0.31607 | R: 0.75837 | A_fpR: 0.36808 | U_fpR: 0.99896\n",
      "Epoch[131/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2730\n",
      "TN: 5961\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.88534 | Acc: 0.72213 | fpR: 0.31412 | R: 0.75837 | A_fpR: 0.36325 | U_fpR: 0.99919\n",
      "Epoch[132/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2734\n",
      "TN: 5957\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.88693 | Acc: 0.72190 | fpR: 0.31458 | R: 0.75837 | A_fpR: 0.35865 | U_fpR: 0.99965\n",
      "Epoch[133/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2806\n",
      "TN: 5885\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.87079 | Acc: 0.71775 | fpR: 0.32286 | R: 0.75837 | A_fpR: 0.36958 | U_fpR: 0.99942\n",
      "Epoch[134/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2820\n",
      "TN: 5871\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.86885 | Acc: 0.71695 | fpR: 0.32447 | R: 0.75837 | A_fpR: 0.35646 | U_fpR: 0.99954\n",
      "Epoch[135/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2768\n",
      "TN: 5923\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.86691 | Acc: 0.71994 | fpR: 0.31849 | R: 0.75837 | A_fpR: 0.35853 | U_fpR: 0.99965\n",
      "Epoch[136/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2899\n",
      "TN: 5792\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.86984 | Acc: 0.71240 | fpR: 0.33356 | R: 0.75837 | A_fpR: 0.35876 | U_fpR: 0.99977\n",
      "Epoch[137/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2889\n",
      "TN: 5802\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.86072 | Acc: 0.71298 | fpR: 0.33241 | R: 0.75837 | A_fpR: 0.36325 | U_fpR: 0.99977\n",
      "Epoch[138/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2869\n",
      "TN: 5822\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.85329 | Acc: 0.71413 | fpR: 0.33011 | R: 0.75837 | A_fpR: 0.36290 | U_fpR: 0.99977\n",
      "Epoch[139/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2818\n",
      "TN: 5873\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.84660 | Acc: 0.71706 | fpR: 0.32424 | R: 0.75837 | A_fpR: 0.37430 | U_fpR: 0.99977\n",
      "Epoch[140/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2848\n",
      "TN: 5843\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.85035 | Acc: 0.71534 | fpR: 0.32770 | R: 0.75837 | A_fpR: 0.37245 | U_fpR: 1.00000\n",
      "Epoch[141/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2901\n",
      "TN: 5790\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.83930 | Acc: 0.71229 | fpR: 0.33379 | R: 0.75837 | A_fpR: 0.37383 | U_fpR: 0.99988\n",
      "Epoch[142/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2855\n",
      "TN: 5836\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.84342 | Acc: 0.71493 | fpR: 0.32850 | R: 0.75837 | A_fpR: 0.37084 | U_fpR: 1.00000\n",
      "Epoch[143/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2933\n",
      "TN: 5758\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.84224 | Acc: 0.71045 | fpR: 0.33748 | R: 0.75837 | A_fpR: 0.37510 | U_fpR: 0.99988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[144/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2910\n",
      "TN: 5781\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.83582 | Acc: 0.71177 | fpR: 0.33483 | R: 0.75837 | A_fpR: 0.38074 | U_fpR: 0.99988\n",
      "Epoch[145/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2925\n",
      "TN: 5766\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.83133 | Acc: 0.71091 | fpR: 0.33656 | R: 0.75837 | A_fpR: 0.37349 | U_fpR: 1.00000\n",
      "Epoch[146/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2894\n",
      "TN: 5797\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.82628 | Acc: 0.71269 | fpR: 0.33299 | R: 0.75837 | A_fpR: 0.37993 | U_fpR: 1.00000\n",
      "Epoch[147/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2955\n",
      "TN: 5736\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.82199 | Acc: 0.70918 | fpR: 0.34001 | R: 0.75837 | A_fpR: 0.37786 | U_fpR: 1.00000\n",
      "Epoch[148/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 2957\n",
      "TN: 5734\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.81744 | Acc: 0.70907 | fpR: 0.34024 | R: 0.75837 | A_fpR: 0.37763 | U_fpR: 1.00000\n",
      "Epoch[149/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3002\n",
      "TN: 5689\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.81864 | Acc: 0.70648 | fpR: 0.34541 | R: 0.75837 | A_fpR: 0.36820 | U_fpR: 1.00000\n",
      "Epoch[150/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3035\n",
      "TN: 5656\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.80842 | Acc: 0.70458 | fpR: 0.34921 | R: 0.75837 | A_fpR: 0.37407 | U_fpR: 1.00000\n",
      "Epoch[151/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3076\n",
      "TN: 5615\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.80997 | Acc: 0.70222 | fpR: 0.35393 | R: 0.75837 | A_fpR: 0.37349 | U_fpR: 1.00000\n",
      "Epoch[152/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3223\n",
      "TN: 5468\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.80403 | Acc: 0.69376 | fpR: 0.37084 | R: 0.75837 | A_fpR: 0.37982 | U_fpR: 1.00000\n",
      "Epoch[153/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3338\n",
      "TN: 5353\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.80130 | Acc: 0.68715 | fpR: 0.38408 | R: 0.75837 | A_fpR: 0.38200 | U_fpR: 0.99988\n",
      "Epoch[154/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3469\n",
      "TN: 5222\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.80146 | Acc: 0.67961 | fpR: 0.39915 | R: 0.75837 | A_fpR: 0.38315 | U_fpR: 1.00000\n",
      "Epoch[155/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3677\n",
      "TN: 5014\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.79197 | Acc: 0.66764 | fpR: 0.42308 | R: 0.75837 | A_fpR: 0.38166 | U_fpR: 1.00000\n",
      "Epoch[156/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 3914\n",
      "TN: 4777\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.78873 | Acc: 0.65401 | fpR: 0.45035 | R: 0.75837 | A_fpR: 0.38235 | U_fpR: 1.00000\n",
      "Epoch[157/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 4206\n",
      "TN: 4485\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.78428 | Acc: 0.63721 | fpR: 0.48395 | R: 0.75837 | A_fpR: 0.38477 | U_fpR: 1.00000\n",
      "Epoch[158/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 4528\n",
      "TN: 4163\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.78114 | Acc: 0.61869 | fpR: 0.52100 | R: 0.75837 | A_fpR: 0.39132 | U_fpR: 1.00000\n",
      "Epoch[159/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 4871\n",
      "TN: 3820\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.77780 | Acc: 0.59895 | fpR: 0.56046 | R: 0.75837 | A_fpR: 0.38223 | U_fpR: 1.00000\n",
      "Epoch[160/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5186\n",
      "TN: 3505\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.77711 | Acc: 0.58083 | fpR: 0.59671 | R: 0.75837 | A_fpR: 0.39063 | U_fpR: 1.00000\n",
      "Epoch[161/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5348\n",
      "TN: 3343\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.77216 | Acc: 0.57151 | fpR: 0.61535 | R: 0.75837 | A_fpR: 0.39317 | U_fpR: 1.00000\n",
      "Epoch[162/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5475\n",
      "TN: 3216\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.76629 | Acc: 0.56420 | fpR: 0.62996 | R: 0.75837 | A_fpR: 0.40375 | U_fpR: 1.00000\n",
      "Epoch[163/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5645\n",
      "TN: 3046\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.75858 | Acc: 0.55442 | fpR: 0.64952 | R: 0.75837 | A_fpR: 0.39616 | U_fpR: 1.00000\n",
      "Epoch[164/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5733\n",
      "TN: 2958\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.75298 | Acc: 0.54936 | fpR: 0.65965 | R: 0.75837 | A_fpR: 0.40962 | U_fpR: 1.00000\n",
      "Epoch[165/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5748\n",
      "TN: 2943\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.74536 | Acc: 0.54850 | fpR: 0.66137 | R: 0.75837 | A_fpR: 0.42182 | U_fpR: 1.00000\n",
      "Epoch[166/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5893\n",
      "TN: 2798\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.74798 | Acc: 0.54016 | fpR: 0.67806 | R: 0.75837 | A_fpR: 0.43056 | U_fpR: 1.00000\n",
      "Epoch[167/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5661\n",
      "TN: 3030\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.73638 | Acc: 0.55350 | fpR: 0.65136 | R: 0.75837 | A_fpR: 0.43988 | U_fpR: 0.99988\n",
      "Epoch[168/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5803\n",
      "TN: 2888\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.74033 | Acc: 0.54533 | fpR: 0.66770 | R: 0.75837 | A_fpR: 0.44690 | U_fpR: 1.00000\n",
      "Epoch[169/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5750\n",
      "TN: 2941\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.72263 | Acc: 0.54838 | fpR: 0.66160 | R: 0.75837 | A_fpR: 0.45887 | U_fpR: 1.00000\n",
      "Epoch[170/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5792\n",
      "TN: 2899\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.72655 | Acc: 0.54597 | fpR: 0.66644 | R: 0.75837 | A_fpR: 0.46439 | U_fpR: 1.00000\n",
      "Epoch[171/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5781\n",
      "TN: 2910\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.71639 | Acc: 0.54660 | fpR: 0.66517 | R: 0.75837 | A_fpR: 0.48349 | U_fpR: 1.00000\n",
      "Epoch[172/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5812\n",
      "TN: 2879\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.70580 | Acc: 0.54482 | fpR: 0.66874 | R: 0.75837 | A_fpR: 0.48291 | U_fpR: 1.00000\n",
      "Epoch[173/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5879\n",
      "TN: 2812\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.70007 | Acc: 0.54096 | fpR: 0.67645 | R: 0.75837 | A_fpR: 0.48752 | U_fpR: 1.00000\n",
      "Epoch[174/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5713\n",
      "TN: 2978\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.69536 | Acc: 0.55051 | fpR: 0.65735 | R: 0.75837 | A_fpR: 0.48763 | U_fpR: 1.00000\n",
      "Epoch[175/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5749\n",
      "TN: 2942\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.68937 | Acc: 0.54844 | fpR: 0.66149 | R: 0.75837 | A_fpR: 0.49557 | U_fpR: 1.00000\n",
      "Epoch[176/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5811\n",
      "TN: 2880\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.68476 | Acc: 0.54487 | fpR: 0.66862 | R: 0.75837 | A_fpR: 0.50121 | U_fpR: 0.99988\n",
      "Epoch[177/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5749\n",
      "TN: 2942\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.67818 | Acc: 0.54844 | fpR: 0.66149 | R: 0.75837 | A_fpR: 0.49097 | U_fpR: 1.00000\n",
      "Epoch[178/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5810\n",
      "TN: 2881\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.67264 | Acc: 0.54493 | fpR: 0.66851 | R: 0.75837 | A_fpR: 0.50777 | U_fpR: 1.00000\n",
      "Epoch[179/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5817\n",
      "TN: 2874\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.66579 | Acc: 0.54453 | fpR: 0.66931 | R: 0.75837 | A_fpR: 0.51352 | U_fpR: 1.00000\n",
      "Epoch[180/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5730\n",
      "TN: 2961\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.66582 | Acc: 0.54953 | fpR: 0.65930 | R: 0.75837 | A_fpR: 0.51594 | U_fpR: 1.00000\n",
      "Epoch[181/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5679\n",
      "TN: 3012\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.65740 | Acc: 0.55247 | fpR: 0.65343 | R: 0.75837 | A_fpR: 0.52825 | U_fpR: 1.00000\n",
      "Epoch[182/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5809\n",
      "TN: 2882\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.64837 | Acc: 0.54499 | fpR: 0.66839 | R: 0.75837 | A_fpR: 0.52157 | U_fpR: 1.00000\n",
      "Epoch[183/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5843\n",
      "TN: 2848\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.63219 | Acc: 0.54303 | fpR: 0.67230 | R: 0.75837 | A_fpR: 0.53883 | U_fpR: 1.00000\n",
      "Epoch[184/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5822\n",
      "TN: 2869\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.63200 | Acc: 0.54424 | fpR: 0.66989 | R: 0.75837 | A_fpR: 0.52664 | U_fpR: 1.00000\n",
      "Epoch[185/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5842\n",
      "TN: 2849\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.62312 | Acc: 0.54309 | fpR: 0.67219 | R: 0.75837 | A_fpR: 0.54401 | U_fpR: 1.00000\n",
      "Epoch[186/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5785\n",
      "TN: 2906\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.61657 | Acc: 0.54637 | fpR: 0.66563 | R: 0.75837 | A_fpR: 0.54965 | U_fpR: 0.99988\n",
      "Epoch[187/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5758\n",
      "TN: 2933\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.60608 | Acc: 0.54792 | fpR: 0.66252 | R: 0.75837 | A_fpR: 0.56875 | U_fpR: 0.99988\n",
      "Epoch[188/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5842\n",
      "TN: 2849\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.60226 | Acc: 0.54309 | fpR: 0.67219 | R: 0.75837 | A_fpR: 0.56242 | U_fpR: 0.99988\n",
      "Epoch[189/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5822\n",
      "TN: 2869\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.58735 | Acc: 0.54424 | fpR: 0.66989 | R: 0.75837 | A_fpR: 0.57968 | U_fpR: 1.00000\n",
      "Epoch[190/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5986\n",
      "TN: 2705\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.57461 | Acc: 0.53481 | fpR: 0.68876 | R: 0.75837 | A_fpR: 0.58912 | U_fpR: 0.99988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[191/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 5920\n",
      "TN: 2771\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.57671 | Acc: 0.53860 | fpR: 0.68116 | R: 0.75837 | A_fpR: 0.59268 | U_fpR: 1.00000\n",
      "Epoch[192/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 6011\n",
      "TN: 2680\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.55960 | Acc: 0.53337 | fpR: 0.69164 | R: 0.75837 | A_fpR: 0.60062 | U_fpR: 0.99988\n",
      "Epoch[193/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 6045\n",
      "TN: 2646\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.54613 | Acc: 0.53141 | fpR: 0.69555 | R: 0.75837 | A_fpR: 0.60453 | U_fpR: 1.00000\n",
      "Epoch[194/5000] Train: GEN \n",
      "\n",
      "TP: 6591\n",
      "FP: 6118\n",
      "TN: 2573\n",
      "FN: 2100\n",
      "\n",
      "| LossD: 0.59019, LossG: 1.54015 | Acc: 0.52721 | fpR: 0.70395 | R: 0.75837 | A_fpR: 0.61489 | U_fpR: 1.00000\n",
      "\n",
      "Push Generator\n",
      "\n",
      "Epoch[195/5000] Train: DISC \n",
      "\n",
      "TP: 7322\n",
      "FP: 6696\n",
      "TN: 1995\n",
      "FN: 1369\n",
      "\n",
      "| LossD: 0.69750, LossG: 1.54015 | Acc: 0.53601 | fpR: 0.77045 | R: 0.84248 | A_fpR: 0.61213 | U_fpR: 0.99988\n",
      "Epoch[196/5000] Train: DISC \n",
      "\n",
      "TP: 7588\n",
      "FP: 7020\n",
      "TN: 1671\n",
      "FN: 1103\n",
      "\n",
      "| LossD: 0.69467, LossG: 1.54015 | Acc: 0.53268 | fpR: 0.80773 | R: 0.87309 | A_fpR: 0.60476 | U_fpR: 1.00000\n",
      "Epoch[197/5000] Train: DISC \n",
      "\n",
      "TP: 7711\n",
      "FP: 7058\n",
      "TN: 1633\n",
      "FN: 980\n",
      "\n",
      "| LossD: 0.69064, LossG: 1.54015 | Acc: 0.53757 | fpR: 0.81210 | R: 0.88724 | A_fpR: 0.61915 | U_fpR: 0.99977\n",
      "Epoch[198/5000] Train: DISC \n",
      "\n",
      "TP: 7759\n",
      "FP: 6813\n",
      "TN: 1878\n",
      "FN: 932\n",
      "\n",
      "| LossD: 0.68229, LossG: 1.54015 | Acc: 0.55442 | fpR: 0.78391 | R: 0.89276 | A_fpR: 0.61224 | U_fpR: 0.99977\n",
      "Epoch[199/5000] Train: DISC \n",
      "\n",
      "TP: 7743\n",
      "FP: 6482\n",
      "TN: 2209\n",
      "FN: 948\n",
      "\n",
      "| LossD: 0.67459, LossG: 1.54015 | Acc: 0.57255 | fpR: 0.74583 | R: 0.89092 | A_fpR: 0.61167 | U_fpR: 1.00000\n",
      "Epoch[200/5000] Train: DISC \n",
      "\n",
      "TP: 7704\n",
      "FP: 4150\n",
      "TN: 4541\n",
      "FN: 987\n",
      "\n",
      "| LossD: 0.66393, LossG: 1.54015 | Acc: 0.70446 | fpR: 0.47751 | R: 0.88643 | A_fpR: 0.61086 | U_fpR: 0.99988\n",
      "Epoch[201/5000] Train: DISC \n",
      "\n",
      "TP: 7654\n",
      "FP: 3321\n",
      "TN: 5370\n",
      "FN: 1037\n",
      "\n",
      "| LossD: 0.65468, LossG: 1.54015 | Acc: 0.74928 | fpR: 0.38212 | R: 0.88068 | A_fpR: 0.61431 | U_fpR: 0.99988\n",
      "Epoch[202/5000] Train: DISC \n",
      "\n",
      "TP: 7589\n",
      "FP: 2772\n",
      "TN: 5919\n",
      "FN: 1102\n",
      "\n",
      "| LossD: 0.64246, LossG: 1.54015 | Acc: 0.77713 | fpR: 0.31895 | R: 0.87320 | A_fpR: 0.62191 | U_fpR: 1.00000\n",
      "Epoch[203/5000] Train: DISC \n",
      "\n",
      "TP: 7537\n",
      "FP: 380\n",
      "TN: 8311\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.54015 | Acc: 0.91175 | fpR: 0.04372 | R: 0.86722 | A_fpR: 0.61754 | U_fpR: 0.99977\n",
      "\n",
      "Pull Generator\n",
      "\n",
      "Epoch[204/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 422\n",
      "TN: 8269\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.60368 | Acc: 0.90933 | fpR: 0.04856 | R: 0.86722 | A_fpR: 0.61857 | U_fpR: 1.00000\n",
      "Epoch[205/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 487\n",
      "TN: 8204\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.58635 | Acc: 0.90559 | fpR: 0.05603 | R: 0.86722 | A_fpR: 0.61777 | U_fpR: 0.99988\n",
      "Epoch[206/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 539\n",
      "TN: 8152\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.58116 | Acc: 0.90260 | fpR: 0.06202 | R: 0.86722 | A_fpR: 0.61374 | U_fpR: 1.00000\n",
      "Epoch[207/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 595\n",
      "TN: 8096\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.57641 | Acc: 0.89938 | fpR: 0.06846 | R: 0.86722 | A_fpR: 0.61788 | U_fpR: 1.00000\n",
      "Epoch[208/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 647\n",
      "TN: 8044\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.55781 | Acc: 0.89639 | fpR: 0.07444 | R: 0.86722 | A_fpR: 0.63595 | U_fpR: 1.00000\n",
      "Epoch[209/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 716\n",
      "TN: 7975\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.55777 | Acc: 0.89242 | fpR: 0.08238 | R: 0.86722 | A_fpR: 0.62858 | U_fpR: 0.99977\n",
      "Epoch[210/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 829\n",
      "TN: 7862\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.54431 | Acc: 0.88592 | fpR: 0.09539 | R: 0.86722 | A_fpR: 0.64365 | U_fpR: 1.00000\n",
      "Epoch[211/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 974\n",
      "TN: 7717\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.54022 | Acc: 0.87757 | fpR: 0.11207 | R: 0.86722 | A_fpR: 0.63479 | U_fpR: 1.00000\n",
      "Epoch[212/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 1076\n",
      "TN: 7615\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.53296 | Acc: 0.87171 | fpR: 0.12381 | R: 0.86722 | A_fpR: 0.65079 | U_fpR: 1.00000\n",
      "Epoch[213/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 1208\n",
      "TN: 7483\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.52511 | Acc: 0.86411 | fpR: 0.13899 | R: 0.86722 | A_fpR: 0.65205 | U_fpR: 1.00000\n",
      "Epoch[214/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 1334\n",
      "TN: 7357\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.51041 | Acc: 0.85686 | fpR: 0.15349 | R: 0.86722 | A_fpR: 0.65366 | U_fpR: 1.00000\n",
      "Epoch[215/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 1488\n",
      "TN: 7203\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.50882 | Acc: 0.84800 | fpR: 0.17121 | R: 0.86722 | A_fpR: 0.65953 | U_fpR: 1.00000\n",
      "Epoch[216/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 1678\n",
      "TN: 7013\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.51160 | Acc: 0.83707 | fpR: 0.19307 | R: 0.86722 | A_fpR: 0.65930 | U_fpR: 1.00000\n",
      "Epoch[217/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 1887\n",
      "TN: 6804\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.49319 | Acc: 0.82505 | fpR: 0.21712 | R: 0.86722 | A_fpR: 0.66344 | U_fpR: 1.00000\n",
      "Epoch[218/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 1999\n",
      "TN: 6692\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.50892 | Acc: 0.81861 | fpR: 0.23001 | R: 0.86722 | A_fpR: 0.66080 | U_fpR: 1.00000\n",
      "Epoch[219/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 2209\n",
      "TN: 6482\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.48556 | Acc: 0.80652 | fpR: 0.25417 | R: 0.86722 | A_fpR: 0.66724 | U_fpR: 1.00000\n",
      "Epoch[220/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 2448\n",
      "TN: 6243\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.48440 | Acc: 0.79277 | fpR: 0.28167 | R: 0.86722 | A_fpR: 0.66287 | U_fpR: 1.00000\n",
      "Epoch[221/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 2601\n",
      "TN: 6090\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.48103 | Acc: 0.78397 | fpR: 0.29928 | R: 0.86722 | A_fpR: 0.66701 | U_fpR: 1.00000\n",
      "Epoch[222/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 2762\n",
      "TN: 5929\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.46895 | Acc: 0.77471 | fpR: 0.31780 | R: 0.86722 | A_fpR: 0.66563 | U_fpR: 1.00000\n",
      "Epoch[223/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 2973\n",
      "TN: 5718\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.45862 | Acc: 0.76257 | fpR: 0.34208 | R: 0.86722 | A_fpR: 0.67829 | U_fpR: 1.00000\n",
      "Epoch[224/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 3100\n",
      "TN: 5591\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.45799 | Acc: 0.75526 | fpR: 0.35669 | R: 0.86722 | A_fpR: 0.68162 | U_fpR: 1.00000\n",
      "Epoch[225/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 3242\n",
      "TN: 5449\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.45549 | Acc: 0.74709 | fpR: 0.37303 | R: 0.86722 | A_fpR: 0.67737 | U_fpR: 1.00000\n",
      "Epoch[226/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 3397\n",
      "TN: 5294\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.45047 | Acc: 0.73818 | fpR: 0.39086 | R: 0.86722 | A_fpR: 0.68657 | U_fpR: 1.00000\n",
      "Epoch[227/5000] Train: GEN \n",
      "\n",
      "TP: 7537\n",
      "FP: 3562\n",
      "TN: 5129\n",
      "FN: 1154\n",
      "\n",
      "| LossD: 0.63086, LossG: 1.45277 | Acc: 0.72868 | fpR: 0.40985 | R: 0.86722 | A_fpR: 0.69808 | U_fpR: 1.00000\n",
      "Epoch[228/5000] Train: GEN "
     ]
    }
   ],
   "source": [
    "#X, y = start_data(\"aggregated_data/aggregated_data.csv\", \"label:SITTING\")\n",
    "#X, y = start_data(\"raw_data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv\", \"label:SITTING\" ) \n",
    "X, y = start_data(\"aggregated_data/aggregated_data.csv\", [\"label:SITTING\", \"label:FIX_walking\", \"label:SLEEPING\"], [\"0BFC35E2-4817-4865-BFA7-764742302A2D\", \"0A986513-7828-4D53-AA1F-E02D6DF9561B\", \"00EABED2-271D-49D8-B599-1D4A09240601\"])\n",
    "#print(len(X))\n",
    "activity_classifier = Activity_Classifier()\n",
    "user_classifier = User_Classifier()\n",
    "activity_classifier.eval()\n",
    "user_classifier.eval()\n",
    "activity_classifier.to(device)\n",
    "user_classifier.to(device)\n",
    "\n",
    "\n",
    "\n",
    "activity_classifier.load_state_dict(torch.load('saved_models/2000Epochs_0.01LR_UserClassifier'), strict = False)\n",
    "user_classifier.load_state_dict(torch.load(\"saved_models/5000Epochs_0.01LR_MutualExclusiveLabelClassifier\"), strict = False)\n",
    "\n",
    "model_output = training_loop(X,y, activity_classifier, user_classifier, gan_id=\"cGAN_trained\", batch_size = len(X), gen_lr=.005, disc_lr =.005, n_epochs=5000, dig=5, constant_train_flag=False, print_batches = False)\n",
    "#plot_metrics(model_output, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c2ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
