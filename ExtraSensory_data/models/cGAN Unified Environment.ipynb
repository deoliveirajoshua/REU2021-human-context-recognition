{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f41269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9876541",
   "metadata": {},
   "source": [
    "# Move Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966083b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaryClare's\n",
    "#os.chdir('/Users/maryclaremartin/Documents/jup/ExtraSensory')\n",
    "\n",
    "# Josh's\n",
    "os.chdir(\"/Users/jdeoliveira/REU2021-human-context-recognition/ExtraSensory_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6afcf",
   "metadata": {},
   "source": [
    "# The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    #torch.manual_seed(0)\n",
    "    return torch.randn(n_samples, z_dim).to(device)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, int(hidden_dim/2)),\n",
    "            generator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            generator_block(int(hidden_dim/4), 30),\n",
    "            generator_block(30, 28),\n",
    "            nn.Linear(28, feature_dim)\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "##calculates generator loss\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#criterion1: loss function1\n",
    "#criterion2: loss function2\n",
    "#batch_size: batch size\n",
    "#z_dim: number of dimensions in the latent space\n",
    "def get_gen_loss(gen, disc, act, usr, criterion1, criterion2, batch_size, z_dim, activities, users):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, activities)\n",
    "    usr_vectors = get_usr_matrix(batch_size, users)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "    fake_features = gen(to_gen)\n",
    "    \n",
    "    pred_disc = disc(fake_features)\n",
    "    pred_act = act(fake_features)\n",
    "    pred_usr = usr(fake_features)\n",
    "    \n",
    "    gen_loss = criterion1(pred_disc, torch.ones_like(pred)) + criterion2(pred_act, act_vectors[0]) + criterion2(pred_usr, usr_vectors[0])\n",
    "    return gen_loss\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot = one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot = one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    \n",
    "    return torch.Tensor(indexes), torch.Tensor(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9681363",
   "metadata": {},
   "source": [
    "# Create Fake Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d27ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_samples(gen, batch_size, z_dim):\n",
    "    \"\"\"\n",
    "    Generates fake acceleration features given a batch size, latent vector dimension, and trained generator.\n",
    "    \n",
    "    \"\"\"\n",
    "    latent_vectors = get_noise(batch_size, z_dim) ### Retrieves a 2D tensor of noise\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    \n",
    "    return fake_features ### Returns a 2D tensor of fake features of size batch_size x z_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc88f35",
   "metadata": {},
   "source": [
    "# The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ea376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "#defines discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim = 26, hidden_dim = 16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            discriminator_block(feature_dim, hidden_dim),\n",
    "            discriminator_block(hidden_dim, int(hidden_dim/2)),\n",
    "            discriminator_block(int(hidden_dim/2), int(hidden_dim/4)),\n",
    "            nn.Linear(int(hidden_dim/4), 1),\n",
    "            nn.Sigmoid()                    \n",
    "        )\n",
    "    def forward(self, feature_vector):\n",
    "        return self.disc(feature_vector)\n",
    "    \n",
    "def get_disc_loss(gen, disc, criterion, real_features, batch_size, z_dim, a_dim, u_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    act_vectors = get_act_matrix(batch_size, a_dim)\n",
    "    usr_vectors = get_usr_matrix(batch_size, u_dim)\n",
    "    \n",
    "    to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "    fake_features = gen(to_gen)\n",
    "    pred_fake = disc(fake_features.detach())\n",
    "    \n",
    "    ground_truth = torch.zeros_like(pred_fake)\n",
    "    loss_fake = criterion(pred_fake, ground_truth)\n",
    "    \n",
    "    pred_real = disc(real_features)\n",
    "    ground_truth = torch.ones_like(pred_real)\n",
    "    loss_real = criterion(pred_real, ground_truth)\n",
    "    \n",
    "    disc_loss = (loss_fake + loss_real) / 2\n",
    "    return disc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13dca3",
   "metadata": {},
   "source": [
    "# User Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575fd06",
   "metadata": {},
   "source": [
    "# Activity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            classifier_block(10, 5),\n",
    "            nn.Linear(5, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #softmax = nn.Softmax(dim = 1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2c788",
   "metadata": {},
   "source": [
    "# Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replaces Nan values with average values\n",
    "#df: data frame of data to use\n",
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9c463",
   "metadata": {},
   "source": [
    "# Visualize Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##prints a plot of a generator batch\n",
    "#gen: generator\n",
    "#b_size: batch size\n",
    "#epochs: current epoch (-1)\n",
    "def visualize_gen_batch(gen, b_size, epochs = -1):\n",
    "    #print(str(b_size))\n",
    "    latent_vectors = get_noise(b_size, z_dim)\n",
    "    #print(latent_vectors.shape)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    #print(fake_features.shape)\n",
    "    \n",
    "    w_img = fake_features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Generated Batch at Epoch ' + str(epochs), fontweight =\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "##prints a plot of a batch of real data\n",
    "#features: real data\n",
    "def visualize_real_batch(features):\n",
    "    w_img = features\n",
    "    wmin = torch.min(w_img)\n",
    "    wmax = torch.max(w_img)\n",
    "    w_img = w_img.cpu()\n",
    "    w_img = w_img.detach().numpy()\n",
    "    c = plt.imshow(w_img, cmap ='Reds', vmin = wmin , vmax = wmax,\n",
    "                        interpolation ='nearest', origin ='upper')\n",
    "    plt.colorbar(c)\n",
    "    plt.title('Real Batch of Data', fontweight =\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1685b",
   "metadata": {},
   "source": [
    "# Calculate Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculates performance statistics for each epoch of training\n",
    "#gen: generator\n",
    "#disc: discriminator\n",
    "#b_size: batch size\n",
    "#z_dim: number of dimensions of the latent space\n",
    "##returns accuracy, precision, recall, fpR, and f1 score\n",
    "def performance_stats(gen, disc, b_size, z_dim, batch = None):\n",
    "    tp = 0 #true positive\n",
    "    fp = 0 #false positive\n",
    "    tn = 0 #true negative\n",
    "    fn = 0 #false negative\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if batch is None:\n",
    "            latent_vectors = get_noise(b_size, z_dim)\n",
    "            fake_features = gen(latent_vectors)\n",
    "            y_hat = torch.round(disc(fake_features))\n",
    "            y_label = [0] * b_size\n",
    "            y_label = torch.Tensor(y_label).to(device)\n",
    "        else:\n",
    "            latent_vectors = get_noise(int(b_size/2), z_dim)\n",
    "            fake_features = gen(latent_vectors.to(device))\n",
    "            y_hat = torch.round(disc(fake_features.to(device)))\n",
    "            y_label = torch.Tensor([0] * int(b_size/2))\n",
    "            \n",
    "            real_y_hat = torch.round(disc(batch[:int(b_size/2)].to(device)))\n",
    "            y_add = torch.Tensor([1] * int(b_size/2))\n",
    "            y_label = torch.cat((y_label, y_add), dim = 0)\n",
    "            #for i in range(0, int(b_size/2)):\n",
    "            # y_label.append(1)\n",
    "            y_hat = torch.cat((y_hat, real_y_hat), dim = 0).to(device)\n",
    "            \n",
    "            #print(y_hat)\n",
    "            #print(y_label)\n",
    "         \n",
    "        \n",
    "        for k in range(len(y_hat)):\n",
    "            if y_label[k] == 1:\n",
    "                if y_hat[k] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            elif y_hat[k] == 0:\n",
    "                tn += 1\n",
    "            elif y_hat[k] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "                exit()\n",
    "            \n",
    "        class_acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "        \n",
    "        if tp + fp == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            \n",
    "        if tp + fn == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "            \n",
    "        if fp + tn == 0:\n",
    "            fpR = 0\n",
    "        else: \n",
    "            fpR = fp / (fp + tn)\n",
    "\n",
    "        #print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "        #print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "        #print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "        #print(f'F-1 Score: {2*(precision * recall / (precision + recall + 0.001)):.2f}')\n",
    "        return class_acc, precision, recall, fpR, 2*(precision * recall / (precision + recall + 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a2ff4",
   "metadata": {},
   "source": [
    "# Create Density Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6ff825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and plot density curves for mean, x, y, z acceleration\n",
    "#reals: real data\n",
    "#fakes: generated data\n",
    "def density_curves(reals, fakes):\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    subplot(2, 2, 1)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,0], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,0], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 2)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,18], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,18], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean X-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 3)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,19], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,19], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Y-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    subplot(2, 2, 4)\n",
    "    sns.kdeplot(fakes.cpu().numpy()[:,20], color = 'r', shade = True, label = 'Fake Distribution')\n",
    "    sns.kdeplot(reals[:,20], color = 'b', shade = True, label = 'Real Distribution')\n",
    "    plt.xlabel('Mean Z-Acceleration')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597b79f",
   "metadata": {},
   "source": [
    "# Calculate Wassertein distance for each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a90334",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate Waaserstein distances for each dimension\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "#feature_dim: number ofd dimensions in the feature space\n",
    "#sample: sample of data\n",
    "def all_Wasserstein_dists(gen, z_dim, feature_dim, sample):\n",
    "    wasser_dim = []\n",
    "    latent_vectors = get_noise(len(sample), z_dim)\n",
    "    fake_features = gen(latent_vectors.to(device))\n",
    "    for k in range(feature_dim):\n",
    "        wasser_dim.append(wasserstein_distance(fake_features[:, k].cpu().detach().numpy(), sample[:, k].cpu().detach().numpy()))\n",
    "    return torch.tensor(wasser_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e631a",
   "metadata": {},
   "source": [
    "# Visualizing Generation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73924dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates and prints a plot of the generated vs real data\n",
    "#data: data used\n",
    "#gen: generator\n",
    "#z_dim: number of dimensions of the latent space\n",
    "def visualize_gen(data, gen, z_dim):\n",
    "    #Number of datum to visualize\n",
    "    sample_size = len(data)\n",
    "    reals = data[0:sample_size, :]\n",
    "    fakes = get_fake_samples(gen, sample_size, z_dim).detach()\n",
    "    density_curves(reals, fakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00a9cc",
   "metadata": {},
   "source": [
    "# Initialize Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initalize parameters that depend on training loop parameters\n",
    "#X: acceleration data\n",
    "#y: labels associated with X data (fake or real)\n",
    "#z_dim: number of dimensions to the latent space\n",
    "#disc_lr: discriminator learning rate\n",
    "#gen_lr: generator learning rate\n",
    "#DISCRIMINATOR: 1 to indicate if discriminator is training\n",
    "#batch_size: batch size\n",
    "#disc: initialized discrimiantor\n",
    "\n",
    "def initialize_params(X, y, z_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc):\n",
    "    #initialize generator\n",
    "    gen = Generator(z_dim + a_dim + u_dim).to(device)\n",
    "    #indicate that discriminator is training\n",
    "    to_train = DISCRIMINATOR\n",
    "    #create training features\n",
    "    train_features = torch.tensor(X)\n",
    "    #create training labels\n",
    "    train_labels = torch.tensor(y)\n",
    "    #concatenate to create training data\n",
    "    train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    #create data loader for training data\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    #initialize generator and discriminator optimizers\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr = disc_lr)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr = gen_lr)\n",
    "    \n",
    "    return gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed8086",
   "metadata": {},
   "source": [
    "# Save / Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac703199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path and name of the Generator and Discriminator accordingly\n",
    "def save_model(gen, disc, model_name):\n",
    "    torch.save(gen.state_dict(), f\"saved_models/{model_name}_gen\")\n",
    "    torch.save(disc.state_dict(), f\"saved_models/{model_name}_disc\")\n",
    "    \n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c21b7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Training loop to train GAN\n",
    "\n",
    "#Parameters to specifiy: \n",
    "    #X: starting accelerometer data\n",
    "    #y: starting labels for X data (fake or real)\n",
    "    \n",
    "#Set parameters (do not change)\n",
    "    #criterion: loss function (BCE)\n",
    "    #dig: number of significant digits for printing (5)\n",
    "    #feature_dim: Number of dimensions of output from generator (26)\n",
    "    #GENERATOR: set generator to zero for training\n",
    "    #DISCRIMINATOR: set discriminator to one for training\n",
    "    #train_string: starting machine to train (DISC)\n",
    "    #disc: initalize discriminator\n",
    "    #rel_epochs: Epochs passed since last switch (constant training) (0)\n",
    "    #rows: initialization of array to save data of each epoch to CSV file ([])\n",
    "    #heading: array of column headings for table ([\"Epoch\", \"Machine Training\", \"Discriminator Loss\", \n",
    "                    #\"Generator Loss\", \"FPR\", \"Recall\", \"Median Wasserstein\", \"Mean Wasserstein\"])\n",
    "    #table: intialize a table as a pretty table to save epoch data\n",
    "    #switch_count: number of switches in dynamic training (0)\n",
    "    \n",
    "#Set parameters (can change):\n",
    "    #z_dim: number of dimensions of latent vector (100)\n",
    "    #gen_lr: generator learning rate (.001)\n",
    "    #disc_lr: discriminator learning rate (.001) (shoud be equal to gen_lr)\n",
    "    #batch_size: batch size (75)\n",
    "    #print_batches: Show model performance per batch (False)\n",
    "    #n_epochs: number of epochs to train (100)\n",
    "    #constant_train_flag: (False)\n",
    "        #Set to true to train based on constant # of epochs per machine \n",
    "        #Set to false to train dynamically based on machine performance\n",
    "        \n",
    "    #Constant training approach:\n",
    "        #disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (5)\n",
    "        #gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #epoch_threshold: Epoch number to change training epoch ratio (50)\n",
    "        #disc_epochs_change: New number of consecutive epochs to train discriminator after epoch threshold is exceeded (1)\n",
    "        #gen_epochs_change: New number of consecutive epochs to train generator after epoch threshold is exceeded (50)\n",
    "    \n",
    "    #Dynamic training approach:                        \n",
    "        #static_threshold: Epoch number to change from static ratio to dynamic (18)\n",
    "        #static_disc_epochs: Number of consecutive epochs to train discriminator before epoch threshold (4)\n",
    "        #static_gen_epochs: Number of consecutive epochs to train generator before epoch threshold (2)\n",
    "        #pull_threshold: Accuracy threshold for switching machine training when the generator is no longer competitive (0.4)\n",
    "        #push_threshold: Accuracy threshold for switching machine training when the discriminator is no longer competitive (0.6)\n",
    "        #recall_threshold: threshold for recall to switch machine training when discriminator is training well\n",
    "        #switch_flag: indicates if we should switch our training machine (False)\n",
    "        \n",
    "def training_loop(X, y, act, usr, criterion1 = nn.BCELoss(), criterion2 = nn.CrossEntropyLoss(), gan_id = \"Mod Test Gan\", dig = 5, feature_dim = 26, \n",
    "                  GENERATOR = 0, DISCRIMINATOR = 1, train_string = \"DISC\", disc = Discriminator(), z_dim = 100, a_dim = 3, u_dim = 3, \n",
    "                  gen_lr =  0.001, disc_lr = 0.001, batch_size = 100, constant_train_flag = False, disc_epochs = 5,\n",
    "                  gen_epochs = 2, epoch_threshold = 50, disc_epochs_change = 5, gen_epochs_change = 2, rel_epochs = 0,\n",
    "                 static_threshold = 18, static_disc_epochs = 5, static_gen_epochs = 2, pull_threshold = 0.2,\n",
    "                 push_threshold = 0.8, recall_threshold = 0.75, print_batches = False, n_epochs = 1000, rows = [],\n",
    "                 heading = [\"Epoch\", \"Machine Training\", \"Discriminator Loss\", \"Generator Loss\", \"Accuracy\", \"FPR\", \"Precision\", \"Recall\", \"F1\", \"Median Wasserstein\", \"Mean Wasserstein\"],\n",
    "                 table = PrettyTable(), switch_flag = False, switch_count = 0, last_real_features = []):\n",
    "    \n",
    "    disc.to(device)\n",
    "    #returns generator, sets discriminator training, creates training tensor, loads data, and initializes optimizers\n",
    "    gen, to_train, train_features, train_labels, train_data, train_loader, opt_disc, opt_gen = initialize_params(X, y, z_dim, a_dim, u_dim, disc_lr, gen_lr, DISCRIMINATOR, batch_size, disc)\n",
    "\n",
    "    #set pretty table field names\n",
    "    table.field_names = heading\n",
    "    \n",
    "    visualize_gen(X, gen, z_dim)\n",
    "\n",
    "    gen_epochs = 0\n",
    "    \n",
    "    last_D_loss = -1.0\n",
    "    last_G_loss = -1.0\n",
    "    \n",
    "    mean_mean = []\n",
    "    mean_median = []\n",
    "    \n",
    "    for epoch in range(n_epochs):  \n",
    "        if constant_train_flag:\n",
    "            if to_train == DISCRIMINATOR and rel_epochs >= disc_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GEN\"\n",
    "\n",
    "            elif to_train == GENERATOR and rel_epochs >= gen_epochs:\n",
    "                rel_epochs = 0\n",
    "                to_train = DISCRIMINATOR\n",
    "                train_string = \"DISC\"\n",
    "\n",
    "            # Change epoch ratio after intial 'leveling out'\n",
    "            if epoch == epoch_threshold:\n",
    "                rel_epochs = 0\n",
    "                to_train = GENERATOR\n",
    "                train_string = \"GENERATOR\"\n",
    "\n",
    "                old_ratio = gen_epochs / disc_epochs\n",
    "                gen_epochs = gen_epochs_change\n",
    "                disc_epochs = disc_epochs_change\n",
    "                new_ratio = gen_epochs / disc_epochs\n",
    "                print(f'\\n\\nTraining ratio of G/D switched from {old_ratio:.{dig}f} to {new_ratio:.{dig}f}\\n\\n')\n",
    "        else:\n",
    "            if epoch < static_threshold:\n",
    "                if to_train == DISCRIMINATOR and rel_epochs >= static_disc_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "\n",
    "                elif to_train == GENERATOR and rel_epochs >= static_gen_epochs:\n",
    "                    rel_epochs = 0\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "\n",
    "            else:\n",
    "                #to_train = DISCRIMINATOR\n",
    "                #train_string = \"DISC\"\n",
    "                if not switch_flag:\n",
    "                    print(\"\\nSwitching to Dynamic Training\\n\")\n",
    "                    switch_flag = True\n",
    "                if to_train == DISCRIMINATOR and fpR <= pull_threshold and R >= recall_threshold:\n",
    "                    to_train = GENERATOR\n",
    "                    train_string = \"GEN\"\n",
    "                    print(\"\\nPull Generator\\n\")\n",
    "                    switch_count += 1\n",
    "                if to_train == GENERATOR and fpR >= push_threshold:\n",
    "                    to_train = DISCRIMINATOR\n",
    "                    train_string = \"DISC\"\n",
    "                    print(\"\\nPush Generator\\n\")\n",
    "                    switch_count += 1\n",
    "        print(f'Epoch [{epoch + 1}/{n_epochs}] Training: {train_string} ', end ='')\n",
    "        for batch_idx, (real_features, _) in enumerate(train_loader):\n",
    "            #batch_size = len(real_features)\n",
    "            \n",
    "            if print_batches:\n",
    "                    print(f'\\n\\tBatch [{batch_idx + 1}/{len(train_loader)}] |', end ='')\n",
    "\n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Training Discriminator\n",
    "                #visualize_real_batch(real_features.float())\n",
    "                opt_disc.zero_grad()\n",
    "                disc_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), batch_size, z_dim, a_dim, u_dim)\n",
    "                disc_loss.backward(retain_graph = True)\n",
    "                opt_disc.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, batch_size, z_dim, batch = real_features.float())\n",
    "                w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                median_w_dist = torch.median(w_dist)\n",
    "                mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                mean_mean.append(mean_w_dist)\n",
    "                mean_median.append(median_w_dist)\n",
    "\n",
    "                last_D_loss = disc_loss.item()\n",
    "                \n",
    "                if last_G_loss == -1.0:\n",
    "                    last_G_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, batch_size, z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Median Wasserstein: {median_w_dist:.{dig}f} | Mean Wasserstein: {mean_w_dist:.{dig}f}')\n",
    "            else:\n",
    "                ### Training Generator\n",
    "                opt_gen.zero_grad()\n",
    "                gen_loss = get_gen_loss(gen, disc, act, usr, criterion1, criterion2, batch_size, z_dim, a_dim, u_dim)\n",
    "                gen_loss.backward()\n",
    "                opt_gen.step()\n",
    "                acc, P, R, fpR, F1 = performance_stats(gen, disc, batch_size, z_dim, batch = real_features.float())\n",
    "                w_dist = all_Wasserstein_dists(gen, z_dim, feature_dim, real_features.float())\n",
    "                median_w_dist = torch.median(w_dist)\n",
    "                mean_w_dist = torch.mean(w_dist)\n",
    "                \n",
    "                mean_mean.append(mean_w_dist)\n",
    "                mean_median.append(median_w_dist)\n",
    "                \n",
    "                last_G_loss = gen_loss.item()\n",
    "                \n",
    "                if last_D_loss == -1.0:\n",
    "                    last_D_loss = get_disc_loss(gen, disc, criterion1, real_features.float(), batch_size, z_dim, a_dim, u_dim)\n",
    "                \n",
    "                if print_batches:\n",
    "                    print(f'Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Median Wasserstein: {median_w_dist:.{dig}f} | Mean Wasserstein: {mean_w_dist:.{dig}f}')\n",
    "\n",
    "        if not print_batches:\n",
    "            \n",
    "            mean_mean_w = torch.mean(torch.Tensor(mean_mean)) \n",
    "            \n",
    "            mean_median_w = torch.mean(torch.Tensor(mean_median))\n",
    "            \n",
    "            if to_train == DISCRIMINATOR:\n",
    "                ### Currently doesn't print Median/Mean Wasserstein --> Change if needed\n",
    "                print(f'| Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Median W: {mean_median_w:.{dig}f} | Mean W: {mean_mean_w:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Discriminator\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{P:.{dig}f}\", f\"{R:.{dig}f}\", f\"{F1:.{dig}f}\", f\"{mean_median_w:.{dig}f}\", f\"{mean_mean_w:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "            else:\n",
    "                print(f'| Loss D: {last_D_loss:.{dig}f}, Loss G: {last_G_loss:.{dig}f} | Acc: {acc:.{dig}f} | fpR: {fpR:.{dig}f} | P: {P:.{dig}f} | R: {R:.{dig}f} | F1: {F1:.{dig}f} | Median W: {mean_median_w:.{dig}f} | Mean W: {mean_mean_w:.{dig}f}')\n",
    "                row_to_add = [f\"{epoch + 1}\", \"Generator\", f\"{last_D_loss:.{dig}f}\", f\"{last_G_loss:.{dig}f}\", f\"{acc:.{dig}f}\", f\"{fpR:.{dig}f}\", f\"{P:.{dig}f}\", f\"{R:.{dig}f}\", f\"{F1:.{dig}f}\", f\"{mean_median_w:.{dig}f}\", f\"{mean_mean_w:.{dig}f}\"]\n",
    "                table.add_row(row_to_add)\n",
    "                rows.append(row_to_add)\n",
    "                gen_epochs += 1\n",
    "        mean_mean.clear()\n",
    "        mean_median.clear()\n",
    "        rel_epochs += 1\n",
    "    print(\"\\n\\nTraining Session Finished\")\n",
    "    print(f\"Encountered {switch_count} non-trivial training swaps\")\n",
    "    percent = gen_epochs / n_epochs\n",
    "    print(f\"Trained Generator {gen_epochs} out of {n_epochs} ({percent:.3f})\")\n",
    "    f = open(\"model_outputs/\" + gan_id + \".txt\", \"w\")\n",
    "    f.write(table.get_string())\n",
    "    f.close()\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".txt\\\"\")\n",
    "\n",
    "    with open(\"model_outputs/\" + gan_id + \".csv\", \"w\") as csvfile: \n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "        # writing the fields \n",
    "        csvwriter.writerow(heading)\n",
    "        # writing the data rows \n",
    "        csvwriter.writerows(rows)\n",
    "    print(\"Model Results Sucessfully Saved to \\\"model_outputs/\" + gan_id + \".csv\\\"\")\n",
    "    save_model(gen, disc, gan_id)\n",
    "    model_output = pd.read_csv(\"model_outputs/\" + gan_id + \".csv\")\n",
    "    visualize_gen(X, gen, z_dim)\n",
    "    \n",
    "    # Change path and name of the Generator and Discriminator accordingly\n",
    "    save_model(gen, disc, gan_id)\n",
    "    \n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16cd118",
   "metadata": {},
   "source": [
    "# Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot metrics based on data (csv)\n",
    "def plot_metrics(data, vanilla = True):\n",
    "    if vanilla:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "    \n",
    "        plt.figure(figsize = (15, 15))\n",
    "        subplot(2, 2, 1)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'FPR', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 2)\n",
    "        sns.scatterplot(x = 'Epoch', y = 'Recall', data = data).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 3)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(2, 2, 4)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.set(style = 'whitegrid', context = 'talk', palette = 'rainbow')\n",
    "        plt.figure(figsize = (15, 8))\n",
    "        \n",
    "        subplot(1, 2, 1)\n",
    "        sns.regplot(x = 'Epoch', y = 'Median Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        subplot(1, 2, 2)\n",
    "        sns.regplot(x = 'Epoch', y = 'Mean Wasserstein', data = data, line_kws = {'color': 'orange'}).set(xlim = (0, None))\n",
    "        sns.despine()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677c28c",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1171105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = start_data(\"aggregated_data/aggregated_data.csv\", \"label:SITTING\")\n",
    "X, y = start_data(\"raw_data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv\", \"label:SITTING\") \n",
    "model_output = training_loop(X,y, activity_classifier, user_classifier, gan_id=\"10\", batch_size = 200, gen_lr=.0001, disc_lr =.0001, n_epochs=10, dig=5, constant_train_flag=True)\n",
    "plot_metrics(model_output, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
