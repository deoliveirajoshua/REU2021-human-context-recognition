{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>label:STAIRS_-_GOING_DOWN</th>\n",
       "      <th>label:ELEVATOR</th>\n",
       "      <th>label:OR_standing</th>\n",
       "      <th>label:AT_SCHOOL</th>\n",
       "      <th>label:PHONE_IN_HAND</th>\n",
       "      <th>label:PHONE_IN_BAG</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "      <th>label:WITH_CO-WORKERS</th>\n",
       "      <th>label:WITH_FRIENDS</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079161</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.995203</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.998502</td>\n",
       "      <td>1.748756</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079221</td>\n",
       "      <td>0.996864</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>1.935573</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079281</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.994797</td>\n",
       "      <td>0.996614</td>\n",
       "      <td>0.998704</td>\n",
       "      <td>2.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079341</td>\n",
       "      <td>0.996874</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>0.995050</td>\n",
       "      <td>0.996907</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>1.865318</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00EABED2-271D-49D8-B599-1D4A09240601</td>\n",
       "      <td>1444079431</td>\n",
       "      <td>0.997371</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.102332</td>\n",
       "      <td>0.995548</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.460806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   UUID   timestamp  \\\n",
       "0  00EABED2-271D-49D8-B599-1D4A09240601  1444079161   \n",
       "1  00EABED2-271D-49D8-B599-1D4A09240601  1444079221   \n",
       "2  00EABED2-271D-49D8-B599-1D4A09240601  1444079281   \n",
       "3  00EABED2-271D-49D8-B599-1D4A09240601  1444079341   \n",
       "4  00EABED2-271D-49D8-B599-1D4A09240601  1444079431   \n",
       "\n",
       "   raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0                      0.996815                     0.003529   \n",
       "1                      0.996864                     0.004172   \n",
       "2                      0.996825                     0.003667   \n",
       "3                      0.996874                     0.003541   \n",
       "4                      0.997371                     0.037653   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.002786                         0.006496   \n",
       "1                        -0.003110                         0.007050   \n",
       "2                         0.003094                         0.006076   \n",
       "3                         0.000626                         0.006059   \n",
       "4                         0.043389                         0.102332   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              0.995203                              0.996825   \n",
       "1                              0.994957                              0.996981   \n",
       "2                              0.994797                              0.996614   \n",
       "3                              0.995050                              0.996907   \n",
       "4                              0.995548                              0.996860   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              0.998502   \n",
       "1                              0.998766   \n",
       "2                              0.998704   \n",
       "3                              0.998690   \n",
       "4                              0.998205   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  ...  label:STAIRS_-_GOING_DOWN  \\\n",
       "0                               1.748756  ...                        NaN   \n",
       "1                               1.935573  ...                        NaN   \n",
       "2                               2.031780  ...                        NaN   \n",
       "3                               1.865318  ...                        NaN   \n",
       "4                               0.460806  ...                        NaN   \n",
       "\n",
       "   label:ELEVATOR  label:OR_standing  label:AT_SCHOOL  label:PHONE_IN_HAND  \\\n",
       "0             NaN                0.0              NaN                  NaN   \n",
       "1             NaN                0.0              NaN                  NaN   \n",
       "2             NaN                0.0              NaN                  NaN   \n",
       "3             NaN                0.0              NaN                  NaN   \n",
       "4             NaN                0.0              NaN                  NaN   \n",
       "\n",
       "   label:PHONE_IN_BAG  label:PHONE_ON_TABLE  label:WITH_CO-WORKERS  \\\n",
       "0                 NaN                   1.0                    1.0   \n",
       "1                 NaN                   1.0                    1.0   \n",
       "2                 NaN                   1.0                    1.0   \n",
       "3                 NaN                   1.0                    1.0   \n",
       "4                 NaN                   1.0                    1.0   \n",
       "\n",
       "   label:WITH_FRIENDS  label_source  \n",
       "0                 NaN             2  \n",
       "1                 NaN             2  \n",
       "2                 NaN             2  \n",
       "3                 NaN             2  \n",
       "4                 NaN             2  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Aggregated User Data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = interpolation(data.iloc[:,2:28])\n",
    "y = interpolation(data.iloc[:,227:-1])\n",
    "\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06795078, -0.36746172, -0.35842925, ...,  0.39622642,\n",
       "         1.84726953,  0.95347118],\n",
       "       [-0.06733515, -0.36076884, -0.36129259, ..., -0.257227  ,\n",
       "         1.27502995,  0.22785565],\n",
       "       [-0.06782514, -0.3660253 , -0.30646495, ...,  0.45060521,\n",
       "         1.66037285,  0.8438587 ],\n",
       "       ...,\n",
       "       [ 0.30153292,  1.62669325,  1.92186691, ..., -0.74879204,\n",
       "         1.25038783,  0.4289404 ],\n",
       "       [ 1.52388973,  2.44210021,  2.43071056, ..., -0.8930017 ,\n",
       "        -0.76180252,  2.05206506],\n",
       "       [ 0.23118678,  1.00799237,  0.58609267, ..., -0.43935668,\n",
       "        -0.02730479,  0.09026333]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 26):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 50),\n",
    "            classifier_block(50, 100),\n",
    "            classifier_block(100, 75),\n",
    "            nn.Linear(75, 51),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "n_epochs = 200\n",
    "batch_size = 200\n",
    "feature_dim = y.shape[1]\n",
    "lr = 0.01\n",
    "\n",
    "train_features = torch.tensor(X).to(device)\n",
    "train_labels = torch.tensor(y).to(device)\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "model = Classifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 276.8947856426239, Final Batch Loss: 0.1366567462682724\n",
      "Epoch 1, Loss: 266.0412206053734, Final Batch Loss: 0.12896481156349182\n",
      "Epoch 2, Loss: 263.09387032687664, Final Batch Loss: 0.12644906342029572\n",
      "Epoch 3, Loss: 261.3870267048478, Final Batch Loss: 0.14045093953609467\n",
      "Epoch 4, Loss: 259.83589163422585, Final Batch Loss: 0.13299328088760376\n",
      "Epoch 5, Loss: 259.2597752660513, Final Batch Loss: 0.1463182419538498\n",
      "Epoch 6, Loss: 258.27568669617176, Final Batch Loss: 0.1462501734495163\n",
      "Epoch 7, Loss: 257.7430736646056, Final Batch Loss: 0.13134463131427765\n",
      "Epoch 8, Loss: 257.2638116851449, Final Batch Loss: 0.15012580156326294\n",
      "Epoch 9, Loss: 257.02130801975727, Final Batch Loss: 0.13630637526512146\n",
      "Epoch 10, Loss: 256.5592309087515, Final Batch Loss: 0.14262062311172485\n",
      "Epoch 11, Loss: 256.3784095197916, Final Batch Loss: 0.13690762221813202\n",
      "Epoch 12, Loss: 256.17999620735645, Final Batch Loss: 0.1315571367740631\n",
      "Epoch 13, Loss: 256.19172935932875, Final Batch Loss: 0.1306341141462326\n",
      "Epoch 14, Loss: 255.8681349530816, Final Batch Loss: 0.13820762932300568\n",
      "Epoch 15, Loss: 255.87220800668, Final Batch Loss: 0.1370946764945984\n",
      "Epoch 16, Loss: 255.75545966625214, Final Batch Loss: 0.14383116364479065\n",
      "Epoch 17, Loss: 255.4430294111371, Final Batch Loss: 0.15455584228038788\n",
      "Epoch 18, Loss: 255.48225462436676, Final Batch Loss: 0.1389753222465515\n",
      "Epoch 19, Loss: 255.45268036425114, Final Batch Loss: 0.14126449823379517\n",
      "Epoch 20, Loss: 255.19526733458042, Final Batch Loss: 0.13974741101264954\n",
      "Epoch 21, Loss: 255.14339236915112, Final Batch Loss: 0.1338881105184555\n",
      "Epoch 22, Loss: 255.19427730888128, Final Batch Loss: 0.1390685737133026\n",
      "Epoch 23, Loss: 255.0808601230383, Final Batch Loss: 0.13526442646980286\n",
      "Epoch 24, Loss: 254.96169988065958, Final Batch Loss: 0.1344567984342575\n",
      "Epoch 25, Loss: 254.96813865751028, Final Batch Loss: 0.13511846959590912\n",
      "Epoch 26, Loss: 254.71950333565474, Final Batch Loss: 0.12419553846120834\n",
      "Epoch 27, Loss: 254.6664151325822, Final Batch Loss: 0.1410667896270752\n",
      "Epoch 28, Loss: 254.6847668364644, Final Batch Loss: 0.13627764582633972\n",
      "Epoch 29, Loss: 254.82142800837755, Final Batch Loss: 0.12762127816677094\n",
      "Epoch 30, Loss: 254.41304691135883, Final Batch Loss: 0.13055607676506042\n",
      "Epoch 31, Loss: 254.57460119575262, Final Batch Loss: 0.13479384779930115\n",
      "Epoch 32, Loss: 254.72573976963758, Final Batch Loss: 0.14877665042877197\n",
      "Epoch 33, Loss: 254.41553428769112, Final Batch Loss: 0.1452152132987976\n",
      "Epoch 34, Loss: 254.5704127550125, Final Batch Loss: 0.1312444657087326\n",
      "Epoch 35, Loss: 254.69753927737474, Final Batch Loss: 0.14091947674751282\n",
      "Epoch 36, Loss: 254.50352768599987, Final Batch Loss: 0.12322842329740524\n",
      "Epoch 37, Loss: 254.30428814142942, Final Batch Loss: 0.13219255208969116\n",
      "Epoch 38, Loss: 254.43836848437786, Final Batch Loss: 0.14032147824764252\n",
      "Epoch 39, Loss: 254.43660233914852, Final Batch Loss: 0.1422465592622757\n",
      "Epoch 40, Loss: 254.382772102952, Final Batch Loss: 0.13326600193977356\n",
      "Epoch 41, Loss: 254.08901359140873, Final Batch Loss: 0.12696455419063568\n",
      "Epoch 42, Loss: 254.19122693687677, Final Batch Loss: 0.14431816339492798\n",
      "Epoch 43, Loss: 254.06558912992477, Final Batch Loss: 0.14037682116031647\n",
      "Epoch 44, Loss: 254.36518174409866, Final Batch Loss: 0.134815514087677\n",
      "Epoch 45, Loss: 254.028719432652, Final Batch Loss: 0.13422314822673798\n",
      "Epoch 46, Loss: 254.4040769636631, Final Batch Loss: 0.137785404920578\n",
      "Epoch 47, Loss: 253.86717269569635, Final Batch Loss: 0.13144104182720184\n",
      "Epoch 48, Loss: 254.23221596330404, Final Batch Loss: 0.11951828002929688\n",
      "Epoch 49, Loss: 254.30013124644756, Final Batch Loss: 0.14243747293949127\n",
      "Epoch 50, Loss: 254.08311028778553, Final Batch Loss: 0.1353183090686798\n",
      "Epoch 51, Loss: 254.1756296902895, Final Batch Loss: 0.12929211556911469\n",
      "Epoch 52, Loss: 253.94680260121822, Final Batch Loss: 0.14572806656360626\n",
      "Epoch 53, Loss: 254.10413042455912, Final Batch Loss: 0.14584881067276\n",
      "Epoch 54, Loss: 253.91811600327492, Final Batch Loss: 0.1406346708536148\n",
      "Epoch 55, Loss: 253.81446985155344, Final Batch Loss: 0.14609184861183167\n",
      "Epoch 56, Loss: 253.7415364459157, Final Batch Loss: 0.12763571739196777\n",
      "Epoch 57, Loss: 253.83314538002014, Final Batch Loss: 0.13462622463703156\n",
      "Epoch 58, Loss: 253.58931678533554, Final Batch Loss: 0.14382968842983246\n",
      "Epoch 59, Loss: 253.87060730159283, Final Batch Loss: 0.1277637928724289\n",
      "Epoch 60, Loss: 253.62706767767668, Final Batch Loss: 0.13779380917549133\n",
      "Epoch 61, Loss: 253.6539725139737, Final Batch Loss: 0.1295364499092102\n",
      "Epoch 62, Loss: 253.57338745892048, Final Batch Loss: 0.1299699991941452\n",
      "Epoch 63, Loss: 253.59143332391977, Final Batch Loss: 0.12775062024593353\n",
      "Epoch 64, Loss: 253.57381015270948, Final Batch Loss: 0.13250377774238586\n",
      "Epoch 65, Loss: 253.3974776044488, Final Batch Loss: 0.14039413630962372\n",
      "Epoch 66, Loss: 253.7407580167055, Final Batch Loss: 0.13783253729343414\n",
      "Epoch 67, Loss: 253.5233854725957, Final Batch Loss: 0.14270317554473877\n",
      "Epoch 68, Loss: 253.49415642023087, Final Batch Loss: 0.13062946498394012\n",
      "Epoch 69, Loss: 253.80883394926786, Final Batch Loss: 0.13374406099319458\n",
      "Epoch 70, Loss: 253.29941038042307, Final Batch Loss: 0.14065352082252502\n",
      "Epoch 71, Loss: 253.53774700313807, Final Batch Loss: 0.13724227249622345\n",
      "Epoch 72, Loss: 253.45631148666143, Final Batch Loss: 0.1355980485677719\n",
      "Epoch 73, Loss: 253.39101274311543, Final Batch Loss: 0.12888412177562714\n",
      "Epoch 74, Loss: 253.3905385658145, Final Batch Loss: 0.13886797428131104\n",
      "Epoch 75, Loss: 253.35397900640965, Final Batch Loss: 0.13243575394153595\n",
      "Epoch 76, Loss: 253.46267595142126, Final Batch Loss: 0.1263291835784912\n",
      "Epoch 77, Loss: 253.291896186769, Final Batch Loss: 0.1305759698152542\n",
      "Epoch 78, Loss: 252.98648142069578, Final Batch Loss: 0.1372624635696411\n",
      "Epoch 79, Loss: 253.3799865320325, Final Batch Loss: 0.15235985815525055\n",
      "Epoch 80, Loss: 253.25173550099134, Final Batch Loss: 0.11993157118558884\n",
      "Epoch 81, Loss: 253.2297463938594, Final Batch Loss: 0.13346093893051147\n",
      "Epoch 82, Loss: 253.50568379461765, Final Batch Loss: 0.12382257729768753\n",
      "Epoch 83, Loss: 253.25048827379942, Final Batch Loss: 0.14553199708461761\n",
      "Epoch 84, Loss: 253.08817701786757, Final Batch Loss: 0.1383218914270401\n",
      "Epoch 85, Loss: 252.99131424725056, Final Batch Loss: 0.12247078865766525\n",
      "Epoch 86, Loss: 253.44066820293665, Final Batch Loss: 0.13532987236976624\n",
      "Epoch 87, Loss: 253.1900798678398, Final Batch Loss: 0.1407463103532791\n",
      "Epoch 88, Loss: 253.05544021725655, Final Batch Loss: 0.13588672876358032\n",
      "Epoch 89, Loss: 253.04618114233017, Final Batch Loss: 0.13314703106880188\n",
      "Epoch 90, Loss: 253.23277501761913, Final Batch Loss: 0.13116370141506195\n",
      "Epoch 91, Loss: 252.999944396317, Final Batch Loss: 0.14392980933189392\n",
      "Epoch 92, Loss: 253.0538844615221, Final Batch Loss: 0.13462427258491516\n",
      "Epoch 93, Loss: 252.91875812411308, Final Batch Loss: 0.14594542980194092\n",
      "Epoch 94, Loss: 253.45107505470514, Final Batch Loss: 0.1321893036365509\n",
      "Epoch 95, Loss: 252.99428840726614, Final Batch Loss: 0.13686257600784302\n",
      "Epoch 96, Loss: 253.02574426680803, Final Batch Loss: 0.140559583902359\n",
      "Epoch 97, Loss: 253.25681257992983, Final Batch Loss: 0.11966399103403091\n",
      "Epoch 98, Loss: 253.0791708007455, Final Batch Loss: 0.13740797340869904\n",
      "Epoch 99, Loss: 253.045155338943, Final Batch Loss: 0.13313788175582886\n",
      "Epoch 100, Loss: 252.98618675768375, Final Batch Loss: 0.13330349326133728\n",
      "Epoch 101, Loss: 252.70970484614372, Final Batch Loss: 0.1393241584300995\n",
      "Epoch 102, Loss: 252.92701371014118, Final Batch Loss: 0.1359560638666153\n",
      "Epoch 103, Loss: 252.99453852325678, Final Batch Loss: 0.11620396375656128\n",
      "Epoch 104, Loss: 252.88335940986872, Final Batch Loss: 0.12775497138500214\n",
      "Epoch 105, Loss: 252.9493569135666, Final Batch Loss: 0.13309243321418762\n",
      "Epoch 106, Loss: 253.12171489745378, Final Batch Loss: 0.14028435945510864\n",
      "Epoch 107, Loss: 252.76405433565378, Final Batch Loss: 0.12527576088905334\n",
      "Epoch 108, Loss: 252.82871429622173, Final Batch Loss: 0.1330648958683014\n",
      "Epoch 109, Loss: 252.92540610581636, Final Batch Loss: 0.1423569619655609\n",
      "Epoch 110, Loss: 253.19232834130526, Final Batch Loss: 0.11481606960296631\n",
      "Epoch 111, Loss: 252.7619182318449, Final Batch Loss: 0.1301281452178955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112, Loss: 252.78821858763695, Final Batch Loss: 0.13389229774475098\n",
      "Epoch 113, Loss: 252.9370829537511, Final Batch Loss: 0.13194067776203156\n",
      "Epoch 114, Loss: 252.7101497799158, Final Batch Loss: 0.13738591969013214\n",
      "Epoch 115, Loss: 252.80763110518456, Final Batch Loss: 0.1245899498462677\n",
      "Epoch 116, Loss: 252.80158676952124, Final Batch Loss: 0.14057523012161255\n",
      "Epoch 117, Loss: 252.72383458912373, Final Batch Loss: 0.1480824500322342\n",
      "Epoch 118, Loss: 252.87635570764542, Final Batch Loss: 0.13836610317230225\n",
      "Epoch 119, Loss: 252.68414817005396, Final Batch Loss: 0.14318394660949707\n",
      "Epoch 120, Loss: 252.50865723192692, Final Batch Loss: 0.1266876757144928\n",
      "Epoch 121, Loss: 252.64927408844233, Final Batch Loss: 0.13406777381896973\n",
      "Epoch 122, Loss: 252.74670501053333, Final Batch Loss: 0.11842267960309982\n",
      "Epoch 123, Loss: 252.5834176018834, Final Batch Loss: 0.13811764121055603\n",
      "Epoch 124, Loss: 252.5010293647647, Final Batch Loss: 0.1282484382390976\n",
      "Epoch 125, Loss: 252.66962334513664, Final Batch Loss: 0.12400843948125839\n",
      "Epoch 126, Loss: 252.84284964203835, Final Batch Loss: 0.13676175475120544\n",
      "Epoch 127, Loss: 252.54025334864855, Final Batch Loss: 0.12401815503835678\n",
      "Epoch 128, Loss: 252.3878373876214, Final Batch Loss: 0.13459457457065582\n",
      "Epoch 129, Loss: 252.6194163635373, Final Batch Loss: 0.12216677516698837\n",
      "Epoch 130, Loss: 252.56255032122135, Final Batch Loss: 0.1387973427772522\n",
      "Epoch 131, Loss: 252.37827199697495, Final Batch Loss: 0.13353820145130157\n",
      "Epoch 132, Loss: 252.68621008098125, Final Batch Loss: 0.12878412008285522\n",
      "Epoch 133, Loss: 252.74145720154047, Final Batch Loss: 0.12910932302474976\n",
      "Epoch 134, Loss: 252.3098550066352, Final Batch Loss: 0.13758324086666107\n",
      "Epoch 135, Loss: 252.54988176375628, Final Batch Loss: 0.13172811269760132\n",
      "Epoch 136, Loss: 252.70605101436377, Final Batch Loss: 0.1303139477968216\n",
      "Epoch 137, Loss: 252.30897134542465, Final Batch Loss: 0.13745148479938507\n",
      "Epoch 138, Loss: 252.6353102400899, Final Batch Loss: 0.1274949461221695\n",
      "Epoch 139, Loss: 252.40769600868225, Final Batch Loss: 0.1324850618839264\n",
      "Epoch 140, Loss: 252.5395573452115, Final Batch Loss: 0.13742493093013763\n",
      "Epoch 141, Loss: 252.771613702178, Final Batch Loss: 0.1326345056295395\n",
      "Epoch 142, Loss: 252.36845137923956, Final Batch Loss: 0.1454339474439621\n",
      "Epoch 143, Loss: 252.51874066144228, Final Batch Loss: 0.13312017917633057\n",
      "Epoch 144, Loss: 252.65052910894156, Final Batch Loss: 0.1399773210287094\n",
      "Epoch 145, Loss: 252.34848672896624, Final Batch Loss: 0.1381264477968216\n",
      "Epoch 146, Loss: 252.7162092924118, Final Batch Loss: 0.1452213078737259\n",
      "Epoch 147, Loss: 252.391820512712, Final Batch Loss: 0.1170245110988617\n",
      "Epoch 148, Loss: 252.36842948943377, Final Batch Loss: 0.1299894005060196\n",
      "Epoch 149, Loss: 252.87224403768778, Final Batch Loss: 0.14293377101421356\n",
      "Epoch 150, Loss: 252.5856992006302, Final Batch Loss: 0.1257718801498413\n",
      "Epoch 151, Loss: 252.25881572067738, Final Batch Loss: 0.12605378031730652\n",
      "Epoch 152, Loss: 252.44767993688583, Final Batch Loss: 0.1402919441461563\n",
      "Epoch 153, Loss: 252.38247552514076, Final Batch Loss: 0.14082346856594086\n",
      "Epoch 154, Loss: 252.37819661945105, Final Batch Loss: 0.1374891698360443\n",
      "Epoch 155, Loss: 252.43825501948595, Final Batch Loss: 0.13100560009479523\n",
      "Epoch 156, Loss: 252.18525142222643, Final Batch Loss: 0.12371324747800827\n",
      "Epoch 157, Loss: 252.25372672826052, Final Batch Loss: 0.13372546434402466\n",
      "Epoch 158, Loss: 252.46441922336817, Final Batch Loss: 0.12234263867139816\n",
      "Epoch 159, Loss: 252.3861884251237, Final Batch Loss: 0.14175492525100708\n",
      "Epoch 160, Loss: 252.23321885615587, Final Batch Loss: 0.12381838262081146\n",
      "Epoch 161, Loss: 252.60182916373014, Final Batch Loss: 0.150898739695549\n",
      "Epoch 162, Loss: 252.1163603812456, Final Batch Loss: 0.13071881234645844\n",
      "Epoch 163, Loss: 252.36831801384687, Final Batch Loss: 0.1335839033126831\n",
      "Epoch 164, Loss: 252.31825940310955, Final Batch Loss: 0.1468857079744339\n",
      "Epoch 165, Loss: 252.4397242218256, Final Batch Loss: 0.1326574981212616\n",
      "Epoch 166, Loss: 252.31570310145617, Final Batch Loss: 0.12659244239330292\n",
      "Epoch 167, Loss: 252.47774974256754, Final Batch Loss: 0.13339297473430634\n",
      "Epoch 168, Loss: 252.3026737049222, Final Batch Loss: 0.14159320294857025\n",
      "Epoch 169, Loss: 252.58048301190138, Final Batch Loss: 0.11766989529132843\n",
      "Epoch 170, Loss: 252.1793491244316, Final Batch Loss: 0.13919849693775177\n",
      "Epoch 171, Loss: 252.0881192535162, Final Batch Loss: 0.12589989602565765\n",
      "Epoch 172, Loss: 252.39332036674023, Final Batch Loss: 0.13664482533931732\n",
      "Epoch 173, Loss: 252.52097313851118, Final Batch Loss: 0.15616296231746674\n",
      "Epoch 174, Loss: 252.31070495396852, Final Batch Loss: 0.12556876242160797\n",
      "Epoch 175, Loss: 252.39168376475573, Final Batch Loss: 0.12440767884254456\n",
      "Epoch 176, Loss: 252.33231810480356, Final Batch Loss: 0.124224953353405\n",
      "Epoch 177, Loss: 252.13398803770542, Final Batch Loss: 0.13348329067230225\n",
      "Epoch 178, Loss: 252.27285374701023, Final Batch Loss: 0.13114488124847412\n",
      "Epoch 179, Loss: 252.45548398047686, Final Batch Loss: 0.13424888253211975\n",
      "Epoch 180, Loss: 252.2210839614272, Final Batch Loss: 0.13190345466136932\n",
      "Epoch 181, Loss: 252.37136580049992, Final Batch Loss: 0.12500998377799988\n",
      "Epoch 182, Loss: 252.236990891397, Final Batch Loss: 0.13161945343017578\n",
      "Epoch 183, Loss: 252.5352130830288, Final Batch Loss: 0.1336251199245453\n",
      "Epoch 184, Loss: 252.23225855082273, Final Batch Loss: 0.13652236759662628\n",
      "Epoch 185, Loss: 252.36077615618706, Final Batch Loss: 0.14478221535682678\n",
      "Epoch 186, Loss: 252.31322696059942, Final Batch Loss: 0.12064595520496368\n",
      "Epoch 187, Loss: 252.50187681615353, Final Batch Loss: 0.13415150344371796\n",
      "Epoch 188, Loss: 252.21671491116285, Final Batch Loss: 0.1313033550977707\n",
      "Epoch 189, Loss: 252.31565108895302, Final Batch Loss: 0.12645147740840912\n",
      "Epoch 190, Loss: 252.2188592404127, Final Batch Loss: 0.1322934478521347\n",
      "Epoch 191, Loss: 252.15283507853746, Final Batch Loss: 0.1321239173412323\n",
      "Epoch 192, Loss: 252.43110397458076, Final Batch Loss: 0.12753775715827942\n",
      "Epoch 193, Loss: 252.19189143925905, Final Batch Loss: 0.12956951558589935\n",
      "Epoch 194, Loss: 252.40392443537712, Final Batch Loss: 0.13195227086544037\n",
      "Epoch 195, Loss: 252.26960438489914, Final Batch Loss: 0.14567221701145172\n",
      "Epoch 196, Loss: 252.1815261170268, Final Batch Loss: 0.12191718816757202\n",
      "Epoch 197, Loss: 252.33433279395103, Final Batch Loss: 0.13538634777069092\n",
      "Epoch 198, Loss: 252.2689548805356, Final Batch Loss: 0.13574416935443878\n",
      "Epoch 199, Loss: 252.14592671394348, Final Batch Loss: 0.1426467001438141\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, label = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        loss = criterion(preds, label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(features.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0', grad_fn=<RoundBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
