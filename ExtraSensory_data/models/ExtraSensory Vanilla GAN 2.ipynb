{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>raw_acc:magnitude_stats:time_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>label:STAIRS_-_GOING_DOWN</th>\n",
       "      <th>label:ELEVATOR</th>\n",
       "      <th>label:OR_standing</th>\n",
       "      <th>label:AT_SCHOOL</th>\n",
       "      <th>label:PHONE_IN_HAND</th>\n",
       "      <th>label:PHONE_IN_BAG</th>\n",
       "      <th>label:PHONE_ON_TABLE</th>\n",
       "      <th>label:WITH_CO-WORKERS</th>\n",
       "      <th>label:WITH_FRIENDS</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1449601597</td>\n",
       "      <td>1.000371</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>-0.016173</td>\n",
       "      <td>0.027860</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>1.000739</td>\n",
       "      <td>1.003265</td>\n",
       "      <td>0.891038</td>\n",
       "      <td>6.684582</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1449601657</td>\n",
       "      <td>1.000243</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.998463</td>\n",
       "      <td>1.000373</td>\n",
       "      <td>1.002088</td>\n",
       "      <td>1.647929</td>\n",
       "      <td>6.684605</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1449601717</td>\n",
       "      <td>1.000811</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>1.000928</td>\n",
       "      <td>1.002032</td>\n",
       "      <td>1.960286</td>\n",
       "      <td>6.684610</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1449601777</td>\n",
       "      <td>1.001245</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.999188</td>\n",
       "      <td>1.001425</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>1.614524</td>\n",
       "      <td>6.684601</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1449601855</td>\n",
       "      <td>1.001354</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>-0.096520</td>\n",
       "      <td>0.165298</td>\n",
       "      <td>1.000807</td>\n",
       "      <td>1.002259</td>\n",
       "      <td>1.003631</td>\n",
       "      <td>0.837790</td>\n",
       "      <td>6.682252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0  1449601597                      1.000371                     0.007671   \n",
       "1  1449601657                      1.000243                     0.003782   \n",
       "2  1449601717                      1.000811                     0.002082   \n",
       "3  1449601777                      1.001245                     0.004715   \n",
       "4  1449601855                      1.001354                     0.065186   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.016173                         0.027860   \n",
       "1                        -0.002713                         0.007046   \n",
       "2                        -0.001922                         0.003575   \n",
       "3                        -0.002895                         0.008881   \n",
       "4                        -0.096520                         0.165298   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              0.998221                              1.000739   \n",
       "1                              0.998463                              1.000373   \n",
       "2                              0.999653                              1.000928   \n",
       "3                              0.999188                              1.001425   \n",
       "4                              1.000807                              1.002259   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              1.003265   \n",
       "1                              1.002088   \n",
       "2                              1.002032   \n",
       "3                              1.003500   \n",
       "4                              1.003631   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  \\\n",
       "0                               0.891038   \n",
       "1                               1.647929   \n",
       "2                               1.960286   \n",
       "3                               1.614524   \n",
       "4                               0.837790   \n",
       "\n",
       "   raw_acc:magnitude_stats:time_entropy  ...  label:STAIRS_-_GOING_DOWN  \\\n",
       "0                              6.684582  ...                        NaN   \n",
       "1                              6.684605  ...                        NaN   \n",
       "2                              6.684610  ...                        NaN   \n",
       "3                              6.684601  ...                        NaN   \n",
       "4                              6.682252  ...                        0.0   \n",
       "\n",
       "   label:ELEVATOR  label:OR_standing  label:AT_SCHOOL  label:PHONE_IN_HAND  \\\n",
       "0             NaN                NaN              NaN                  NaN   \n",
       "1             NaN                NaN              NaN                  NaN   \n",
       "2             NaN                NaN              NaN                  NaN   \n",
       "3             NaN                NaN              NaN                  NaN   \n",
       "4             NaN                0.0              1.0                  NaN   \n",
       "\n",
       "   label:PHONE_IN_BAG  label:PHONE_ON_TABLE  label:WITH_CO-WORKERS  \\\n",
       "0                 NaN                   NaN                    NaN   \n",
       "1                 NaN                   NaN                    NaN   \n",
       "2                 NaN                   NaN                    NaN   \n",
       "3                 NaN                   NaN                    NaN   \n",
       "4                 NaN                   NaN                    NaN   \n",
       "\n",
       "   label:WITH_FRIENDS  label_source  \n",
       "0                 NaN            -1  \n",
       "1                 NaN            -1  \n",
       "2                 NaN            -1  \n",
       "3                 NaN            -1  \n",
       "4                 0.0             2  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Change the data file directory below appropriately\n",
    "data = pd.read_csv('data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolating acceleration columns with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(df):\n",
    "    col_to_avg = list(df.columns) #Start with keeping all the columns as columns to use an average interpolation on\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if list(df.columns)[k].startswith(('discrete', 'label')): #Remove label and discrete columns from col_to_avg\n",
    "            col_to_avg.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_avg = df[col_to_avg].fillna(df[col_to_avg].mean()) #Interpolate nan columns for all continuous-valued columns with average\n",
    "    \n",
    "    col_to_zero = list(df.columns)\n",
    "    for k in range(len(list(df.columns))):\n",
    "        if not list(df.columns)[k].startswith(('discrete', 'label')): #Remove all columns except label and discrete\n",
    "            col_to_zero.remove(list(df.columns)[k])\n",
    "    \n",
    "    df_with_zero = df[col_to_zero].fillna(0) #Interpolate nan values for label and discrete columns with 0\n",
    "    \n",
    "    return pd.concat([df_with_avg, df_with_zero], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:27]\n",
    "y = data[['label:SITTING']]\n",
    "\n",
    "X = interpolation(X)\n",
    "y = interpolation(y)\n",
    "\n",
    "X = X[y['label:SITTING'] == 1].reset_index() ### Select samples of acceleration where the person is sitting\n",
    "X.drop(columns = ['index'], inplace = True)\n",
    "y = y[y['label:SITTING'] == 1].reset_index()\n",
    "y.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03055181, 0.11061779, 0.12862242, ..., 0.44251652, 0.73594585,\n",
       "        0.48495746],\n",
       "       [0.03274136, 0.11517124, 0.39588783, ..., 0.61975644, 0.43366874,\n",
       "        0.22504885],\n",
       "       [0.03146445, 0.0019387 , 0.22650965, ..., 0.61342338, 0.47548982,\n",
       "        0.7131712 ],\n",
       "       ...,\n",
       "       [0.03284124, 0.04673182, 0.2552768 , ..., 0.76676471, 0.64817891,\n",
       "        0.5024305 ],\n",
       "       [0.03297246, 0.00750709, 0.22582391, ..., 0.45546619, 0.73917111,\n",
       "        0.77494757],\n",
       "       [0.03262581, 0.00437283, 0.23225078, ..., 0.58757284, 0.49265173,\n",
       "        0.4306445 ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "X = mm.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(X)\n",
    "train_labels = torch.tensor(y)\n",
    "batch_size = 60\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True) ### 66 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, hidden_dim * 2),\n",
    "            generator_block(hidden_dim * 2, hidden_dim),\n",
    "            generator_block(hidden_dim, int(hidden_dim * 0.5)),\n",
    "            nn.Linear(int(hidden_dim * 0.5), feature_dim),\n",
    "            #generator_block(int(hidden_dim * 0.5), feature_dim),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def discriminator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout()\n",
    "    )\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim = 26, hidden_dim = 128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            discriminator_block(feature_dim, hidden_dim),\n",
    "            discriminator_block(hidden_dim, int(hidden_dim * 0.5)),\n",
    "            discriminator_block(int(hidden_dim * 0.5), hidden_dim),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, feature_vector):\n",
    "        return self.disc(feature_vector)\n",
    "\n",
    "def get_disc_loss(gen, disc, criterion, real_features, batch_size, z_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    pred_fake = disc(fake_features.detach())\n",
    "    \n",
    "    ground_truth = torch.zeros_like(pred_fake)\n",
    "    loss_fake = criterion(pred_fake, ground_truth)\n",
    "    \n",
    "    pred_real = disc(real_features)\n",
    "    ground_truth = torch.ones_like(pred_real)\n",
    "    loss_real = criterion(pred_real, ground_truth)\n",
    "    \n",
    "    disc_loss = (loss_fake + loss_real) / 2\n",
    "    \n",
    "    return disc_loss\n",
    "\n",
    "def get_gen_loss(gen, disc, criterion, batch_size, z_dim):\n",
    "    latent_vectors = get_noise(batch_size, z_dim)\n",
    "    fake_features = gen(latent_vectors)\n",
    "    pred = disc(fake_features)\n",
    "    gen_loss = criterion(pred, torch.ones_like(pred))\n",
    "    \n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 50\n",
    "z_dim = 100\n",
    "lr = 0.000001\n",
    "\n",
    "disc = Discriminator()\n",
    "gen = Generator(z_dim)\n",
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr = lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "def get_fake_samples(gen, batch_size, z_dim):\n",
    "    \"\"\"\n",
    "    Generates fake acceleration features given a batch size, latent vector dimension, and trained generator.\n",
    "    \n",
    "    \"\"\"\n",
    "    latent_vectors = get_noise(batch_size, z_dim) ### Retrieves a 2D tensor of noise\n",
    "    fake_features = gen(latent_vectors)\n",
    "    \n",
    "    return fake_features ### Returns a 2D tensor of fake features of size batch_size x z_dim\n",
    "    \n",
    "\n",
    "def create_fake_dataset(gen, z_dim, data):\n",
    "    '''\n",
    "    Creates a training/test set with 50% fake sitting features and 50% real non-sitting features.\n",
    "    '''\n",
    "    ### Retrieve random real samples where the user wasn't sitting\n",
    "    X = interpolation(data.iloc[:,1:27]) \n",
    "    y = interpolation(data[['label:SITTING']]) \n",
    "    X = X[y['label:SITTING'] == 0].reset_index().drop(columns = ['index']) ### Selects non-sitting features\n",
    "    y = y[y['label:SITTING'] == 0].reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    X = X.values ### Converts the dataframes into arrays\n",
    "    y = y.values \n",
    "    \n",
    "    X_real, _, _, _ = train_test_split(X, y, test_size = 0.2) ### Use 80% of real non-sitting samples\n",
    "    \n",
    "    X_real_length = len(X_real) ### Storing the length to create an equal number of fake samples\n",
    "    \n",
    "    X_real = torch.tensor(X_real) ### All real samples where the user wasn't sitting\n",
    "    \n",
    "    fake_sitting = get_fake_samples(gen, X_real_length, z_dim)\n",
    "    \n",
    "    dataset = torch.cat((fake_sitting, X_real), dim = 0).detach()\n",
    "    \n",
    "    one_labels = torch.ones(len(fake_sitting), 1) ### 1s correspond to sitting\n",
    "    zero_labels = torch.zeros(len(X_real), 1) ### 0s correspond to not sitting\n",
    "    labels = torch.cat((one_labels, zero_labels), dim = 0).detach()\n",
    "    \n",
    "    ### Splitting into training and testing sets\n",
    "    dataset = dataset.numpy()\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.2)\n",
    "    \n",
    "    ### Converting to tensors\n",
    "    X_train = torch.tensor(X_train)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_test = torch.tensor(y_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_real_dataset(data):\n",
    "    \"\"\"\n",
    "    Returns a train/test split of the real dataset.\n",
    "    \"\"\"\n",
    "    ### Not a guaranteed 50-50 split between sitting and not sitting\n",
    "    X = interpolation(data.iloc[:,1:27])\n",
    "    y = interpolation(data[['label:SITTING']])\n",
    "\n",
    "    ### Converting dataframe to arrays\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    ### Converting to tensors\n",
    "    X_train = torch.tensor(X_train)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_test = torch.tensor(y_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "def create_dataloader(X_train, y_train, X_test, y_test, batch_size):\n",
    "    \"\"\"\n",
    "    Creates the train_loader and test_loader iterables.\n",
    "    \"\"\"\n",
    "    train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    test_data = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True) \n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model_fake(gen, criterion, n_epochs, z_dim, data, batch_size, input_size, output_size):\n",
    "    \"\"\"\n",
    "    Trains a classifier on a combination of real and fake training examples. Evaluates it on real testing examples.\n",
    "    \"\"\"\n",
    "    ### Create a training/test set with fake features\n",
    "    X_train, y_train, X_test, y_test = create_fake_dataset(gen, z_dim, data)\n",
    "    train_loader, test_loader = create_dataloader(X_train, y_train, X_test, y_test, batch_size)\n",
    "    \n",
    "    ### Instantiate the model and optimizer\n",
    "    model = Classifier(input_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "            y_preds = model(features.float())\n",
    "            loss = criterion(y_preds, labels.float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch: {epoch + 1} | Total Batch Loss: {total_loss}')\n",
    "        \n",
    "    ### Evalute Model's Performance\n",
    "    evaluate_model(X_test, y_test, test_loader, model)\n",
    "\n",
    "def train_model_real(criterion, n_epochs, data, batch_size):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a classifier on only real training and testing examples.\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test = create_real_dataset(data)\n",
    "    train_loader, test_loader = create_dataloader(X_train, y_train, X_test, y_test, batch_size)\n",
    "    \n",
    "    model = Classifier(input_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "            y_preds = model(features.float())\n",
    "            loss = criterion(y_preds, labels.float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch: {epoch + 1} | Total Batch Loss: {total_loss}')\n",
    "        \n",
    "    ### Evalute Model's Performance\n",
    "    evaluate_model(X_test, y_test, test_loader, model)\n",
    "\n",
    "def evaluate_model(X_test, y_test, test_loader, model):\n",
    "    \"\"\"\n",
    "    Returns the classification accuracy, precision, recall, and F-1 score of a model.\n",
    "    \"\"\"\n",
    "    total_wrong = 0\n",
    "    positive_preds = 0 \n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test_data, y_test in test_loader:\n",
    "            y_test_preds = model(X_test_data.float())\n",
    "            y_test_preds = torch.round(y_test_preds)\n",
    "\n",
    "            for k in range(len(y_test_preds)):\n",
    "                if y_test_preds[k].item() == 1:\n",
    "                    positive_preds += 1\n",
    "                if y_test_preds[k].item() == y_test[k].item() == 1:\n",
    "                    true_positives += 1\n",
    "                if y_test_preds[k].item() == 0 and y_test[k].item() == 1:\n",
    "                    false_negatives += 1\n",
    "\n",
    "            current_wrong = (abs(y_test_preds - y_test)).sum().item()\n",
    "            total_wrong += current_wrong\n",
    "\n",
    "        class_acc = (len(X_test) - total_wrong) / len(X_test) * 100\n",
    "        precision = true_positives / positive_preds\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        print(f'Classification Accuracy: {class_acc:.2f}')\n",
    "        print(f'Precision: {precision:.2f}') #What percentage of a model's positive predictions were actually positive\n",
    "        print(f'Recall: {recall:.2f}') #What percent of the true positives were identified\n",
    "        print(f'F-1 Score: {2*(precision * recall / (precision + recall)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Total Batch Loss: 10.670419368892908\n",
      "Epoch: 2 | Total Batch Loss: 0.2582301759393886\n",
      "Epoch: 3 | Total Batch Loss: 0.048022057802882046\n",
      "Epoch: 4 | Total Batch Loss: 0.02718917521997355\n",
      "Epoch: 5 | Total Batch Loss: 0.017476253357017413\n",
      "Epoch: 6 | Total Batch Loss: 0.01221564230218064\n",
      "Epoch: 7 | Total Batch Loss: 0.008981192964711227\n",
      "Epoch: 8 | Total Batch Loss: 0.006847762182587758\n",
      "Epoch: 9 | Total Batch Loss: 0.0053979304793756455\n",
      "Epoch: 10 | Total Batch Loss: 0.004357019912276883\n",
      "Epoch: 11 | Total Batch Loss: 0.0035702220375242177\n",
      "Epoch: 12 | Total Batch Loss: 0.0029781910379824694\n",
      "Epoch: 13 | Total Batch Loss: 0.002517574088415131\n",
      "Epoch: 14 | Total Batch Loss: 0.00215756164288905\n",
      "Epoch: 15 | Total Batch Loss: 0.0018641315637069056\n",
      "Epoch: 16 | Total Batch Loss: 0.0016313301921400125\n",
      "Epoch: 17 | Total Batch Loss: 0.001433808663932723\n",
      "Epoch: 18 | Total Batch Loss: 0.001271617616112053\n",
      "Epoch: 19 | Total Batch Loss: 0.0011284047868684866\n",
      "Epoch: 20 | Total Batch Loss: 0.0010192563449891168\n",
      "Classification Accuracy: 100.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F-1 Score: 1.00\n",
      "Epoch: 1 | Total Batch Loss: 41.81705766916275\n",
      "Epoch: 2 | Total Batch Loss: 37.20606967806816\n",
      "Epoch: 3 | Total Batch Loss: 34.766855359077454\n",
      "Epoch: 4 | Total Batch Loss: 33.14767801761627\n",
      "Epoch: 5 | Total Batch Loss: 31.57024821639061\n",
      "Epoch: 6 | Total Batch Loss: 31.064330145716667\n",
      "Epoch: 7 | Total Batch Loss: 29.53078642487526\n",
      "Epoch: 8 | Total Batch Loss: 28.54469186067581\n",
      "Epoch: 9 | Total Batch Loss: 27.93160229921341\n",
      "Epoch: 10 | Total Batch Loss: 27.30267497897148\n",
      "Epoch: 11 | Total Batch Loss: 26.85891756415367\n",
      "Epoch: 12 | Total Batch Loss: 26.599408701062202\n",
      "Epoch: 13 | Total Batch Loss: 25.928730696439743\n",
      "Epoch: 14 | Total Batch Loss: 25.369958981871605\n",
      "Epoch: 15 | Total Batch Loss: 24.850296705961227\n",
      "Epoch: 16 | Total Batch Loss: 24.64471523463726\n",
      "Epoch: 17 | Total Batch Loss: 24.580173328518867\n",
      "Epoch: 18 | Total Batch Loss: 24.109289094805717\n",
      "Epoch: 19 | Total Batch Loss: 24.126288428902626\n",
      "Epoch: 20 | Total Batch Loss: 23.53654932975769\n",
      "Classification Accuracy: 83.96\n",
      "Precision: 0.89\n",
      "Recall: 0.82\n",
      "F-1 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "n_epochs = 20\n",
    "z_dim = 100\n",
    "batch_size = 50\n",
    "input_size = 26\n",
    "output_size = 1\n",
    "\n",
    "train_model_fake(gen, loss_function, n_epochs, z_dim, data, batch_size, input_size, output_size)\n",
    "train_model_real(loss_function, n_epochs, data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 20\n",
    "z_dim = 100\n",
    "lr = 0.000001\n",
    "\n",
    "disc = Discriminator()\n",
    "gen = Generator(z_dim)\n",
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr = lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 20] Loss D: 0.6909, Loss G: 0.6937 \n",
      "Epoch [2 / 20] Loss D: 0.7011, Loss G: 0.6908 \n",
      "Epoch [3 / 20] Loss D: 0.6981, Loss G: 0.6927 \n",
      "Epoch [4 / 20] Loss D: 0.6946, Loss G: 0.6946 \n",
      "Epoch [5 / 20] Loss D: 0.6908, Loss G: 0.6877 \n",
      "Epoch [6 / 20] Loss D: 0.6897, Loss G: 0.6928 \n",
      "Epoch [7 / 20] Loss D: 0.6855, Loss G: 0.6944 \n",
      "Epoch [8 / 20] Loss D: 0.6926, Loss G: 0.6924 \n",
      "Epoch [9 / 20] Loss D: 0.6892, Loss G: 0.6962 \n",
      "Epoch [10 / 20] Loss D: 0.6954, Loss G: 0.6894 \n",
      "Epoch: 1 | Total Batch Loss: 12.186321257497184\n",
      "Epoch: 2 | Total Batch Loss: 0.11870711937081069\n",
      "Epoch: 3 | Total Batch Loss: 0.03152973535907222\n",
      "Epoch: 4 | Total Batch Loss: 0.015313363255700096\n",
      "Epoch: 5 | Total Batch Loss: 0.008897444244212238\n",
      "Epoch: 6 | Total Batch Loss: 0.0059209794490016066\n",
      "Epoch: 7 | Total Batch Loss: 0.004194332048427896\n",
      "Epoch: 8 | Total Batch Loss: 0.0031090115753613645\n",
      "Epoch: 9 | Total Batch Loss: 0.002444064371047716\n",
      "Epoch: 10 | Total Batch Loss: 0.0019151010828863946\n",
      "Epoch: 11 | Total Batch Loss: 0.0015519320186285768\n",
      "Epoch: 12 | Total Batch Loss: 0.0013159739564798656\n",
      "Epoch: 13 | Total Batch Loss: 0.001085399824205524\n",
      "Epoch: 14 | Total Batch Loss: 0.0009267775244552467\n",
      "Epoch: 15 | Total Batch Loss: 0.0008005605213838862\n",
      "Epoch: 16 | Total Batch Loss: 0.0006968929014874448\n",
      "Epoch: 17 | Total Batch Loss: 0.0006097057655551907\n",
      "Epoch: 18 | Total Batch Loss: 0.0005415833541064785\n",
      "Epoch: 19 | Total Batch Loss: 0.0004823730487260036\n",
      "Epoch: 20 | Total Batch Loss: 0.00043190218900690525\n",
      "Classification Accuracy: 100.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F-1 Score: 1.00\n",
      "Epoch: 1 | Total Batch Loss: 59.802238285541534\n",
      "Epoch: 2 | Total Batch Loss: 52.42542392015457\n",
      "Epoch: 3 | Total Batch Loss: 49.215456426143646\n",
      "Epoch: 4 | Total Batch Loss: 47.854842990636826\n",
      "Epoch: 5 | Total Batch Loss: 44.51123121380806\n",
      "Epoch: 6 | Total Batch Loss: 43.216186463832855\n",
      "Epoch: 7 | Total Batch Loss: 41.92445705831051\n",
      "Epoch: 8 | Total Batch Loss: 41.59702226519585\n",
      "Epoch: 9 | Total Batch Loss: 40.79344642162323\n",
      "Epoch: 10 | Total Batch Loss: 40.28171291947365\n",
      "Epoch: 11 | Total Batch Loss: 39.74936485290527\n",
      "Epoch: 12 | Total Batch Loss: 39.302589148283005\n",
      "Epoch: 13 | Total Batch Loss: 37.372805804014206\n",
      "Epoch: 14 | Total Batch Loss: 37.05405142903328\n",
      "Epoch: 15 | Total Batch Loss: 37.81897833943367\n",
      "Epoch: 16 | Total Batch Loss: 36.82931864261627\n",
      "Epoch: 17 | Total Batch Loss: 36.08904033899307\n",
      "Epoch: 18 | Total Batch Loss: 36.0933833271265\n",
      "Epoch: 19 | Total Batch Loss: 34.96864449977875\n",
      "Epoch: 20 | Total Batch Loss: 34.815822035074234\n",
      "Classification Accuracy: 83.08\n",
      "Precision: 0.86\n",
      "Recall: 0.83\n",
      "F-1 Score: 0.84\n",
      "Epoch [11 / 20] Loss D: 0.6884, Loss G: 0.6863 \n",
      "Epoch [12 / 20] Loss D: 0.6832, Loss G: 0.6882 \n",
      "Epoch [13 / 20] Loss D: 0.6871, Loss G: 0.6890 \n",
      "Epoch [14 / 20] Loss D: 0.6736, Loss G: 0.6941 \n",
      "Epoch [15 / 20] Loss D: 0.6854, Loss G: 0.6997 \n",
      "Epoch [16 / 20] Loss D: 0.6800, Loss G: 0.6907 \n",
      "Epoch [17 / 20] Loss D: 0.6866, Loss G: 0.6916 \n",
      "Epoch [18 / 20] Loss D: 0.6806, Loss G: 0.6911 \n",
      "Epoch [19 / 20] Loss D: 0.6776, Loss G: 0.6959 \n",
      "Epoch [20 / 20] Loss D: 0.6892, Loss G: 0.6890 \n",
      "Epoch: 1 | Total Batch Loss: 10.602623298065737\n",
      "Epoch: 2 | Total Batch Loss: 0.12467116623884067\n",
      "Epoch: 3 | Total Batch Loss: 0.04214072480681352\n",
      "Epoch: 4 | Total Batch Loss: 0.019680910900206072\n",
      "Epoch: 5 | Total Batch Loss: 0.010901792862568982\n",
      "Epoch: 6 | Total Batch Loss: 0.007147441400320531\n",
      "Epoch: 7 | Total Batch Loss: 0.005143617259818711\n",
      "Epoch: 8 | Total Batch Loss: 0.0037498441436127905\n",
      "Epoch: 9 | Total Batch Loss: 0.0028940106258232845\n",
      "Epoch: 10 | Total Batch Loss: 0.002297246893135707\n",
      "Epoch: 11 | Total Batch Loss: 0.001853548074450373\n",
      "Epoch: 12 | Total Batch Loss: 0.0014981388728187994\n",
      "Epoch: 13 | Total Batch Loss: 0.0012552781023487114\n",
      "Epoch: 14 | Total Batch Loss: 0.0010530650470172986\n",
      "Epoch: 15 | Total Batch Loss: 0.0008831658295775924\n",
      "Epoch: 16 | Total Batch Loss: 0.0007555063692166186\n",
      "Epoch: 17 | Total Batch Loss: 0.0006501992434664317\n",
      "Epoch: 18 | Total Batch Loss: 0.0005643403290491733\n",
      "Epoch: 19 | Total Batch Loss: 0.0004891577385848223\n",
      "Epoch: 20 | Total Batch Loss: 0.00042734650890707826\n",
      "Classification Accuracy: 100.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F-1 Score: 1.00\n",
      "Epoch: 1 | Total Batch Loss: 61.94152534008026\n",
      "Epoch: 2 | Total Batch Loss: 54.10854348540306\n",
      "Epoch: 3 | Total Batch Loss: 50.29649770259857\n",
      "Epoch: 4 | Total Batch Loss: 47.90861365199089\n",
      "Epoch: 5 | Total Batch Loss: 46.648550033569336\n",
      "Epoch: 6 | Total Batch Loss: 45.863101840019226\n",
      "Epoch: 7 | Total Batch Loss: 43.10359796881676\n",
      "Epoch: 8 | Total Batch Loss: 41.72850711643696\n",
      "Epoch: 9 | Total Batch Loss: 41.650719076395035\n",
      "Epoch: 10 | Total Batch Loss: 40.11628246307373\n",
      "Epoch: 11 | Total Batch Loss: 39.50655263662338\n",
      "Epoch: 12 | Total Batch Loss: 38.508078932762146\n",
      "Epoch: 13 | Total Batch Loss: 37.55440264940262\n",
      "Epoch: 14 | Total Batch Loss: 37.90720668435097\n",
      "Epoch: 15 | Total Batch Loss: 36.33101660013199\n",
      "Epoch: 16 | Total Batch Loss: 35.786379262804985\n",
      "Epoch: 17 | Total Batch Loss: 35.44160261750221\n",
      "Epoch: 18 | Total Batch Loss: 34.59980830550194\n",
      "Epoch: 19 | Total Batch Loss: 34.53080312907696\n",
      "Epoch: 20 | Total Batch Loss: 33.93537160754204\n",
      "Classification Accuracy: 82.83\n",
      "Precision: 0.82\n",
      "Recall: 0.89\n",
      "F-1 Score: 0.85\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (real_features, _) in enumerate(train_loader):\n",
    "        batch_size = len(real_features)\n",
    "        \n",
    "        ### Training Discriminator\n",
    "        for k in range(5):\n",
    "            opt_disc.zero_grad()\n",
    "            disc_loss = get_disc_loss(gen, disc, criterion, real_features.float(), batch_size, z_dim)\n",
    "            disc_loss.backward(retain_graph = True)\n",
    "            opt_disc.step()\n",
    "\n",
    "        ### Training Generator\n",
    "        opt_gen.zero_grad()\n",
    "        gen_loss = get_gen_loss(gen, disc, criterion, batch_size, z_dim)\n",
    "        gen_loss.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f'Epoch [{epoch + 1} / {n_epochs}] Loss D: {disc_loss.item():.4f}, Loss G: {gen_loss.item():.4f} '\n",
    "            )\n",
    "            \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        train_model_fake(gen, loss_function, n_epochs, z_dim, data, batch_size, input_size, output_size)\n",
    "        train_model_real(loss_function, n_epochs, data, batch_size)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
